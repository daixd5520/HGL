Parsed from filename: Pretrained backbone is Hyperbolic.
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.7908 cls=1.9493 smmd=2.3581 ct=9.2262 rec=1.3889 | train/val/test=0.155/0.120/0.128 | c=0.999014
[Epoch 0001] loss=18.6842 cls=1.9389 smmd=2.2304 ct=9.2249 rec=1.3889 | train/val/test=0.172/0.124/0.129 | c=0.999014
[Epoch 0002] loss=18.5636 cls=1.9481 smmd=2.0523 ct=9.2174 rec=1.3889 | train/val/test=0.293/0.122/0.155 | c=0.999014
[Epoch 0003] loss=18.3727 cls=1.9380 smmd=1.8181 ct=9.2082 rec=1.3889 | train/val/test=0.293/0.144/0.148 | c=0.999014
[Epoch 0004] loss=18.1383 cls=1.9316 smmd=1.5297 ct=9.1906 rec=1.3890 | train/val/test=0.276/0.130/0.141 | c=0.999014
[Epoch 0005] loss=17.8708 cls=1.9233 smmd=1.2104 ct=9.1662 rec=1.3890 | train/val/test=0.310/0.192/0.192 | c=0.999014
[Epoch 0006] loss=17.5788 cls=1.9175 smmd=0.8755 ct=9.1288 rec=1.3890 | train/val/test=0.362/0.228/0.232 | c=0.999014
[Epoch 0007] loss=17.2667 cls=1.9007 smmd=0.5883 ct=9.0627 rec=1.3890 | train/val/test=0.466/0.236/0.242 | c=0.999014
[Epoch 0008] loss=17.0210 cls=1.8964 smmd=0.4046 ct=8.9777 rec=1.3889 | train/val/test=0.724/0.374/0.365 | c=0.999014
[Epoch 0009] loss=16.8274 cls=1.8421 smmd=0.3770 ct=8.9063 rec=1.3888 | train/val/test=0.793/0.432/0.440 | c=0.999014
[Epoch 0010] loss=16.7765 cls=1.8189 smmd=0.4530 ct=8.8532 rec=1.3887 | train/val/test=0.707/0.350/0.357 | c=0.999014
[Epoch 0011] loss=17.6465 cls=1.7832 smmd=0.5894 ct=9.4906 rec=1.3884 | train/val/test=0.707/0.364/0.360 | c=0.999014
[Epoch 0012] loss=17.3350 cls=1.7445 smmd=0.7575 ct=9.2058 rec=1.3878 | train/val/test=0.724/0.402/0.390 | c=0.999014
[Epoch 0013] loss=17.1062 cls=1.6448 smmd=0.9531 ct=9.0412 rec=1.3865 | train/val/test=0.862/0.466/0.447 | c=0.999014
[Epoch 0014] loss=16.9869 cls=1.5549 smmd=1.1347 ct=8.9594 rec=1.3832 | train/val/test=0.914/0.530/0.526 | c=0.999014
[Epoch 0015] loss=16.9056 cls=1.4885 smmd=1.2612 ct=8.9136 rec=1.3764 | train/val/test=0.914/0.564/0.563 | c=0.999014
[Epoch 0016] loss=16.8161 cls=1.4165 smmd=1.3498 ct=8.8965 rec=1.3632 | train/val/test=0.914/0.570/0.577 | c=0.999014
[Epoch 0017] loss=17.5191 cls=1.8997 smmd=1.3543 ct=8.8958 rec=1.3481 | train/val/test=0.948/0.698/0.673 | c=0.999014
[Epoch 0018] loss=16.7472 cls=1.4197 smmd=1.3061 ct=8.8952 rec=1.3372 | train/val/test=0.948/0.740/0.714 | c=0.999014
[Epoch 0019] loss=16.7209 cls=1.4330 smmd=1.2216 ct=8.9040 rec=1.3382 | train/val/test=0.966/0.728/0.710 | c=0.999014
[Epoch 0020] loss=16.6545 cls=1.4310 smmd=1.0909 ct=8.9250 rec=1.3387 | train/val/test=0.931/0.712/0.675 | c=0.999014
[Epoch 0021] loss=16.7016 cls=1.5031 smmd=0.9367 ct=8.9559 rec=1.3429 | train/val/test=0.931/0.682/0.669 | c=0.999014
[Epoch 0022] loss=16.8695 cls=1.6494 smmd=0.7909 ct=8.9910 rec=1.3459 | train/val/test=0.931/0.694/0.681 | c=0.999014
[Epoch 0023] loss=16.4775 cls=1.4085 smmd=0.6633 ct=9.0327 rec=1.3487 | train/val/test=0.931/0.692/0.681 | c=0.999014
[Epoch 0024] loss=16.1149 cls=1.1870 smmd=0.5635 ct=9.0588 rec=1.3522 | train/val/test=0.931/0.666/0.662 | c=0.999014
[Epoch 0025] loss=16.5074 cls=1.4437 smmd=0.5021 ct=9.0950 rec=1.3542 | train/val/test=0.931/0.672/0.679 | c=0.999014
[Epoch 0026] loss=16.5623 cls=1.4782 smmd=0.4620 ct=9.1165 rec=1.3564 | train/val/test=0.931/0.678/0.695 | c=0.999014
[Epoch 0027] loss=16.8298 cls=1.6571 smmd=0.4231 ct=9.1389 rec=1.3547 | train/val/test=0.948/0.690/0.714 | c=0.999014
[Epoch 0028] loss=16.5703 cls=1.4940 smmd=0.4003 ct=9.1424 rec=1.3524 | train/val/test=0.948/0.718/0.721 | c=0.999014
[Epoch 0029] loss=16.5915 cls=1.5092 smmd=0.3945 ct=9.1399 rec=1.3560 | train/val/test=0.948/0.718/0.707 | c=0.999014
[Epoch 0030] loss=16.0590 cls=1.1570 smmd=0.3975 ct=9.1355 rec=1.3557 | train/val/test=0.931/0.724/0.688 | c=0.999014
[Epoch 0031] loss=16.3887 cls=1.3822 smmd=0.4092 ct=9.1261 rec=1.3532 | train/val/test=0.931/0.700/0.666 | c=0.999014
[Epoch 0032] loss=16.4166 cls=1.4081 smmd=0.4213 ct=9.1142 rec=1.3506 | train/val/test=0.931/0.692/0.661 | c=0.999014
[Epoch 0033] loss=16.3666 cls=1.3879 smmd=0.4230 ct=9.0954 rec=1.3530 | train/val/test=0.931/0.680/0.654 | c=0.999014
[Epoch 0034] loss=16.3258 cls=1.3679 smmd=0.4296 ct=9.0890 rec=1.3485 | train/val/test=0.948/0.684/0.650 | c=0.999014
[Epoch 0035] loss=16.6135 cls=1.5655 smmd=0.4319 ct=9.0815 rec=1.3482 | train/val/test=0.948/0.688/0.652 | c=0.999014
[Epoch 0036] loss=16.2541 cls=1.3326 smmd=0.4288 ct=9.0769 rec=1.3469 | train/val/test=0.948/0.674/0.653 | c=0.999014
[Epoch 0037] loss=16.1718 cls=1.2788 smmd=0.4218 ct=9.0832 rec=1.3439 | train/val/test=0.948/0.664/0.647 | c=0.999014
[Epoch 0038] loss=15.9547 cls=1.1357 smmd=0.4157 ct=9.0832 rec=1.3450 | train/val/test=0.948/0.666/0.642 | c=0.999014
[Epoch 0039] loss=16.1073 cls=1.2472 smmd=0.3976 ct=9.0875 rec=1.3403 | train/val/test=0.931/0.674/0.649 | c=0.999014
[Epoch 0040] loss=16.0778 cls=1.2318 smmd=0.3925 ct=9.0855 rec=1.3402 | train/val/test=0.931/0.672/0.647 | c=0.999014
[Epoch 0041] loss=16.3674 cls=1.4258 smmd=0.3820 ct=9.0912 rec=1.3391 | train/val/test=0.948/0.660/0.655 | c=0.999014
[Epoch 0042] loss=15.6387 cls=0.9444 smmd=0.3808 ct=9.0919 rec=1.3351 | train/val/test=0.966/0.654/0.644 | c=0.999014
[Epoch 0043] loss=15.9575 cls=1.1569 smmd=0.3702 ct=9.0967 rec=1.3358 | train/val/test=0.966/0.652/0.640 | c=0.999014
[Epoch 0044] loss=16.2061 cls=1.3237 smmd=0.3696 ct=9.0962 rec=1.3355 | train/val/test=0.966/0.652/0.641 | c=0.999014
[Epoch 0045] loss=15.8562 cls=1.0960 smmd=0.3657 ct=9.0942 rec=1.3337 | train/val/test=0.966/0.658/0.643 | c=0.999014
[Epoch 0046] loss=15.9940 cls=1.1874 smmd=0.3599 ct=9.1013 rec=1.3309 | train/val/test=0.966/0.666/0.655 | c=0.999014
[Epoch 0047] loss=16.0225 cls=1.2116 smmd=0.3533 ct=9.0986 rec=1.3310 | train/val/test=0.948/0.670/0.655 | c=0.999014
[Epoch 0048] loss=16.1697 cls=1.3203 smmd=0.3417 ct=9.0937 rec=1.3302 | train/val/test=0.948/0.668/0.655 | c=0.999014
[Epoch 0049] loss=15.8394 cls=1.0992 smmd=0.3450 ct=9.0968 rec=1.3270 | train/val/test=0.948/0.668/0.657 | c=0.999014
[Epoch 0050] loss=15.5341 cls=0.9032 smmd=0.3368 ct=9.0934 rec=1.3263 | train/val/test=0.948/0.676/0.661 | c=0.999014
[Epoch 0051] loss=15.8215 cls=1.0938 smmd=0.3360 ct=9.0891 rec=1.3311 | train/val/test=0.966/0.680/0.674 | c=0.999014
[Epoch 0052] loss=16.3304 cls=1.4322 smmd=0.3335 ct=9.0934 rec=1.3295 | train/val/test=0.966/0.676/0.673 | c=0.999014
[Epoch 0053] loss=15.7798 cls=1.0740 smmd=0.3312 ct=9.0891 rec=1.3257 | train/val/test=0.966/0.684/0.675 | c=0.999014
[Epoch 0054] loss=16.2380 cls=1.3720 smmd=0.3302 ct=9.0951 rec=1.3283 | train/val/test=0.966/0.688/0.675 | c=0.999014
[Epoch 0055] loss=16.0773 cls=1.2696 smmd=0.3276 ct=9.0948 rec=1.3252 | train/val/test=0.966/0.694/0.676 | c=0.999014
[Epoch 0056] loss=15.9078 cls=1.1623 smmd=0.3229 ct=9.0912 rec=1.3249 | train/val/test=0.966/0.700/0.684 | c=0.999014
[Epoch 0057] loss=15.8999 cls=1.1645 smmd=0.3195 ct=9.0882 rec=1.3218 | train/val/test=0.983/0.714/0.700 | c=0.999014
[Epoch 0058] loss=15.9436 cls=1.1920 smmd=0.3145 ct=9.0917 rec=1.3227 | train/val/test=0.983/0.720/0.707 | c=0.999014
[Epoch 0059] loss=16.2195 cls=1.3698 smmd=0.3186 ct=9.0908 rec=1.3274 | train/val/test=0.983/0.734/0.715 | c=0.999014
[Epoch 0060] loss=15.5306 cls=0.9164 smmd=0.3121 ct=9.0940 rec=1.3221 | train/val/test=0.983/0.736/0.720 | c=0.999014
[Epoch 0061] loss=16.0815 cls=1.2796 smmd=0.3143 ct=9.0967 rec=1.3227 | train/val/test=0.983/0.746/0.721 | c=0.999014
[Epoch 0062] loss=15.5272 cls=0.9157 smmd=0.3075 ct=9.0948 rec=1.3220 | train/val/test=0.983/0.746/0.729 | c=0.999014
[Epoch 0063] loss=15.7577 cls=1.0732 smmd=0.3026 ct=9.0917 rec=1.3230 | train/val/test=0.983/0.744/0.729 | c=0.999014
[Epoch 0064] loss=15.7674 cls=1.0792 smmd=0.3023 ct=9.0917 rec=1.3236 | train/val/test=0.983/0.746/0.730 | c=0.999014
[Epoch 0065] loss=15.7052 cls=1.0377 smmd=0.3023 ct=9.0937 rec=1.3220 | train/val/test=0.983/0.750/0.727 | c=0.999014
[Epoch 0066] loss=15.5308 cls=0.9278 smmd=0.3018 ct=9.0891 rec=1.3201 | train/val/test=0.983/0.752/0.722 | c=0.999014
[Epoch 0067] loss=15.8881 cls=1.1673 smmd=0.2995 ct=9.0886 rec=1.3203 | train/val/test=0.983/0.746/0.722 | c=0.999014
[Epoch 0068] loss=16.1131 cls=1.3122 smmd=0.3018 ct=9.0901 rec=1.3229 | train/val/test=0.983/0.748/0.719 | c=0.999014
[Epoch 0069] loss=15.6857 cls=1.0364 smmd=0.3059 ct=9.0811 rec=1.3197 | train/val/test=0.983/0.746/0.719 | c=0.999014
[Epoch 0070] loss=15.3299 cls=0.7952 smmd=0.3061 ct=9.0912 rec=1.3152 | train/val/test=0.983/0.744/0.720 | c=0.999014
[Epoch 0071] loss=15.4837 cls=0.9013 smmd=0.3094 ct=9.0853 rec=1.3152 | train/val/test=0.983/0.742/0.719 | c=0.999014
[Epoch 0072] loss=15.8890 cls=1.1697 smmd=0.3091 ct=9.0844 rec=1.3177 | train/val/test=0.966/0.736/0.718 | c=0.999014
[Epoch 0073] loss=15.5573 cls=0.9539 smmd=0.3105 ct=9.0819 rec=1.3141 | train/val/test=0.966/0.730/0.717 | c=0.999014
[Epoch 0074] loss=15.4449 cls=0.8752 smmd=0.3134 ct=9.0806 rec=1.3174 | train/val/test=0.966/0.734/0.715 | c=0.999014
[Epoch 0075] loss=15.5083 cls=0.9197 smmd=0.3084 ct=9.0818 rec=1.3166 | train/val/test=0.966/0.730/0.714 | c=0.999014
[Epoch 0076] loss=15.2631 cls=0.7541 smmd=0.3135 ct=9.0834 rec=1.3150 | train/val/test=0.966/0.730/0.715 | c=0.999014
[Epoch 0077] loss=15.9296 cls=1.1970 smmd=0.3119 ct=9.0860 rec=1.3150 | train/val/test=0.966/0.726/0.716 | c=0.999014
[Epoch 0078] loss=15.7414 cls=1.0768 smmd=0.3074 ct=9.0842 rec=1.3134 | train/val/test=0.966/0.728/0.715 | c=0.999014
[Epoch 0079] loss=15.5533 cls=0.9512 smmd=0.3104 ct=9.0791 rec=1.3165 | train/val/test=0.966/0.728/0.712 | c=0.999014
[Epoch 0080] loss=15.4882 cls=0.9089 smmd=0.3091 ct=9.0805 rec=1.3149 | train/val/test=0.966/0.728/0.713 | c=0.999014
[Epoch 0081] loss=15.2706 cls=0.7676 smmd=0.3089 ct=9.0792 rec=1.3125 | train/val/test=0.966/0.730/0.712 | c=0.999014
[Epoch 0082] loss=15.7390 cls=1.0799 smmd=0.3055 ct=9.0795 rec=1.3137 | train/val/test=0.966/0.730/0.711 | c=0.999014
[Epoch 0083] loss=15.6201 cls=0.9971 smmd=0.3075 ct=9.0857 rec=1.3112 | train/val/test=0.966/0.732/0.711 | c=0.999014
[Epoch 0084] loss=15.1667 cls=0.7009 smmd=0.3060 ct=9.0818 rec=1.3092 | train/val/test=0.966/0.734/0.714 | c=0.999014
[Epoch 0085] loss=15.3028 cls=0.7906 smmd=0.3068 ct=9.0835 rec=1.3084 | train/val/test=0.966/0.730/0.715 | c=0.999014
[Epoch 0086] loss=15.3566 cls=0.8293 smmd=0.3051 ct=9.0821 rec=1.3077 | train/val/test=0.966/0.728/0.718 | c=0.999014
[Epoch 0087] loss=15.7830 cls=1.1124 smmd=0.3057 ct=9.0815 rec=1.3091 | train/val/test=0.966/0.730/0.722 | c=0.999014
[Epoch 0088] loss=15.4646 cls=0.8974 smmd=0.3048 ct=9.0874 rec=1.3072 | train/val/test=0.966/0.730/0.722 | c=0.999014
[Epoch 0089] loss=15.4562 cls=0.8955 smmd=0.3051 ct=9.0846 rec=1.3059 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0090] loss=16.1721 cls=1.3707 smmd=0.3040 ct=9.0822 rec=1.3102 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0091] loss=15.3076 cls=0.7957 smmd=0.3060 ct=9.0842 rec=1.3065 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0092] loss=15.7408 cls=1.0886 smmd=0.3048 ct=9.0807 rec=1.3060 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0093] loss=15.5046 cls=0.9303 smmd=0.3059 ct=9.0796 rec=1.3072 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0094] loss=15.3692 cls=0.8376 smmd=0.3038 ct=9.0821 rec=1.3084 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0095] loss=15.2705 cls=0.7685 smmd=0.3046 ct=9.0861 rec=1.3079 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0096] loss=15.8950 cls=1.1833 smmd=0.3057 ct=9.0876 rec=1.3076 | train/val/test=0.966/0.730/0.721 | c=0.999014
[Epoch 0097] loss=15.9993 cls=1.2547 smmd=0.3033 ct=9.0810 rec=1.3123 | train/val/test=0.966/0.732/0.721 | c=0.999014
[Epoch 0098] loss=15.7380 cls=1.0788 smmd=0.3057 ct=9.0831 rec=1.3111 | train/val/test=0.966/0.732/0.721 | c=0.999014
[Epoch 0099] loss=15.3020 cls=0.7933 smmd=0.3053 ct=9.0814 rec=1.3079 | train/val/test=0.966/0.732/0.721 | c=0.999014
=== Best @ epoch 66: val=0.7520, test=0.7220 ===
