Parsed from filename: Pretrained backbone is Hyperbolic.
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.8056 cls=1.9548 smmd=2.3650 ct=9.2275 rec=1.3889 | train/val/test=0.138/0.182/0.154 | c=0.999014
[Epoch 0001] loss=18.7085 cls=1.9462 smmd=2.2481 ct=9.2256 rec=1.3889 | train/val/test=0.155/0.162/0.149 | c=0.999014
[Epoch 0002] loss=18.5606 cls=1.9399 smmd=2.0646 ct=9.2179 rec=1.3889 | train/val/test=0.155/0.162/0.151 | c=0.999014
[Epoch 0003] loss=18.3792 cls=1.9340 smmd=1.8388 ct=9.2067 rec=1.3889 | train/val/test=0.155/0.164/0.156 | c=0.999014
[Epoch 0004] loss=18.1445 cls=1.9302 smmd=1.5424 ct=9.1901 rec=1.3890 | train/val/test=0.276/0.220/0.195 | c=0.999014
[Epoch 0005] loss=17.8665 cls=1.9257 smmd=1.2033 ct=9.1640 rec=1.3890 | train/val/test=0.431/0.260/0.270 | c=0.999014
[Epoch 0006] loss=17.5663 cls=1.9197 smmd=0.8698 ct=9.1197 rec=1.3890 | train/val/test=0.431/0.226/0.247 | c=0.999014
[Epoch 0007] loss=17.2623 cls=1.9122 smmd=0.5736 ct=9.0540 rec=1.3890 | train/val/test=0.414/0.222/0.248 | c=0.999014
[Epoch 0008] loss=17.0106 cls=1.8906 smmd=0.4011 ct=8.9782 rec=1.3889 | train/val/test=0.534/0.242/0.272 | c=0.999014
[Epoch 0009] loss=16.9264 cls=1.8959 smmd=0.3771 ct=8.9202 rec=1.3889 | train/val/test=0.655/0.310/0.323 | c=0.999014
[Epoch 0010] loss=16.7970 cls=1.8189 smmd=0.4605 ct=8.8650 rec=1.3887 | train/val/test=0.759/0.430/0.425 | c=0.999014
[Epoch 0011] loss=16.7628 cls=1.7686 smmd=0.5832 ct=8.8308 rec=1.3885 | train/val/test=0.759/0.500/0.466 | c=0.999014
[Epoch 0012] loss=16.7144 cls=1.7128 smmd=0.7099 ct=8.7907 rec=1.3878 | train/val/test=0.793/0.514/0.489 | c=0.999014
[Epoch 0013] loss=16.5373 cls=1.5869 smmd=0.8044 ct=8.7508 rec=1.3862 | train/val/test=0.828/0.532/0.513 | c=0.999014
[Epoch 0014] loss=16.4522 cls=1.5328 smmd=0.8414 ct=8.7316 rec=1.3831 | train/val/test=0.862/0.558/0.539 | c=0.999014
[Epoch 0015] loss=16.2877 cls=1.4491 smmd=0.8213 ct=8.7215 rec=1.3757 | train/val/test=0.879/0.594/0.576 | c=0.999014
[Epoch 0016] loss=15.7731 cls=1.1580 smmd=0.7432 ct=8.7237 rec=1.3594 | train/val/test=0.879/0.624/0.600 | c=0.999014
[Epoch 0017] loss=16.5209 cls=1.1442 smmd=0.6395 ct=9.3994 rec=1.3361 | train/val/test=0.948/0.724/0.704 | c=0.999014
[Epoch 0018] loss=16.9261 cls=1.6309 smmd=0.5570 ct=9.2454 rec=1.2943 | train/val/test=0.948/0.706/0.721 | c=0.999014
[Epoch 0019] loss=16.7187 cls=1.6223 smmd=0.5434 ct=9.1081 rec=1.2903 | train/val/test=0.914/0.694/0.716 | c=0.999014
[Epoch 0020] loss=16.9024 cls=1.7675 smmd=0.5895 ct=9.0569 rec=1.2903 | train/val/test=0.966/0.682/0.686 | c=0.999014
[Epoch 0021] loss=16.3224 cls=1.3831 smmd=0.6530 ct=9.0188 rec=1.2913 | train/val/test=0.948/0.668/0.668 | c=0.999014
[Epoch 0022] loss=16.6800 cls=1.5994 smmd=0.7163 ct=9.0045 rec=1.2960 | train/val/test=0.948/0.692/0.681 | c=0.999014
[Epoch 0023] loss=16.7040 cls=1.5870 smmd=0.7613 ct=8.9953 rec=1.3105 | train/val/test=0.966/0.716/0.721 | c=0.999014
[Epoch 0024] loss=16.3756 cls=1.3480 smmd=0.7742 ct=8.9983 rec=1.3212 | train/val/test=0.966/0.734/0.748 | c=0.999014
[Epoch 0025] loss=16.8503 cls=1.6513 smmd=0.7539 ct=9.0151 rec=1.3288 | train/val/test=0.948/0.720/0.714 | c=0.999014
[Epoch 0026] loss=16.9118 cls=1.6917 smmd=0.7190 ct=9.0251 rec=1.3364 | train/val/test=0.931/0.692/0.686 | c=0.999014
[Epoch 0027] loss=16.9515 cls=1.7158 smmd=0.6642 ct=9.0509 rec=1.3416 | train/val/test=0.948/0.660/0.642 | c=0.999014
[Epoch 0028] loss=16.5465 cls=1.4616 smmd=0.5997 ct=9.0607 rec=1.3471 | train/val/test=0.931/0.632/0.612 | c=0.999014
[Epoch 0029] loss=16.9180 cls=1.7282 smmd=0.5345 ct=9.0741 rec=1.3469 | train/val/test=0.931/0.618/0.591 | c=0.999014
[Epoch 0030] loss=16.4950 cls=1.4480 smmd=0.4852 ct=9.0914 rec=1.3529 | train/val/test=0.931/0.610/0.583 | c=0.999014
[Epoch 0031] loss=16.3675 cls=1.3677 smmd=0.4439 ct=9.1069 rec=1.3539 | train/val/test=0.931/0.612/0.583 | c=0.999014
[Epoch 0032] loss=16.3315 cls=1.3575 smmd=0.4077 ct=9.1078 rec=1.3561 | train/val/test=0.931/0.616/0.583 | c=0.999014
[Epoch 0033] loss=16.3862 cls=1.4067 smmd=0.3825 ct=9.1054 rec=1.3571 | train/val/test=0.931/0.624/0.590 | c=0.999014
[Epoch 0034] loss=16.5810 cls=1.5420 smmd=0.3653 ct=9.1079 rec=1.3575 | train/val/test=0.931/0.628/0.589 | c=0.999014
[Epoch 0035] loss=16.2085 cls=1.2898 smmd=0.3650 ct=9.1119 rec=1.3580 | train/val/test=0.931/0.626/0.593 | c=0.999014
[Epoch 0036] loss=16.3049 cls=1.3513 smmd=0.3585 ct=9.1156 rec=1.3604 | train/val/test=0.931/0.636/0.603 | c=0.999014
[Epoch 0037] loss=16.2020 cls=1.2930 smmd=0.3553 ct=9.1085 rec=1.3580 | train/val/test=0.931/0.650/0.610 | c=0.999014
[Epoch 0038] loss=16.1656 cls=1.2723 smmd=0.3600 ct=9.1000 rec=1.3595 | train/val/test=0.931/0.662/0.620 | c=0.999014
[Epoch 0039] loss=16.2385 cls=1.3158 smmd=0.3622 ct=9.1113 rec=1.3541 | train/val/test=0.948/0.662/0.620 | c=0.999014
[Epoch 0040] loss=15.9169 cls=1.1100 smmd=0.3600 ct=9.0994 rec=1.3567 | train/val/test=0.948/0.662/0.618 | c=0.999014
[Epoch 0041] loss=16.7445 cls=1.6593 smmd=0.3599 ct=9.1002 rec=1.3583 | train/val/test=0.948/0.658/0.621 | c=0.999014
[Epoch 0042] loss=16.2651 cls=1.3461 smmd=0.3708 ct=9.0905 rec=1.3555 | train/val/test=0.948/0.656/0.621 | c=0.999014
[Epoch 0043] loss=16.2320 cls=1.3196 smmd=0.3664 ct=9.1016 rec=1.3525 | train/val/test=0.948/0.660/0.627 | c=0.999014
[Epoch 0044] loss=16.1015 cls=1.2423 smmd=0.3638 ct=9.0921 rec=1.3523 | train/val/test=0.948/0.660/0.630 | c=0.999014
[Epoch 0045] loss=16.2010 cls=1.3132 smmd=0.3608 ct=9.0897 rec=1.3512 | train/val/test=0.948/0.666/0.634 | c=0.999014
[Epoch 0046] loss=15.9694 cls=1.1686 smmd=0.3594 ct=9.0827 rec=1.3484 | train/val/test=0.948/0.676/0.637 | c=0.999014
[Epoch 0047] loss=16.1567 cls=1.2916 smmd=0.3552 ct=9.0864 rec=1.3489 | train/val/test=0.948/0.682/0.638 | c=0.999014
[Epoch 0048] loss=16.2097 cls=1.3248 smmd=0.3463 ct=9.0954 rec=1.3476 | train/val/test=0.948/0.686/0.649 | c=0.999014
[Epoch 0049] loss=15.9851 cls=1.1882 smmd=0.3432 ct=9.0846 rec=1.3454 | train/val/test=0.948/0.684/0.650 | c=0.999014
[Epoch 0050] loss=15.9173 cls=1.1414 smmd=0.3360 ct=9.0928 rec=1.3434 | train/val/test=0.948/0.694/0.653 | c=0.999014
[Epoch 0051] loss=16.0493 cls=1.2392 smmd=0.3289 ct=9.0880 rec=1.3412 | train/val/test=0.948/0.692/0.657 | c=0.999014
[Epoch 0052] loss=15.9378 cls=1.1632 smmd=0.3303 ct=9.0918 rec=1.3390 | train/val/test=0.948/0.690/0.658 | c=0.999014
[Epoch 0053] loss=16.0138 cls=1.2176 smmd=0.3206 ct=9.0937 rec=1.3383 | train/val/test=0.948/0.690/0.656 | c=0.999014
[Epoch 0054] loss=15.8445 cls=1.1045 smmd=0.3197 ct=9.0932 rec=1.3392 | train/val/test=0.948/0.686/0.656 | c=0.999014
[Epoch 0055] loss=16.1513 cls=1.3176 smmd=0.3159 ct=9.0837 rec=1.3406 | train/val/test=0.948/0.684/0.661 | c=0.999014
[Epoch 0056] loss=15.7929 cls=1.0846 smmd=0.3187 ct=9.0805 rec=1.3364 | train/val/test=0.948/0.682/0.661 | c=0.999014
[Epoch 0057] loss=16.1749 cls=1.3313 smmd=0.3114 ct=9.0934 rec=1.3366 | train/val/test=0.948/0.676/0.666 | c=0.999014
[Epoch 0058] loss=15.6882 cls=1.0125 smmd=0.3157 ct=9.0865 rec=1.3350 | train/val/test=0.948/0.674/0.666 | c=0.999014
[Epoch 0059] loss=15.8992 cls=1.1621 smmd=0.3173 ct=9.0768 rec=1.3338 | train/val/test=0.948/0.674/0.668 | c=0.999014
[Epoch 0060] loss=15.8185 cls=1.1090 smmd=0.3133 ct=9.0835 rec=1.3295 | train/val/test=0.948/0.672/0.667 | c=0.999014
[Epoch 0061] loss=16.0752 cls=1.2786 smmd=0.3103 ct=9.0846 rec=1.3313 | train/val/test=0.948/0.672/0.670 | c=0.999014
[Epoch 0062] loss=15.8837 cls=1.1584 smmd=0.3068 ct=9.0809 rec=1.3289 | train/val/test=0.948/0.680/0.670 | c=0.999014
[Epoch 0063] loss=16.1588 cls=1.3337 smmd=0.3068 ct=9.0855 rec=1.3327 | train/val/test=0.948/0.690/0.673 | c=0.999014
[Epoch 0064] loss=15.8726 cls=1.1473 smmd=0.3058 ct=9.0881 rec=1.3269 | train/val/test=0.948/0.694/0.675 | c=0.999014
[Epoch 0065] loss=15.7657 cls=1.0698 smmd=0.3037 ct=9.0974 rec=1.3261 | train/val/test=0.948/0.702/0.685 | c=0.999014
[Epoch 0066] loss=15.9801 cls=1.2261 smmd=0.3029 ct=9.0808 rec=1.3275 | train/val/test=0.948/0.712/0.686 | c=0.999014
[Epoch 0067] loss=15.8621 cls=1.1423 smmd=0.3043 ct=9.0841 rec=1.3289 | train/val/test=0.948/0.718/0.686 | c=0.999014
[Epoch 0068] loss=15.4338 cls=0.8663 smmd=0.3014 ct=9.0820 rec=1.3230 | train/val/test=0.948/0.724/0.690 | c=0.999014
[Epoch 0069] loss=15.5232 cls=0.9211 smmd=0.3035 ct=9.0817 rec=1.3268 | train/val/test=0.948/0.722/0.696 | c=0.999014
[Epoch 0070] loss=15.4214 cls=0.8537 smmd=0.3075 ct=9.0806 rec=1.3255 | train/val/test=0.948/0.722/0.692 | c=0.999014
[Epoch 0071] loss=15.3934 cls=0.8408 smmd=0.3069 ct=9.0786 rec=1.3220 | train/val/test=0.948/0.720/0.693 | c=0.999014
[Epoch 0072] loss=15.6284 cls=0.9934 smmd=0.3050 ct=9.0839 rec=1.3223 | train/val/test=0.948/0.714/0.692 | c=0.999014
[Epoch 0073] loss=15.5577 cls=0.9489 smmd=0.3023 ct=9.0812 rec=1.3233 | train/val/test=0.948/0.716/0.693 | c=0.999014
[Epoch 0074] loss=15.9749 cls=1.2236 smmd=0.3023 ct=9.0815 rec=1.3262 | train/val/test=0.948/0.712/0.691 | c=0.999014
[Epoch 0075] loss=16.3545 cls=1.4767 smmd=0.2994 ct=9.0819 rec=1.3271 | train/val/test=0.948/0.714/0.691 | c=0.999014
[Epoch 0076] loss=15.7834 cls=1.1062 smmd=0.2986 ct=9.0757 rec=1.3229 | train/val/test=0.948/0.718/0.692 | c=0.999014
[Epoch 0077] loss=15.6050 cls=0.9844 smmd=0.2975 ct=9.0786 rec=1.3237 | train/val/test=0.948/0.718/0.690 | c=0.999014
[Epoch 0078] loss=15.5507 cls=0.9530 smmd=0.2987 ct=9.0746 rec=1.3219 | train/val/test=0.948/0.720/0.689 | c=0.999014
[Epoch 0079] loss=15.7364 cls=1.0703 smmd=0.2923 ct=9.0875 rec=1.3204 | train/val/test=0.948/0.724/0.693 | c=0.999014
[Epoch 0080] loss=15.9697 cls=1.2259 smmd=0.2924 ct=9.0841 rec=1.3231 | train/val/test=0.948/0.728/0.693 | c=0.999014
[Epoch 0081] loss=16.2176 cls=1.3922 smmd=0.2927 ct=9.0833 rec=1.3225 | train/val/test=0.948/0.726/0.693 | c=0.999014
[Epoch 0082] loss=15.8960 cls=1.1728 smmd=0.2928 ct=9.0879 rec=1.3234 | train/val/test=0.948/0.728/0.689 | c=0.999014
[Epoch 0083] loss=15.4860 cls=0.9118 smmd=0.2936 ct=9.0777 rec=1.3199 | train/val/test=0.948/0.728/0.690 | c=0.999014
[Epoch 0084] loss=15.9405 cls=1.2034 smmd=0.2957 ct=9.0863 rec=1.3226 | train/val/test=0.948/0.726/0.689 | c=0.999014
[Epoch 0085] loss=15.5434 cls=0.9475 smmd=0.2902 ct=9.0834 rec=1.3191 | train/val/test=0.948/0.726/0.690 | c=0.999014
[Epoch 0086] loss=15.6370 cls=0.9988 smmd=0.2925 ct=9.0864 rec=1.3261 | train/val/test=0.948/0.726/0.691 | c=0.999014
[Epoch 0087] loss=15.4998 cls=0.9198 smmd=0.2947 ct=9.0748 rec=1.3229 | train/val/test=0.948/0.722/0.692 | c=0.999014
[Epoch 0088] loss=15.4035 cls=0.8496 smmd=0.2975 ct=9.0824 rec=1.3211 | train/val/test=0.948/0.722/0.695 | c=0.999014
[Epoch 0089] loss=15.6271 cls=0.9982 smmd=0.2932 ct=9.0874 rec=1.3194 | train/val/test=0.948/0.722/0.695 | c=0.999014
[Epoch 0090] loss=16.1007 cls=1.3177 smmd=0.2938 ct=9.0807 rec=1.3210 | train/val/test=0.948/0.722/0.695 | c=0.999014
[Epoch 0091] loss=15.7895 cls=1.1095 smmd=0.2976 ct=9.0823 rec=1.3187 | train/val/test=0.948/0.722/0.695 | c=0.999014
[Epoch 0092] loss=15.8870 cls=1.1715 smmd=0.2970 ct=9.0804 rec=1.3233 | train/val/test=0.948/0.724/0.695 | c=0.999014
[Epoch 0093] loss=15.1880 cls=0.7140 smmd=0.2984 ct=9.0749 rec=1.3192 | train/val/test=0.948/0.724/0.695 | c=0.999014
[Epoch 0094] loss=15.5705 cls=0.9694 smmd=0.2948 ct=9.0746 rec=1.3207 | train/val/test=0.948/0.724/0.695 | c=0.999014
[Epoch 0095] loss=15.7471 cls=1.0808 smmd=0.2957 ct=9.0801 rec=1.3217 | train/val/test=0.948/0.724/0.695 | c=0.999014
[Epoch 0096] loss=16.1421 cls=1.3413 smmd=0.2940 ct=9.0832 rec=1.3226 | train/val/test=0.948/0.724/0.696 | c=0.999014
[Epoch 0097] loss=15.7337 cls=1.0738 smmd=0.2960 ct=9.0792 rec=1.3206 | train/val/test=0.948/0.724/0.696 | c=0.999014
[Epoch 0098] loss=15.6284 cls=1.0019 smmd=0.2943 ct=9.0794 rec=1.3228 | train/val/test=0.948/0.724/0.696 | c=0.999014
[Epoch 0099] loss=15.4253 cls=0.8691 smmd=0.2950 ct=9.0785 rec=1.3207 | train/val/test=0.948/0.724/0.696 | c=0.999014
=== Best @ epoch 24: val=0.7340, test=0.7480 ===
