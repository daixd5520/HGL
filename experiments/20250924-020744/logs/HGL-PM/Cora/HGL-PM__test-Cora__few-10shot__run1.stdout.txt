Parsed from filename: Pretrained backbone is Hyperbolic.
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.7410 cls=1.9474 smmd=2.2952 ct=9.2238 rec=1.3889 | train/val/test=0.172/0.116/0.133 | c=0.999014
[Epoch 0001] loss=18.6341 cls=1.9388 smmd=2.1738 ct=9.2170 rec=1.3889 | train/val/test=0.224/0.130/0.141 | c=0.999014
[Epoch 0002] loss=18.4914 cls=1.9332 smmd=1.9916 ct=9.2117 rec=1.3889 | train/val/test=0.207/0.122/0.130 | c=0.999014
[Epoch 0003] loss=18.2970 cls=1.9311 smmd=1.7534 ct=9.1929 rec=1.3889 | train/val/test=0.276/0.138/0.149 | c=0.999014
[Epoch 0004] loss=18.0739 cls=1.9339 smmd=1.4667 ct=9.1723 rec=1.3889 | train/val/test=0.397/0.218/0.225 | c=0.999014
[Epoch 0005] loss=17.7867 cls=1.9253 smmd=1.1483 ct=9.1328 rec=1.3890 | train/val/test=0.448/0.304/0.300 | c=0.999014
[Epoch 0006] loss=17.4655 cls=1.9224 smmd=0.8267 ct=9.0621 rec=1.3890 | train/val/test=0.466/0.256/0.267 | c=0.999014
[Epoch 0007] loss=17.1313 cls=1.8906 smmd=0.5795 ct=8.9750 rec=1.3889 | train/val/test=0.552/0.262/0.275 | c=0.999014
[Epoch 0008] loss=16.9190 cls=1.8587 smmd=0.4383 ct=8.9246 rec=1.3889 | train/val/test=0.672/0.352/0.368 | c=0.999014
[Epoch 0009] loss=16.7784 cls=1.8084 smmd=0.4042 ct=8.8930 rec=1.3887 | train/val/test=0.690/0.404/0.415 | c=0.999014
[Epoch 0010] loss=16.7899 cls=1.8169 smmd=0.4534 ct=8.8659 rec=1.3885 | train/val/test=0.776/0.444/0.459 | c=0.999014
[Epoch 0011] loss=16.5708 cls=1.6603 smmd=0.5508 ct=8.8264 rec=1.3878 | train/val/test=0.776/0.522/0.510 | c=0.999014
[Epoch 0012] loss=17.4281 cls=1.6039 smmd=0.6519 ct=9.4981 rec=1.3865 | train/val/test=0.810/0.542/0.524 | c=0.999014
[Epoch 0013] loss=17.0539 cls=1.5048 smmd=0.7409 ct=9.2809 rec=1.3831 | train/val/test=0.862/0.626/0.606 | c=0.999014
[Epoch 0014] loss=16.7657 cls=1.4157 smmd=0.8559 ct=9.1095 rec=1.3754 | train/val/test=0.914/0.712/0.698 | c=0.999014
[Epoch 0015] loss=16.4713 cls=1.2542 smmd=0.9763 ct=9.0199 rec=1.3630 | train/val/test=0.931/0.726/0.721 | c=0.999014
[Epoch 0016] loss=16.4816 cls=1.2870 smmd=1.0703 ct=8.9715 rec=1.3368 | train/val/test=0.931/0.732/0.710 | c=0.999014
[Epoch 0017] loss=16.2083 cls=1.1289 smmd=1.1351 ct=8.9458 rec=1.3068 | train/val/test=0.931/0.686/0.703 | c=0.999014
[Epoch 0018] loss=16.1454 cls=1.1201 smmd=1.1664 ct=8.9386 rec=1.2680 | train/val/test=0.931/0.730/0.743 | c=0.999014
[Epoch 0019] loss=16.0646 cls=1.0991 smmd=1.1557 ct=8.9373 rec=1.2428 | train/val/test=0.931/0.722/0.724 | c=0.999014
[Epoch 0020] loss=16.9877 cls=1.7038 smmd=1.1210 ct=8.9569 rec=1.2520 | train/val/test=0.948/0.760/0.758 | c=0.999014
[Epoch 0021] loss=17.3935 cls=1.9923 smmd=1.0524 ct=8.9591 rec=1.2636 | train/val/test=0.948/0.708/0.734 | c=0.999014
[Epoch 0022] loss=16.5838 cls=1.4566 smmd=0.9625 ct=8.9858 rec=1.2773 | train/val/test=0.966/0.686/0.720 | c=0.999014
[Epoch 0023] loss=16.6702 cls=1.4924 smmd=0.8844 ct=9.0273 rec=1.2982 | train/val/test=0.931/0.660/0.685 | c=0.999014
[Epoch 0024] loss=17.0880 cls=1.7568 smmd=0.8050 ct=9.0713 rec=1.3104 | train/val/test=0.914/0.650/0.646 | c=0.999014
[Epoch 0025] loss=16.4639 cls=1.3355 smmd=0.7314 ct=9.1032 rec=1.3215 | train/val/test=0.931/0.626/0.626 | c=0.999014
[Epoch 0026] loss=16.3277 cls=1.2446 smmd=0.6506 ct=9.1320 rec=1.3337 | train/val/test=0.914/0.612/0.626 | c=0.999014
[Epoch 0027] loss=16.7687 cls=1.5533 smmd=0.5673 ct=9.1524 rec=1.3397 | train/val/test=0.931/0.652/0.653 | c=0.999014
[Epoch 0028] loss=16.2755 cls=1.2478 smmd=0.4917 ct=9.1660 rec=1.3398 | train/val/test=0.948/0.684/0.676 | c=0.999014
[Epoch 0029] loss=16.1680 cls=1.2021 smmd=0.4268 ct=9.1647 rec=1.3450 | train/val/test=0.931/0.694/0.695 | c=0.999014
[Epoch 0030] loss=16.5208 cls=1.4415 smmd=0.3900 ct=9.1792 rec=1.3453 | train/val/test=0.931/0.700/0.693 | c=0.999014
[Epoch 0031] loss=16.5269 cls=1.4411 smmd=0.3691 ct=9.1919 rec=1.3483 | train/val/test=0.931/0.700/0.683 | c=0.999014
[Epoch 0032] loss=16.4489 cls=1.3861 smmd=0.3633 ct=9.1964 rec=1.3500 | train/val/test=0.931/0.694/0.676 | c=0.999014
[Epoch 0033] loss=16.5753 cls=1.4760 smmd=0.3544 ct=9.1956 rec=1.3493 | train/val/test=0.931/0.696/0.682 | c=0.999014
[Epoch 0034] loss=16.3163 cls=1.3154 smmd=0.3498 ct=9.1848 rec=1.3488 | train/val/test=0.931/0.686/0.684 | c=0.999014
[Epoch 0035] loss=16.2078 cls=1.2599 smmd=0.3501 ct=9.1665 rec=1.3478 | train/val/test=0.931/0.676/0.673 | c=0.999014
[Epoch 0036] loss=16.3665 cls=1.3756 smmd=0.3585 ct=9.1517 rec=1.3468 | train/val/test=0.931/0.654/0.646 | c=0.999014
[Epoch 0037] loss=16.2077 cls=1.2767 smmd=0.3684 ct=9.1367 rec=1.3482 | train/val/test=0.931/0.626/0.624 | c=0.999014
[Epoch 0038] loss=16.2985 cls=1.3329 smmd=0.3894 ct=9.1294 rec=1.3489 | train/val/test=0.931/0.610/0.604 | c=0.999014
[Epoch 0039] loss=16.5414 cls=1.4945 smmd=0.3992 ct=9.1232 rec=1.3500 | train/val/test=0.931/0.588/0.582 | c=0.999014
[Epoch 0040] loss=16.1043 cls=1.2034 smmd=0.4139 ct=9.1173 rec=1.3481 | train/val/test=0.931/0.598/0.587 | c=0.999014
[Epoch 0041] loss=16.2220 cls=1.2936 smmd=0.4164 ct=9.1067 rec=1.3447 | train/val/test=0.948/0.624/0.605 | c=0.999014
[Epoch 0042] loss=16.2294 cls=1.3042 smmd=0.4157 ct=9.1006 rec=1.3445 | train/val/test=0.948/0.650/0.624 | c=0.999014
[Epoch 0043] loss=16.1550 cls=1.2520 smmd=0.4118 ct=9.1116 rec=1.3398 | train/val/test=0.931/0.664/0.633 | c=0.999014
[Epoch 0044] loss=16.0139 cls=1.1637 smmd=0.4011 ct=9.1120 rec=1.3387 | train/val/test=0.931/0.672/0.639 | c=0.999014
[Epoch 0045] loss=16.1849 cls=1.2892 smmd=0.3841 ct=9.1063 rec=1.3400 | train/val/test=0.931/0.670/0.648 | c=0.999014
[Epoch 0046] loss=16.1955 cls=1.2895 smmd=0.3797 ct=9.1179 rec=1.3388 | train/val/test=0.914/0.672/0.648 | c=0.999014
[Epoch 0047] loss=16.0159 cls=1.1767 smmd=0.3580 ct=9.1241 rec=1.3368 | train/val/test=0.948/0.674/0.648 | c=0.999014
[Epoch 0048] loss=15.9325 cls=1.1284 smmd=0.3507 ct=9.1241 rec=1.3332 | train/val/test=0.948/0.678/0.655 | c=0.999014
[Epoch 0049] loss=15.6748 cls=0.9589 smmd=0.3496 ct=9.1266 rec=1.3295 | train/val/test=0.948/0.680/0.659 | c=0.999014
[Epoch 0050] loss=16.3412 cls=1.4038 smmd=0.3328 ct=9.1299 rec=1.3335 | train/val/test=0.948/0.684/0.655 | c=0.999014
[Epoch 0051] loss=16.1904 cls=1.3032 smmd=0.3310 ct=9.1370 rec=1.3286 | train/val/test=0.966/0.680/0.660 | c=0.999014
[Epoch 0052] loss=16.1386 cls=1.2754 smmd=0.3280 ct=9.1330 rec=1.3269 | train/val/test=0.948/0.684/0.661 | c=0.999014
[Epoch 0053] loss=15.8548 cls=1.0927 smmd=0.3233 ct=9.1266 rec=1.3279 | train/val/test=0.948/0.690/0.663 | c=0.999014
[Epoch 0054] loss=15.6566 cls=0.9630 smmd=0.3249 ct=9.1267 rec=1.3250 | train/val/test=0.948/0.688/0.666 | c=0.999014
[Epoch 0055] loss=15.9429 cls=1.1588 smmd=0.3228 ct=9.1233 rec=1.3241 | train/val/test=0.948/0.692/0.664 | c=0.999014
[Epoch 0056] loss=15.6894 cls=0.9884 smmd=0.3226 ct=9.1239 rec=1.3249 | train/val/test=0.948/0.700/0.673 | c=0.999014
[Epoch 0057] loss=15.7397 cls=1.0260 smmd=0.3252 ct=9.1202 rec=1.3230 | train/val/test=0.948/0.706/0.676 | c=0.999014
[Epoch 0058] loss=15.7940 cls=1.0642 smmd=0.3201 ct=9.1198 rec=1.3236 | train/val/test=0.948/0.710/0.677 | c=0.999014
[Epoch 0059] loss=16.0004 cls=1.2054 smmd=0.3180 ct=9.1170 rec=1.3236 | train/val/test=0.948/0.714/0.679 | c=0.999014
[Epoch 0060] loss=15.6214 cls=0.9540 smmd=0.3177 ct=9.1219 rec=1.3184 | train/val/test=0.931/0.720/0.682 | c=0.999014
[Epoch 0061] loss=15.7547 cls=1.0439 smmd=0.3154 ct=9.1219 rec=1.3185 | train/val/test=0.931/0.720/0.684 | c=0.999014
[Epoch 0062] loss=15.5640 cls=0.9178 smmd=0.3133 ct=9.1214 rec=1.3189 | train/val/test=0.931/0.716/0.686 | c=0.999014
[Epoch 0063] loss=16.0353 cls=1.2374 smmd=0.3130 ct=9.1184 rec=1.3163 | train/val/test=0.931/0.718/0.687 | c=0.999014
[Epoch 0064] loss=15.6902 cls=1.0036 smmd=0.3088 ct=9.1248 rec=1.3164 | train/val/test=0.931/0.714/0.688 | c=0.999014
[Epoch 0065] loss=15.4489 cls=0.8453 smmd=0.3101 ct=9.1215 rec=1.3162 | train/val/test=0.931/0.714/0.690 | c=0.999014
[Epoch 0066] loss=15.6464 cls=0.9796 smmd=0.3063 ct=9.1216 rec=1.3153 | train/val/test=0.931/0.716/0.692 | c=0.999014
[Epoch 0067] loss=15.7731 cls=1.0693 smmd=0.3063 ct=9.1136 rec=1.3169 | train/val/test=0.931/0.718/0.691 | c=0.999014
[Epoch 0068] loss=15.9750 cls=1.2015 smmd=0.3039 ct=9.1199 rec=1.3151 | train/val/test=0.931/0.714/0.693 | c=0.999014
[Epoch 0069] loss=15.7603 cls=1.0640 smmd=0.3028 ct=9.1139 rec=1.3151 | train/val/test=0.931/0.718/0.696 | c=0.999014
[Epoch 0070] loss=15.5361 cls=0.9113 smmd=0.3063 ct=9.1160 rec=1.3150 | train/val/test=0.931/0.716/0.699 | c=0.999014
[Epoch 0071] loss=15.5277 cls=0.9044 smmd=0.3065 ct=9.1225 rec=1.3108 | train/val/test=0.931/0.718/0.700 | c=0.999014
[Epoch 0072] loss=16.0960 cls=1.2917 smmd=0.3011 ct=9.1156 rec=1.3109 | train/val/test=0.931/0.722/0.702 | c=0.999014
[Epoch 0073] loss=15.3507 cls=0.7943 smmd=0.3033 ct=9.1201 rec=1.3068 | train/val/test=0.931/0.724/0.702 | c=0.999014
[Epoch 0074] loss=15.6517 cls=0.9943 smmd=0.3023 ct=9.1152 rec=1.3118 | train/val/test=0.931/0.724/0.700 | c=0.999014
[Epoch 0075] loss=15.6436 cls=0.9907 smmd=0.2997 ct=9.1143 rec=1.3120 | train/val/test=0.931/0.718/0.698 | c=0.999014
[Epoch 0076] loss=16.2166 cls=1.3712 smmd=0.3019 ct=9.1158 rec=1.3112 | train/val/test=0.931/0.720/0.697 | c=0.999014
[Epoch 0077] loss=15.4993 cls=0.8929 smmd=0.3025 ct=9.1196 rec=1.3080 | train/val/test=0.931/0.720/0.700 | c=0.999014
[Epoch 0078] loss=15.9376 cls=1.1742 smmd=0.2994 ct=9.1255 rec=1.3148 | train/val/test=0.931/0.722/0.701 | c=0.999014
[Epoch 0079] loss=15.6393 cls=0.9875 smmd=0.3006 ct=9.1174 rec=1.3094 | train/val/test=0.931/0.722/0.701 | c=0.999014
[Epoch 0080] loss=15.6484 cls=0.9892 smmd=0.3022 ct=9.1203 rec=1.3104 | train/val/test=0.931/0.722/0.708 | c=0.999014
[Epoch 0081] loss=15.5117 cls=0.9009 smmd=0.3033 ct=9.1154 rec=1.3112 | train/val/test=0.931/0.722/0.707 | c=0.999014
[Epoch 0082] loss=15.3597 cls=0.8030 smmd=0.2976 ct=9.1160 rec=1.3100 | train/val/test=0.931/0.722/0.708 | c=0.999014
[Epoch 0083] loss=15.4924 cls=0.8916 smmd=0.2999 ct=9.1151 rec=1.3096 | train/val/test=0.931/0.722/0.710 | c=0.999014
[Epoch 0084] loss=15.4287 cls=0.8501 smmd=0.2997 ct=9.1159 rec=1.3082 | train/val/test=0.931/0.724/0.713 | c=0.999014
[Epoch 0085] loss=15.5269 cls=0.9168 smmd=0.2973 ct=9.1154 rec=1.3085 | train/val/test=0.931/0.724/0.713 | c=0.999014
[Epoch 0086] loss=15.8321 cls=1.1216 smmd=0.2937 ct=9.1122 rec=1.3114 | train/val/test=0.931/0.724/0.712 | c=0.999014
[Epoch 0087] loss=15.8190 cls=1.1042 smmd=0.2983 ct=9.1135 rec=1.3165 | train/val/test=0.931/0.722/0.711 | c=0.999014
[Epoch 0088] loss=15.4812 cls=0.8891 smmd=0.2980 ct=9.1119 rec=1.3084 | train/val/test=0.931/0.722/0.711 | c=0.999014
[Epoch 0089] loss=15.6459 cls=0.9923 smmd=0.2980 ct=9.1168 rec=1.3107 | train/val/test=0.931/0.724/0.711 | c=0.999014
[Epoch 0090] loss=15.2290 cls=0.7192 smmd=0.3015 ct=9.1130 rec=1.3076 | train/val/test=0.948/0.724/0.712 | c=0.999014
[Epoch 0091] loss=15.6745 cls=1.0155 smmd=0.2975 ct=9.1115 rec=1.3112 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0092] loss=15.8819 cls=1.1529 smmd=0.2992 ct=9.1131 rec=1.3100 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0093] loss=15.5600 cls=0.9389 smmd=0.2999 ct=9.1132 rec=1.3091 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0094] loss=15.4486 cls=0.8666 smmd=0.2974 ct=9.1127 rec=1.3088 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0095] loss=15.8018 cls=1.0988 smmd=0.2960 ct=9.1174 rec=1.3085 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0096] loss=15.7598 cls=1.0731 smmd=0.2995 ct=9.1133 rec=1.3083 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0097] loss=15.4009 cls=0.8370 smmd=0.2944 ct=9.1165 rec=1.3050 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0098] loss=15.8912 cls=1.1609 smmd=0.2978 ct=9.1095 rec=1.3119 | train/val/test=0.948/0.724/0.713 | c=0.999014
[Epoch 0099] loss=15.5449 cls=0.9299 smmd=0.2986 ct=9.1126 rec=1.3091 | train/val/test=0.948/0.724/0.713 | c=0.999014
=== Best @ epoch 20: val=0.7600, test=0.7580 ===
