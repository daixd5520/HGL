Parsed from filename: Pretrained backbone is Hyperbolic.
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=23.1490 cls=1.1016 smmd=4.0613 ct=11.2692 rec=1.4137 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0001] loss=22.7404 cls=1.0985 smmd=3.8312 ct=11.2665 rec=1.4137 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0002] loss=22.0860 cls=1.1026 smmd=3.4597 ct=11.2608 rec=1.4137 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0003] loss=21.1112 cls=1.1104 smmd=2.9071 ct=11.2505 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0004] loss=19.7205 cls=1.1044 smmd=2.1292 ct=11.2313 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0005] loss=18.0582 cls=1.0907 smmd=1.2100 ct=11.1978 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0006] loss=17.1204 cls=1.0991 smmd=0.6869 ct=11.1757 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0007] loss=17.8250 cls=1.0883 smmd=1.0917 ct=11.1791 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0008] loss=18.8207 cls=1.0928 smmd=1.6648 ct=11.1709 rec=1.4136 | train/val/test=0.308/0.416/0.407 | c=0.999014
[Epoch 0009] loss=19.0155 cls=1.0730 smmd=1.8174 ct=11.1268 rec=1.4136 | train/val/test=0.692/0.522/0.537 | c=0.999014
[Epoch 0010] loss=18.3665 cls=1.0779 smmd=1.5075 ct=11.0420 rec=1.4135 | train/val/test=0.846/0.580/0.616 | c=0.999014
[Epoch 0011] loss=17.3390 cls=1.0641 smmd=0.9906 ct=10.9554 rec=1.4134 | train/val/test=0.923/0.632/0.636 | c=0.999014
[Epoch 0012] loss=16.8312 cls=1.0440 smmd=0.7345 ct=10.9211 rec=1.4132 | train/val/test=0.923/0.634/0.644 | c=0.999014
[Epoch 0013] loss=16.9465 cls=1.0369 smmd=0.8092 ct=10.9135 rec=1.4130 | train/val/test=0.923/0.632/0.643 | c=0.999014
[Epoch 0014] loss=17.0114 cls=1.0163 smmd=0.8648 ct=10.9006 rec=1.4126 | train/val/test=0.923/0.624/0.649 | c=0.999014
[Epoch 0015] loss=16.8232 cls=1.0205 smmd=0.7645 ct=10.8886 rec=1.4120 | train/val/test=0.923/0.650/0.655 | c=0.999014
[Epoch 0016] loss=16.4696 cls=0.9698 smmd=0.5874 ct=10.8845 rec=1.4113 | train/val/test=0.923/0.660/0.659 | c=0.999014
[Epoch 0017] loss=16.3394 cls=0.9549 smmd=0.5161 ct=10.8891 rec=1.4106 | train/val/test=0.923/0.660/0.660 | c=0.999014
[Epoch 0018] loss=16.4952 cls=0.9167 smmd=0.6207 ct=10.8903 rec=1.4100 | train/val/test=0.923/0.664/0.670 | c=0.999014
[Epoch 0019] loss=16.7294 cls=0.9083 smmd=0.7590 ct=10.8893 rec=1.4097 | train/val/test=0.846/0.662/0.670 | c=0.999014
[Epoch 0020] loss=16.8901 cls=0.9838 smmd=0.8130 ct=10.8967 rec=1.4096 | train/val/test=0.923/0.658/0.675 | c=0.999014
[Epoch 0021] loss=16.8317 cls=0.9553 smmd=0.7857 ct=10.9051 rec=1.4090 | train/val/test=0.923/0.672/0.685 | c=0.999014
[Epoch 0022] loss=16.7369 cls=0.8610 smmd=0.7704 ct=10.9076 rec=1.4079 | train/val/test=0.923/0.678/0.687 | c=0.999014
[Epoch 0023] loss=16.8483 cls=0.9405 smmd=0.8062 ct=10.8997 rec=1.4059 | train/val/test=0.923/0.688/0.688 | c=0.999014
[Epoch 0024] loss=16.6389 cls=0.7446 smmd=0.7828 ct=10.8842 rec=1.4027 | train/val/test=0.923/0.686/0.693 | c=0.999014
[Epoch 0025] loss=16.4243 cls=0.7169 smmd=0.6854 ct=10.8676 rec=1.3984 | train/val/test=0.923/0.684/0.686 | c=0.999014
[Epoch 0026] loss=16.2961 cls=0.7915 smmd=0.5873 ct=10.8596 rec=1.3929 | train/val/test=0.923/0.690/0.690 | c=0.999014
[Epoch 0027] loss=16.2347 cls=0.8220 smmd=0.5446 ct=10.8537 rec=1.3881 | train/val/test=0.923/0.694/0.688 | c=0.999014
[Epoch 0028] loss=16.0539 cls=0.5893 smmd=0.5465 ct=10.8478 rec=1.3838 | train/val/test=0.923/0.692/0.684 | c=0.999014
[Epoch 0029] loss=16.1969 cls=0.7867 smmd=0.5474 ct=10.8435 rec=1.3807 | train/val/test=0.923/0.688/0.693 | c=0.999014
[Epoch 0030] loss=16.1250 cls=0.6927 smmd=0.5470 ct=10.8433 rec=1.3798 | train/val/test=0.923/0.696/0.696 | c=0.999014
[Epoch 0031] loss=16.2404 cls=0.7859 smmd=0.5694 ct=10.8484 rec=1.3791 | train/val/test=0.923/0.702/0.696 | c=0.999014
[Epoch 0032] loss=16.3706 cls=0.8656 smmd=0.6083 ct=10.8501 rec=1.3792 | train/val/test=0.923/0.698/0.699 | c=0.999014
[Epoch 0033] loss=16.1811 cls=0.5516 smmd=0.6341 ct=10.8498 rec=1.3822 | train/val/test=0.923/0.712/0.708 | c=0.999014
[Epoch 0034] loss=17.1508 cls=0.6440 smmd=0.6347 ct=11.5424 rec=1.3801 | train/val/test=0.923/0.710/0.704 | c=0.999014
[Epoch 0035] loss=17.2621 cls=0.8189 smmd=0.8720 ct=11.2086 rec=1.3766 | train/val/test=0.923/0.718/0.701 | c=0.999014
[Epoch 0036] loss=17.0556 cls=0.7063 smmd=0.8160 ct=11.1908 rec=1.3742 | train/val/test=0.923/0.718/0.701 | c=0.999014
[Epoch 0037] loss=16.9151 cls=0.5267 smmd=0.6911 ct=11.3560 rec=1.3699 | train/val/test=0.923/0.718/0.688 | c=0.999014
[Epoch 0038] loss=16.8594 cls=0.6341 smmd=0.6192 ct=11.3493 rec=1.3653 | train/val/test=0.923/0.726/0.692 | c=0.999014
[Epoch 0039] loss=16.8521 cls=0.6686 smmd=0.6474 ct=11.2870 rec=1.3618 | train/val/test=0.923/0.720/0.702 | c=0.999014
[Epoch 0040] loss=16.7577 cls=0.5778 smmd=0.6301 ct=11.2910 rec=1.3587 | train/val/test=0.923/0.726/0.700 | c=0.999014
[Epoch 0041] loss=16.8265 cls=0.6953 smmd=0.5750 ct=11.3500 rec=1.3596 | train/val/test=1.000/0.730/0.708 | c=0.999014
[Epoch 0042] loss=16.8716 cls=0.7810 smmd=0.5722 ct=11.3396 rec=1.3576 | train/val/test=1.000/0.716/0.714 | c=0.999014
[Epoch 0043] loss=17.0933 cls=1.0478 smmd=0.6297 ct=11.2785 rec=1.3584 | train/val/test=1.000/0.708/0.699 | c=0.999014
[Epoch 0044] loss=16.7221 cls=0.5189 smmd=0.6516 ct=11.2675 rec=1.3621 | train/val/test=1.000/0.706/0.705 | c=0.999014
[Epoch 0045] loss=16.8719 cls=0.7379 smmd=0.6332 ct=11.2815 rec=1.3610 | train/val/test=1.000/0.678/0.709 | c=0.999014
[Epoch 0046] loss=16.6214 cls=0.3629 smmd=0.6400 ct=11.2964 rec=1.3598 | train/val/test=1.000/0.680/0.719 | c=0.999014
[Epoch 0047] loss=16.8774 cls=0.7052 smmd=0.6482 ct=11.2845 rec=1.3607 | train/val/test=1.000/0.682/0.711 | c=0.999014
[Epoch 0048] loss=16.9266 cls=0.7755 smmd=0.6502 ct=11.2780 rec=1.3645 | train/val/test=1.000/0.674/0.694 | c=0.999014
[Epoch 0049] loss=17.1375 cls=1.0728 smmd=0.6275 ct=11.2981 rec=1.3681 | train/val/test=1.000/0.682/0.722 | c=0.999014
[Epoch 0050] loss=17.2256 cls=1.2621 smmd=0.6071 ct=11.2857 rec=1.3630 | train/val/test=1.000/0.666/0.713 | c=0.999014
[Epoch 0051] loss=16.9103 cls=0.8501 smmd=0.6163 ct=11.2677 rec=1.3655 | train/val/test=1.000/0.674/0.704 | c=0.999014
[Epoch 0052] loss=16.5958 cls=0.5339 smmd=0.5901 ct=11.2441 rec=1.3631 | train/val/test=1.000/0.680/0.700 | c=0.999014
[Epoch 0053] loss=16.6421 cls=0.5897 smmd=0.5692 ct=11.2751 rec=1.3652 | train/val/test=1.000/0.672/0.696 | c=0.999014
[Epoch 0054] loss=16.5807 cls=0.5307 smmd=0.5362 ct=11.3064 rec=1.3650 | train/val/test=1.000/0.678/0.708 | c=0.999014
[Epoch 0055] loss=17.2408 cls=1.4489 smmd=0.5247 ct=11.3014 rec=1.3603 | train/val/test=1.000/0.672/0.698 | c=0.999014
[Epoch 0056] loss=16.7754 cls=0.8288 smmd=0.5421 ct=11.2775 rec=1.3609 | train/val/test=1.000/0.666/0.693 | c=0.999014
[Epoch 0057] loss=16.8967 cls=0.9805 smmd=0.5662 ct=11.2510 rec=1.3604 | train/val/test=1.000/0.660/0.682 | c=0.999014
[Epoch 0058] loss=16.6290 cls=0.6112 smmd=0.5688 ct=11.2546 rec=1.3609 | train/val/test=0.923/0.668/0.682 | c=0.999014
[Epoch 0059] loss=16.8015 cls=0.8244 smmd=0.5623 ct=11.2731 rec=1.3607 | train/val/test=0.923/0.680/0.686 | c=0.999014
[Epoch 0060] loss=16.9319 cls=0.9783 smmd=0.5676 ct=11.2774 rec=1.3604 | train/val/test=0.923/0.678/0.681 | c=0.999014
[Epoch 0061] loss=16.5584 cls=0.4653 smmd=0.5825 ct=11.2657 rec=1.3616 | train/val/test=0.923/0.672/0.676 | c=0.999014
[Epoch 0062] loss=16.8814 cls=0.8786 smmd=0.6015 ct=11.2496 rec=1.3633 | train/val/test=0.923/0.664/0.677 | c=0.999014
[Epoch 0063] loss=16.5032 cls=0.3593 smmd=0.6030 ct=11.2559 rec=1.3647 | train/val/test=0.923/0.666/0.677 | c=0.999014
[Epoch 0064] loss=16.8091 cls=0.7604 smmd=0.5917 ct=11.2749 rec=1.3649 | train/val/test=0.923/0.668/0.681 | c=0.999014
[Epoch 0065] loss=16.5190 cls=0.3536 smmd=0.5892 ct=11.2899 rec=1.3647 | train/val/test=0.923/0.670/0.686 | c=0.999014
[Epoch 0066] loss=16.6782 cls=0.5539 smmd=0.5946 ct=11.2896 rec=1.3644 | train/val/test=0.923/0.676/0.696 | c=0.999014
[Epoch 0067] loss=16.8205 cls=0.7371 smmd=0.6142 ct=11.2678 rec=1.3618 | train/val/test=0.923/0.678/0.693 | c=0.999014
[Epoch 0068] loss=16.6990 cls=0.5539 smmd=0.6358 ct=11.2512 rec=1.3605 | train/val/test=0.923/0.686/0.690 | c=0.999014
[Epoch 0069] loss=16.7882 cls=0.6610 smmd=0.6424 ct=11.2493 rec=1.3603 | train/val/test=0.923/0.688/0.695 | c=0.999014
[Epoch 0070] loss=16.7339 cls=0.6075 smmd=0.6324 ct=11.2519 rec=1.3604 | train/val/test=0.923/0.686/0.704 | c=0.999014
[Epoch 0071] loss=16.8921 cls=0.8170 smmd=0.6215 ct=11.2671 rec=1.3612 | train/val/test=0.923/0.684/0.705 | c=0.999014
[Epoch 0072] loss=16.7586 cls=0.6423 smmd=0.6194 ct=11.2680 rec=1.3611 | train/val/test=0.923/0.688/0.704 | c=0.999014
[Epoch 0073] loss=16.7518 cls=0.6440 smmd=0.6150 ct=11.2675 rec=1.3618 | train/val/test=0.923/0.682/0.700 | c=0.999014
[Epoch 0074] loss=16.5377 cls=0.3695 smmd=0.6135 ct=11.2635 rec=1.3612 | train/val/test=0.923/0.688/0.708 | c=0.999014
[Epoch 0075] loss=16.7628 cls=0.6788 smmd=0.6128 ct=11.2593 rec=1.3603 | train/val/test=1.000/0.686/0.707 | c=0.999014
[Epoch 0076] loss=16.6481 cls=0.5252 smmd=0.6166 ct=11.2550 rec=1.3592 | train/val/test=1.000/0.688/0.715 | c=0.999014
[Epoch 0077] loss=16.7274 cls=0.6316 smmd=0.6157 ct=11.2558 rec=1.3593 | train/val/test=1.000/0.690/0.708 | c=0.999014
[Epoch 0078] loss=16.5228 cls=0.3645 smmd=0.6099 ct=11.2602 rec=1.3601 | train/val/test=1.000/0.690/0.702 | c=0.999014
[Epoch 0079] loss=16.6620 cls=0.5457 smmd=0.6100 ct=11.2621 rec=1.3612 | train/val/test=1.000/0.686/0.700 | c=0.999014
[Epoch 0080] loss=16.6681 cls=0.5447 smmd=0.6115 ct=11.2650 rec=1.3626 | train/val/test=1.000/0.684/0.695 | c=0.999014
[Epoch 0081] loss=16.7645 cls=0.6685 smmd=0.6132 ct=11.2652 rec=1.3633 | train/val/test=1.000/0.688/0.696 | c=0.999014
[Epoch 0082] loss=16.7326 cls=0.6275 smmd=0.6164 ct=11.2600 rec=1.3631 | train/val/test=1.000/0.690/0.696 | c=0.999014
[Epoch 0083] loss=16.6121 cls=0.4628 smmd=0.6210 ct=11.2563 rec=1.3626 | train/val/test=1.000/0.698/0.701 | c=0.999014
[Epoch 0084] loss=16.8123 cls=0.7285 smmd=0.6236 ct=11.2535 rec=1.3624 | train/val/test=1.000/0.694/0.704 | c=0.999014
[Epoch 0085] loss=16.6801 cls=0.5471 smmd=0.6261 ct=11.2533 rec=1.3623 | train/val/test=1.000/0.694/0.706 | c=0.999014
[Epoch 0086] loss=16.5355 cls=0.3467 smmd=0.6293 ct=11.2532 rec=1.3625 | train/val/test=1.000/0.694/0.707 | c=0.999014
[Epoch 0087] loss=16.7907 cls=0.6914 smmd=0.6281 ct=11.2523 rec=1.3627 | train/val/test=1.000/0.694/0.710 | c=0.999014
[Epoch 0088] loss=16.6294 cls=0.4763 smmd=0.6288 ct=11.2513 rec=1.3628 | train/val/test=1.000/0.692/0.714 | c=0.999014
[Epoch 0089] loss=16.6667 cls=0.5231 smmd=0.6299 ct=11.2514 rec=1.3631 | train/val/test=1.000/0.692/0.717 | c=0.999014
[Epoch 0090] loss=16.5185 cls=0.3228 smmd=0.6316 ct=11.2506 rec=1.3633 | train/val/test=1.000/0.696/0.716 | c=0.999014
[Epoch 0091] loss=16.8895 cls=0.8153 smmd=0.6325 ct=11.2506 rec=1.3637 | train/val/test=1.000/0.696/0.714 | c=0.999014
[Epoch 0092] loss=16.5153 cls=0.3151 smmd=0.6334 ct=11.2500 rec=1.3638 | train/val/test=1.000/0.696/0.712 | c=0.999014
[Epoch 0093] loss=16.7345 cls=0.6087 smmd=0.6339 ct=11.2486 rec=1.3639 | train/val/test=1.000/0.696/0.714 | c=0.999014
[Epoch 0094] loss=16.7304 cls=0.6014 smmd=0.6351 ct=11.2479 rec=1.3639 | train/val/test=1.000/0.696/0.714 | c=0.999014
[Epoch 0095] loss=16.6802 cls=0.5304 smmd=0.6377 ct=11.2468 rec=1.3640 | train/val/test=1.000/0.696/0.714 | c=0.999014
[Epoch 0096] loss=16.7742 cls=0.6584 smmd=0.6361 ct=11.2475 rec=1.3640 | train/val/test=1.000/0.696/0.715 | c=0.999014
[Epoch 0097] loss=16.8498 cls=0.7534 smmd=0.6392 ct=11.2466 rec=1.3640 | train/val/test=1.000/0.694/0.715 | c=0.999014
[Epoch 0098] loss=16.8027 cls=0.6949 smmd=0.6374 ct=11.2466 rec=1.3639 | train/val/test=1.000/0.694/0.715 | c=0.999014
[Epoch 0099] loss=16.5801 cls=0.4002 smmd=0.6365 ct=11.2466 rec=1.3638 | train/val/test=1.000/0.694/0.715 | c=0.999014
=== Best @ epoch 41: val=0.7300, test=0.7080 ===
