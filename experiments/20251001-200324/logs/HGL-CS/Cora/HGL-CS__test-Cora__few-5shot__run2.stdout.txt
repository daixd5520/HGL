Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> CiteSeer
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.9285 cls=2.0940 smmd=4.7653 ct=9.2870 rec=1.3911 | train/val/test=0.241/0.100/0.126 | c=0.998896
[Epoch 0001] loss=18.6670 cls=1.8830 smmd=4.7118 ct=9.2854 rec=1.3934 | train/val/test=0.241/0.120/0.111 | c=0.998896
[Epoch 0002] loss=18.6781 cls=1.9935 smmd=4.6180 ct=9.2802 rec=1.3932 | train/val/test=0.690/0.314/0.300 | c=0.998896
[Epoch 0003] loss=18.2639 cls=1.7384 smmd=4.4846 ct=9.2665 rec=1.3872 | train/val/test=0.793/0.398/0.379 | c=0.998896
[Epoch 0004] loss=17.8595 cls=1.5519 smmd=4.3048 ct=9.2363 rec=1.3833 | train/val/test=0.862/0.512/0.543 | c=0.998896
[Epoch 0005] loss=17.2126 cls=1.2156 smmd=4.0924 ct=9.1634 rec=1.3706 | train/val/test=0.931/0.418/0.440 | c=0.998896
[Epoch 0006] loss=16.7094 cls=1.1638 smmd=3.8233 ct=9.0513 rec=1.3356 | train/val/test=0.966/0.466/0.475 | c=0.998896
[Epoch 0007] loss=16.0235 cls=0.9662 smmd=3.4908 ct=8.9745 rec=1.2960 | train/val/test=0.931/0.562/0.565 | c=0.998896
[Epoch 0008] loss=15.2019 cls=0.6627 smmd=3.0855 ct=8.9165 rec=1.2686 | train/val/test=1.000/0.632/0.656 | c=0.998896
[Epoch 0009] loss=14.9055 cls=1.0525 smmd=2.6005 ct=8.8082 rec=1.2221 | train/val/test=0.966/0.554/0.589 | c=0.998896
[Epoch 0010] loss=14.4223 cls=1.1368 smmd=2.0465 ct=8.7883 rec=1.2253 | train/val/test=0.966/0.568/0.599 | c=0.998896
[Epoch 0011] loss=14.5496 cls=1.1321 smmd=1.4824 ct=9.5011 rec=1.2170 | train/val/test=1.000/0.574/0.621 | c=0.998896
[Epoch 0012] loss=13.7640 cls=0.9120 smmd=1.0232 ct=9.3902 rec=1.2193 | train/val/test=1.000/0.596/0.627 | c=0.998896
[Epoch 0013] loss=13.4209 cls=0.8629 smmd=0.8317 ct=9.2778 rec=1.2242 | train/val/test=0.966/0.636/0.654 | c=0.998896
[Epoch 0014] loss=13.4457 cls=0.8096 smmd=0.9529 ct=9.2425 rec=1.2204 | train/val/test=0.966/0.644/0.664 | c=0.998896
[Epoch 0015] loss=14.0986 cls=1.1951 smmd=1.2189 ct=9.2385 rec=1.2230 | train/val/test=0.966/0.606/0.643 | c=0.998896
[Epoch 0016] loss=14.1058 cls=0.8771 smmd=1.4898 ct=9.2608 rec=1.2391 | train/val/test=1.000/0.586/0.599 | c=0.998896
[Epoch 0017] loss=14.3913 cls=1.0728 smmd=1.6594 ct=9.2016 rec=1.2287 | train/val/test=1.000/0.538/0.569 | c=0.998896
[Epoch 0018] loss=14.2822 cls=0.9239 smmd=1.7368 ct=9.1778 rec=1.2218 | train/val/test=0.966/0.562/0.593 | c=0.998896
[Epoch 0019] loss=14.2510 cls=0.9181 smmd=1.7208 ct=9.1870 rec=1.2126 | train/val/test=0.931/0.634/0.640 | c=0.998896
[Epoch 0020] loss=14.4273 cls=1.1901 smmd=1.6145 ct=9.2151 rec=1.2038 | train/val/test=0.931/0.638/0.669 | c=0.998896
[Epoch 0021] loss=13.6528 cls=0.5490 smmd=1.4475 ct=9.2537 rec=1.2013 | train/val/test=0.931/0.648/0.671 | c=0.998896
[Epoch 0022] loss=13.7688 cls=0.8529 smmd=1.2449 ct=9.2649 rec=1.2031 | train/val/test=0.966/0.642/0.669 | c=0.998896
[Epoch 0023] loss=13.5042 cls=0.7963 smmd=1.0498 ct=9.2495 rec=1.2043 | train/val/test=0.966/0.626/0.642 | c=0.998896
[Epoch 0024] loss=13.2544 cls=0.6706 smmd=0.9126 ct=9.2505 rec=1.2103 | train/val/test=0.966/0.630/0.632 | c=0.998896
[Epoch 0025] loss=13.6579 cls=1.1293 smmd=0.8350 ct=9.2658 rec=1.2139 | train/val/test=0.966/0.630/0.631 | c=0.998896
[Epoch 0026] loss=13.3200 cls=0.8064 smmd=0.8004 ct=9.2843 rec=1.2145 | train/val/test=0.966/0.640/0.650 | c=0.998896
[Epoch 0027] loss=13.4370 cls=0.9276 smmd=0.7892 ct=9.3014 rec=1.2094 | train/val/test=0.966/0.644/0.657 | c=0.998896
[Epoch 0028] loss=14.0863 cls=1.5894 smmd=0.7741 ct=9.3175 rec=1.2027 | train/val/test=0.966/0.648/0.666 | c=0.998896
[Epoch 0029] loss=13.5991 cls=1.1130 smmd=0.7516 ct=9.3334 rec=1.2006 | train/val/test=0.966/0.648/0.674 | c=0.998896
[Epoch 0030] loss=13.2993 cls=0.8515 smmd=0.7086 ct=9.3352 rec=1.2020 | train/val/test=0.966/0.654/0.682 | c=0.998896
[Epoch 0031] loss=13.3990 cls=0.9826 smmd=0.6607 ct=9.3331 rec=1.2113 | train/val/test=0.966/0.650/0.681 | c=0.998896
[Epoch 0032] loss=13.6208 cls=1.2796 smmd=0.6124 ct=9.3131 rec=1.2079 | train/val/test=0.966/0.646/0.667 | c=0.998896
[Epoch 0033] loss=13.1749 cls=0.8880 smmd=0.5687 ct=9.2888 rec=1.2147 | train/val/test=1.000/0.642/0.675 | c=0.998896
[Epoch 0034] loss=13.1967 cls=0.9611 smmd=0.5399 ct=9.2737 rec=1.2110 | train/val/test=0.966/0.656/0.681 | c=0.998896
[Epoch 0035] loss=12.9823 cls=0.7693 smmd=0.5161 ct=9.2700 rec=1.2135 | train/val/test=0.966/0.666/0.687 | c=0.998896
[Epoch 0036] loss=13.1354 cls=0.9481 smmd=0.4850 ct=9.2737 rec=1.2143 | train/val/test=0.966/0.664/0.679 | c=0.998896
[Epoch 0037] loss=13.1399 cls=0.9920 smmd=0.4542 ct=9.2794 rec=1.2071 | train/val/test=0.966/0.664/0.688 | c=0.998896
[Epoch 0038] loss=13.0863 cls=0.9692 smmd=0.4136 ct=9.2873 rec=1.2081 | train/val/test=0.966/0.670/0.689 | c=0.998896
[Epoch 0039] loss=12.7063 cls=0.6309 smmd=0.3727 ct=9.2923 rec=1.2052 | train/val/test=0.966/0.670/0.693 | c=0.998896
[Epoch 0040] loss=12.8615 cls=0.8282 smmd=0.3405 ct=9.2899 rec=1.2015 | train/val/test=1.000/0.672/0.690 | c=0.998896
[Epoch 0041] loss=12.7988 cls=0.7944 smmd=0.3127 ct=9.2869 rec=1.2024 | train/val/test=1.000/0.666/0.682 | c=0.998896
[Epoch 0042] loss=13.5109 cls=1.5061 smmd=0.2957 ct=9.2889 rec=1.2101 | train/val/test=1.000/0.662/0.680 | c=0.998896
[Epoch 0043] loss=13.2142 cls=1.2599 smmd=0.2754 ct=9.2746 rec=1.2022 | train/val/test=1.000/0.662/0.683 | c=0.998896
[Epoch 0044] loss=12.7462 cls=0.8051 smmd=0.2592 ct=9.2782 rec=1.2018 | train/val/test=1.000/0.650/0.691 | c=0.998896
[Epoch 0045] loss=13.0453 cls=1.1193 smmd=0.2413 ct=9.2824 rec=1.2012 | train/val/test=0.966/0.664/0.693 | c=0.998896
[Epoch 0046] loss=12.6297 cls=0.7431 smmd=0.2232 ct=9.2728 rec=1.1953 | train/val/test=0.966/0.670/0.692 | c=0.998896
[Epoch 0047] loss=12.7959 cls=0.9140 smmd=0.2116 ct=9.2719 rec=1.1992 | train/val/test=0.966/0.668/0.692 | c=0.998896
[Epoch 0048] loss=12.6721 cls=0.8260 smmd=0.1974 ct=9.2685 rec=1.1901 | train/val/test=1.000/0.666/0.689 | c=0.998896
[Epoch 0049] loss=12.4282 cls=0.5773 smmd=0.1922 ct=9.2712 rec=1.1938 | train/val/test=1.000/0.644/0.679 | c=0.998896
[Epoch 0050] loss=12.7950 cls=0.9605 smmd=0.1856 ct=9.2633 rec=1.1929 | train/val/test=1.000/0.632/0.678 | c=0.998896
[Epoch 0051] loss=12.9135 cls=1.0805 smmd=0.1702 ct=9.2626 rec=1.2001 | train/val/test=0.966/0.640/0.680 | c=0.998896
[Epoch 0052] loss=13.0642 cls=1.2653 smmd=0.1616 ct=9.2565 rec=1.1904 | train/val/test=0.966/0.656/0.683 | c=0.998896
[Epoch 0053] loss=13.1204 cls=1.2856 smmd=0.1599 ct=9.2784 rec=1.1982 | train/val/test=0.966/0.654/0.693 | c=0.998896
[Epoch 0054] loss=13.2755 cls=1.4630 smmd=0.1555 ct=9.2677 rec=1.1946 | train/val/test=1.000/0.656/0.703 | c=0.998896
[Epoch 0055] loss=12.5767 cls=0.7710 smmd=0.1528 ct=9.2652 rec=1.1939 | train/val/test=1.000/0.654/0.693 | c=0.998896
[Epoch 0056] loss=12.9087 cls=1.1185 smmd=0.1511 ct=9.2491 rec=1.1950 | train/val/test=1.000/0.654/0.687 | c=0.998896
[Epoch 0057] loss=13.1313 cls=1.3102 smmd=0.1512 ct=9.2491 rec=1.2104 | train/val/test=1.000/0.642/0.669 | c=0.998896
[Epoch 0058] loss=13.1012 cls=1.2873 smmd=0.1494 ct=9.2501 rec=1.2072 | train/val/test=1.000/0.630/0.663 | c=0.998896
[Epoch 0059] loss=12.7806 cls=0.9762 smmd=0.1439 ct=9.2470 rec=1.2067 | train/val/test=1.000/0.636/0.684 | c=0.998896
[Epoch 0060] loss=13.0310 cls=1.2297 smmd=0.1391 ct=9.2646 rec=1.1988 | train/val/test=1.000/0.660/0.702 | c=0.998896
[Epoch 0061] loss=12.9214 cls=1.0858 smmd=0.1364 ct=9.2882 rec=1.2055 | train/val/test=1.000/0.658/0.695 | c=0.998896
[Epoch 0062] loss=12.5791 cls=0.7563 smmd=0.1315 ct=9.2816 rec=1.2048 | train/val/test=1.000/0.646/0.687 | c=0.998896
[Epoch 0063] loss=12.8851 cls=1.0804 smmd=0.1309 ct=9.2653 rec=1.2043 | train/val/test=1.000/0.632/0.662 | c=0.998896
[Epoch 0064] loss=13.0354 cls=1.2289 smmd=0.1338 ct=9.2520 rec=1.2104 | train/val/test=1.000/0.632/0.652 | c=0.998896
[Epoch 0065] loss=13.1868 cls=1.3714 smmd=0.1377 ct=9.2495 rec=1.2141 | train/val/test=1.000/0.628/0.651 | c=0.998896
[Epoch 0066] loss=12.5831 cls=0.7938 smmd=0.1345 ct=9.2441 rec=1.2053 | train/val/test=1.000/0.632/0.662 | c=0.998896
[Epoch 0067] loss=12.9723 cls=1.1732 smmd=0.1306 ct=9.2460 rec=1.2112 | train/val/test=1.000/0.636/0.672 | c=0.998896
[Epoch 0068] loss=12.8337 cls=1.0359 smmd=0.1289 ct=9.2597 rec=1.2046 | train/val/test=1.000/0.642/0.680 | c=0.998896
[Epoch 0069] loss=12.6606 cls=0.8518 smmd=0.1269 ct=9.2703 rec=1.2058 | train/val/test=1.000/0.646/0.684 | c=0.998896
[Epoch 0070] loss=12.5654 cls=0.7619 smmd=0.1238 ct=9.2654 rec=1.2072 | train/val/test=1.000/0.646/0.683 | c=0.998896
[Epoch 0071] loss=13.1281 cls=1.3293 smmd=0.1224 ct=9.2595 rec=1.2084 | train/val/test=1.000/0.640/0.674 | c=0.998896
[Epoch 0072] loss=12.6834 cls=0.8838 smmd=0.1227 ct=9.2572 rec=1.2099 | train/val/test=1.000/0.646/0.671 | c=0.998896
[Epoch 0073] loss=13.1051 cls=1.3204 smmd=0.1249 ct=9.2473 rec=1.2063 | train/val/test=1.000/0.650/0.670 | c=0.998896
[Epoch 0074] loss=12.8542 cls=1.0582 smmd=0.1254 ct=9.2500 rec=1.2103 | train/val/test=1.000/0.648/0.669 | c=0.998896
[Epoch 0075] loss=12.7729 cls=0.9798 smmd=0.1241 ct=9.2517 rec=1.2087 | train/val/test=1.000/0.648/0.669 | c=0.998896
[Epoch 0076] loss=13.1887 cls=1.4012 smmd=0.1227 ct=9.2530 rec=1.2059 | train/val/test=1.000/0.642/0.670 | c=0.998896
[Epoch 0077] loss=13.1002 cls=1.3012 smmd=0.1208 ct=9.2590 rec=1.2096 | train/val/test=1.000/0.642/0.671 | c=0.998896
[Epoch 0078] loss=12.8502 cls=1.0613 smmd=0.1202 ct=9.2578 rec=1.2054 | train/val/test=1.000/0.646/0.676 | c=0.998896
[Epoch 0079] loss=13.0731 cls=1.2828 smmd=0.1181 ct=9.2579 rec=1.2072 | train/val/test=1.000/0.650/0.675 | c=0.998896
[Epoch 0080] loss=12.6356 cls=0.8660 smmd=0.1170 ct=9.2512 rec=1.2006 | train/val/test=1.000/0.648/0.679 | c=0.998896
[Epoch 0081] loss=13.0389 cls=1.2506 smmd=0.1175 ct=9.2572 rec=1.2068 | train/val/test=1.000/0.650/0.683 | c=0.998896
[Epoch 0082] loss=12.8150 cls=1.0391 smmd=0.1190 ct=9.2455 rec=1.2057 | train/val/test=1.000/0.648/0.680 | c=0.998896
[Epoch 0083] loss=12.3923 cls=0.6015 smmd=0.1205 ct=9.2534 rec=1.2084 | train/val/test=1.000/0.650/0.681 | c=0.998896
[Epoch 0084] loss=12.4227 cls=0.6493 smmd=0.1203 ct=9.2525 rec=1.2003 | train/val/test=1.000/0.654/0.679 | c=0.998896
[Epoch 0085] loss=12.8888 cls=1.1208 smmd=0.1189 ct=9.2456 rec=1.2018 | train/val/test=1.000/0.652/0.682 | c=0.998896
[Epoch 0086] loss=12.8283 cls=1.0571 smmd=0.1184 ct=9.2487 rec=1.2020 | train/val/test=1.000/0.646/0.678 | c=0.998896
[Epoch 0087] loss=12.5614 cls=0.7764 smmd=0.1160 ct=9.2537 rec=1.2077 | train/val/test=1.000/0.644/0.677 | c=0.998896
[Epoch 0088] loss=12.9438 cls=1.1569 smmd=0.1156 ct=9.2545 rec=1.2084 | train/val/test=1.000/0.646/0.675 | c=0.998896
[Epoch 0089] loss=13.1594 cls=1.3953 smmd=0.1149 ct=9.2462 rec=1.2015 | train/val/test=1.000/0.646/0.674 | c=0.998896
[Epoch 0090] loss=12.3542 cls=0.5925 smmd=0.1151 ct=9.2472 rec=1.1997 | train/val/test=1.000/0.644/0.675 | c=0.998896
[Epoch 0091] loss=12.8953 cls=1.1147 smmd=0.1141 ct=9.2489 rec=1.2088 | train/val/test=1.000/0.644/0.676 | c=0.998896
[Epoch 0092] loss=12.3862 cls=0.6227 smmd=0.1147 ct=9.2495 rec=1.1996 | train/val/test=1.000/0.646/0.676 | c=0.998896
[Epoch 0093] loss=12.5590 cls=0.7901 smmd=0.1129 ct=9.2500 rec=1.2030 | train/val/test=1.000/0.646/0.676 | c=0.998896
[Epoch 0094] loss=12.3658 cls=0.6080 smmd=0.1146 ct=9.2467 rec=1.1982 | train/val/test=1.000/0.644/0.675 | c=0.998896
[Epoch 0095] loss=12.8228 cls=1.0651 smmd=0.1146 ct=9.2471 rec=1.1980 | train/val/test=1.000/0.644/0.676 | c=0.998896
[Epoch 0096] loss=12.6998 cls=0.9454 smmd=0.1136 ct=9.2474 rec=1.1967 | train/val/test=1.000/0.644/0.677 | c=0.998896
[Epoch 0097] loss=12.7401 cls=0.9851 smmd=0.1136 ct=9.2471 rec=1.1971 | train/val/test=1.000/0.644/0.677 | c=0.998896
[Epoch 0098] loss=12.3820 cls=0.6146 smmd=0.1146 ct=9.2514 rec=1.2007 | train/val/test=1.000/0.646/0.677 | c=0.998896
[Epoch 0099] loss=13.0190 cls=1.2639 smmd=0.1132 ct=9.2439 rec=1.1990 | train/val/test=1.000/0.646/0.677 | c=0.998896
=== Best @ epoch 40: val=0.6720, test=0.6900 ===
