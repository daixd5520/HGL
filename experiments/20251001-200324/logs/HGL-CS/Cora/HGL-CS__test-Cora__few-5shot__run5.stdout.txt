Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> CiteSeer
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.8918 cls=2.1139 smmd=4.7044 ct=9.2837 rec=1.3949 | train/val/test=0.414/0.228/0.213 | c=0.998896
[Epoch 0001] loss=18.5819 cls=1.8636 smmd=4.6552 ct=9.2828 rec=1.3902 | train/val/test=0.241/0.120/0.137 | c=0.998896
[Epoch 0002] loss=18.4756 cls=1.8628 smmd=4.5558 ct=9.2776 rec=1.3897 | train/val/test=0.552/0.162/0.164 | c=0.998896
[Epoch 0003] loss=18.0954 cls=1.6382 smmd=4.4189 ct=9.2639 rec=1.3872 | train/val/test=0.793/0.366/0.388 | c=0.998896
[Epoch 0004] loss=17.7071 cls=1.4775 smmd=4.2509 ct=9.2281 rec=1.3753 | train/val/test=0.793/0.530/0.537 | c=0.998896
[Epoch 0005] loss=17.3851 cls=1.4856 smmd=4.0290 ct=9.1535 rec=1.3585 | train/val/test=0.966/0.552/0.586 | c=0.998896
[Epoch 0006] loss=16.4778 cls=1.0000 smmd=3.7634 ct=9.0475 rec=1.3334 | train/val/test=0.897/0.460/0.480 | c=0.998896
[Epoch 0007] loss=16.0919 cls=1.0193 smmd=3.4321 ct=9.0115 rec=1.3145 | train/val/test=0.897/0.506/0.495 | c=0.998896
[Epoch 0008] loss=15.3890 cls=0.9100 smmd=3.0398 ct=8.9161 rec=1.2615 | train/val/test=0.966/0.532/0.534 | c=0.998896
[Epoch 0009] loss=15.1292 cls=1.2682 smmd=2.5726 ct=8.8074 rec=1.2405 | train/val/test=0.931/0.600/0.606 | c=0.998896
[Epoch 0010] loss=14.1077 cls=0.8078 smmd=2.0371 ct=8.8098 rec=1.2265 | train/val/test=0.931/0.628/0.644 | c=0.998896
[Epoch 0011] loss=13.5120 cls=0.7914 smmd=1.4840 ct=8.8138 rec=1.2114 | train/val/test=0.966/0.646/0.647 | c=0.998896
[Epoch 0012] loss=13.0698 cls=0.8422 smmd=1.0108 ct=8.8014 rec=1.2077 | train/val/test=1.000/0.638/0.618 | c=0.998896
[Epoch 0013] loss=12.6525 cls=0.6847 smmd=0.7729 ct=8.7863 rec=1.2043 | train/val/test=0.931/0.636/0.632 | c=0.998896
[Epoch 0014] loss=13.0452 cls=1.0413 smmd=0.8421 ct=8.7692 rec=1.1963 | train/val/test=0.931/0.636/0.645 | c=0.998896
[Epoch 0015] loss=12.8206 cls=0.5833 smmd=1.0777 ct=8.7734 rec=1.1931 | train/val/test=0.966/0.662/0.667 | c=0.998896
[Epoch 0016] loss=13.9843 cls=0.8923 smmd=1.3191 ct=9.3969 rec=1.1880 | train/val/test=0.966/0.674/0.690 | c=0.998896
[Epoch 0017] loss=14.2938 cls=1.0713 smmd=1.4959 ct=9.3418 rec=1.1924 | train/val/test=0.966/0.676/0.696 | c=0.998896
[Epoch 0018] loss=13.9853 cls=0.7290 smmd=1.5926 ct=9.2706 rec=1.1966 | train/val/test=1.000/0.680/0.698 | c=0.998896
[Epoch 0019] loss=13.6877 cls=0.4608 smmd=1.6034 ct=9.2275 rec=1.1980 | train/val/test=1.000/0.650/0.681 | c=0.998896
[Epoch 0020] loss=14.2057 cls=1.0660 smmd=1.5448 ct=9.1907 rec=1.2021 | train/val/test=1.000/0.638/0.657 | c=0.998896
[Epoch 0021] loss=13.6185 cls=0.5848 smmd=1.4298 ct=9.1870 rec=1.2085 | train/val/test=1.000/0.634/0.634 | c=0.998896
[Epoch 0022] loss=13.8910 cls=1.0085 smmd=1.2723 ct=9.1817 rec=1.2143 | train/val/test=1.000/0.650/0.640 | c=0.998896
[Epoch 0023] loss=13.9254 cls=1.2327 smmd=1.0949 ct=9.1733 rec=1.2123 | train/val/test=1.000/0.664/0.674 | c=0.998896
[Epoch 0024] loss=13.7188 cls=1.1838 smmd=0.9430 ct=9.1880 rec=1.2020 | train/val/test=1.000/0.676/0.697 | c=0.998896
[Epoch 0025] loss=13.3572 cls=0.8732 smmd=0.8492 ct=9.2371 rec=1.1988 | train/val/test=1.000/0.676/0.699 | c=0.998896
[Epoch 0026] loss=13.5617 cls=1.0740 smmd=0.8029 ct=9.2901 rec=1.1973 | train/val/test=1.000/0.666/0.688 | c=0.998896
[Epoch 0027] loss=13.8855 cls=1.3698 smmd=0.7844 ct=9.3253 rec=1.2030 | train/val/test=1.000/0.672/0.678 | c=0.998896
[Epoch 0028] loss=13.1046 cls=0.6296 smmd=0.7542 ct=9.3270 rec=1.1969 | train/val/test=1.000/0.674/0.658 | c=0.998896
[Epoch 0029] loss=13.2379 cls=0.7639 smmd=0.7352 ct=9.3207 rec=1.2091 | train/val/test=1.000/0.652/0.648 | c=0.998896
[Epoch 0030] loss=13.8275 cls=1.3365 smmd=0.7234 ct=9.3164 rec=1.2256 | train/val/test=1.000/0.652/0.653 | c=0.998896
[Epoch 0031] loss=13.2151 cls=0.7743 smmd=0.6897 ct=9.3051 rec=1.2230 | train/val/test=1.000/0.658/0.666 | c=0.998896
[Epoch 0032] loss=13.3232 cls=0.9780 smmd=0.6308 ct=9.2800 rec=1.2172 | train/val/test=1.000/0.670/0.686 | c=0.998896
[Epoch 0033] loss=13.1334 cls=0.8618 smmd=0.5760 ct=9.2744 rec=1.2106 | train/val/test=1.000/0.678/0.697 | c=0.998896
[Epoch 0034] loss=12.9669 cls=0.7159 smmd=0.5451 ct=9.2818 rec=1.2121 | train/val/test=1.000/0.678/0.689 | c=0.998896
[Epoch 0035] loss=13.5862 cls=1.3844 smmd=0.5128 ct=9.2776 rec=1.2057 | train/val/test=1.000/0.678/0.687 | c=0.998896
[Epoch 0036] loss=12.8589 cls=0.7043 smmd=0.4733 ct=9.2781 rec=1.2016 | train/val/test=1.000/0.674/0.682 | c=0.998896
[Epoch 0037] loss=12.9384 cls=0.8414 smmd=0.4345 ct=9.2703 rec=1.1962 | train/val/test=1.000/0.666/0.682 | c=0.998896
[Epoch 0038] loss=13.0889 cls=1.0094 smmd=0.4174 ct=9.2616 rec=1.2002 | train/val/test=1.000/0.656/0.683 | c=0.998896
[Epoch 0039] loss=12.9570 cls=0.8849 smmd=0.4097 ct=9.2662 rec=1.1981 | train/val/test=1.000/0.660/0.683 | c=0.998896
[Epoch 0040] loss=13.2332 cls=1.1644 smmd=0.3950 ct=9.2570 rec=1.2085 | train/val/test=1.000/0.670/0.691 | c=0.998896
[Epoch 0041] loss=12.8969 cls=0.8720 smmd=0.3614 ct=9.2554 rec=1.2041 | train/val/test=1.000/0.672/0.693 | c=0.998896
[Epoch 0042] loss=12.7523 cls=0.7905 smmd=0.3178 ct=9.2554 rec=1.1943 | train/val/test=1.000/0.674/0.696 | c=0.998896
[Epoch 0043] loss=12.6176 cls=0.6980 smmd=0.2787 ct=9.2694 rec=1.1858 | train/val/test=1.000/0.676/0.689 | c=0.998896
[Epoch 0044] loss=12.7675 cls=0.8624 smmd=0.2474 ct=9.2849 rec=1.1864 | train/val/test=1.000/0.674/0.672 | c=0.998896
[Epoch 0045] loss=12.6336 cls=0.7467 smmd=0.2281 ct=9.2873 rec=1.1858 | train/val/test=1.000/0.672/0.679 | c=0.998896
[Epoch 0046] loss=12.4518 cls=0.5954 smmd=0.2163 ct=9.2762 rec=1.1820 | train/val/test=1.000/0.672/0.681 | c=0.998896
[Epoch 0047] loss=12.7439 cls=0.8809 smmd=0.2213 ct=9.2557 rec=1.1930 | train/val/test=1.000/0.672/0.688 | c=0.998896
[Epoch 0048] loss=12.9442 cls=1.1170 smmd=0.2142 ct=9.2413 rec=1.1859 | train/val/test=1.000/0.670/0.686 | c=0.998896
[Epoch 0049] loss=12.8961 cls=1.0767 smmd=0.1984 ct=9.2406 rec=1.1902 | train/val/test=1.000/0.676/0.689 | c=0.998896
[Epoch 0050] loss=13.1761 cls=1.3690 smmd=0.1821 ct=9.2542 rec=1.1854 | train/val/test=1.000/0.676/0.697 | c=0.998896
[Epoch 0051] loss=12.6590 cls=0.8753 smmd=0.1711 ct=9.2439 rec=1.1844 | train/val/test=1.000/0.680/0.697 | c=0.998896
[Epoch 0052] loss=12.8021 cls=1.0087 smmd=0.1699 ct=9.2487 rec=1.1874 | train/val/test=1.000/0.682/0.694 | c=0.998896
[Epoch 0053] loss=12.9683 cls=1.1648 smmd=0.1696 ct=9.2467 rec=1.1936 | train/val/test=1.000/0.682/0.689 | c=0.998896
[Epoch 0054] loss=12.7414 cls=0.9533 smmd=0.1599 ct=9.2391 rec=1.1946 | train/val/test=1.000/0.668/0.679 | c=0.998896
[Epoch 0055] loss=13.1392 cls=1.3452 smmd=0.1479 ct=9.2386 rec=1.2038 | train/val/test=1.000/0.634/0.661 | c=0.998896
[Epoch 0056] loss=12.7818 cls=0.9548 smmd=0.1534 ct=9.2592 rec=1.2072 | train/val/test=1.000/0.644/0.672 | c=0.998896
[Epoch 0057] loss=12.8869 cls=1.0625 smmd=0.1467 ct=9.2504 rec=1.2136 | train/val/test=1.000/0.682/0.700 | c=0.998896
[Epoch 0058] loss=13.3182 cls=1.5388 smmd=0.1414 ct=9.2290 rec=1.2045 | train/val/test=1.000/0.684/0.692 | c=0.998896
[Epoch 0059] loss=13.2651 cls=1.4396 smmd=0.1571 ct=9.2320 rec=1.2182 | train/val/test=1.000/0.678/0.691 | c=0.998896
[Epoch 0060] loss=12.7345 cls=0.9066 smmd=0.1545 ct=9.2357 rec=1.2189 | train/val/test=1.000/0.682/0.699 | c=0.998896
[Epoch 0061] loss=13.0923 cls=1.2703 smmd=0.1383 ct=9.2407 rec=1.2215 | train/val/test=1.000/0.668/0.691 | c=0.998896
[Epoch 0062] loss=12.7765 cls=0.9904 smmd=0.1243 ct=9.2350 rec=1.2134 | train/val/test=1.000/0.654/0.680 | c=0.998896
[Epoch 0063] loss=12.8309 cls=1.0219 smmd=0.1271 ct=9.2436 rec=1.2192 | train/val/test=1.000/0.644/0.675 | c=0.998896
[Epoch 0064] loss=12.6420 cls=0.8207 smmd=0.1297 ct=9.2519 rec=1.2199 | train/val/test=1.000/0.660/0.688 | c=0.998896
[Epoch 0065] loss=12.8054 cls=1.0090 smmd=0.1232 ct=9.2393 rec=1.2169 | train/val/test=1.000/0.670/0.695 | c=0.998896
[Epoch 0066] loss=12.7282 cls=0.9441 smmd=0.1235 ct=9.2205 rec=1.2201 | train/val/test=1.000/0.674/0.699 | c=0.998896
[Epoch 0067] loss=12.7260 cls=0.9390 smmd=0.1283 ct=9.2220 rec=1.2184 | train/val/test=1.000/0.674/0.702 | c=0.998896
[Epoch 0068] loss=13.0320 cls=1.2436 smmd=0.1281 ct=9.2260 rec=1.2172 | train/val/test=1.000/0.666/0.701 | c=0.998896
[Epoch 0069] loss=12.9902 cls=1.2130 smmd=0.1194 ct=9.2282 rec=1.2148 | train/val/test=1.000/0.662/0.699 | c=0.998896
[Epoch 0070] loss=12.9015 cls=1.1374 smmd=0.1145 ct=9.2284 rec=1.2106 | train/val/test=1.000/0.666/0.696 | c=0.998896
[Epoch 0071] loss=12.4994 cls=0.7421 smmd=0.1118 ct=9.2299 rec=1.2078 | train/val/test=1.000/0.666/0.697 | c=0.998896
[Epoch 0072] loss=12.8599 cls=1.0847 smmd=0.1135 ct=9.2369 rec=1.2124 | train/val/test=1.000/0.668/0.699 | c=0.998896
[Epoch 0073] loss=12.7639 cls=1.0069 smmd=0.1130 ct=9.2293 rec=1.2074 | train/val/test=1.000/0.670/0.699 | c=0.998896
[Epoch 0074] loss=12.9700 cls=1.2156 smmd=0.1150 ct=9.2247 rec=1.2074 | train/val/test=1.000/0.672/0.697 | c=0.998896
[Epoch 0075] loss=13.2370 cls=1.4762 smmd=0.1159 ct=9.2197 rec=1.2126 | train/val/test=1.000/0.670/0.696 | c=0.998896
[Epoch 0076] loss=12.8359 cls=1.0603 smmd=0.1197 ct=9.2172 rec=1.2194 | train/val/test=1.000/0.672/0.697 | c=0.998896
[Epoch 0077] loss=13.3615 cls=1.6072 smmd=0.1222 ct=9.2184 rec=1.2069 | train/val/test=1.000/0.670/0.700 | c=0.998896
[Epoch 0078] loss=12.4761 cls=0.7198 smmd=0.1214 ct=9.2196 rec=1.2076 | train/val/test=1.000/0.674/0.701 | c=0.998896
[Epoch 0079] loss=12.5868 cls=0.8402 smmd=0.1214 ct=9.2132 rec=1.2060 | train/val/test=1.000/0.672/0.701 | c=0.998896
[Epoch 0080] loss=12.8104 cls=1.0501 smmd=0.1193 ct=9.2107 rec=1.2151 | train/val/test=1.000/0.668/0.701 | c=0.998896
[Epoch 0081] loss=13.2383 cls=1.4892 smmd=0.1165 ct=9.2206 rec=1.2060 | train/val/test=1.000/0.662/0.700 | c=0.998896
[Epoch 0082] loss=13.0212 cls=1.2772 smmd=0.1136 ct=9.2181 rec=1.2061 | train/val/test=1.000/0.664/0.703 | c=0.998896
[Epoch 0083] loss=12.7668 cls=1.0118 smmd=0.1152 ct=9.2180 rec=1.2109 | train/val/test=1.000/0.664/0.703 | c=0.998896
[Epoch 0084] loss=12.8579 cls=1.1158 smmd=0.1131 ct=9.2178 rec=1.2056 | train/val/test=1.000/0.664/0.703 | c=0.998896
[Epoch 0085] loss=12.4699 cls=0.7266 smmd=0.1132 ct=9.2214 rec=1.2044 | train/val/test=1.000/0.664/0.705 | c=0.998896
[Epoch 0086] loss=12.6491 cls=0.8908 smmd=0.1128 ct=9.2303 rec=1.2076 | train/val/test=1.000/0.664/0.704 | c=0.998896
[Epoch 0087] loss=12.3772 cls=0.6351 smmd=0.1118 ct=9.2210 rec=1.2047 | train/val/test=1.000/0.664/0.703 | c=0.998896
[Epoch 0088] loss=12.4077 cls=0.6717 smmd=0.1117 ct=9.2212 rec=1.2016 | train/val/test=1.000/0.668/0.704 | c=0.998896
[Epoch 0089] loss=12.5902 cls=0.8585 smmd=0.1108 ct=9.2224 rec=1.1993 | train/val/test=1.000/0.668/0.703 | c=0.998896
[Epoch 0090] loss=12.9358 cls=1.1988 smmd=0.1109 ct=9.2225 rec=1.2018 | train/val/test=1.000/0.668/0.703 | c=0.998896
[Epoch 0091] loss=12.7351 cls=1.0026 smmd=0.1100 ct=9.2212 rec=1.2006 | train/val/test=1.000/0.668/0.702 | c=0.998896
[Epoch 0092] loss=12.7255 cls=0.9939 smmd=0.1118 ct=9.2178 rec=1.2010 | train/val/test=1.000/0.668/0.703 | c=0.998896
[Epoch 0093] loss=12.8861 cls=1.1619 smmd=0.1089 ct=9.2144 rec=1.2004 | train/val/test=1.000/0.668/0.703 | c=0.998896
[Epoch 0094] loss=12.9075 cls=1.1712 smmd=0.1114 ct=9.2230 rec=1.2010 | train/val/test=1.000/0.672/0.703 | c=0.998896
[Epoch 0095] loss=12.8456 cls=1.0903 smmd=0.1113 ct=9.2243 rec=1.2098 | train/val/test=1.000/0.672/0.703 | c=0.998896
[Epoch 0096] loss=12.6665 cls=0.9306 smmd=0.1128 ct=9.2187 rec=1.2022 | train/val/test=1.000/0.670/0.703 | c=0.998896
[Epoch 0097] loss=12.5462 cls=0.8175 smmd=0.1121 ct=9.2161 rec=1.2003 | train/val/test=1.000/0.668/0.704 | c=0.998896
[Epoch 0098] loss=12.9056 cls=1.1563 smmd=0.1103 ct=9.2238 rec=1.2076 | train/val/test=1.000/0.668/0.704 | c=0.998896
[Epoch 0099] loss=13.1984 cls=1.4460 smmd=0.1120 ct=9.2194 rec=1.2105 | train/val/test=1.000/0.668/0.704 | c=0.998896
=== Best @ epoch 58: val=0.6840, test=0.6920 ===
