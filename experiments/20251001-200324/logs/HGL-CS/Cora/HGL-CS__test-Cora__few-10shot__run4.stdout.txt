Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> CiteSeer
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.8928 cls=2.1576 smmd=4.6904 ct=9.2542 rec=1.3953 | train/val/test=0.276/0.096/0.125 | c=0.998896
[Epoch 0001] loss=18.6746 cls=1.9976 smmd=4.6460 ct=9.2516 rec=1.3897 | train/val/test=0.414/0.262/0.294 | c=0.998896
[Epoch 0002] loss=18.4902 cls=1.9157 smmd=4.5498 ct=9.2438 rec=1.3904 | train/val/test=0.621/0.316/0.330 | c=0.998896
[Epoch 0003] loss=18.2670 cls=1.8579 smmd=4.4162 ct=9.2210 rec=1.3859 | train/val/test=0.672/0.326/0.330 | c=0.998896
[Epoch 0004] loss=17.7328 cls=1.5648 smmd=4.2374 ct=9.1747 rec=1.3780 | train/val/test=0.828/0.472/0.459 | c=0.998896
[Epoch 0005] loss=17.2078 cls=1.4089 smmd=4.0054 ct=9.0822 rec=1.3556 | train/val/test=0.845/0.518/0.511 | c=0.998896
[Epoch 0006] loss=16.7416 cls=1.4014 smmd=3.7066 ct=8.9873 rec=1.3231 | train/val/test=0.914/0.614/0.611 | c=0.998896
[Epoch 0007] loss=16.1529 cls=1.2830 smmd=3.3438 ct=8.9178 rec=1.3041 | train/val/test=0.897/0.656/0.630 | c=0.998896
[Epoch 0008] loss=15.6269 cls=1.4153 smmd=2.9155 ct=8.7932 rec=1.2515 | train/val/test=0.948/0.662/0.641 | c=0.998896
[Epoch 0009] loss=14.5725 cls=0.9322 smmd=2.4104 ct=8.7618 rec=1.2341 | train/val/test=0.948/0.702/0.677 | c=0.998896
[Epoch 0010] loss=14.1438 cls=1.0790 smmd=1.8411 ct=8.7729 rec=1.2254 | train/val/test=0.931/0.730/0.706 | c=0.998896
[Epoch 0011] loss=13.5384 cls=1.0637 smmd=1.2693 ct=8.7709 rec=1.2173 | train/val/test=0.966/0.720/0.708 | c=0.998896
[Epoch 0012] loss=13.2192 cls=1.2090 smmd=0.8524 ct=8.7479 rec=1.2049 | train/val/test=0.983/0.726/0.721 | c=0.998896
[Epoch 0013] loss=12.8202 cls=0.9462 smmd=0.7530 ct=8.7295 rec=1.1958 | train/val/test=0.983/0.736/0.729 | c=0.998896
[Epoch 0014] loss=13.0982 cls=1.0389 smmd=0.9467 ct=8.7306 rec=1.1910 | train/val/test=0.948/0.740/0.740 | c=0.998896
[Epoch 0015] loss=13.3164 cls=0.9864 smmd=1.2257 ct=8.7292 rec=1.1875 | train/val/test=0.966/0.758/0.745 | c=0.998896
[Epoch 0016] loss=13.4894 cls=0.9431 smmd=1.4474 ct=8.7314 rec=1.1837 | train/val/test=0.966/0.738/0.721 | c=0.998896
[Epoch 0017] loss=13.9255 cls=1.2499 smmd=1.5637 ct=8.7356 rec=1.1881 | train/val/test=0.983/0.734/0.695 | c=0.998896
[Epoch 0018] loss=14.0993 cls=1.4317 smmd=1.5596 ct=8.7294 rec=1.1894 | train/val/test=0.983/0.726/0.705 | c=0.998896
[Epoch 0019] loss=13.2996 cls=0.7579 smmd=1.4524 ct=8.7109 rec=1.1892 | train/val/test=0.983/0.730/0.719 | c=0.998896
[Epoch 0020] loss=13.5461 cls=1.2084 smmd=1.2653 ct=8.6972 rec=1.1876 | train/val/test=0.983/0.738/0.734 | c=0.998896
[Epoch 0021] loss=13.2148 cls=1.0894 smmd=1.0465 ct=8.6991 rec=1.1899 | train/val/test=0.983/0.740/0.736 | c=0.998896
[Epoch 0022] loss=13.9066 cls=1.2648 smmd=0.8693 ct=9.3822 rec=1.1952 | train/val/test=0.966/0.722/0.716 | c=0.998896
[Epoch 0023] loss=13.6334 cls=1.1129 smmd=0.7786 ct=9.3449 rec=1.1985 | train/val/test=0.966/0.714/0.683 | c=0.998896
[Epoch 0024] loss=13.5172 cls=1.0206 smmd=0.7833 ct=9.3019 rec=1.2057 | train/val/test=0.948/0.700/0.678 | c=0.998896
[Epoch 0025] loss=13.5168 cls=0.9963 smmd=0.8417 ct=9.2521 rec=1.2133 | train/val/test=0.948/0.704/0.686 | c=0.998896
[Epoch 0026] loss=13.6712 cls=1.1251 smmd=0.8865 ct=9.2300 rec=1.2148 | train/val/test=0.966/0.710/0.702 | c=0.998896
[Epoch 0027] loss=13.7971 cls=1.2571 smmd=0.8990 ct=9.2203 rec=1.2104 | train/val/test=0.983/0.714/0.710 | c=0.998896
[Epoch 0028] loss=13.8216 cls=1.3132 smmd=0.8586 ct=9.2246 rec=1.2126 | train/val/test=1.000/0.716/0.710 | c=0.998896
[Epoch 0029] loss=13.6924 cls=1.2468 smmd=0.7794 ct=9.2431 rec=1.2115 | train/val/test=1.000/0.718/0.702 | c=0.998896
[Epoch 0030] loss=13.3852 cls=1.0275 smmd=0.6840 ct=9.2578 rec=1.2079 | train/val/test=0.983/0.712/0.696 | c=0.998896
[Epoch 0031] loss=13.3779 cls=1.0776 smmd=0.5949 ct=9.2758 rec=1.2148 | train/val/test=0.983/0.706/0.684 | c=0.998896
[Epoch 0032] loss=13.8381 cls=1.6063 smmd=0.5245 ct=9.2795 rec=1.2139 | train/val/test=0.983/0.696/0.686 | c=0.998896
[Epoch 0033] loss=13.2970 cls=1.0958 smmd=0.4847 ct=9.2862 rec=1.2151 | train/val/test=0.983/0.696/0.689 | c=0.998896
[Epoch 0034] loss=13.4607 cls=1.2757 smmd=0.4628 ct=9.2899 rec=1.2162 | train/val/test=0.966/0.702/0.692 | c=0.998896
[Epoch 0035] loss=13.1212 cls=0.9852 smmd=0.4418 ct=9.2820 rec=1.2061 | train/val/test=0.966/0.700/0.695 | c=0.998896
[Epoch 0036] loss=13.1255 cls=1.0130 smmd=0.4171 ct=9.2843 rec=1.2055 | train/val/test=0.983/0.702/0.694 | c=0.998896
[Epoch 0037] loss=13.0151 cls=0.9312 smmd=0.3901 ct=9.2851 rec=1.2044 | train/val/test=0.983/0.706/0.690 | c=0.998896
[Epoch 0038] loss=13.4716 cls=1.4227 smmd=0.3615 ct=9.2715 rec=1.2080 | train/val/test=0.983/0.710/0.682 | c=0.998896
[Epoch 0039] loss=13.2084 cls=1.2062 smmd=0.3374 ct=9.2603 rec=1.2023 | train/val/test=1.000/0.712/0.679 | c=0.998896
[Epoch 0040] loss=13.0313 cls=1.0600 smmd=0.3211 ct=9.2482 rec=1.2010 | train/val/test=1.000/0.706/0.683 | c=0.998896
[Epoch 0041] loss=12.9025 cls=0.9553 smmd=0.3009 ct=9.2469 rec=1.1997 | train/val/test=0.983/0.706/0.685 | c=0.998896
[Epoch 0042] loss=12.9378 cls=1.0199 smmd=0.2792 ct=9.2453 rec=1.1967 | train/val/test=0.983/0.694/0.685 | c=0.998896
[Epoch 0043] loss=13.3566 cls=1.4433 smmd=0.2551 ct=9.2592 rec=1.1995 | train/val/test=0.983/0.688/0.672 | c=0.998896
[Epoch 0044] loss=13.1081 cls=1.2118 smmd=0.2381 ct=9.2552 rec=1.2015 | train/val/test=1.000/0.694/0.673 | c=0.998896
[Epoch 0045] loss=13.2776 cls=1.3953 smmd=0.2229 ct=9.2532 rec=1.2031 | train/val/test=1.000/0.696/0.672 | c=0.998896
[Epoch 0046] loss=12.8250 cls=0.9529 smmd=0.2172 ct=9.2560 rec=1.1995 | train/val/test=1.000/0.704/0.680 | c=0.998896
[Epoch 0047] loss=13.1176 cls=1.2649 smmd=0.2076 ct=9.2488 rec=1.1981 | train/val/test=1.000/0.700/0.688 | c=0.998896
[Epoch 0048] loss=13.1029 cls=1.2609 smmd=0.2019 ct=9.2470 rec=1.1965 | train/val/test=1.000/0.702/0.687 | c=0.998896
[Epoch 0049] loss=12.9566 cls=1.0989 smmd=0.1976 ct=9.2551 rec=1.2025 | train/val/test=1.000/0.692/0.673 | c=0.998896
[Epoch 0050] loss=12.9342 cls=1.1050 smmd=0.1825 ct=9.2379 rec=1.2044 | train/val/test=1.000/0.698/0.671 | c=0.998896
[Epoch 0051] loss=13.1615 cls=1.3390 smmd=0.1775 ct=9.2398 rec=1.2026 | train/val/test=1.000/0.708/0.672 | c=0.998896
[Epoch 0052] loss=13.2533 cls=1.4069 smmd=0.1763 ct=9.2460 rec=1.2121 | train/val/test=1.000/0.706/0.682 | c=0.998896
[Epoch 0053] loss=13.1011 cls=1.2943 smmd=0.1642 ct=9.2321 rec=1.2052 | train/val/test=1.000/0.700/0.693 | c=0.998896
[Epoch 0054] loss=13.0031 cls=1.1988 smmd=0.1582 ct=9.2373 rec=1.2044 | train/val/test=1.000/0.694/0.684 | c=0.998896
[Epoch 0055] loss=13.2032 cls=1.3864 smmd=0.1596 ct=9.2399 rec=1.2087 | train/val/test=1.000/0.694/0.669 | c=0.998896
[Epoch 0056] loss=13.2088 cls=1.3892 smmd=0.1538 ct=9.2400 rec=1.2129 | train/val/test=1.000/0.684/0.672 | c=0.998896
[Epoch 0057] loss=12.8669 cls=1.0524 smmd=0.1526 ct=9.2314 rec=1.2153 | train/val/test=1.000/0.706/0.682 | c=0.998896
[Epoch 0058] loss=12.9103 cls=1.0983 smmd=0.1576 ct=9.2308 rec=1.2118 | train/val/test=0.983/0.702/0.678 | c=0.998896
[Epoch 0059] loss=13.3183 cls=1.4852 smmd=0.1573 ct=9.2400 rec=1.2179 | train/val/test=1.000/0.698/0.677 | c=0.998896
[Epoch 0060] loss=13.0848 cls=1.2690 smmd=0.1462 ct=9.2366 rec=1.2165 | train/val/test=1.000/0.696/0.673 | c=0.998896
[Epoch 0061] loss=12.8671 cls=1.0618 smmd=0.1415 ct=9.2336 rec=1.2151 | train/val/test=1.000/0.702/0.678 | c=0.998896
[Epoch 0062] loss=13.2267 cls=1.4087 smmd=0.1438 ct=9.2404 rec=1.2169 | train/val/test=1.000/0.708/0.678 | c=0.998896
[Epoch 0063] loss=13.2106 cls=1.3920 smmd=0.1411 ct=9.2348 rec=1.2213 | train/val/test=1.000/0.708/0.685 | c=0.998896
[Epoch 0064] loss=12.9928 cls=1.1820 smmd=0.1374 ct=9.2377 rec=1.2178 | train/val/test=0.983/0.702/0.688 | c=0.998896
[Epoch 0065] loss=12.8837 cls=1.0903 smmd=0.1390 ct=9.2333 rec=1.2105 | train/val/test=0.983/0.708/0.675 | c=0.998896
[Epoch 0066] loss=13.2940 cls=1.5024 smmd=0.1410 ct=9.2246 rec=1.2130 | train/val/test=0.983/0.702/0.669 | c=0.998896
[Epoch 0067] loss=12.8181 cls=1.0321 smmd=0.1428 ct=9.2275 rec=1.2079 | train/val/test=0.983/0.702/0.671 | c=0.998896
[Epoch 0068] loss=13.1465 cls=1.3615 smmd=0.1424 ct=9.2181 rec=1.2123 | train/val/test=0.983/0.702/0.675 | c=0.998896
[Epoch 0069] loss=12.8187 cls=1.0361 smmd=0.1380 ct=9.2253 rec=1.2097 | train/val/test=0.983/0.706/0.682 | c=0.998896
[Epoch 0070] loss=12.8536 cls=1.0812 smmd=0.1345 ct=9.2287 rec=1.2046 | train/val/test=0.983/0.710/0.685 | c=0.998896
[Epoch 0071] loss=12.7983 cls=1.0252 smmd=0.1330 ct=9.2267 rec=1.2067 | train/val/test=0.983/0.714/0.684 | c=0.998896
[Epoch 0072] loss=12.9311 cls=1.1529 smmd=0.1337 ct=9.2292 rec=1.2077 | train/val/test=1.000/0.710/0.683 | c=0.998896
[Epoch 0073] loss=12.9884 cls=1.2146 smmd=0.1338 ct=9.2198 rec=1.2101 | train/val/test=0.983/0.710/0.684 | c=0.998896
[Epoch 0074] loss=12.8133 cls=1.0499 smmd=0.1335 ct=9.2226 rec=1.2036 | train/val/test=0.983/0.702/0.682 | c=0.998896
[Epoch 0075] loss=12.7567 cls=1.0004 smmd=0.1339 ct=9.2172 rec=1.2026 | train/val/test=0.983/0.704/0.681 | c=0.998896
[Epoch 0076] loss=12.9468 cls=1.1872 smmd=0.1357 ct=9.2107 rec=1.2066 | train/val/test=0.983/0.702/0.680 | c=0.998896
[Epoch 0077] loss=12.9025 cls=1.1633 smmd=0.1357 ct=9.2130 rec=1.1953 | train/val/test=0.983/0.700/0.681 | c=0.998896
[Epoch 0078] loss=12.7102 cls=0.9637 smmd=0.1346 ct=9.2134 rec=1.1993 | train/val/test=0.983/0.702/0.679 | c=0.998896
[Epoch 0079] loss=12.8620 cls=1.1131 smmd=0.1331 ct=9.2157 rec=1.2000 | train/val/test=0.983/0.702/0.677 | c=0.998896
[Epoch 0080] loss=12.6810 cls=0.9421 smmd=0.1296 ct=9.2158 rec=1.1968 | train/val/test=0.983/0.704/0.678 | c=0.998896
[Epoch 0081] loss=12.9296 cls=1.1864 smmd=0.1268 ct=9.2263 rec=1.1950 | train/val/test=0.983/0.712/0.683 | c=0.998896
[Epoch 0082] loss=12.7950 cls=1.0432 smmd=0.1276 ct=9.2201 rec=1.2021 | train/val/test=0.983/0.708/0.687 | c=0.998896
[Epoch 0083] loss=12.7466 cls=1.0042 smmd=0.1297 ct=9.2212 rec=1.1957 | train/val/test=1.000/0.710/0.689 | c=0.998896
[Epoch 0084] loss=12.8955 cls=1.1340 smmd=0.1295 ct=9.2285 rec=1.2017 | train/val/test=1.000/0.710/0.689 | c=0.998896
[Epoch 0085] loss=12.9540 cls=1.2086 smmd=0.1305 ct=9.2214 rec=1.1968 | train/val/test=0.983/0.712/0.690 | c=0.998896
[Epoch 0086] loss=12.4669 cls=0.7339 smmd=0.1328 ct=9.2167 rec=1.1918 | train/val/test=0.983/0.714/0.687 | c=0.998896
[Epoch 0087] loss=12.5591 cls=0.8258 smmd=0.1343 ct=9.2184 rec=1.1903 | train/val/test=0.983/0.710/0.687 | c=0.998896
[Epoch 0088] loss=12.7171 cls=0.9800 smmd=0.1324 ct=9.2108 rec=1.1970 | train/val/test=0.983/0.710/0.689 | c=0.998896
[Epoch 0089] loss=12.6115 cls=0.8591 smmd=0.1336 ct=9.2165 rec=1.2011 | train/val/test=0.983/0.708/0.688 | c=0.998896
[Epoch 0090] loss=13.0362 cls=1.2951 smmd=0.1338 ct=9.2108 rec=1.1982 | train/val/test=0.983/0.708/0.688 | c=0.998896
[Epoch 0091] loss=12.9447 cls=1.2139 smmd=0.1331 ct=9.2118 rec=1.1930 | train/val/test=0.983/0.708/0.688 | c=0.998896
[Epoch 0092] loss=12.6618 cls=0.9336 smmd=0.1322 ct=9.2098 rec=1.1930 | train/val/test=0.983/0.708/0.688 | c=0.998896
[Epoch 0093] loss=13.0350 cls=1.2962 smmd=0.1329 ct=9.2112 rec=1.1974 | train/val/test=0.983/0.706/0.688 | c=0.998896
[Epoch 0094] loss=12.6112 cls=0.8883 smmd=0.1330 ct=9.2085 rec=1.1907 | train/val/test=1.000/0.706/0.688 | c=0.998896
[Epoch 0095] loss=12.8493 cls=1.1230 smmd=0.1335 ct=9.2087 rec=1.1921 | train/val/test=1.000/0.706/0.688 | c=0.998896
[Epoch 0096] loss=12.8400 cls=1.1063 smmd=0.1334 ct=9.2108 rec=1.1947 | train/val/test=1.000/0.706/0.688 | c=0.998896
[Epoch 0097] loss=12.9155 cls=1.1775 smmd=0.1331 ct=9.2138 rec=1.1956 | train/val/test=1.000/0.706/0.688 | c=0.998896
[Epoch 0098] loss=12.6309 cls=0.9115 smmd=0.1324 ct=9.2062 rec=1.1904 | train/val/test=1.000/0.708/0.688 | c=0.998896
[Epoch 0099] loss=12.8911 cls=1.1521 smmd=0.1323 ct=9.2102 rec=1.1982 | train/val/test=1.000/0.708/0.688 | c=0.998896
=== Best @ epoch 15: val=0.7580, test=0.7450 ===
