Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> CiteSeer
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.9105 cls=2.1396 smmd=4.7326 ct=9.2513 rec=1.3935 | train/val/test=0.276/0.138/0.152 | c=0.998896
[Epoch 0001] loss=18.5818 cls=1.8802 smmd=4.6806 ct=9.2434 rec=1.3888 | train/val/test=0.241/0.162/0.157 | c=0.998896
[Epoch 0002] loss=18.5526 cls=1.9572 smmd=4.5864 ct=9.2303 rec=1.3894 | train/val/test=0.586/0.372/0.351 | c=0.998896
[Epoch 0003] loss=18.0958 cls=1.6771 smmd=4.4524 ct=9.2019 rec=1.3822 | train/val/test=0.707/0.308/0.310 | c=0.998896
[Epoch 0004] loss=17.6509 cls=1.4978 smmd=4.2725 ct=9.1377 rec=1.3714 | train/val/test=0.845/0.432/0.440 | c=0.998896
[Epoch 0005] loss=17.2172 cls=1.4126 smmd=4.0444 ct=9.0499 rec=1.3551 | train/val/test=0.897/0.602/0.583 | c=0.998896
[Epoch 0006] loss=16.5692 cls=1.2660 smmd=3.7682 ct=8.9348 rec=1.3000 | train/val/test=0.845/0.556/0.531 | c=0.998896
[Epoch 0007] loss=16.0257 cls=1.1342 smmd=3.4219 ct=8.8935 rec=1.2880 | train/val/test=0.966/0.688/0.670 | c=0.998896
[Epoch 0008] loss=15.4534 cls=1.1819 smmd=3.0286 ct=8.7834 rec=1.2297 | train/val/test=0.931/0.694/0.686 | c=0.998896
[Epoch 0009] loss=14.6993 cls=0.9260 smmd=2.5697 ct=8.7693 rec=1.2172 | train/val/test=0.914/0.696/0.695 | c=0.998896
[Epoch 0010] loss=14.2977 cls=1.0535 smmd=2.0405 ct=8.7878 rec=1.2079 | train/val/test=1.000/0.736/0.748 | c=0.998896
[Epoch 0011] loss=13.5611 cls=0.9383 smmd=1.4871 ct=8.7527 rec=1.1915 | train/val/test=1.000/0.762/0.771 | c=0.998896
[Epoch 0012] loss=13.1727 cls=1.0451 smmd=1.0254 ct=8.7323 rec=1.1850 | train/val/test=0.966/0.750/0.768 | c=0.998896
[Epoch 0013] loss=13.5864 cls=1.6773 smmd=0.8155 ct=8.7173 rec=1.1882 | train/val/test=1.000/0.710/0.730 | c=0.998896
[Epoch 0014] loss=13.1301 cls=1.1363 smmd=0.9063 ct=8.7195 rec=1.1840 | train/val/test=0.983/0.692/0.699 | c=0.998896
[Epoch 0015] loss=14.2861 cls=1.3633 smmd=1.1358 ct=9.4126 rec=1.1872 | train/val/test=0.966/0.696/0.690 | c=0.998896
[Epoch 0016] loss=14.1859 cls=1.0969 smmd=1.3566 ct=9.3475 rec=1.1925 | train/val/test=0.966/0.692/0.687 | c=0.998896
[Epoch 0017] loss=14.4736 cls=1.2873 smmd=1.5154 ct=9.2830 rec=1.1940 | train/val/test=0.966/0.716/0.705 | c=0.998896
[Epoch 0018] loss=14.2175 cls=0.9950 smmd=1.5922 ct=9.2371 rec=1.1966 | train/val/test=0.966/0.718/0.712 | c=0.998896
[Epoch 0019] loss=14.5447 cls=1.3493 smmd=1.5869 ct=9.1976 rec=1.2055 | train/val/test=0.966/0.728/0.724 | c=0.998896
[Epoch 0020] loss=14.1368 cls=1.0234 smmd=1.5154 ct=9.1789 rec=1.2095 | train/val/test=0.983/0.746/0.735 | c=0.998896
[Epoch 0021] loss=14.2294 cls=1.2370 smmd=1.3816 ct=9.1737 rec=1.2186 | train/val/test=0.983/0.750/0.725 | c=0.998896
[Epoch 0022] loss=13.7553 cls=0.9336 smmd=1.2229 ct=9.1713 rec=1.2137 | train/val/test=0.983/0.738/0.712 | c=0.998896
[Epoch 0023] loss=13.7697 cls=1.0811 smmd=1.0697 ct=9.1875 rec=1.2157 | train/val/test=0.983/0.728/0.708 | c=0.998896
[Epoch 0024] loss=13.6159 cls=1.0211 smmd=0.9450 ct=9.2165 rec=1.2167 | train/val/test=0.983/0.720/0.705 | c=0.998896
[Epoch 0025] loss=13.6609 cls=1.1072 smmd=0.8554 ct=9.2598 rec=1.2193 | train/val/test=0.983/0.716/0.697 | c=0.998896
[Epoch 0026] loss=13.9557 cls=1.4363 smmd=0.8025 ct=9.2847 rec=1.2161 | train/val/test=0.983/0.720/0.700 | c=0.998896
[Epoch 0027] loss=13.4956 cls=0.9931 smmd=0.7750 ct=9.3047 rec=1.2114 | train/val/test=0.983/0.734/0.707 | c=0.998896
[Epoch 0028] loss=13.6162 cls=1.1241 smmd=0.7556 ct=9.3106 rec=1.2130 | train/val/test=0.983/0.742/0.714 | c=0.998896
[Epoch 0029] loss=13.6171 cls=1.1727 smmd=0.7191 ct=9.3104 rec=1.2075 | train/val/test=0.966/0.756/0.718 | c=0.998896
[Epoch 0030] loss=13.9105 cls=1.4894 smmd=0.6750 ct=9.3177 rec=1.2142 | train/val/test=0.966/0.756/0.715 | c=0.998896
[Epoch 0031] loss=13.4344 cls=1.1130 smmd=0.6256 ct=9.2913 rec=1.2022 | train/val/test=0.966/0.744/0.712 | c=0.998896
[Epoch 0032] loss=13.3115 cls=1.0401 smmd=0.5764 ct=9.2830 rec=1.2060 | train/val/test=0.966/0.730/0.708 | c=0.998896
[Epoch 0033] loss=13.3187 cls=1.1118 smmd=0.5354 ct=9.2627 rec=1.2043 | train/val/test=0.966/0.724/0.701 | c=0.998896
[Epoch 0034] loss=13.5265 cls=1.3545 smmd=0.5036 ct=9.2569 rec=1.2057 | train/val/test=0.966/0.728/0.707 | c=0.998896
[Epoch 0035] loss=13.2268 cls=1.0821 smmd=0.4744 ct=9.2574 rec=1.2064 | train/val/test=0.966/0.730/0.705 | c=0.998896
[Epoch 0036] loss=13.5104 cls=1.3974 smmd=0.4483 ct=9.2547 rec=1.2050 | train/val/test=0.966/0.734/0.709 | c=0.998896
[Epoch 0037] loss=13.0789 cls=1.0107 smmd=0.4143 ct=9.2588 rec=1.1976 | train/val/test=0.966/0.738/0.709 | c=0.998896
[Epoch 0038] loss=13.3186 cls=1.2723 smmd=0.3805 ct=9.2613 rec=1.2023 | train/val/test=0.966/0.734/0.702 | c=0.998896
[Epoch 0039] loss=13.1629 cls=1.1457 smmd=0.3490 ct=9.2584 rec=1.2049 | train/val/test=0.966/0.730/0.698 | c=0.998896
[Epoch 0040] loss=13.2858 cls=1.2927 smmd=0.3223 ct=9.2594 rec=1.2057 | train/val/test=0.966/0.720/0.690 | c=0.998896
[Epoch 0041] loss=13.1012 cls=1.1348 smmd=0.2991 ct=9.2576 rec=1.2049 | train/val/test=0.966/0.720/0.697 | c=0.998896
[Epoch 0042] loss=13.1960 cls=1.2441 smmd=0.2845 ct=9.2557 rec=1.2058 | train/val/test=0.966/0.722/0.695 | c=0.998896
[Epoch 0043] loss=13.1761 cls=1.2593 smmd=0.2675 ct=9.2423 rec=1.2035 | train/val/test=0.966/0.720/0.698 | c=0.998896
[Epoch 0044] loss=13.0266 cls=1.1253 smmd=0.2507 ct=9.2512 rec=1.1997 | train/val/test=0.966/0.728/0.696 | c=0.998896
[Epoch 0045] loss=12.7408 cls=0.8507 smmd=0.2400 ct=9.2481 rec=1.2009 | train/val/test=0.966/0.720/0.696 | c=0.998896
[Epoch 0046] loss=12.8885 cls=1.0082 smmd=0.2237 ct=9.2512 rec=1.2027 | train/val/test=0.983/0.722/0.703 | c=0.998896
[Epoch 0047] loss=12.8338 cls=0.9696 smmd=0.2144 ct=9.2450 rec=1.2024 | train/val/test=0.983/0.708/0.691 | c=0.998896
[Epoch 0048] loss=13.1262 cls=1.2560 smmd=0.2188 ct=9.2371 rec=1.2072 | train/val/test=0.983/0.706/0.688 | c=0.998896
[Epoch 0049] loss=12.9076 cls=1.0535 smmd=0.2125 ct=9.2277 rec=1.2069 | train/val/test=0.983/0.710/0.694 | c=0.998896
[Epoch 0050] loss=12.8609 cls=1.0288 smmd=0.2017 ct=9.2294 rec=1.2005 | train/val/test=0.983/0.724/0.707 | c=0.998896
[Epoch 0051] loss=12.9582 cls=1.1193 smmd=0.1918 ct=9.2444 rec=1.2013 | train/val/test=0.983/0.716/0.703 | c=0.998896
[Epoch 0052] loss=13.0994 cls=1.2808 smmd=0.1820 ct=9.2424 rec=1.1971 | train/val/test=0.983/0.714/0.687 | c=0.998896
[Epoch 0053] loss=13.0654 cls=1.2359 smmd=0.1847 ct=9.2503 rec=1.1972 | train/val/test=0.983/0.706/0.677 | c=0.998896
[Epoch 0054] loss=13.0643 cls=1.2205 smmd=0.1910 ct=9.2408 rec=1.2060 | train/val/test=0.983/0.696/0.680 | c=0.998896
[Epoch 0055] loss=13.1054 cls=1.2676 smmd=0.1885 ct=9.2283 rec=1.2105 | train/val/test=0.983/0.710/0.690 | c=0.998896
[Epoch 0056] loss=13.2238 cls=1.3762 smmd=0.1794 ct=9.2292 rec=1.2195 | train/val/test=0.983/0.720/0.708 | c=0.998896
[Epoch 0057] loss=13.1909 cls=1.3688 smmd=0.1653 ct=9.2379 rec=1.2095 | train/val/test=0.983/0.712/0.708 | c=0.998896
[Epoch 0058] loss=12.9687 cls=1.1505 smmd=0.1621 ct=9.2416 rec=1.2073 | train/val/test=0.983/0.708/0.710 | c=0.998896
[Epoch 0059] loss=12.9985 cls=1.1689 smmd=0.1665 ct=9.2336 rec=1.2148 | train/val/test=0.983/0.702/0.697 | c=0.998896
[Epoch 0060] loss=13.1860 cls=1.3589 smmd=0.1665 ct=9.2265 rec=1.2170 | train/val/test=0.983/0.712/0.689 | c=0.998896
[Epoch 0061] loss=12.9406 cls=1.1356 smmd=0.1705 ct=9.2178 rec=1.2084 | train/val/test=0.983/0.716/0.694 | c=0.998896
[Epoch 0062] loss=13.2789 cls=1.4686 smmd=0.1685 ct=9.2156 rec=1.2131 | train/val/test=0.983/0.716/0.701 | c=0.998896
[Epoch 0063] loss=12.8702 cls=1.0736 smmd=0.1587 ct=9.2221 rec=1.2079 | train/val/test=0.983/0.730/0.705 | c=0.998896
[Epoch 0064] loss=13.1408 cls=1.3297 smmd=0.1547 ct=9.2319 rec=1.2122 | train/val/test=0.983/0.706/0.699 | c=0.998896
[Epoch 0065] loss=13.1560 cls=1.3275 smmd=0.1550 ct=9.2390 rec=1.2173 | train/val/test=0.983/0.710/0.689 | c=0.998896
[Epoch 0066] loss=13.3281 cls=1.5068 smmd=0.1537 ct=9.2276 rec=1.2200 | train/val/test=0.983/0.704/0.679 | c=0.998896
[Epoch 0067] loss=13.1960 cls=1.3811 smmd=0.1566 ct=9.2269 rec=1.2157 | train/val/test=0.983/0.704/0.679 | c=0.998896
[Epoch 0068] loss=12.9520 cls=1.1389 smmd=0.1602 ct=9.2209 rec=1.2160 | train/val/test=0.983/0.726/0.690 | c=0.998896
[Epoch 0069] loss=12.9272 cls=1.1348 smmd=0.1548 ct=9.2153 rec=1.2112 | train/val/test=0.983/0.728/0.693 | c=0.998896
[Epoch 0070] loss=12.9206 cls=1.1181 smmd=0.1503 ct=9.2234 rec=1.2144 | train/val/test=0.983/0.728/0.699 | c=0.998896
[Epoch 0071] loss=13.0488 cls=1.2443 smmd=0.1494 ct=9.2278 rec=1.2137 | train/val/test=0.983/0.716/0.696 | c=0.998896
[Epoch 0072] loss=13.1696 cls=1.3536 smmd=0.1496 ct=9.2279 rec=1.2193 | train/val/test=0.983/0.718/0.695 | c=0.998896
[Epoch 0073] loss=12.8123 cls=1.0064 smmd=0.1501 ct=9.2197 rec=1.2181 | train/val/test=0.983/0.710/0.688 | c=0.998896
[Epoch 0074] loss=12.9666 cls=1.1758 smmd=0.1468 ct=9.2160 rec=1.2141 | train/val/test=1.000/0.712/0.689 | c=0.998896
[Epoch 0075] loss=12.9343 cls=1.1499 smmd=0.1481 ct=9.2155 rec=1.2104 | train/val/test=0.983/0.714/0.683 | c=0.998896
[Epoch 0076] loss=12.8548 cls=1.0645 smmd=0.1482 ct=9.2168 rec=1.2126 | train/val/test=0.983/0.718/0.685 | c=0.998896
[Epoch 0077] loss=12.8374 cls=1.0561 smmd=0.1476 ct=9.2168 rec=1.2084 | train/val/test=0.983/0.714/0.685 | c=0.998896
[Epoch 0078] loss=12.9135 cls=1.1199 smmd=0.1452 ct=9.2179 rec=1.2152 | train/val/test=0.983/0.716/0.684 | c=0.998896
[Epoch 0079] loss=12.9619 cls=1.1680 smmd=0.1448 ct=9.2244 rec=1.2124 | train/val/test=0.983/0.718/0.686 | c=0.998896
[Epoch 0080] loss=12.8830 cls=1.1073 smmd=0.1444 ct=9.2176 rec=1.2068 | train/val/test=0.983/0.716/0.687 | c=0.998896
[Epoch 0081] loss=12.8948 cls=1.1127 smmd=0.1432 ct=9.2243 rec=1.2073 | train/val/test=0.983/0.720/0.690 | c=0.998896
[Epoch 0082] loss=12.6306 cls=0.8446 smmd=0.1440 ct=9.2275 rec=1.2073 | train/val/test=0.983/0.722/0.694 | c=0.998896
[Epoch 0083] loss=12.7525 cls=0.9749 smmd=0.1435 ct=9.2167 rec=1.2087 | train/val/test=0.983/0.722/0.692 | c=0.998896
[Epoch 0084] loss=12.9887 cls=1.2075 smmd=0.1453 ct=9.2161 rec=1.2099 | train/val/test=0.983/0.726/0.692 | c=0.998896
[Epoch 0085] loss=12.9287 cls=1.1502 smmd=0.1461 ct=9.2183 rec=1.2071 | train/val/test=0.983/0.722/0.691 | c=0.998896
[Epoch 0086] loss=12.7319 cls=0.9649 smmd=0.1446 ct=9.2189 rec=1.2018 | train/val/test=0.983/0.722/0.694 | c=0.998896
[Epoch 0087] loss=12.7667 cls=0.9911 smmd=0.1444 ct=9.2186 rec=1.2063 | train/val/test=0.983/0.720/0.692 | c=0.998896
[Epoch 0088] loss=12.7843 cls=1.0171 smmd=0.1433 ct=9.2229 rec=1.2005 | train/val/test=0.983/0.724/0.691 | c=0.998896
[Epoch 0089] loss=13.0532 cls=1.2886 smmd=0.1419 ct=9.2193 rec=1.2017 | train/val/test=0.983/0.724/0.690 | c=0.998896
[Epoch 0090] loss=12.8879 cls=1.1262 smmd=0.1406 ct=9.2122 rec=1.2045 | train/val/test=0.983/0.724/0.686 | c=0.998896
[Epoch 0091] loss=12.9083 cls=1.1415 smmd=0.1418 ct=9.2163 rec=1.2043 | train/val/test=0.983/0.722/0.688 | c=0.998896
[Epoch 0092] loss=13.0291 cls=1.2563 smmd=0.1417 ct=9.2138 rec=1.2086 | train/val/test=0.983/0.722/0.691 | c=0.998896
[Epoch 0093] loss=12.8034 cls=1.0405 smmd=0.1424 ct=9.2153 rec=1.2026 | train/val/test=0.983/0.724/0.691 | c=0.998896
[Epoch 0094] loss=12.7623 cls=1.0140 smmd=0.1425 ct=9.2124 rec=1.1967 | train/val/test=0.983/0.724/0.692 | c=0.998896
[Epoch 0095] loss=12.9151 cls=1.1571 smmd=0.1419 ct=9.2147 rec=1.2007 | train/val/test=0.983/0.724/0.692 | c=0.998896
[Epoch 0096] loss=12.8378 cls=1.0709 smmd=0.1406 ct=9.2207 rec=1.2028 | train/val/test=0.983/0.724/0.692 | c=0.998896
[Epoch 0097] loss=12.8962 cls=1.1338 smmd=0.1416 ct=9.2191 rec=1.2008 | train/val/test=0.983/0.722/0.692 | c=0.998896
[Epoch 0098] loss=13.0831 cls=1.2876 smmd=0.1416 ct=9.2266 rec=1.2136 | train/val/test=0.983/0.722/0.692 | c=0.998896
[Epoch 0099] loss=12.7224 cls=0.9489 smmd=0.1422 ct=9.2170 rec=1.2072 | train/val/test=0.983/0.722/0.692 | c=0.998896
=== Best @ epoch 11: val=0.7620, test=0.7710 ===
