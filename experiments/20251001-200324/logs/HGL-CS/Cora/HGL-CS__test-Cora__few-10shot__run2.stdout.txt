Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> CiteSeer
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.8078 cls=2.0361 smmd=4.7271 ct=9.2629 rec=1.3908 | train/val/test=0.155/0.074/0.094 | c=0.998896
[Epoch 0001] loss=18.6610 cls=1.9354 smmd=4.6825 ct=9.2602 rec=1.3915 | train/val/test=0.241/0.162/0.154 | c=0.998896
[Epoch 0002] loss=18.4319 cls=1.8171 smmd=4.5889 ct=9.2480 rec=1.3889 | train/val/test=0.672/0.402/0.413 | c=0.998896
[Epoch 0003] loss=18.2817 cls=1.8218 smmd=4.4669 ct=9.2244 rec=1.3843 | train/val/test=0.759/0.420/0.425 | c=0.998896
[Epoch 0004] loss=17.6648 cls=1.4218 smmd=4.3010 ct=9.1838 rec=1.3791 | train/val/test=0.810/0.432/0.423 | c=0.998896
[Epoch 0005] loss=17.5159 cls=1.6074 smmd=4.0853 ct=9.0981 rec=1.3625 | train/val/test=0.948/0.548/0.545 | c=0.998896
[Epoch 0006] loss=16.7740 cls=1.3488 smmd=3.8367 ct=8.9582 rec=1.3151 | train/val/test=0.879/0.532/0.553 | c=0.998896
[Epoch 0007] loss=16.3824 cls=1.2960 smmd=3.5336 ct=8.9425 rec=1.3052 | train/val/test=0.966/0.678/0.668 | c=0.998896
[Epoch 0008] loss=15.6176 cls=1.0516 smmd=3.1647 ct=8.8582 rec=1.2715 | train/val/test=0.931/0.678/0.662 | c=0.998896
[Epoch 0009] loss=14.9157 cls=0.9818 smmd=2.7345 ct=8.7551 rec=1.2221 | train/val/test=0.931/0.620/0.601 | c=0.998896
[Epoch 0010] loss=14.6904 cls=1.2213 smmd=2.2225 ct=8.7859 rec=1.2304 | train/val/test=0.931/0.662/0.635 | c=0.998896
[Epoch 0011] loss=14.0843 cls=1.1944 smmd=1.6732 ct=8.7849 rec=1.2159 | train/val/test=0.948/0.680/0.654 | c=0.998896
[Epoch 0012] loss=13.6969 cls=1.3319 smmd=1.1776 ct=8.7701 rec=1.2086 | train/val/test=0.983/0.698/0.708 | c=0.998896
[Epoch 0013] loss=12.9853 cls=0.9385 smmd=0.8689 ct=8.7588 rec=1.2096 | train/val/test=0.983/0.712/0.721 | c=0.998896
[Epoch 0014] loss=13.0642 cls=1.0721 smmd=0.8386 ct=8.7447 rec=1.2044 | train/val/test=0.966/0.744/0.734 | c=0.998896
[Epoch 0015] loss=14.1510 cls=1.3008 smmd=1.0191 ct=9.4295 rec=1.2008 | train/val/test=0.966/0.724/0.728 | c=0.998896
[Epoch 0016] loss=14.4692 cls=1.4282 smmd=1.2609 ct=9.3637 rec=1.2082 | train/val/test=0.966/0.716/0.716 | c=0.998896
[Epoch 0017] loss=14.2467 cls=1.0831 smmd=1.4515 ct=9.2837 rec=1.2142 | train/val/test=0.966/0.718/0.692 | c=0.998896
[Epoch 0018] loss=14.2863 cls=1.0495 smmd=1.5782 ct=9.2309 rec=1.2138 | train/val/test=0.948/0.714/0.684 | c=0.998896
[Epoch 0019] loss=14.3330 cls=1.0864 smmd=1.6100 ct=9.2013 rec=1.2176 | train/val/test=0.948/0.724/0.699 | c=0.998896
[Epoch 0020] loss=14.3335 cls=1.1497 smmd=1.5645 ct=9.1667 rec=1.2263 | train/val/test=0.966/0.732/0.721 | c=0.998896
[Epoch 0021] loss=14.1296 cls=1.0703 smmd=1.4530 ct=9.1586 rec=1.2239 | train/val/test=0.983/0.742/0.738 | c=0.998896
[Epoch 0022] loss=13.9762 cls=1.0767 smmd=1.2911 ct=9.1713 rec=1.2186 | train/val/test=0.966/0.744/0.739 | c=0.998896
[Epoch 0023] loss=13.7672 cls=1.0294 smmd=1.1244 ct=9.1952 rec=1.2091 | train/val/test=0.983/0.748/0.740 | c=0.998896
[Epoch 0024] loss=13.6387 cls=1.0332 smmd=0.9762 ct=9.2135 rec=1.2079 | train/val/test=0.983/0.742/0.736 | c=0.998896
[Epoch 0025] loss=13.4594 cls=0.9293 smmd=0.8643 ct=9.2666 rec=1.1996 | train/val/test=0.983/0.730/0.732 | c=0.998896
[Epoch 0026] loss=13.4519 cls=0.9733 smmd=0.7929 ct=9.2755 rec=1.2051 | train/val/test=0.983/0.720/0.712 | c=0.998896
[Epoch 0027] loss=13.4415 cls=1.0032 smmd=0.7560 ct=9.2852 rec=1.1986 | train/val/test=0.983/0.718/0.708 | c=0.998896
[Epoch 0028] loss=13.3946 cls=0.9722 smmd=0.7447 ct=9.2866 rec=1.1956 | train/val/test=0.983/0.720/0.711 | c=0.998896
[Epoch 0029] loss=13.7012 cls=1.2875 smmd=0.7293 ct=9.2891 rec=1.1976 | train/val/test=0.983/0.712/0.702 | c=0.998896
[Epoch 0030] loss=13.9546 cls=1.5637 smmd=0.7006 ct=9.2926 rec=1.1989 | train/val/test=0.983/0.708/0.694 | c=0.998896
[Epoch 0031] loss=13.3247 cls=1.0078 smmd=0.6517 ct=9.2802 rec=1.1925 | train/val/test=0.983/0.708/0.700 | c=0.998896
[Epoch 0032] loss=13.6827 cls=1.4215 smmd=0.5945 ct=9.2703 rec=1.1982 | train/val/test=0.983/0.708/0.702 | c=0.998896
[Epoch 0033] loss=13.3988 cls=1.2041 smmd=0.5394 ct=9.2604 rec=1.1975 | train/val/test=0.983/0.704/0.706 | c=0.998896
[Epoch 0034] loss=13.3951 cls=1.2467 smmd=0.4975 ct=9.2537 rec=1.1986 | train/val/test=0.983/0.706/0.704 | c=0.998896
[Epoch 0035] loss=13.2549 cls=1.1255 smmd=0.4649 ct=9.2536 rec=1.2055 | train/val/test=0.983/0.706/0.698 | c=0.998896
[Epoch 0036] loss=13.1262 cls=1.0409 smmd=0.4389 ct=9.2437 rec=1.2013 | train/val/test=0.983/0.698/0.693 | c=0.998896
[Epoch 0037] loss=13.0852 cls=1.0348 smmd=0.4099 ct=9.2381 rec=1.2012 | train/val/test=0.983/0.698/0.687 | c=0.998896
[Epoch 0038] loss=13.3223 cls=1.2700 smmd=0.3873 ct=9.2498 rec=1.2076 | train/val/test=0.983/0.696/0.682 | c=0.998896
[Epoch 0039] loss=13.1990 cls=1.1925 smmd=0.3579 ct=9.2449 rec=1.2019 | train/val/test=0.983/0.700/0.684 | c=0.998896
[Epoch 0040] loss=13.1556 cls=1.1722 smmd=0.3274 ct=9.2522 rec=1.2019 | train/val/test=0.983/0.696/0.695 | c=0.998896
[Epoch 0041] loss=13.2504 cls=1.3019 smmd=0.2987 ct=9.2391 rec=1.2054 | train/val/test=1.000/0.704/0.695 | c=0.998896
[Epoch 0042] loss=12.9060 cls=0.9873 smmd=0.2794 ct=9.2357 rec=1.2018 | train/val/test=1.000/0.710/0.704 | c=0.998896
[Epoch 0043] loss=13.2823 cls=1.3654 smmd=0.2641 ct=9.2335 rec=1.2096 | train/val/test=1.000/0.710/0.707 | c=0.998896
[Epoch 0044] loss=13.3768 cls=1.4791 smmd=0.2456 ct=9.2323 rec=1.2099 | train/val/test=1.000/0.702/0.687 | c=0.998896
[Epoch 0045] loss=13.2127 cls=1.3405 smmd=0.2345 ct=9.2241 rec=1.2069 | train/val/test=1.000/0.696/0.678 | c=0.998896
[Epoch 0046] loss=12.8939 cls=1.0195 smmd=0.2236 ct=9.2270 rec=1.2119 | train/val/test=1.000/0.702/0.680 | c=0.998896
[Epoch 0047] loss=13.1158 cls=1.2549 smmd=0.2102 ct=9.2270 rec=1.2118 | train/val/test=1.000/0.706/0.692 | c=0.998896
[Epoch 0048] loss=13.0504 cls=1.2006 smmd=0.2024 ct=9.2233 rec=1.2120 | train/val/test=1.000/0.700/0.692 | c=0.998896
[Epoch 0049] loss=13.0798 cls=1.2404 smmd=0.1956 ct=9.2169 rec=1.2134 | train/val/test=1.000/0.704/0.692 | c=0.998896
[Epoch 0050] loss=12.9632 cls=1.1239 smmd=0.1905 ct=9.2196 rec=1.2146 | train/val/test=0.983/0.696/0.681 | c=0.998896
[Epoch 0051] loss=13.3909 cls=1.5437 smmd=0.1832 ct=9.2042 rec=1.2299 | train/val/test=1.000/0.704/0.680 | c=0.998896
[Epoch 0052] loss=13.1281 cls=1.3010 smmd=0.1762 ct=9.2149 rec=1.2179 | train/val/test=1.000/0.704/0.684 | c=0.998896
[Epoch 0053] loss=13.1700 cls=1.3578 smmd=0.1717 ct=9.2060 rec=1.2173 | train/val/test=0.983/0.700/0.686 | c=0.998896
[Epoch 0054] loss=13.1058 cls=1.2967 smmd=0.1733 ct=9.2027 rec=1.2166 | train/val/test=0.983/0.700/0.689 | c=0.998896
[Epoch 0055] loss=13.0604 cls=1.2415 smmd=0.1769 ct=9.2039 rec=1.2190 | train/val/test=0.983/0.704/0.694 | c=0.998896
[Epoch 0056] loss=12.9791 cls=1.1671 smmd=0.1694 ct=9.2048 rec=1.2189 | train/val/test=1.000/0.708/0.689 | c=0.998896
[Epoch 0057] loss=13.1842 cls=1.3906 smmd=0.1594 ct=9.2066 rec=1.2138 | train/val/test=1.000/0.706/0.680 | c=0.998896
[Epoch 0058] loss=13.3308 cls=1.5089 smmd=0.1618 ct=9.2172 rec=1.2215 | train/val/test=0.983/0.708/0.683 | c=0.998896
[Epoch 0059] loss=13.3641 cls=1.5478 smmd=0.1563 ct=9.2229 rec=1.2186 | train/val/test=0.983/0.712/0.685 | c=0.998896
[Epoch 0060] loss=13.0541 cls=1.2637 smmd=0.1543 ct=9.2008 rec=1.2176 | train/val/test=0.983/0.710/0.697 | c=0.998896
[Epoch 0061] loss=12.7940 cls=0.9965 smmd=0.1624 ct=9.1969 rec=1.2191 | train/val/test=1.000/0.716/0.707 | c=0.998896
[Epoch 0062] loss=13.1270 cls=1.3192 smmd=0.1609 ct=9.2018 rec=1.2226 | train/val/test=1.000/0.710/0.708 | c=0.998896
[Epoch 0063] loss=13.3502 cls=1.5497 smmd=0.1538 ct=9.2082 rec=1.2193 | train/val/test=1.000/0.704/0.702 | c=0.998896
[Epoch 0064] loss=12.9650 cls=1.1851 smmd=0.1490 ct=9.1991 rec=1.2159 | train/val/test=1.000/0.702/0.698 | c=0.998896
[Epoch 0065] loss=13.2482 cls=1.4564 smmd=0.1487 ct=9.2081 rec=1.2175 | train/val/test=1.000/0.708/0.697 | c=0.998896
[Epoch 0066] loss=13.0879 cls=1.2841 smmd=0.1473 ct=9.2181 rec=1.2192 | train/val/test=1.000/0.718/0.696 | c=0.998896
[Epoch 0067] loss=13.2089 cls=1.4199 smmd=0.1476 ct=9.1962 rec=1.2227 | train/val/test=1.000/0.720/0.698 | c=0.998896
[Epoch 0068] loss=13.0173 cls=1.2284 smmd=0.1436 ct=9.2032 rec=1.2210 | train/val/test=1.000/0.718/0.706 | c=0.998896
[Epoch 0069] loss=12.9632 cls=1.1878 smmd=0.1437 ct=9.1971 rec=1.2173 | train/val/test=1.000/0.716/0.702 | c=0.998896
[Epoch 0070] loss=12.6679 cls=0.8868 smmd=0.1421 ct=9.2029 rec=1.2181 | train/val/test=1.000/0.720/0.703 | c=0.998896
[Epoch 0071] loss=12.6744 cls=0.9003 smmd=0.1398 ct=9.2110 rec=1.2116 | train/val/test=1.000/0.714/0.702 | c=0.998896
[Epoch 0072] loss=12.8661 cls=1.1126 smmd=0.1356 ct=9.2001 rec=1.2089 | train/val/test=1.000/0.716/0.706 | c=0.998896
[Epoch 0073] loss=12.9587 cls=1.2020 smmd=0.1337 ct=9.2022 rec=1.2104 | train/val/test=0.983/0.718/0.702 | c=0.998896
[Epoch 0074] loss=12.7302 cls=0.9787 smmd=0.1339 ct=9.2031 rec=1.2073 | train/val/test=0.983/0.716/0.701 | c=0.998896
[Epoch 0075] loss=12.6220 cls=0.8611 smmd=0.1351 ct=9.2060 rec=1.2099 | train/val/test=0.983/0.716/0.701 | c=0.998896
[Epoch 0076] loss=12.8195 cls=1.0734 smmd=0.1348 ct=9.1978 rec=1.2068 | train/val/test=0.983/0.722/0.700 | c=0.998896
[Epoch 0077] loss=12.6418 cls=0.9016 smmd=0.1361 ct=9.1937 rec=1.2052 | train/val/test=1.000/0.720/0.700 | c=0.998896
[Epoch 0078] loss=13.4814 cls=1.7180 smmd=0.1375 ct=9.1999 rec=1.2130 | train/val/test=1.000/0.720/0.700 | c=0.998896
[Epoch 0079] loss=12.9938 cls=1.2533 smmd=0.1380 ct=9.1963 rec=1.2031 | train/val/test=1.000/0.720/0.702 | c=0.998896
[Epoch 0080] loss=13.1126 cls=1.3712 smmd=0.1386 ct=9.1967 rec=1.2030 | train/val/test=1.000/0.720/0.704 | c=0.998896
[Epoch 0081] loss=12.8410 cls=1.1074 smmd=0.1387 ct=9.1877 rec=1.2036 | train/val/test=1.000/0.720/0.704 | c=0.998896
[Epoch 0082] loss=12.6823 cls=0.9571 smmd=0.1389 ct=9.1921 rec=1.1971 | train/val/test=1.000/0.714/0.705 | c=0.998896
[Epoch 0083] loss=13.1066 cls=1.3788 smmd=0.1385 ct=9.1917 rec=1.1988 | train/val/test=1.000/0.718/0.704 | c=0.998896
[Epoch 0084] loss=12.9703 cls=1.2351 smmd=0.1386 ct=9.1906 rec=1.2030 | train/val/test=1.000/0.720/0.703 | c=0.998896
[Epoch 0085] loss=13.1849 cls=1.4522 smmd=0.1388 ct=9.1938 rec=1.2001 | train/val/test=1.000/0.722/0.707 | c=0.998896
[Epoch 0086] loss=12.8102 cls=1.0831 smmd=0.1384 ct=9.1842 rec=1.2023 | train/val/test=1.000/0.724/0.708 | c=0.998896
[Epoch 0087] loss=12.6127 cls=0.8937 smmd=0.1384 ct=9.1907 rec=1.1949 | train/val/test=1.000/0.724/0.705 | c=0.998896
[Epoch 0088] loss=12.9292 cls=1.2102 smmd=0.1367 ct=9.1890 rec=1.1967 | train/val/test=1.000/0.722/0.704 | c=0.998896
[Epoch 0089] loss=12.5391 cls=0.8122 smmd=0.1381 ct=9.1937 rec=1.1975 | train/val/test=1.000/0.722/0.703 | c=0.998896
[Epoch 0090] loss=12.5941 cls=0.8742 smmd=0.1362 ct=9.1944 rec=1.1947 | train/val/test=1.000/0.720/0.703 | c=0.998896
[Epoch 0091] loss=12.8294 cls=1.0974 smmd=0.1370 ct=9.1964 rec=1.1993 | train/val/test=1.000/0.720/0.703 | c=0.998896
[Epoch 0092] loss=12.9345 cls=1.1844 smmd=0.1367 ct=9.1976 rec=1.2079 | train/val/test=0.983/0.722/0.702 | c=0.998896
[Epoch 0093] loss=12.6410 cls=0.9155 smmd=0.1373 ct=9.1971 rec=1.1955 | train/val/test=0.983/0.722/0.702 | c=0.998896
[Epoch 0094] loss=12.8622 cls=1.1429 smmd=0.1370 ct=9.1894 rec=1.1964 | train/val/test=1.000/0.722/0.703 | c=0.998896
[Epoch 0095] loss=12.7630 cls=1.0378 smmd=0.1384 ct=9.1974 rec=1.1947 | train/val/test=1.000/0.724/0.703 | c=0.998896
[Epoch 0096] loss=12.6371 cls=0.9156 smmd=0.1365 ct=9.1876 rec=1.1988 | train/val/test=1.000/0.724/0.703 | c=0.998896
[Epoch 0097] loss=13.3070 cls=1.5693 smmd=0.1361 ct=9.1943 rec=1.2037 | train/val/test=1.000/0.726/0.703 | c=0.998896
[Epoch 0098] loss=12.5634 cls=0.8351 smmd=0.1383 ct=9.1955 rec=1.1973 | train/val/test=1.000/0.726/0.703 | c=0.998896
[Epoch 0099] loss=12.9396 cls=1.2248 smmd=0.1371 ct=9.1840 rec=1.1969 | train/val/test=1.000/0.726/0.703 | c=0.998896
=== Best @ epoch 23: val=0.7480, test=0.7400 ===
