Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2 - 2025-09-21 04:29:25:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2154 cls=1.9496 smmd=4.2264 ct=9.2616 rec=1.3889 | train/val/test=0.379/0.174/0.189 | c=0.998437
[Epoch 0001] loss=16.6897 cls=1.9199 smmd=2.7912 ct=9.1993 rec=1.3896 | train/val/test=0.397/0.320/0.292 | c=0.998437
[Epoch 0002] loss=15.2948 cls=1.7964 smmd=1.5703 ct=9.1509 rec=1.3886 | train/val/test=0.603/0.304/0.309 | c=0.998437
[Epoch 0003] loss=14.8680 cls=1.5776 smmd=1.4460 ct=9.0700 rec=1.3872 | train/val/test=0.776/0.480/0.479 | c=0.998437
[Epoch 0004] loss=14.6802 cls=1.2815 smmd=1.6852 ct=8.9580 rec=1.3778 | train/val/test=0.810/0.522/0.507 | c=0.998437
[Epoch 0005] loss=14.1424 cls=0.9465 smmd=1.6197 ct=8.8631 rec=1.3565 | train/val/test=0.862/0.524/0.514 | c=0.998437
[Epoch 0006] loss=13.5262 cls=0.6837 smmd=1.3436 ct=8.8420 rec=1.3285 | train/val/test=0.948/0.582/0.583 | c=0.998437
[Epoch 0007] loss=13.6319 cls=0.4689 smmd=1.0612 ct=9.5060 rec=1.2979 | train/val/test=0.948/0.634/0.604 | c=0.998437
[Epoch 0008] loss=13.2664 cls=0.3160 smmd=1.0027 ct=9.4043 rec=1.2717 | train/val/test=1.000/0.646/0.624 | c=0.998437
[Epoch 0009] loss=13.2170 cls=0.2041 smmd=1.1797 ct=9.3356 rec=1.2489 | train/val/test=1.000/0.658/0.640 | c=0.998437
[Epoch 0010] loss=13.1801 cls=0.1280 smmd=1.2816 ct=9.3093 rec=1.2306 | train/val/test=1.000/0.654/0.668 | c=0.998437
[Epoch 0011] loss=13.0092 cls=0.0794 smmd=1.1858 ct=9.3136 rec=1.2152 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0012] loss=12.8101 cls=0.0509 smmd=1.0114 ct=9.3401 rec=1.2039 | train/val/test=1.000/0.664/0.672 | c=0.998437
[Epoch 0013] loss=12.5819 cls=0.0329 smmd=0.7804 ct=9.3760 rec=1.1963 | train/val/test=1.000/0.664/0.674 | c=0.998437
[Epoch 0014] loss=12.5227 cls=0.0213 smmd=0.7152 ct=9.4037 rec=1.1912 | train/val/test=1.000/0.666/0.679 | c=0.998437
[Epoch 0015] loss=12.5085 cls=0.0149 smmd=0.7113 ct=9.4066 rec=1.1879 | train/val/test=1.000/0.670/0.684 | c=0.998437
[Epoch 0016] loss=12.4409 cls=0.0118 smmd=0.6705 ct=9.3867 rec=1.1859 | train/val/test=1.000/0.670/0.682 | c=0.998437
[Epoch 0017] loss=12.3342 cls=0.0105 smmd=0.5890 ct=9.3623 rec=1.1862 | train/val/test=1.000/0.668/0.683 | c=0.998437
[Epoch 0018] loss=12.2364 cls=0.0106 smmd=0.5032 ct=9.3480 rec=1.1873 | train/val/test=1.000/0.678/0.674 | c=0.998437
[Epoch 0019] loss=12.2008 cls=0.0118 smmd=0.4601 ct=9.3506 rec=1.1891 | train/val/test=1.000/0.670/0.672 | c=0.998437
[Epoch 0020] loss=12.1710 cls=0.0120 smmd=0.4218 ct=9.3567 rec=1.1902 | train/val/test=1.000/0.672/0.676 | c=0.998437
[Epoch 0021] loss=12.1230 cls=0.0132 smmd=0.3650 ct=9.3615 rec=1.1916 | train/val/test=1.000/0.674/0.685 | c=0.998437
[Epoch 0022] loss=12.0756 cls=0.0156 smmd=0.3119 ct=9.3620 rec=1.1930 | train/val/test=1.000/0.670/0.685 | c=0.998437
[Epoch 0023] loss=12.0341 cls=0.0191 smmd=0.2684 ct=9.3609 rec=1.1929 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0024] loss=12.0107 cls=0.0242 smmd=0.2407 ct=9.3625 rec=1.1917 | train/val/test=1.000/0.688/0.687 | c=0.998437
[Epoch 0025] loss=11.9881 cls=0.0299 smmd=0.2088 ct=9.3700 rec=1.1897 | train/val/test=1.000/0.696/0.694 | c=0.998437
[Epoch 0026] loss=11.9707 cls=0.0345 smmd=0.1912 ct=9.3729 rec=1.1861 | train/val/test=1.000/0.692/0.688 | c=0.998437
[Epoch 0027] loss=11.9475 cls=0.0370 smmd=0.1712 ct=9.3762 rec=1.1815 | train/val/test=1.000/0.692/0.702 | c=0.998437
[Epoch 0028] loss=11.9130 cls=0.0359 smmd=0.1536 ct=9.3677 rec=1.1779 | train/val/test=1.000/0.698/0.694 | c=0.998437
[Epoch 0029] loss=11.8789 cls=0.0326 smmd=0.1440 ct=9.3559 rec=1.1732 | train/val/test=1.000/0.694/0.703 | c=0.998437
[Epoch 0030] loss=11.8683 cls=0.0286 smmd=0.1547 ct=9.3445 rec=1.1703 | train/val/test=1.000/0.686/0.693 | c=0.998437
[Epoch 0031] loss=11.8320 cls=0.0258 smmd=0.1268 ct=9.3441 rec=1.1676 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0032] loss=11.8066 cls=0.0239 smmd=0.0973 ct=9.3530 rec=1.1662 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0033] loss=11.8053 cls=0.0211 smmd=0.0989 ct=9.3544 rec=1.1654 | train/val/test=1.000/0.692/0.694 | c=0.998437
[Epoch 0034] loss=11.7917 cls=0.0196 smmd=0.0943 ct=9.3484 rec=1.1647 | train/val/test=1.000/0.686/0.688 | c=0.998437
[Epoch 0035] loss=11.7644 cls=0.0182 smmd=0.0781 ct=9.3387 rec=1.1647 | train/val/test=1.000/0.690/0.698 | c=0.998437
[Epoch 0036] loss=11.7646 cls=0.0171 smmd=0.0831 ct=9.3334 rec=1.1655 | train/val/test=1.000/0.684/0.689 | c=0.998437
[Epoch 0037] loss=11.7596 cls=0.0173 smmd=0.0792 ct=9.3316 rec=1.1657 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0038] loss=11.7520 cls=0.0168 smmd=0.0627 ct=9.3382 rec=1.1672 | train/val/test=1.000/0.684/0.691 | c=0.998437
[Epoch 0039] loss=11.7563 cls=0.0191 smmd=0.0639 ct=9.3382 rec=1.1675 | train/val/test=1.000/0.686/0.697 | c=0.998437
[Epoch 0040] loss=11.7614 cls=0.0183 smmd=0.0667 ct=9.3398 rec=1.1683 | train/val/test=1.000/0.684/0.695 | c=0.998437
[Epoch 0041] loss=11.7450 cls=0.0217 smmd=0.0527 ct=9.3363 rec=1.1672 | train/val/test=1.000/0.690/0.692 | c=0.998437
[Epoch 0042] loss=11.7307 cls=0.0188 smmd=0.0402 ct=9.3400 rec=1.1658 | train/val/test=1.000/0.688/0.706 | c=0.998437
[Epoch 0043] loss=11.7178 cls=0.0193 smmd=0.0461 ct=9.3256 rec=1.1634 | train/val/test=1.000/0.692/0.695 | c=0.998437
[Epoch 0044] loss=11.7104 cls=0.0182 smmd=0.0429 ct=9.3242 rec=1.1625 | train/val/test=1.000/0.690/0.703 | c=0.998437
[Epoch 0045] loss=11.7014 cls=0.0179 smmd=0.0350 ct=9.3231 rec=1.1627 | train/val/test=1.000/0.694/0.700 | c=0.998437
[Epoch 0046] loss=11.7039 cls=0.0188 smmd=0.0381 ct=9.3217 rec=1.1626 | train/val/test=1.000/0.690/0.697 | c=0.998437
[Epoch 0047] loss=11.6992 cls=0.0191 smmd=0.0272 ct=9.3260 rec=1.1635 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0048] loss=11.6884 cls=0.0195 smmd=0.0242 ct=9.3184 rec=1.1632 | train/val/test=1.000/0.694/0.697 | c=0.998437
[Epoch 0049] loss=11.6830 cls=0.0194 smmd=0.0188 ct=9.3179 rec=1.1634 | train/val/test=1.000/0.694/0.699 | c=0.998437
[Epoch 0050] loss=11.6809 cls=0.0195 smmd=0.0228 ct=9.3110 rec=1.1638 | train/val/test=1.000/0.694/0.699 | c=0.998437
[Epoch 0051] loss=11.6832 cls=0.0201 smmd=0.0258 ct=9.3090 rec=1.1642 | train/val/test=1.000/0.688/0.693 | c=0.998437
[Epoch 0052] loss=11.6870 cls=0.0203 smmd=0.0180 ct=9.3178 rec=1.1655 | train/val/test=1.000/0.694/0.698 | c=0.998437
[Epoch 0053] loss=11.6954 cls=0.0228 smmd=0.0264 ct=9.3136 rec=1.1663 | train/val/test=1.000/0.680/0.697 | c=0.998437
[Epoch 0054] loss=11.7158 cls=0.0228 smmd=0.0288 ct=9.3259 rec=1.1692 | train/val/test=1.000/0.688/0.695 | c=0.998437
[Epoch 0055] loss=11.7477 cls=0.0302 smmd=0.0524 ct=9.3228 rec=1.1711 | train/val/test=1.000/0.676/0.698 | c=0.998437
[Epoch 0056] loss=11.7564 cls=0.0247 smmd=0.0523 ct=9.3340 rec=1.1727 | train/val/test=1.000/0.688/0.695 | c=0.998437
[Epoch 0057] loss=11.7080 cls=0.0220 smmd=0.0386 ct=9.3174 rec=1.1650 | train/val/test=1.000/0.692/0.699 | c=0.998437
[Epoch 0058] loss=11.6633 cls=0.0157 smmd=0.0179 ct=9.3097 rec=1.1600 | train/val/test=1.000/0.684/0.701 | c=0.998437
[Epoch 0059] loss=11.6957 cls=0.0173 smmd=0.0347 ct=9.3177 rec=1.1630 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0060] loss=11.6899 cls=0.0174 smmd=0.0332 ct=9.3130 rec=1.1632 | train/val/test=1.000/0.686/0.697 | c=0.998437
[Epoch 0061] loss=11.6582 cls=0.0154 smmd=0.0130 ct=9.3071 rec=1.1614 | train/val/test=1.000/0.686/0.703 | c=0.998437
[Epoch 0062] loss=11.6748 cls=0.0173 smmd=0.0201 ct=9.3091 rec=1.1641 | train/val/test=1.000/0.684/0.693 | c=0.998437
[Epoch 0063] loss=11.6939 cls=0.0196 smmd=0.0256 ct=9.3124 rec=1.1682 | train/val/test=1.000/0.682/0.701 | c=0.998437
[Epoch 0064] loss=11.6825 cls=0.0206 smmd=0.0127 ct=9.3120 rec=1.1686 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0065] loss=11.6655 cls=0.0198 smmd=0.0148 ct=9.2976 rec=1.1667 | train/val/test=1.000/0.686/0.694 | c=0.998437
[Epoch 0066] loss=11.6617 cls=0.0200 smmd=0.0055 ct=9.3018 rec=1.1672 | train/val/test=1.000/0.686/0.705 | c=0.998437
[Epoch 0067] loss=11.6707 cls=0.0215 smmd=0.0102 ct=9.3035 rec=1.1678 | train/val/test=1.000/0.688/0.690 | c=0.998437
[Epoch 0068] loss=11.6756 cls=0.0235 smmd=0.0124 ct=9.3034 rec=1.1681 | train/val/test=1.000/0.690/0.704 | c=0.998437
[Epoch 0069] loss=11.6684 cls=0.0216 smmd=0.0072 ct=9.3046 rec=1.1675 | train/val/test=1.000/0.692/0.700 | c=0.998437
[Epoch 0070] loss=11.6610 cls=0.0228 smmd=0.0161 ct=9.2918 rec=1.1652 | train/val/test=1.000/0.690/0.689 | c=0.998437
[Epoch 0071] loss=11.6558 cls=0.0205 smmd=0.0062 ct=9.2991 rec=1.1650 | train/val/test=1.000/0.692/0.702 | c=0.998437
[Epoch 0072] loss=11.6534 cls=0.0201 smmd=0.0070 ct=9.2968 rec=1.1648 | train/val/test=1.000/0.680/0.691 | c=0.998437
[Epoch 0073] loss=11.6466 cls=0.0205 smmd=0.0021 ct=9.2942 rec=1.1649 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0074] loss=11.6473 cls=0.0195 smmd=0.0047 ct=9.2915 rec=1.1658 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0075] loss=11.6405 cls=0.0199 smmd=0.0033 ct=9.2858 rec=1.1657 | train/val/test=1.000/0.684/0.688 | c=0.998437
[Epoch 0076] loss=11.6452 cls=0.0199 smmd=0.0029 ct=9.2887 rec=1.1668 | train/val/test=1.000/0.686/0.701 | c=0.998437
[Epoch 0077] loss=11.6407 cls=0.0201 smmd=-0.0012 ct=9.2877 rec=1.1671 | train/val/test=1.000/0.680/0.690 | c=0.998437
[Epoch 0078] loss=11.6353 cls=0.0207 smmd=-0.0053 ct=9.2857 rec=1.1671 | train/val/test=1.000/0.682/0.700 | c=0.998437
[Epoch 0079] loss=11.6402 cls=0.0203 smmd=0.0015 ct=9.2835 rec=1.1674 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0080] loss=11.6402 cls=0.0210 smmd=0.0053 ct=9.2804 rec=1.1667 | train/val/test=1.000/0.680/0.694 | c=0.998437
[Epoch 0081] loss=11.6406 cls=0.0205 smmd=0.0013 ct=9.2846 rec=1.1671 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0082] loss=11.6411 cls=0.0219 smmd=0.0043 ct=9.2805 rec=1.1672 | train/val/test=1.000/0.680/0.694 | c=0.998437
[Epoch 0083] loss=11.6459 cls=0.0220 smmd=0.0025 ct=9.2861 rec=1.1676 | train/val/test=1.000/0.688/0.702 | c=0.998437
[Epoch 0084] loss=11.6542 cls=0.0236 smmd=0.0121 ct=9.2806 rec=1.1690 | train/val/test=1.000/0.678/0.686 | c=0.998437
[Epoch 0085] loss=11.6650 cls=0.0238 smmd=0.0080 ct=9.2936 rec=1.1698 | train/val/test=1.000/0.690/0.704 | c=0.998437
[Epoch 0086] loss=11.6827 cls=0.0262 smmd=0.0289 ct=9.2847 rec=1.1714 | train/val/test=1.000/0.678/0.690 | c=0.998437
[Epoch 0087] loss=11.6954 cls=0.0247 smmd=0.0230 ct=9.3030 rec=1.1724 | train/val/test=1.000/0.692/0.695 | c=0.998437
[Epoch 0088] loss=11.6845 cls=0.0245 smmd=0.0314 ct=9.2890 rec=1.1698 | train/val/test=1.000/0.680/0.700 | c=0.998437
[Epoch 0089] loss=11.6526 cls=0.0185 smmd=0.0118 ct=9.2898 rec=1.1663 | train/val/test=1.000/0.684/0.691 | c=0.998437
[Epoch 0090] loss=11.6246 cls=0.0160 smmd=0.0061 ct=9.2784 rec=1.1620 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0091] loss=11.6300 cls=0.0167 smmd=0.0096 ct=9.2774 rec=1.1631 | train/val/test=1.000/0.678/0.695 | c=0.998437
[Epoch 0092] loss=11.6490 cls=0.0178 smmd=0.0090 ct=9.2903 rec=1.1660 | train/val/test=1.000/0.684/0.692 | c=0.998437
[Epoch 0093] loss=11.6376 cls=0.0188 smmd=0.0089 ct=9.2782 rec=1.1659 | train/val/test=1.000/0.688/0.702 | c=0.998437
[Epoch 0094] loss=11.6255 cls=0.0180 smmd=0.0007 ct=9.2749 rec=1.1660 | train/val/test=1.000/0.680/0.686 | c=0.998437
[Epoch 0095] loss=11.6298 cls=0.0198 smmd=-0.0040 ct=9.2783 rec=1.1678 | train/val/test=1.000/0.686/0.701 | c=0.998437
[Epoch 0096] loss=11.6442 cls=0.0228 smmd=0.0073 ct=9.2743 rec=1.1699 | train/val/test=1.000/0.676/0.682 | c=0.998437
[Epoch 0097] loss=11.6558 cls=0.0232 smmd=0.0020 ct=9.2875 rec=1.1716 | train/val/test=1.000/0.688/0.697 | c=0.998437
[Epoch 0098] loss=11.6549 cls=0.0260 smmd=0.0089 ct=9.2768 rec=1.1716 | train/val/test=1.000/0.680/0.698 | c=0.998437
[Epoch 0099] loss=11.6652 cls=0.0240 smmd=0.0074 ct=9.2893 rec=1.1722 | train/val/test=1.000/0.688/0.688 | c=0.998437
=== Best @ epoch 28: val=0.6980, test=0.6940 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2 - 2025-09-21 04:29:25:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2154 cls=1.9496 smmd=4.2264 ct=9.2616 rec=1.3889 | train/val/test=0.379/0.174/0.189 | c=0.998437
[Epoch 0001] loss=16.6897 cls=1.9199 smmd=2.7912 ct=9.1993 rec=1.3896 | train/val/test=0.397/0.320/0.292 | c=0.998437
[Epoch 0002] loss=15.2948 cls=1.7964 smmd=1.5703 ct=9.1509 rec=1.3886 | train/val/test=0.603/0.304/0.309 | c=0.998437
[Epoch 0003] loss=14.8680 cls=1.5776 smmd=1.4460 ct=9.0700 rec=1.3872 | train/val/test=0.776/0.480/0.479 | c=0.998437
[Epoch 0004] loss=14.6802 cls=1.2815 smmd=1.6852 ct=8.9580 rec=1.3778 | train/val/test=0.810/0.522/0.507 | c=0.998437
[Epoch 0005] loss=14.1424 cls=0.9465 smmd=1.6197 ct=8.8631 rec=1.3565 | train/val/test=0.862/0.524/0.514 | c=0.998437
[Epoch 0006] loss=13.5262 cls=0.6837 smmd=1.3436 ct=8.8420 rec=1.3285 | train/val/test=0.948/0.582/0.583 | c=0.998437
[Epoch 0007] loss=13.6319 cls=0.4689 smmd=1.0612 ct=9.5060 rec=1.2979 | train/val/test=0.948/0.634/0.604 | c=0.998437
[Epoch 0008] loss=13.2664 cls=0.3160 smmd=1.0027 ct=9.4043 rec=1.2717 | train/val/test=1.000/0.646/0.624 | c=0.998437
[Epoch 0009] loss=13.2170 cls=0.2041 smmd=1.1797 ct=9.3356 rec=1.2489 | train/val/test=1.000/0.658/0.640 | c=0.998437
[Epoch 0010] loss=13.1801 cls=0.1280 smmd=1.2816 ct=9.3093 rec=1.2306 | train/val/test=1.000/0.654/0.668 | c=0.998437
[Epoch 0011] loss=13.0092 cls=0.0794 smmd=1.1858 ct=9.3136 rec=1.2152 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0012] loss=12.8101 cls=0.0509 smmd=1.0114 ct=9.3401 rec=1.2039 | train/val/test=1.000/0.664/0.672 | c=0.998437
[Epoch 0013] loss=12.5819 cls=0.0329 smmd=0.7804 ct=9.3760 rec=1.1963 | train/val/test=1.000/0.664/0.674 | c=0.998437
[Epoch 0014] loss=12.5227 cls=0.0213 smmd=0.7152 ct=9.4037 rec=1.1912 | train/val/test=1.000/0.666/0.679 | c=0.998437
[Epoch 0015] loss=12.5085 cls=0.0149 smmd=0.7113 ct=9.4066 rec=1.1879 | train/val/test=1.000/0.670/0.684 | c=0.998437
[Epoch 0016] loss=12.4409 cls=0.0118 smmd=0.6705 ct=9.3867 rec=1.1859 | train/val/test=1.000/0.670/0.682 | c=0.998437
[Epoch 0017] loss=12.3342 cls=0.0105 smmd=0.5890 ct=9.3623 rec=1.1862 | train/val/test=1.000/0.668/0.683 | c=0.998437
[Epoch 0018] loss=12.2364 cls=0.0106 smmd=0.5032 ct=9.3480 rec=1.1873 | train/val/test=1.000/0.678/0.674 | c=0.998437
[Epoch 0019] loss=12.2008 cls=0.0118 smmd=0.4601 ct=9.3506 rec=1.1891 | train/val/test=1.000/0.670/0.672 | c=0.998437
[Epoch 0020] loss=12.1710 cls=0.0120 smmd=0.4218 ct=9.3567 rec=1.1902 | train/val/test=1.000/0.672/0.676 | c=0.998437
[Epoch 0021] loss=12.1230 cls=0.0132 smmd=0.3650 ct=9.3615 rec=1.1916 | train/val/test=1.000/0.674/0.685 | c=0.998437
[Epoch 0022] loss=12.0756 cls=0.0156 smmd=0.3119 ct=9.3620 rec=1.1930 | train/val/test=1.000/0.670/0.685 | c=0.998437
[Epoch 0023] loss=12.0341 cls=0.0191 smmd=0.2684 ct=9.3609 rec=1.1929 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0024] loss=12.0107 cls=0.0242 smmd=0.2407 ct=9.3625 rec=1.1917 | train/val/test=1.000/0.688/0.687 | c=0.998437
[Epoch 0025] loss=11.9881 cls=0.0299 smmd=0.2088 ct=9.3700 rec=1.1897 | train/val/test=1.000/0.696/0.694 | c=0.998437
[Epoch 0026] loss=11.9707 cls=0.0345 smmd=0.1912 ct=9.3729 rec=1.1861 | train/val/test=1.000/0.692/0.688 | c=0.998437
[Epoch 0027] loss=11.9475 cls=0.0370 smmd=0.1712 ct=9.3762 rec=1.1815 | train/val/test=1.000/0.692/0.702 | c=0.998437
[Epoch 0028] loss=11.9130 cls=0.0359 smmd=0.1536 ct=9.3677 rec=1.1779 | train/val/test=1.000/0.698/0.694 | c=0.998437
[Epoch 0029] loss=11.8789 cls=0.0326 smmd=0.1440 ct=9.3559 rec=1.1732 | train/val/test=1.000/0.694/0.703 | c=0.998437
[Epoch 0030] loss=11.8683 cls=0.0286 smmd=0.1547 ct=9.3445 rec=1.1703 | train/val/test=1.000/0.686/0.693 | c=0.998437
[Epoch 0031] loss=11.8320 cls=0.0258 smmd=0.1268 ct=9.3441 rec=1.1676 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0032] loss=11.8066 cls=0.0239 smmd=0.0973 ct=9.3530 rec=1.1662 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0033] loss=11.8053 cls=0.0211 smmd=0.0989 ct=9.3544 rec=1.1654 | train/val/test=1.000/0.692/0.694 | c=0.998437
[Epoch 0034] loss=11.7917 cls=0.0196 smmd=0.0943 ct=9.3484 rec=1.1647 | train/val/test=1.000/0.686/0.688 | c=0.998437
[Epoch 0035] loss=11.7644 cls=0.0182 smmd=0.0781 ct=9.3387 rec=1.1647 | train/val/test=1.000/0.690/0.698 | c=0.998437
[Epoch 0036] loss=11.7646 cls=0.0171 smmd=0.0831 ct=9.3334 rec=1.1655 | train/val/test=1.000/0.684/0.689 | c=0.998437
[Epoch 0037] loss=11.7596 cls=0.0173 smmd=0.0792 ct=9.3316 rec=1.1657 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0038] loss=11.7520 cls=0.0168 smmd=0.0627 ct=9.3382 rec=1.1672 | train/val/test=1.000/0.684/0.691 | c=0.998437
[Epoch 0039] loss=11.7563 cls=0.0191 smmd=0.0639 ct=9.3382 rec=1.1675 | train/val/test=1.000/0.686/0.697 | c=0.998437
[Epoch 0040] loss=11.7614 cls=0.0183 smmd=0.0667 ct=9.3398 rec=1.1683 | train/val/test=1.000/0.684/0.695 | c=0.998437
[Epoch 0041] loss=11.7450 cls=0.0217 smmd=0.0527 ct=9.3363 rec=1.1672 | train/val/test=1.000/0.690/0.692 | c=0.998437
[Epoch 0042] loss=11.7307 cls=0.0188 smmd=0.0402 ct=9.3400 rec=1.1658 | train/val/test=1.000/0.688/0.706 | c=0.998437
[Epoch 0043] loss=11.7178 cls=0.0193 smmd=0.0461 ct=9.3256 rec=1.1634 | train/val/test=1.000/0.692/0.695 | c=0.998437
[Epoch 0044] loss=11.7104 cls=0.0182 smmd=0.0429 ct=9.3242 rec=1.1625 | train/val/test=1.000/0.690/0.703 | c=0.998437
[Epoch 0045] loss=11.7014 cls=0.0179 smmd=0.0350 ct=9.3231 rec=1.1627 | train/val/test=1.000/0.694/0.700 | c=0.998437
[Epoch 0046] loss=11.7039 cls=0.0188 smmd=0.0381 ct=9.3217 rec=1.1626 | train/val/test=1.000/0.690/0.697 | c=0.998437
[Epoch 0047] loss=11.6992 cls=0.0191 smmd=0.0272 ct=9.3260 rec=1.1635 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0048] loss=11.6884 cls=0.0195 smmd=0.0242 ct=9.3184 rec=1.1632 | train/val/test=1.000/0.694/0.697 | c=0.998437
[Epoch 0049] loss=11.6830 cls=0.0194 smmd=0.0188 ct=9.3179 rec=1.1634 | train/val/test=1.000/0.694/0.699 | c=0.998437
[Epoch 0050] loss=11.6809 cls=0.0195 smmd=0.0228 ct=9.3110 rec=1.1638 | train/val/test=1.000/0.694/0.699 | c=0.998437
[Epoch 0051] loss=11.6832 cls=0.0201 smmd=0.0258 ct=9.3090 rec=1.1642 | train/val/test=1.000/0.688/0.693 | c=0.998437
[Epoch 0052] loss=11.6870 cls=0.0203 smmd=0.0180 ct=9.3178 rec=1.1655 | train/val/test=1.000/0.694/0.698 | c=0.998437
[Epoch 0053] loss=11.6954 cls=0.0228 smmd=0.0264 ct=9.3136 rec=1.1663 | train/val/test=1.000/0.680/0.697 | c=0.998437
[Epoch 0054] loss=11.7158 cls=0.0228 smmd=0.0288 ct=9.3259 rec=1.1692 | train/val/test=1.000/0.688/0.695 | c=0.998437
[Epoch 0055] loss=11.7477 cls=0.0302 smmd=0.0524 ct=9.3228 rec=1.1711 | train/val/test=1.000/0.676/0.698 | c=0.998437
[Epoch 0056] loss=11.7564 cls=0.0247 smmd=0.0523 ct=9.3340 rec=1.1727 | train/val/test=1.000/0.688/0.695 | c=0.998437
[Epoch 0057] loss=11.7080 cls=0.0220 smmd=0.0386 ct=9.3174 rec=1.1650 | train/val/test=1.000/0.692/0.699 | c=0.998437
[Epoch 0058] loss=11.6633 cls=0.0157 smmd=0.0179 ct=9.3097 rec=1.1600 | train/val/test=1.000/0.684/0.701 | c=0.998437
[Epoch 0059] loss=11.6957 cls=0.0173 smmd=0.0347 ct=9.3177 rec=1.1630 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0060] loss=11.6899 cls=0.0174 smmd=0.0332 ct=9.3130 rec=1.1632 | train/val/test=1.000/0.686/0.697 | c=0.998437
[Epoch 0061] loss=11.6582 cls=0.0154 smmd=0.0130 ct=9.3071 rec=1.1614 | train/val/test=1.000/0.686/0.703 | c=0.998437
[Epoch 0062] loss=11.6748 cls=0.0173 smmd=0.0201 ct=9.3091 rec=1.1641 | train/val/test=1.000/0.684/0.693 | c=0.998437
[Epoch 0063] loss=11.6939 cls=0.0196 smmd=0.0256 ct=9.3124 rec=1.1682 | train/val/test=1.000/0.682/0.701 | c=0.998437
[Epoch 0064] loss=11.6825 cls=0.0206 smmd=0.0127 ct=9.3120 rec=1.1686 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0065] loss=11.6655 cls=0.0198 smmd=0.0148 ct=9.2976 rec=1.1667 | train/val/test=1.000/0.686/0.694 | c=0.998437
[Epoch 0066] loss=11.6617 cls=0.0200 smmd=0.0055 ct=9.3018 rec=1.1672 | train/val/test=1.000/0.686/0.705 | c=0.998437
[Epoch 0067] loss=11.6707 cls=0.0215 smmd=0.0102 ct=9.3035 rec=1.1678 | train/val/test=1.000/0.688/0.690 | c=0.998437
[Epoch 0068] loss=11.6756 cls=0.0235 smmd=0.0124 ct=9.3034 rec=1.1681 | train/val/test=1.000/0.690/0.704 | c=0.998437
[Epoch 0069] loss=11.6684 cls=0.0216 smmd=0.0072 ct=9.3046 rec=1.1675 | train/val/test=1.000/0.692/0.700 | c=0.998437
[Epoch 0070] loss=11.6610 cls=0.0228 smmd=0.0161 ct=9.2918 rec=1.1652 | train/val/test=1.000/0.690/0.689 | c=0.998437
[Epoch 0071] loss=11.6558 cls=0.0205 smmd=0.0062 ct=9.2991 rec=1.1650 | train/val/test=1.000/0.692/0.702 | c=0.998437
[Epoch 0072] loss=11.6534 cls=0.0201 smmd=0.0070 ct=9.2968 rec=1.1648 | train/val/test=1.000/0.680/0.691 | c=0.998437
[Epoch 0073] loss=11.6466 cls=0.0205 smmd=0.0021 ct=9.2942 rec=1.1649 | train/val/test=1.000/0.688/0.699 | c=0.998437
[Epoch 0074] loss=11.6473 cls=0.0195 smmd=0.0047 ct=9.2915 rec=1.1658 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0075] loss=11.6405 cls=0.0199 smmd=0.0033 ct=9.2858 rec=1.1657 | train/val/test=1.000/0.684/0.688 | c=0.998437
[Epoch 0076] loss=11.6452 cls=0.0199 smmd=0.0029 ct=9.2887 rec=1.1668 | train/val/test=1.000/0.686/0.701 | c=0.998437
[Epoch 0077] loss=11.6407 cls=0.0201 smmd=-0.0012 ct=9.2877 rec=1.1671 | train/val/test=1.000/0.680/0.690 | c=0.998437
[Epoch 0078] loss=11.6353 cls=0.0207 smmd=-0.0053 ct=9.2857 rec=1.1671 | train/val/test=1.000/0.682/0.700 | c=0.998437
[Epoch 0079] loss=11.6402 cls=0.0203 smmd=0.0015 ct=9.2835 rec=1.1674 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0080] loss=11.6402 cls=0.0210 smmd=0.0053 ct=9.2804 rec=1.1667 | train/val/test=1.000/0.680/0.694 | c=0.998437
[Epoch 0081] loss=11.6406 cls=0.0205 smmd=0.0013 ct=9.2846 rec=1.1671 | train/val/test=1.000/0.688/0.698 | c=0.998437
[Epoch 0082] loss=11.6411 cls=0.0219 smmd=0.0043 ct=9.2805 rec=1.1672 | train/val/test=1.000/0.680/0.694 | c=0.998437
[Epoch 0083] loss=11.6459 cls=0.0220 smmd=0.0025 ct=9.2861 rec=1.1676 | train/val/test=1.000/0.688/0.702 | c=0.998437
[Epoch 0084] loss=11.6542 cls=0.0236 smmd=0.0121 ct=9.2806 rec=1.1690 | train/val/test=1.000/0.678/0.686 | c=0.998437
[Epoch 0085] loss=11.6650 cls=0.0238 smmd=0.0080 ct=9.2936 rec=1.1698 | train/val/test=1.000/0.690/0.704 | c=0.998437
[Epoch 0086] loss=11.6827 cls=0.0262 smmd=0.0289 ct=9.2847 rec=1.1714 | train/val/test=1.000/0.678/0.690 | c=0.998437
[Epoch 0087] loss=11.6954 cls=0.0247 smmd=0.0230 ct=9.3030 rec=1.1724 | train/val/test=1.000/0.692/0.695 | c=0.998437
[Epoch 0088] loss=11.6845 cls=0.0245 smmd=0.0314 ct=9.2890 rec=1.1698 | train/val/test=1.000/0.680/0.700 | c=0.998437
[Epoch 0089] loss=11.6526 cls=0.0185 smmd=0.0118 ct=9.2898 rec=1.1663 | train/val/test=1.000/0.684/0.691 | c=0.998437
[Epoch 0090] loss=11.6246 cls=0.0160 smmd=0.0061 ct=9.2784 rec=1.1620 | train/val/test=1.000/0.690/0.699 | c=0.998437
[Epoch 0091] loss=11.6300 cls=0.0167 smmd=0.0096 ct=9.2774 rec=1.1631 | train/val/test=1.000/0.678/0.695 | c=0.998437
[Epoch 0092] loss=11.6490 cls=0.0178 smmd=0.0090 ct=9.2903 rec=1.1660 | train/val/test=1.000/0.684/0.692 | c=0.998437
[Epoch 0093] loss=11.6376 cls=0.0188 smmd=0.0089 ct=9.2782 rec=1.1659 | train/val/test=1.000/0.688/0.702 | c=0.998437
[Epoch 0094] loss=11.6255 cls=0.0180 smmd=0.0007 ct=9.2749 rec=1.1660 | train/val/test=1.000/0.680/0.686 | c=0.998437
[Epoch 0095] loss=11.6298 cls=0.0198 smmd=-0.0040 ct=9.2783 rec=1.1678 | train/val/test=1.000/0.686/0.701 | c=0.998437
[Epoch 0096] loss=11.6442 cls=0.0228 smmd=0.0073 ct=9.2743 rec=1.1699 | train/val/test=1.000/0.676/0.682 | c=0.998437
[Epoch 0097] loss=11.6558 cls=0.0232 smmd=0.0020 ct=9.2875 rec=1.1716 | train/val/test=1.000/0.688/0.697 | c=0.998437
[Epoch 0098] loss=11.6549 cls=0.0260 smmd=0.0089 ct=9.2768 rec=1.1716 | train/val/test=1.000/0.680/0.698 | c=0.998437
[Epoch 0099] loss=11.6652 cls=0.0240 smmd=0.0074 ct=9.2893 rec=1.1722 | train/val/test=1.000/0.688/0.688 | c=0.998437
=== Best @ epoch 28: val=0.6980, test=0.6940 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-2 completed in 29.64 seconds.
==================================================
