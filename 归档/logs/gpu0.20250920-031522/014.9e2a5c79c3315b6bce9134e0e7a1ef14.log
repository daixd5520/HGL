[2025-09-20 04:03:03] START attempt 1: python main.py --is_transfer True --test_dataset Cora --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth --few True --shot 5 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1837 cls=1.9498 smmd=4.1903 ct=9.2658 rec=1.3889 | train/val/test=0.207/0.100/0.120 | c=0.998437
[Epoch 0001] loss=16.6956 cls=1.8943 smmd=2.8268 ct=9.1956 rec=1.3895 | train/val/test=0.552/0.356/0.386 | c=0.998437
[Epoch 0002] loss=15.0979 cls=1.7567 smmd=1.4954 ct=9.0683 rec=1.3887 | train/val/test=0.862/0.330/0.340 | c=0.998437
[Epoch 0003] loss=14.7672 cls=1.4887 smmd=1.4741 ct=9.0311 rec=1.3866 | train/val/test=0.931/0.568/0.555 | c=0.998437
[Epoch 0004] loss=14.5431 cls=1.1407 smmd=1.6995 ct=8.9433 rec=1.3798 | train/val/test=0.966/0.636/0.637 | c=0.998437
[Epoch 0005] loss=14.0207 cls=0.7652 smmd=1.6445 ct=8.8794 rec=1.3659 | train/val/test=1.000/0.538/0.572 | c=0.998437
[Epoch 0006] loss=13.3701 cls=0.4936 smmd=1.3370 ct=8.8488 rec=1.3454 | train/val/test=1.000/0.590/0.617 | c=0.998437
[Epoch 0007] loss=12.8189 cls=0.2989 smmd=1.0522 ct=8.8262 rec=1.3208 | train/val/test=1.000/0.620/0.658 | c=0.998437
[Epoch 0008] loss=12.5776 cls=0.1792 smmd=0.9906 ct=8.8232 rec=1.2923 | train/val/test=1.000/0.628/0.652 | c=0.998437
[Epoch 0009] loss=12.5140 cls=0.1019 smmd=1.0605 ct=8.8158 rec=1.2679 | train/val/test=1.000/0.608/0.641 | c=0.998437
[Epoch 0010] loss=12.4573 cls=0.0546 smmd=1.0945 ct=8.8077 rec=1.2502 | train/val/test=1.000/0.612/0.649 | c=0.998437
[Epoch 0011] loss=12.2673 cls=0.0282 smmd=0.9688 ct=8.8011 rec=1.2346 | train/val/test=1.000/0.644/0.680 | c=0.998437
[Epoch 0012] loss=12.7723 cls=0.0149 smmd=0.8015 ct=9.5168 rec=1.2196 | train/val/test=1.000/0.618/0.669 | c=0.998437
[Epoch 0013] loss=12.6076 cls=0.0087 smmd=0.7440 ct=9.4306 rec=1.2121 | train/val/test=1.000/0.618/0.647 | c=0.998437
[Epoch 0014] loss=12.5704 cls=0.0061 smmd=0.7885 ct=9.3594 rec=1.2082 | train/val/test=1.000/0.642/0.676 | c=0.998437
[Epoch 0015] loss=12.5614 cls=0.0049 smmd=0.8136 ct=9.3419 rec=1.2005 | train/val/test=1.000/0.666/0.689 | c=0.998437
[Epoch 0016] loss=12.4780 cls=0.0046 smmd=0.7146 ct=9.3706 rec=1.1941 | train/val/test=1.000/0.664/0.703 | c=0.998437
[Epoch 0017] loss=12.3712 cls=0.0044 smmd=0.5864 ct=9.3958 rec=1.1923 | train/val/test=1.000/0.658/0.700 | c=0.998437
[Epoch 0018] loss=12.2591 cls=0.0048 smmd=0.4545 ct=9.4144 rec=1.1927 | train/val/test=1.000/0.674/0.711 | c=0.998437
[Epoch 0019] loss=12.2363 cls=0.0057 smmd=0.4207 ct=9.4294 rec=1.1902 | train/val/test=1.000/0.690/0.713 | c=0.998437
[Epoch 0020] loss=12.2006 cls=0.0074 smmd=0.3925 ct=9.4214 rec=1.1896 | train/val/test=1.000/0.682/0.712 | c=0.998437
[Epoch 0021] loss=12.1386 cls=0.0092 smmd=0.3570 ct=9.3904 rec=1.1910 | train/val/test=1.000/0.686/0.722 | c=0.998437
[Epoch 0022] loss=12.0729 cls=0.0116 smmd=0.2938 ct=9.3854 rec=1.1911 | train/val/test=1.000/0.690/0.729 | c=0.998437
[Epoch 0023] loss=12.0593 cls=0.0151 smmd=0.2751 ct=9.3881 rec=1.1905 | train/val/test=1.000/0.692/0.732 | c=0.998437
[Epoch 0024] loss=12.0348 cls=0.0199 smmd=0.2450 ct=9.3892 rec=1.1904 | train/val/test=1.000/0.708/0.734 | c=0.998437
[Epoch 0025] loss=12.0173 cls=0.0238 smmd=0.2183 ct=9.4023 rec=1.1864 | train/val/test=1.000/0.698/0.739 | c=0.998437
[Epoch 0026] loss=11.9698 cls=0.0277 smmd=0.1819 ct=9.3916 rec=1.1843 | train/val/test=1.000/0.700/0.738 | c=0.998437
[Epoch 0027] loss=11.9542 cls=0.0300 smmd=0.1727 ct=9.3903 rec=1.1806 | train/val/test=1.000/0.706/0.740 | c=0.998437
[Epoch 0028] loss=11.9278 cls=0.0293 smmd=0.1620 ct=9.3840 rec=1.1762 | train/val/test=1.000/0.704/0.739 | c=0.998437
[Epoch 0029] loss=11.8960 cls=0.0274 smmd=0.1495 ct=9.3748 rec=1.1721 | train/val/test=1.000/0.702/0.738 | c=0.998437
[Epoch 0030] loss=11.8549 cls=0.0232 smmd=0.1215 ct=9.3736 rec=1.1683 | train/val/test=1.000/0.706/0.737 | c=0.998437
[Epoch 0031] loss=11.8349 cls=0.0196 smmd=0.1148 ct=9.3689 rec=1.1658 | train/val/test=1.000/0.704/0.735 | c=0.998437
[Epoch 0032] loss=11.8184 cls=0.0166 smmd=0.1106 ct=9.3632 rec=1.1640 | train/val/test=1.000/0.710/0.730 | c=0.998437
[Epoch 0033] loss=11.7926 cls=0.0145 smmd=0.0964 ct=9.3548 rec=1.1635 | train/val/test=1.000/0.702/0.736 | c=0.998437
[Epoch 0034] loss=11.7781 cls=0.0126 smmd=0.0892 ct=9.3499 rec=1.1632 | train/val/test=1.000/0.706/0.733 | c=0.998437
[Epoch 0035] loss=11.7777 cls=0.0118 smmd=0.0875 ct=9.3512 rec=1.1636 | train/val/test=1.000/0.708/0.735 | c=0.998437
[Epoch 0036] loss=11.7613 cls=0.0115 smmd=0.0706 ct=9.3496 rec=1.1648 | train/val/test=1.000/0.706/0.735 | c=0.998437
[Epoch 0037] loss=11.7385 cls=0.0115 smmd=0.0523 ct=9.3431 rec=1.1659 | train/val/test=1.000/0.708/0.736 | c=0.998437
[Epoch 0038] loss=11.7403 cls=0.0118 smmd=0.0546 ct=9.3401 rec=1.1669 | train/val/test=1.000/0.700/0.738 | c=0.998437
[Epoch 0039] loss=11.7315 cls=0.0120 smmd=0.0492 ct=9.3347 rec=1.1678 | train/val/test=1.000/0.708/0.737 | c=0.998437
[Epoch 0040] loss=11.7288 cls=0.0128 smmd=0.0370 ct=9.3437 rec=1.1676 | train/val/test=1.000/0.702/0.739 | c=0.998437
[Epoch 0041] loss=11.7304 cls=0.0136 smmd=0.0544 ct=9.3252 rec=1.1685 | train/val/test=1.000/0.710/0.734 | c=0.998437
[Epoch 0042] loss=11.7398 cls=0.0154 smmd=0.0425 ct=9.3464 rec=1.1677 | train/val/test=1.000/0.704/0.737 | c=0.998437
[Epoch 0043] loss=11.7550 cls=0.0184 smmd=0.0719 ct=9.3233 rec=1.1707 | train/val/test=1.000/0.712/0.730 | c=0.998437
[Epoch 0044] loss=11.7670 cls=0.0196 smmd=0.0455 ct=9.3649 rec=1.1685 | train/val/test=1.000/0.702/0.740 | c=0.998437
[Epoch 0045] loss=11.7396 cls=0.0176 smmd=0.0687 ct=9.3198 rec=1.1668 | train/val/test=1.000/0.702/0.737 | c=0.998437
[Epoch 0046] loss=11.6842 cls=0.0136 smmd=0.0238 ct=9.3234 rec=1.1617 | train/val/test=1.000/0.710/0.735 | c=0.998437
[Epoch 0047] loss=11.6975 cls=0.0142 smmd=0.0293 ct=9.3297 rec=1.1622 | train/val/test=1.000/0.700/0.741 | c=0.998437
[Epoch 0048] loss=11.7040 cls=0.0147 smmd=0.0524 ct=9.3082 rec=1.1643 | train/val/test=1.000/0.706/0.739 | c=0.998437
[Epoch 0049] loss=11.6732 cls=0.0126 smmd=0.0138 ct=9.3222 rec=1.1623 | train/val/test=1.000/0.706/0.738 | c=0.998437
[Epoch 0050] loss=11.6699 cls=0.0129 smmd=0.0103 ct=9.3203 rec=1.1632 | train/val/test=1.000/0.696/0.742 | c=0.998437
[Epoch 0051] loss=11.6893 cls=0.0139 smmd=0.0368 ct=9.3055 rec=1.1665 | train/val/test=1.000/0.708/0.736 | c=0.998437
[Epoch 0052] loss=11.6792 cls=0.0144 smmd=0.0103 ct=9.3213 rec=1.1666 | train/val/test=1.000/0.696/0.740 | c=0.998437
[Epoch 0053] loss=11.6645 cls=0.0139 smmd=0.0193 ct=9.2965 rec=1.1674 | train/val/test=1.000/0.698/0.739 | c=0.998437
[Epoch 0054] loss=11.6598 cls=0.0136 smmd=0.0083 ct=9.3030 rec=1.1674 | train/val/test=1.000/0.704/0.735 | c=0.998437
[Epoch 0055] loss=11.6613 cls=0.0147 smmd=0.0064 ct=9.3047 rec=1.1678 | train/val/test=1.000/0.696/0.740 | c=0.998437
[Epoch 0056] loss=11.6577 cls=0.0147 smmd=0.0099 ct=9.2953 rec=1.1689 | train/val/test=1.000/0.706/0.737 | c=0.998437
[Epoch 0057] loss=11.6572 cls=0.0150 smmd=-0.0000 ct=9.3067 rec=1.1677 | train/val/test=1.000/0.702/0.739 | c=0.998437
[Epoch 0058] loss=11.6487 cls=0.0146 smmd=0.0124 ct=9.2867 rec=1.1675 | train/val/test=1.000/0.706/0.739 | c=0.998437
[Epoch 0059] loss=11.6450 cls=0.0140 smmd=0.0043 ct=9.2944 rec=1.1661 | train/val/test=1.000/0.704/0.738 | c=0.998437
[Epoch 0060] loss=11.6365 cls=0.0141 smmd=0.0022 ct=9.2891 rec=1.1656 | train/val/test=1.000/0.706/0.738 | c=0.998437
[Epoch 0061] loss=11.6342 cls=0.0145 smmd=0.0012 ct=9.2862 rec=1.1661 | train/val/test=1.000/0.704/0.739 | c=0.998437
[Epoch 0062] loss=11.6414 cls=0.0149 smmd=0.0003 ct=9.2933 rec=1.1665 | train/val/test=1.000/0.706/0.738 | c=0.998437
[Epoch 0063] loss=11.6364 cls=0.0155 smmd=0.0059 ct=9.2795 rec=1.1677 | train/val/test=1.000/0.702/0.737 | c=0.998437
[Epoch 0064] loss=11.6301 cls=0.0158 smmd=-0.0077 ct=9.2859 rec=1.1680 | train/val/test=1.000/0.706/0.741 | c=0.998437
[Epoch 0065] loss=11.6293 cls=0.0161 smmd=-0.0019 ct=9.2764 rec=1.1693 | train/val/test=1.000/0.704/0.737 | c=0.998437
[Epoch 0066] loss=11.6276 cls=0.0165 smmd=-0.0080 ct=9.2791 rec=1.1700 | train/val/test=1.000/0.706/0.739 | c=0.998437
[Epoch 0067] loss=11.6291 cls=0.0167 smmd=-0.0073 ct=9.2780 rec=1.1708 | train/val/test=1.000/0.704/0.738 | c=0.998437
[Epoch 0068] loss=11.6238 cls=0.0169 smmd=-0.0073 ct=9.2711 rec=1.1716 | train/val/test=1.000/0.704/0.740 | c=0.998437
[Epoch 0069] loss=11.6269 cls=0.0167 smmd=-0.0120 ct=9.2800 rec=1.1711 | train/val/test=1.000/0.704/0.742 | c=0.998437
[Epoch 0070] loss=11.6275 cls=0.0166 smmd=0.0004 ct=9.2668 rec=1.1719 | train/val/test=1.000/0.708/0.738 | c=0.998437
[Epoch 0071] loss=11.6384 cls=0.0168 smmd=-0.0085 ct=9.2886 rec=1.1708 | train/val/test=1.000/0.702/0.743 | c=0.998437
[Epoch 0072] loss=11.6426 cls=0.0165 smmd=0.0175 ct=9.2649 rec=1.1718 | train/val/test=1.000/0.708/0.736 | c=0.998437
[Epoch 0073] loss=11.6399 cls=0.0166 smmd=-0.0083 ct=9.2915 rec=1.1701 | train/val/test=1.000/0.706/0.744 | c=0.998437
[Epoch 0074] loss=11.6326 cls=0.0149 smmd=0.0156 ct=9.2635 rec=1.1693 | train/val/test=1.000/0.708/0.740 | c=0.998437
[Epoch 0075] loss=11.6086 cls=0.0135 smmd=-0.0151 ct=9.2770 rec=1.1666 | train/val/test=1.000/0.706/0.740 | c=0.998437
[Epoch 0076] loss=11.5978 cls=0.0131 smmd=-0.0139 ct=9.2665 rec=1.1661 | train/val/test=1.000/0.704/0.743 | c=0.998437
[Epoch 0077] loss=11.6090 cls=0.0140 smmd=-0.0009 ct=9.2599 rec=1.1680 | train/val/test=1.000/0.706/0.740 | c=0.998437
[Epoch 0078] loss=11.6156 cls=0.0158 smmd=-0.0125 ct=9.2746 rec=1.1689 | train/val/test=1.000/0.704/0.743 | c=0.998437
[Epoch 0079] loss=11.6124 cls=0.0160 smmd=-0.0034 ct=9.2576 rec=1.1711 | train/val/test=1.000/0.702/0.739 | c=0.998437
[Epoch 0080] loss=11.6093 cls=0.0162 smmd=-0.0189 ct=9.2719 rec=1.1700 | train/val/test=1.000/0.704/0.742 | c=0.998437
[Epoch 0081] loss=11.5973 cls=0.0160 smmd=-0.0139 ct=9.2540 rec=1.1706 | train/val/test=1.000/0.708/0.741 | c=0.998437
[Epoch 0082] loss=11.5966 cls=0.0158 smmd=-0.0202 ct=9.2602 rec=1.1704 | train/val/test=1.000/0.704/0.737 | c=0.998437
[Epoch 0083] loss=11.5995 cls=0.0168 smmd=-0.0146 ct=9.2549 rec=1.1712 | train/val/test=1.000/0.706/0.741 | c=0.998437
[Epoch 0084] loss=11.6194 cls=0.0168 smmd=-0.0094 ct=9.2649 rec=1.1735 | train/val/test=1.000/0.706/0.726 | c=0.998437
[Epoch 0085] loss=11.6456 cls=0.0203 smmd=0.0125 ct=9.2600 rec=1.1764 | train/val/test=1.000/0.706/0.744 | c=0.998437
[Epoch 0086] loss=11.6870 cls=0.0205 smmd=0.0204 ct=9.2849 rec=1.1806 | train/val/test=1.000/0.708/0.722 | c=0.998437
[Epoch 0087] loss=11.6595 cls=0.0182 smmd=0.0315 ct=9.2614 rec=1.1742 | train/val/test=1.000/0.710/0.737 | c=0.998437
[Epoch 0088] loss=11.5947 cls=0.0121 smmd=-0.0130 ct=9.2674 rec=1.1641 | train/val/test=1.000/0.700/0.745 | c=0.998437
[Epoch 0089] loss=11.6098 cls=0.0124 smmd=0.0043 ct=9.2626 rec=1.1652 | train/val/test=1.000/0.710/0.724 | c=0.998437
[Epoch 0090] loss=11.6234 cls=0.0136 smmd=0.0132 ct=9.2637 rec=1.1664 | train/val/test=1.000/0.708/0.740 | c=0.998437
[Epoch 0091] loss=11.5878 cls=0.0115 smmd=-0.0099 ct=9.2587 rec=1.1638 | train/val/test=1.000/0.698/0.747 | c=0.998437
[Epoch 0092] loss=11.5990 cls=0.0131 smmd=-0.0054 ct=9.2561 rec=1.1676 | train/val/test=1.000/0.706/0.723 | c=0.998437
[Epoch 0093] loss=11.6214 cls=0.0167 smmd=-0.0011 ct=9.2633 rec=1.1712 | train/val/test=1.000/0.702/0.743 | c=0.998437
[Epoch 0094] loss=11.6037 cls=0.0157 smmd=-0.0126 ct=9.2540 rec=1.1733 | train/val/test=1.000/0.708/0.743 | c=0.998437
[Epoch 0095] loss=11.5885 cls=0.0153 smmd=-0.0222 ct=9.2532 rec=1.1711 | train/val/test=1.000/0.700/0.729 | c=0.998437
[Epoch 0096] loss=11.6084 cls=0.0180 smmd=-0.0060 ct=9.2489 rec=1.1738 | train/val/test=1.000/0.700/0.751 | c=0.998437
[Epoch 0097] loss=11.6287 cls=0.0193 smmd=-0.0095 ct=9.2606 rec=1.1792 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0098] loss=11.6221 cls=0.0191 smmd=0.0031 ct=9.2521 rec=1.1739 | train/val/test=1.000/0.706/0.738 | c=0.998437
[Epoch 0099] loss=11.5946 cls=0.0157 smmd=-0.0220 ct=9.2611 rec=1.1699 | train/val/test=1.000/0.702/0.743 | c=0.998437
=== Best @ epoch 43: val=0.7120, test=0.7300 ===
[2025-09-20 04:03:37] END attempt 1: exit_code=0
