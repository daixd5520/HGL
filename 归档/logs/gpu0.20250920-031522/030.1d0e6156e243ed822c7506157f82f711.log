[2025-09-20 04:55:33] START attempt 1: python main.py --is_transfer True --test_dataset Cora --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth --few True --shot 10 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2623 cls=1.9455 smmd=4.2829 ct=9.2562 rec=1.3889 | train/val/test=0.241/0.084/0.111 | c=0.998347
[Epoch 0001] loss=16.7827 cls=1.9019 smmd=2.8986 ct=9.2038 rec=1.3892 | train/val/test=0.603/0.278/0.299 | c=0.998347
[Epoch 0002] loss=15.1616 cls=1.7790 smmd=1.5367 ct=9.0685 rec=1.3887 | train/val/test=0.638/0.418/0.421 | c=0.998347
[Epoch 0003] loss=14.8723 cls=1.5670 smmd=1.4937 ct=9.0403 rec=1.3857 | train/val/test=0.879/0.520/0.511 | c=0.998347
[Epoch 0004] loss=14.7036 cls=1.2796 smmd=1.7622 ct=8.9135 rec=1.3742 | train/val/test=0.845/0.584/0.570 | c=0.998347
[Epoch 0005] loss=14.2639 cls=0.9763 smmd=1.6952 ct=8.8831 rec=1.3547 | train/val/test=0.897/0.598/0.601 | c=0.998347
[Epoch 0006] loss=13.5945 cls=0.7080 smmd=1.4058 ct=8.8291 rec=1.3258 | train/val/test=0.914/0.616/0.626 | c=0.998347
[Epoch 0007] loss=12.9529 cls=0.4784 smmd=1.0635 ct=8.8188 rec=1.2961 | train/val/test=0.948/0.662/0.653 | c=0.998347
[Epoch 0008] loss=12.6360 cls=0.3000 smmd=1.0024 ct=8.8047 rec=1.2645 | train/val/test=0.983/0.686/0.670 | c=0.998347
[Epoch 0009] loss=12.5596 cls=0.1782 smmd=1.1111 ct=8.7935 rec=1.2384 | train/val/test=0.983/0.702/0.679 | c=0.998347
[Epoch 0010] loss=13.1292 cls=0.0943 smmd=1.1581 ct=9.4442 rec=1.2163 | train/val/test=1.000/0.732/0.721 | c=0.998347
[Epoch 0011] loss=12.9214 cls=0.0499 smmd=1.0730 ct=9.4030 rec=1.1978 | train/val/test=1.000/0.738/0.733 | c=0.998347
[Epoch 0012] loss=12.7113 cls=0.0307 smmd=0.9414 ct=9.3653 rec=1.1869 | train/val/test=1.000/0.736/0.727 | c=0.998347
[Epoch 0013] loss=12.5671 cls=0.0187 smmd=0.8360 ct=9.3535 rec=1.1795 | train/val/test=1.000/0.738/0.713 | c=0.998347
[Epoch 0014] loss=12.4935 cls=0.0129 smmd=0.7539 ct=9.3760 rec=1.1753 | train/val/test=1.000/0.726/0.706 | c=0.998347
[Epoch 0015] loss=12.4719 cls=0.0097 smmd=0.7051 ct=9.4097 rec=1.1737 | train/val/test=1.000/0.714/0.713 | c=0.998347
[Epoch 0016] loss=12.4408 cls=0.0074 smmd=0.6633 ct=9.4231 rec=1.1735 | train/val/test=1.000/0.718/0.728 | c=0.998347
[Epoch 0017] loss=12.3218 cls=0.0071 smmd=0.5524 ct=9.4123 rec=1.1750 | train/val/test=1.000/0.710/0.713 | c=0.998347
[Epoch 0018] loss=12.2227 cls=0.0074 smmd=0.4700 ct=9.3934 rec=1.1759 | train/val/test=1.000/0.712/0.710 | c=0.998347
[Epoch 0019] loss=12.1817 cls=0.0087 smmd=0.4343 ct=9.3848 rec=1.1769 | train/val/test=1.000/0.706/0.721 | c=0.998347
[Epoch 0020] loss=12.1698 cls=0.0120 smmd=0.4146 ct=9.3860 rec=1.1786 | train/val/test=1.000/0.718/0.722 | c=0.998347
[Epoch 0021] loss=12.1181 cls=0.0147 smmd=0.3508 ct=9.3938 rec=1.1794 | train/val/test=1.000/0.716/0.713 | c=0.998347
[Epoch 0022] loss=12.0477 cls=0.0170 smmd=0.2671 ct=9.4054 rec=1.1791 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0023] loss=12.0081 cls=0.0223 smmd=0.2139 ct=9.4125 rec=1.1797 | train/val/test=1.000/0.728/0.726 | c=0.998347
[Epoch 0024] loss=12.0115 cls=0.0245 smmd=0.2232 ct=9.4086 rec=1.1776 | train/val/test=1.000/0.732/0.729 | c=0.998347
[Epoch 0025] loss=11.9922 cls=0.0281 smmd=0.2141 ct=9.4000 rec=1.1750 | train/val/test=1.000/0.734/0.734 | c=0.998347
[Epoch 0026] loss=11.9388 cls=0.0315 smmd=0.1684 ct=9.3949 rec=1.1721 | train/val/test=1.000/0.734/0.736 | c=0.998347
[Epoch 0027] loss=11.9186 cls=0.0330 smmd=0.1470 ct=9.4023 rec=1.1682 | train/val/test=1.000/0.736/0.739 | c=0.998347
[Epoch 0028] loss=11.9008 cls=0.0308 smmd=0.1369 ct=9.4031 rec=1.1650 | train/val/test=1.000/0.738/0.740 | c=0.998347
[Epoch 0029] loss=11.8852 cls=0.0286 smmd=0.1362 ct=9.3962 rec=1.1621 | train/val/test=1.000/0.734/0.738 | c=0.998347
[Epoch 0030] loss=11.8423 cls=0.0242 smmd=0.1103 ct=9.3874 rec=1.1602 | train/val/test=1.000/0.736/0.738 | c=0.998347
[Epoch 0031] loss=11.8322 cls=0.0215 smmd=0.1092 ct=9.3826 rec=1.1595 | train/val/test=1.000/0.732/0.736 | c=0.998347
[Epoch 0032] loss=11.8262 cls=0.0177 smmd=0.1113 ct=9.3804 rec=1.1584 | train/val/test=1.000/0.728/0.733 | c=0.998347
[Epoch 0033] loss=11.8070 cls=0.0172 smmd=0.0826 ct=9.3880 rec=1.1596 | train/val/test=1.000/0.726/0.732 | c=0.998347
[Epoch 0034] loss=11.8068 cls=0.0161 smmd=0.0920 ct=9.3803 rec=1.1592 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0035] loss=11.8068 cls=0.0164 smmd=0.0843 ct=9.3833 rec=1.1614 | train/val/test=1.000/0.718/0.730 | c=0.998347
[Epoch 0036] loss=11.7932 cls=0.0165 smmd=0.0807 ct=9.3755 rec=1.1603 | train/val/test=1.000/0.716/0.726 | c=0.998347
[Epoch 0037] loss=11.7757 cls=0.0144 smmd=0.0591 ct=9.3830 rec=1.1596 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0038] loss=11.7538 cls=0.0135 smmd=0.0543 ct=9.3714 rec=1.1572 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0039] loss=11.7434 cls=0.0136 smmd=0.0481 ct=9.3679 rec=1.1569 | train/val/test=1.000/0.724/0.728 | c=0.998347
[Epoch 0040] loss=11.7460 cls=0.0152 smmd=0.0420 ct=9.3734 rec=1.1577 | train/val/test=1.000/0.722/0.722 | c=0.998347
[Epoch 0041] loss=11.7480 cls=0.0175 smmd=0.0436 ct=9.3681 rec=1.1594 | train/val/test=1.000/0.732/0.733 | c=0.998347
[Epoch 0042] loss=11.7474 cls=0.0192 smmd=0.0327 ct=9.3755 rec=1.1599 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0043] loss=11.7487 cls=0.0197 smmd=0.0430 ct=9.3663 rec=1.1599 | train/val/test=1.000/0.738/0.741 | c=0.998347
[Epoch 0044] loss=11.7393 cls=0.0209 smmd=0.0304 ct=9.3685 rec=1.1598 | train/val/test=1.000/0.730/0.728 | c=0.998347
[Epoch 0045] loss=11.7280 cls=0.0196 smmd=0.0239 ct=9.3658 rec=1.1593 | train/val/test=1.000/0.738/0.738 | c=0.998347
[Epoch 0046] loss=11.7192 cls=0.0200 smmd=0.0180 ct=9.3645 rec=1.1584 | train/val/test=1.000/0.728/0.730 | c=0.998347
[Epoch 0047] loss=11.7096 cls=0.0169 smmd=0.0217 ct=9.3564 rec=1.1573 | train/val/test=1.000/0.730/0.731 | c=0.998347
[Epoch 0048] loss=11.7085 cls=0.0168 smmd=0.0176 ct=9.3610 rec=1.1565 | train/val/test=1.000/0.732/0.732 | c=0.998347
[Epoch 0049] loss=11.7062 cls=0.0166 smmd=0.0147 ct=9.3602 rec=1.1573 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0050] loss=11.6981 cls=0.0163 smmd=0.0109 ct=9.3573 rec=1.1568 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0051] loss=11.6927 cls=0.0162 smmd=0.0078 ct=9.3551 rec=1.1568 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0052] loss=11.6966 cls=0.0167 smmd=0.0106 ct=9.3537 rec=1.1578 | train/val/test=1.000/0.728/0.722 | c=0.998347
[Epoch 0053] loss=11.7027 cls=0.0181 smmd=0.0077 ct=9.3593 rec=1.1588 | train/val/test=1.000/0.726/0.730 | c=0.998347
[Epoch 0054] loss=11.6994 cls=0.0182 smmd=0.0067 ct=9.3544 rec=1.1601 | train/val/test=1.000/0.730/0.723 | c=0.998347
[Epoch 0055] loss=11.6984 cls=0.0201 smmd=0.0029 ct=9.3557 rec=1.1598 | train/val/test=1.000/0.724/0.728 | c=0.998347
[Epoch 0056] loss=11.6968 cls=0.0181 smmd=0.0100 ct=9.3497 rec=1.1596 | train/val/test=1.000/0.730/0.731 | c=0.998347
[Epoch 0057] loss=11.6872 cls=0.0197 smmd=-0.0037 ct=9.3543 rec=1.1584 | train/val/test=1.000/0.728/0.728 | c=0.998347
[Epoch 0058] loss=11.6909 cls=0.0182 smmd=0.0043 ct=9.3516 rec=1.1584 | train/val/test=1.000/0.736/0.730 | c=0.998347
[Epoch 0059] loss=11.6921 cls=0.0194 smmd=0.0019 ct=9.3521 rec=1.1593 | train/val/test=1.000/0.728/0.725 | c=0.998347
[Epoch 0060] loss=11.6992 cls=0.0198 smmd=0.0088 ct=9.3509 rec=1.1599 | train/val/test=1.000/0.740/0.733 | c=0.998347
[Epoch 0061] loss=11.7063 cls=0.0208 smmd=0.0090 ct=9.3536 rec=1.1615 | train/val/test=1.000/0.724/0.722 | c=0.998347
[Epoch 0062] loss=11.7074 cls=0.0204 smmd=0.0140 ct=9.3495 rec=1.1617 | train/val/test=1.000/0.740/0.729 | c=0.998347
[Epoch 0063] loss=11.7131 cls=0.0218 smmd=0.0083 ct=9.3584 rec=1.1623 | train/val/test=1.000/0.724/0.724 | c=0.998347
[Epoch 0064] loss=11.7188 cls=0.0210 smmd=0.0199 ct=9.3500 rec=1.1639 | train/val/test=1.000/0.736/0.721 | c=0.998347
[Epoch 0065] loss=11.7344 cls=0.0238 smmd=0.0205 ct=9.3639 rec=1.1631 | train/val/test=1.000/0.722/0.725 | c=0.998347
[Epoch 0066] loss=11.7170 cls=0.0187 smmd=0.0270 ct=9.3468 rec=1.1622 | train/val/test=1.000/0.736/0.727 | c=0.998347
[Epoch 0067] loss=11.6805 cls=0.0160 smmd=0.0042 ct=9.3469 rec=1.1567 | train/val/test=1.000/0.732/0.728 | c=0.998347
[Epoch 0068] loss=11.6754 cls=0.0154 smmd=-0.0009 ct=9.3477 rec=1.1566 | train/val/test=1.000/0.724/0.722 | c=0.998347
[Epoch 0069] loss=11.7002 cls=0.0170 smmd=0.0158 ct=9.3462 rec=1.1606 | train/val/test=1.000/0.738/0.724 | c=0.998347
[Epoch 0070] loss=11.7021 cls=0.0199 smmd=0.0065 ct=9.3532 rec=1.1612 | train/val/test=1.000/0.728/0.723 | c=0.998347
[Epoch 0071] loss=11.6815 cls=0.0178 smmd=0.0008 ct=9.3418 rec=1.1605 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0072] loss=11.6835 cls=0.0183 smmd=-0.0006 ct=9.3422 rec=1.1618 | train/val/test=1.000/0.738/0.717 | c=0.998347
[Epoch 0073] loss=11.6988 cls=0.0220 smmd=0.0007 ct=9.3489 rec=1.1636 | train/val/test=1.000/0.730/0.726 | c=0.998347
[Epoch 0074] loss=11.7049 cls=0.0216 smmd=0.0071 ct=9.3446 rec=1.1658 | train/val/test=1.000/0.736/0.719 | c=0.998347
[Epoch 0075] loss=11.6933 cls=0.0223 smmd=-0.0010 ct=9.3464 rec=1.1628 | train/val/test=1.000/0.730/0.722 | c=0.998347
[Epoch 0076] loss=11.6753 cls=0.0194 smmd=-0.0043 ct=9.3380 rec=1.1611 | train/val/test=1.000/0.736/0.724 | c=0.998347
[Epoch 0077] loss=11.6754 cls=0.0194 smmd=-0.0047 ct=9.3394 rec=1.1606 | train/val/test=1.000/0.736/0.718 | c=0.998347
[Epoch 0078] loss=11.6826 cls=0.0199 smmd=0.0031 ct=9.3402 rec=1.1597 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0079] loss=11.6778 cls=0.0186 smmd=0.0002 ct=9.3382 rec=1.1604 | train/val/test=1.000/0.734/0.722 | c=0.998347
[Epoch 0080] loss=11.6648 cls=0.0182 smmd=-0.0088 ct=9.3371 rec=1.1591 | train/val/test=1.000/0.732/0.729 | c=0.998347
[Epoch 0081] loss=11.6658 cls=0.0188 smmd=-0.0055 ct=9.3333 rec=1.1596 | train/val/test=1.000/0.734/0.726 | c=0.998347
[Epoch 0082] loss=11.6734 cls=0.0189 smmd=-0.0021 ct=9.3332 rec=1.1617 | train/val/test=1.000/0.732/0.721 | c=0.998347
[Epoch 0083] loss=11.6743 cls=0.0203 smmd=-0.0053 ct=9.3352 rec=1.1621 | train/val/test=1.000/0.736/0.728 | c=0.998347
[Epoch 0084] loss=11.6715 cls=0.0201 smmd=-0.0075 ct=9.3329 rec=1.1630 | train/val/test=1.000/0.730/0.720 | c=0.998347
[Epoch 0085] loss=11.6704 cls=0.0206 smmd=-0.0082 ct=9.3305 rec=1.1637 | train/val/test=1.000/0.742/0.729 | c=0.998347
[Epoch 0086] loss=11.6858 cls=0.0237 smmd=-0.0028 ct=9.3354 rec=1.1648 | train/val/test=1.000/0.722/0.717 | c=0.998347
[Epoch 0087] loss=11.7085 cls=0.0239 smmd=0.0142 ct=9.3343 rec=1.1680 | train/val/test=1.000/0.732/0.726 | c=0.998347
[Epoch 0088] loss=11.7662 cls=0.0357 smmd=0.0243 ct=9.3584 rec=1.1739 | train/val/test=1.000/0.704/0.703 | c=0.998347
[Epoch 0089] loss=11.8545 cls=0.0402 smmd=0.0866 ct=9.3581 rec=1.1848 | train/val/test=1.000/0.720/0.716 | c=0.998347
[Epoch 0090] loss=11.9193 cls=0.0513 smmd=0.1069 ct=9.3894 rec=1.1859 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0091] loss=11.7212 cls=0.0148 smmd=0.0486 ct=9.3389 rec=1.1595 | train/val/test=1.000/0.722/0.721 | c=0.998347
[Epoch 0092] loss=11.7165 cls=0.0114 smmd=0.0552 ct=9.3354 rec=1.1572 | train/val/test=1.000/0.742/0.727 | c=0.998347
[Epoch 0093] loss=11.7626 cls=0.0126 smmd=0.0673 ct=9.3625 rec=1.1601 | train/val/test=1.000/0.740/0.722 | c=0.998347
[Epoch 0094] loss=11.6813 cls=0.0082 smmd=0.0297 ct=9.3390 rec=1.1522 | train/val/test=1.000/0.708/0.711 | c=0.998347
[Epoch 0095] loss=11.7693 cls=0.0147 smmd=0.0858 ct=9.3449 rec=1.1619 | train/val/test=1.000/0.734/0.726 | c=0.998347
[Epoch 0096] loss=11.6670 cls=0.0090 smmd=0.0162 ct=9.3308 rec=1.1555 | train/val/test=1.000/0.742/0.728 | c=0.998347
[Epoch 0097] loss=11.7335 cls=0.0146 smmd=0.0343 ct=9.3539 rec=1.1653 | train/val/test=1.000/0.716/0.719 | c=0.998347
[Epoch 0098] loss=11.7112 cls=0.0174 smmd=0.0312 ct=9.3330 rec=1.1648 | train/val/test=1.000/0.722/0.724 | c=0.998347
[Epoch 0099] loss=11.6708 cls=0.0165 smmd=-0.0024 ct=9.3284 rec=1.1641 | train/val/test=1.000/0.742/0.730 | c=0.998347
=== Best @ epoch 85: val=0.7420, test=0.7290 ===
[2025-09-20 04:56:07] END attempt 1: exit_code=0
