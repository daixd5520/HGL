[2025-09-20 04:55:02] START attempt 1: python main.py --is_transfer True --test_dataset Cora --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth --few True --shot 5 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.2035 cls=1.9461 smmd=4.1925 ct=9.2871 rec=1.3889 | train/val/test=0.172/0.306/0.321 | c=0.998347
[Epoch 0001] loss=16.6700 cls=1.9214 smmd=2.7237 ct=9.2446 rec=1.3902 | train/val/test=0.276/0.136/0.155 | c=0.998347
[Epoch 0002] loss=15.3036 cls=1.7895 smmd=1.5491 ct=9.1874 rec=1.3888 | train/val/test=0.793/0.242/0.269 | c=0.998347
[Epoch 0003] loss=14.8809 cls=1.5325 smmd=1.4812 ct=9.0911 rec=1.3881 | train/val/test=0.828/0.512/0.514 | c=0.998347
[Epoch 0004] loss=14.5990 cls=1.1900 smmd=1.6698 ct=8.9744 rec=1.3824 | train/val/test=0.862/0.420/0.439 | c=0.998347
[Epoch 0005] loss=14.1004 cls=0.8484 smmd=1.6369 ct=8.8798 rec=1.3677 | train/val/test=0.931/0.562/0.568 | c=0.998347
[Epoch 0006] loss=13.4273 cls=0.5715 smmd=1.3475 ct=8.8282 rec=1.3401 | train/val/test=0.966/0.594/0.588 | c=0.998347
[Epoch 0007] loss=12.8958 cls=0.3848 smmd=1.0743 ct=8.8166 rec=1.3100 | train/val/test=0.966/0.566/0.575 | c=0.998347
[Epoch 0008] loss=12.6238 cls=0.2390 smmd=1.0031 ct=8.8131 rec=1.2843 | train/val/test=1.000/0.566/0.582 | c=0.998347
[Epoch 0009] loss=12.5526 cls=0.1293 smmd=1.0868 ct=8.8114 rec=1.2625 | train/val/test=1.000/0.602/0.600 | c=0.998347
[Epoch 0010] loss=12.4592 cls=0.0610 smmd=1.1074 ct=8.8073 rec=1.2417 | train/val/test=1.000/0.614/0.611 | c=0.998347
[Epoch 0011] loss=12.2675 cls=0.0312 smmd=0.9828 ct=8.8022 rec=1.2257 | train/val/test=1.000/0.610/0.613 | c=0.998347
[Epoch 0012] loss=12.0572 cls=0.0196 smmd=0.8170 ct=8.7930 rec=1.2138 | train/val/test=1.000/0.614/0.616 | c=0.998347
[Epoch 0013] loss=11.9627 cls=0.0159 smmd=0.7495 ct=8.7866 rec=1.2053 | train/val/test=1.000/0.618/0.617 | c=0.998347
[Epoch 0014] loss=11.8838 cls=0.0135 smmd=0.6921 ct=8.7835 rec=1.1973 | train/val/test=1.000/0.628/0.630 | c=0.998347
[Epoch 0015] loss=11.8473 cls=0.0108 smmd=0.6703 ct=8.7849 rec=1.1906 | train/val/test=1.000/0.642/0.640 | c=0.998347
[Epoch 0016] loss=11.7880 cls=0.0093 smmd=0.6176 ct=8.7883 rec=1.1864 | train/val/test=1.000/0.638/0.639 | c=0.998347
[Epoch 0017] loss=11.6733 cls=0.0084 smmd=0.5066 ct=8.7908 rec=1.1837 | train/val/test=1.000/0.634/0.638 | c=0.998347
[Epoch 0018] loss=11.5910 cls=0.0070 smmd=0.4236 ct=8.7958 rec=1.1823 | train/val/test=1.000/0.636/0.644 | c=0.998347
[Epoch 0019] loss=11.5949 cls=0.0059 smmd=0.4236 ct=8.8027 rec=1.1814 | train/val/test=1.000/0.634/0.646 | c=0.998347
[Epoch 0020] loss=11.5370 cls=0.0061 smmd=0.3559 ct=8.8113 rec=1.1818 | train/val/test=1.000/0.632/0.640 | c=0.998347
[Epoch 0021] loss=11.4795 cls=0.0075 smmd=0.2856 ct=8.8202 rec=1.1831 | train/val/test=1.000/0.622/0.642 | c=0.998347
[Epoch 0022] loss=11.4340 cls=0.0099 smmd=0.2266 ct=8.8288 rec=1.1844 | train/val/test=1.000/0.622/0.644 | c=0.998347
[Epoch 0023] loss=11.4230 cls=0.0135 smmd=0.2064 ct=8.8337 rec=1.1847 | train/val/test=1.000/0.622/0.645 | c=0.998347
[Epoch 0024] loss=11.4063 cls=0.0176 smmd=0.1874 ct=8.8346 rec=1.1833 | train/val/test=1.000/0.622/0.648 | c=0.998347
[Epoch 0025] loss=12.0918 cls=0.0227 smmd=0.1768 ct=9.5299 rec=1.1812 | train/val/test=1.000/0.630/0.640 | c=0.998347
[Epoch 0026] loss=12.0521 cls=0.0342 smmd=0.2198 ct=9.4385 rec=1.1798 | train/val/test=1.000/0.632/0.644 | c=0.998347
[Epoch 0027] loss=12.0569 cls=0.0278 smmd=0.2309 ct=9.4469 rec=1.1757 | train/val/test=1.000/0.630/0.652 | c=0.998347
[Epoch 0028] loss=12.0272 cls=0.0311 smmd=0.1787 ct=9.4698 rec=1.1738 | train/val/test=1.000/0.632/0.645 | c=0.998347
[Epoch 0029] loss=11.9813 cls=0.0246 smmd=0.1366 ct=9.4845 rec=1.1678 | train/val/test=1.000/0.632/0.652 | c=0.998347
[Epoch 0030] loss=11.9502 cls=0.0203 smmd=0.1340 ct=9.4651 rec=1.1654 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0031] loss=11.9315 cls=0.0184 smmd=0.1370 ct=9.4481 rec=1.1640 | train/val/test=1.000/0.632/0.651 | c=0.998347
[Epoch 0032] loss=11.9076 cls=0.0162 smmd=0.1168 ct=9.4512 rec=1.1617 | train/val/test=1.000/0.632/0.654 | c=0.998347
[Epoch 0033] loss=11.8790 cls=0.0140 smmd=0.1001 ct=9.4433 rec=1.1608 | train/val/test=1.000/0.634/0.657 | c=0.998347
[Epoch 0034] loss=11.8657 cls=0.0124 smmd=0.0977 ct=9.4336 rec=1.1610 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0035] loss=11.8598 cls=0.0116 smmd=0.0938 ct=9.4332 rec=1.1606 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0036] loss=11.8319 cls=0.0105 smmd=0.0644 ct=9.4344 rec=1.1613 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0037] loss=11.8298 cls=0.0102 smmd=0.0699 ct=9.4253 rec=1.1622 | train/val/test=1.000/0.634/0.653 | c=0.998347
[Epoch 0038] loss=11.8163 cls=0.0104 smmd=0.0579 ct=9.4223 rec=1.1628 | train/val/test=1.000/0.638/0.652 | c=0.998347
[Epoch 0039] loss=11.8128 cls=0.0114 smmd=0.0512 ct=9.4224 rec=1.1639 | train/val/test=1.000/0.636/0.648 | c=0.998347
[Epoch 0040] loss=11.8076 cls=0.0120 smmd=0.0491 ct=9.4183 rec=1.1641 | train/val/test=1.000/0.642/0.652 | c=0.998347
[Epoch 0041] loss=11.7903 cls=0.0129 smmd=0.0367 ct=9.4123 rec=1.1642 | train/val/test=1.000/0.638/0.651 | c=0.998347
[Epoch 0042] loss=11.7858 cls=0.0137 smmd=0.0379 ct=9.4066 rec=1.1638 | train/val/test=1.000/0.638/0.651 | c=0.998347
[Epoch 0043] loss=11.7829 cls=0.0148 smmd=0.0374 ct=9.4036 rec=1.1635 | train/val/test=1.000/0.640/0.654 | c=0.998347
[Epoch 0044] loss=11.7635 cls=0.0143 smmd=0.0147 ct=9.4079 rec=1.1633 | train/val/test=1.000/0.634/0.652 | c=0.998347
[Epoch 0045] loss=11.7627 cls=0.0150 smmd=0.0216 ct=9.4017 rec=1.1622 | train/val/test=1.000/0.640/0.653 | c=0.998347
[Epoch 0046] loss=11.7562 cls=0.0142 smmd=0.0221 ct=9.3970 rec=1.1614 | train/val/test=1.000/0.640/0.655 | c=0.998347
[Epoch 0047] loss=11.7491 cls=0.0139 smmd=0.0215 ct=9.3919 rec=1.1608 | train/val/test=1.000/0.638/0.652 | c=0.998347
[Epoch 0048] loss=11.7437 cls=0.0139 smmd=0.0205 ct=9.3884 rec=1.1605 | train/val/test=1.000/0.638/0.657 | c=0.998347
[Epoch 0049] loss=11.7423 cls=0.0135 smmd=0.0166 ct=9.3912 rec=1.1605 | train/val/test=1.000/0.638/0.653 | c=0.998347
[Epoch 0050] loss=11.7316 cls=0.0133 smmd=0.0098 ct=9.3873 rec=1.1606 | train/val/test=1.000/0.638/0.655 | c=0.998347
[Epoch 0051] loss=11.7336 cls=0.0130 smmd=0.0128 ct=9.3856 rec=1.1611 | train/val/test=1.000/0.640/0.656 | c=0.998347
[Epoch 0052] loss=11.7286 cls=0.0130 smmd=0.0089 ct=9.3838 rec=1.1615 | train/val/test=1.000/0.640/0.655 | c=0.998347
[Epoch 0053] loss=11.7291 cls=0.0131 smmd=0.0088 ct=9.3831 rec=1.1620 | train/val/test=1.000/0.638/0.655 | c=0.998347
[Epoch 0054] loss=11.7224 cls=0.0132 smmd=0.0025 ct=9.3816 rec=1.1626 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0055] loss=11.7226 cls=0.0134 smmd=-0.0007 ct=9.3840 rec=1.1629 | train/val/test=1.000/0.638/0.656 | c=0.998347
[Epoch 0056] loss=11.7215 cls=0.0133 smmd=0.0005 ct=9.3814 rec=1.1632 | train/val/test=1.000/0.632/0.653 | c=0.998347
[Epoch 0057] loss=11.7249 cls=0.0136 smmd=0.0076 ct=9.3774 rec=1.1631 | train/val/test=1.000/0.638/0.657 | c=0.998347
[Epoch 0058] loss=11.7257 cls=0.0133 smmd=0.0068 ct=9.3793 rec=1.1632 | train/val/test=1.000/0.632/0.652 | c=0.998347
[Epoch 0059] loss=11.7244 cls=0.0139 smmd=0.0015 ct=9.3824 rec=1.1633 | train/val/test=1.000/0.642/0.660 | c=0.998347
[Epoch 0060] loss=11.7283 cls=0.0135 smmd=0.0095 ct=9.3783 rec=1.1635 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0061] loss=11.7300 cls=0.0144 smmd=0.0061 ct=9.3819 rec=1.1638 | train/val/test=1.000/0.638/0.659 | c=0.998347
[Epoch 0062] loss=11.7365 cls=0.0143 smmd=0.0204 ct=9.3740 rec=1.1640 | train/val/test=1.000/0.636/0.654 | c=0.998347
[Epoch 0063] loss=11.7411 cls=0.0146 smmd=0.0132 ct=9.3843 rec=1.1645 | train/val/test=1.000/0.634/0.654 | c=0.998347
[Epoch 0064] loss=11.7334 cls=0.0147 smmd=0.0171 ct=9.3728 rec=1.1644 | train/val/test=1.000/0.632/0.657 | c=0.998347
[Epoch 0065] loss=11.7253 cls=0.0137 smmd=0.0015 ct=9.3821 rec=1.1640 | train/val/test=1.000/0.638/0.655 | c=0.998347
[Epoch 0066] loss=11.7132 cls=0.0135 smmd=0.0041 ct=9.3704 rec=1.1626 | train/val/test=1.000/0.636/0.658 | c=0.998347
[Epoch 0067] loss=11.7055 cls=0.0126 smmd=-0.0043 ct=9.3728 rec=1.1622 | train/val/test=1.000/0.634/0.654 | c=0.998347
[Epoch 0068] loss=11.7097 cls=0.0131 smmd=-0.0016 ct=9.3727 rec=1.1627 | train/val/test=1.000/0.638/0.657 | c=0.998347
[Epoch 0069] loss=11.7200 cls=0.0140 smmd=0.0112 ct=9.3668 rec=1.1641 | train/val/test=1.000/0.638/0.655 | c=0.998347
[Epoch 0070] loss=11.7302 cls=0.0141 smmd=0.0053 ct=9.3796 rec=1.1656 | train/val/test=1.000/0.642/0.656 | c=0.998347
[Epoch 0071] loss=11.7328 cls=0.0151 smmd=0.0143 ct=9.3712 rec=1.1661 | train/val/test=1.000/0.636/0.654 | c=0.998347
[Epoch 0072] loss=11.7325 cls=0.0146 smmd=0.0084 ct=9.3778 rec=1.1658 | train/val/test=1.000/0.640/0.661 | c=0.998347
[Epoch 0073] loss=11.7197 cls=0.0139 smmd=0.0067 ct=9.3696 rec=1.1647 | train/val/test=1.000/0.634/0.653 | c=0.998347
[Epoch 0074] loss=11.7095 cls=0.0141 smmd=0.0001 ct=9.3685 rec=1.1634 | train/val/test=1.000/0.640/0.658 | c=0.998347
[Epoch 0075] loss=11.7048 cls=0.0127 smmd=-0.0015 ct=9.3672 rec=1.1632 | train/val/test=1.000/0.636/0.652 | c=0.998347
[Epoch 0076] loss=11.7027 cls=0.0139 smmd=0.0026 ct=9.3609 rec=1.1627 | train/val/test=1.000/0.636/0.657 | c=0.998347
[Epoch 0077] loss=11.7088 cls=0.0134 smmd=-0.0024 ct=9.3700 rec=1.1639 | train/val/test=1.000/0.640/0.658 | c=0.998347
[Epoch 0078] loss=11.7104 cls=0.0144 smmd=0.0041 ct=9.3631 rec=1.1644 | train/val/test=1.000/0.634/0.654 | c=0.998347
[Epoch 0079] loss=11.7090 cls=0.0147 smmd=-0.0015 ct=9.3660 rec=1.1649 | train/val/test=1.000/0.642/0.661 | c=0.998347
[Epoch 0080] loss=11.7091 cls=0.0143 smmd=0.0028 ct=9.3617 rec=1.1652 | train/val/test=1.000/0.632/0.655 | c=0.998347
[Epoch 0081] loss=11.7021 cls=0.0147 smmd=-0.0038 ct=9.3616 rec=1.1648 | train/val/test=1.000/0.640/0.660 | c=0.998347
[Epoch 0082] loss=11.7006 cls=0.0138 smmd=-0.0053 ct=9.3619 rec=1.1651 | train/val/test=1.000/0.636/0.652 | c=0.998347
[Epoch 0083] loss=11.6976 cls=0.0146 smmd=-0.0050 ct=9.3583 rec=1.1649 | train/val/test=1.000/0.636/0.659 | c=0.998347
[Epoch 0084] loss=11.7054 cls=0.0140 smmd=-0.0017 ct=9.3613 rec=1.1659 | train/val/test=1.000/0.632/0.652 | c=0.998347
[Epoch 0085] loss=11.7155 cls=0.0156 smmd=0.0099 ct=9.3578 rec=1.1661 | train/val/test=1.000/0.634/0.658 | c=0.998347
[Epoch 0086] loss=11.7136 cls=0.0143 smmd=0.0050 ct=9.3611 rec=1.1666 | train/val/test=1.000/0.632/0.652 | c=0.998347
[Epoch 0087] loss=11.7091 cls=0.0155 smmd=0.0053 ct=9.3567 rec=1.1658 | train/val/test=1.000/0.636/0.656 | c=0.998347
[Epoch 0088] loss=11.7018 cls=0.0130 smmd=0.0014 ct=9.3586 rec=1.1644 | train/val/test=1.000/0.636/0.656 | c=0.998347
[Epoch 0089] loss=11.6859 cls=0.0129 smmd=-0.0004 ct=9.3481 rec=1.1626 | train/val/test=1.000/0.634/0.655 | c=0.998347
[Epoch 0090] loss=11.6922 cls=0.0127 smmd=-0.0015 ct=9.3551 rec=1.1629 | train/val/test=1.000/0.632/0.657 | c=0.998347
[Epoch 0091] loss=11.6927 cls=0.0131 smmd=-0.0022 ct=9.3531 rec=1.1643 | train/val/test=1.000/0.626/0.653 | c=0.998347
[Epoch 0092] loss=11.6982 cls=0.0143 smmd=-0.0012 ct=9.3549 rec=1.1651 | train/val/test=1.000/0.632/0.659 | c=0.998347
[Epoch 0093] loss=11.7055 cls=0.0133 smmd=0.0091 ct=9.3519 rec=1.1656 | train/val/test=1.000/0.630/0.654 | c=0.998347
[Epoch 0094] loss=11.6897 cls=0.0141 smmd=-0.0013 ct=9.3482 rec=1.1644 | train/val/test=1.000/0.636/0.662 | c=0.998347
[Epoch 0095] loss=11.6822 cls=0.0127 smmd=-0.0080 ct=9.3497 rec=1.1640 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0096] loss=11.6786 cls=0.0135 smmd=-0.0069 ct=9.3446 rec=1.1637 | train/val/test=1.000/0.634/0.658 | c=0.998347
[Epoch 0097] loss=11.6817 cls=0.0138 smmd=-0.0062 ct=9.3448 rec=1.1646 | train/val/test=1.000/0.634/0.655 | c=0.998347
[Epoch 0098] loss=11.6849 cls=0.0141 smmd=-0.0039 ct=9.3432 rec=1.1658 | train/val/test=1.000/0.628/0.654 | c=0.998347
[Epoch 0099] loss=11.6868 cls=0.0155 smmd=-0.0070 ct=9.3453 rec=1.1665 | train/val/test=1.000/0.634/0.662 | c=0.998347
=== Best @ epoch 15: val=0.6420, test=0.6400 ===
[2025-09-20 04:55:28] END attempt 1: exit_code=0
