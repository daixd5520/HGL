[2025-09-20 04:41:04] START attempt 1: python main.py --is_transfer True --test_dataset PubMed --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth --few False --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.4497 cls=1.0898 smmd=5.6704 ct=7.2468 rec=1.4137 | train/val/test=0.458/0.439/0.441 | c=0.998347
[Epoch 0001] loss=53.4160 cls=1.0601 smmd=3.7806 ct=7.1858 rec=1.4158 | train/val/test=0.592/0.591/0.581 | c=0.998347
[Epoch 0002] loss=37.2804 cls=1.0611 smmd=2.1851 ct=7.0959 rec=1.4139 | train/val/test=0.584/0.577/0.563 | c=0.998347
[Epoch 0003] loss=40.6631 cls=1.0312 smmd=2.5302 ct=7.0694 rec=1.4136 | train/val/test=0.566/0.558/0.547 | c=0.998347
[Epoch 0004] loss=41.4354 cls=0.9785 smmd=2.4887 ct=7.6766 rec=1.4127 | train/val/test=0.561/0.553/0.542 | c=0.998347
[Epoch 0005] loss=36.3932 cls=0.9251 smmd=2.0400 ct=7.4132 rec=1.4087 | train/val/test=0.571/0.563/0.556 | c=0.998347
[Epoch 0006] loss=31.9304 cls=0.8939 smmd=1.6008 ct=7.3871 rec=1.4019 | train/val/test=0.570/0.561/0.549 | c=0.998347
[Epoch 0007] loss=34.7917 cls=0.8470 smmd=1.8803 ct=7.4350 rec=1.3900 | train/val/test=0.581/0.573/0.560 | c=0.998347
[Epoch 0008] loss=35.5226 cls=0.8002 smmd=1.9544 ct=7.4447 rec=1.3774 | train/val/test=0.666/0.667/0.655 | c=0.998347
[Epoch 0009] loss=29.3768 cls=0.7612 smmd=1.3401 ct=7.4563 rec=1.3659 | train/val/test=0.697/0.700/0.688 | c=0.998347
[Epoch 0010] loss=28.2989 cls=0.7294 smmd=1.2240 ct=7.5072 rec=1.3607 | train/val/test=0.682/0.679/0.673 | c=0.998347
[Epoch 0011] loss=30.0827 cls=0.7034 smmd=1.4075 ct=7.4882 rec=1.3594 | train/val/test=0.717/0.719/0.710 | c=0.998347
[Epoch 0012] loss=27.6520 cls=0.6738 smmd=1.1735 ct=7.4507 rec=1.3566 | train/val/test=0.765/0.772/0.751 | c=0.998347
[Epoch 0013] loss=26.5864 cls=0.6414 smmd=1.0628 ct=7.4812 rec=1.3506 | train/val/test=0.769/0.779/0.757 | c=0.998347
[Epoch 0014] loss=25.4150 cls=0.6332 smmd=0.9412 ct=7.5078 rec=1.3424 | train/val/test=0.774/0.780/0.759 | c=0.998347
[Epoch 0015] loss=25.7049 cls=0.6128 smmd=0.9810 ct=7.4594 rec=1.3396 | train/val/test=0.783/0.787/0.769 | c=0.998347
[Epoch 0016] loss=24.8432 cls=0.5882 smmd=0.8994 ct=7.4425 rec=1.3400 | train/val/test=0.807/0.813/0.791 | c=0.998347
[Epoch 0017] loss=23.4403 cls=0.5475 smmd=0.7581 ct=7.4597 rec=1.3317 | train/val/test=0.801/0.811/0.795 | c=0.998347
[Epoch 0018] loss=23.2432 cls=0.5498 smmd=0.7307 ct=7.4982 rec=1.3291 | train/val/test=0.803/0.814/0.792 | c=0.998347
[Epoch 0019] loss=22.8684 cls=0.5419 smmd=0.6958 ct=7.4891 rec=1.3215 | train/val/test=0.814/0.821/0.794 | c=0.998347
[Epoch 0020] loss=22.2168 cls=0.5170 smmd=0.6383 ct=7.4581 rec=1.3185 | train/val/test=0.815/0.820/0.797 | c=0.998347
[Epoch 0021] loss=21.4557 cls=0.5046 smmd=0.5651 ct=7.4467 rec=1.3185 | train/val/test=0.819/0.823/0.802 | c=0.998347
[Epoch 0022] loss=21.7175 cls=0.4958 smmd=0.5924 ct=7.4434 rec=1.3168 | train/val/test=0.824/0.828/0.808 | c=0.998347
[Epoch 0023] loss=20.9842 cls=0.4916 smmd=0.5171 ct=7.4551 rec=1.3142 | train/val/test=0.823/0.830/0.808 | c=0.998347
[Epoch 0024] loss=20.5514 cls=0.4979 smmd=0.4685 ct=7.4798 rec=1.3159 | train/val/test=0.826/0.830/0.812 | c=0.998347
[Epoch 0025] loss=20.5271 cls=0.4791 smmd=0.4686 ct=7.4722 rec=1.3137 | train/val/test=0.823/0.828/0.808 | c=0.998347
[Epoch 0026] loss=20.0378 cls=0.4787 smmd=0.4231 ct=7.4546 rec=1.3160 | train/val/test=0.831/0.835/0.818 | c=0.998347
[Epoch 0027] loss=19.8471 cls=0.4698 smmd=0.4063 ct=7.4462 rec=1.3145 | train/val/test=0.834/0.835/0.820 | c=0.998347
[Epoch 0028] loss=19.6281 cls=0.4667 smmd=0.3820 ct=7.4591 rec=1.3125 | train/val/test=0.835/0.838/0.821 | c=0.998347
[Epoch 0029] loss=19.3344 cls=0.4623 smmd=0.3512 ct=7.4676 rec=1.3114 | train/val/test=0.836/0.840/0.823 | c=0.998347
[Epoch 0030] loss=19.2050 cls=0.4544 smmd=0.3403 ct=7.4596 rec=1.3120 | train/val/test=0.837/0.841/0.826 | c=0.998347
[Epoch 0031] loss=18.8988 cls=0.4498 smmd=0.3133 ct=7.4425 rec=1.3111 | train/val/test=0.840/0.844/0.824 | c=0.998347
[Epoch 0032] loss=18.7709 cls=0.4513 smmd=0.2995 ct=7.4478 rec=1.3085 | train/val/test=0.838/0.845/0.826 | c=0.998347
[Epoch 0033] loss=18.6204 cls=0.4516 smmd=0.2826 ct=7.4577 rec=1.3070 | train/val/test=0.840/0.843/0.828 | c=0.998347
[Epoch 0034] loss=18.3653 cls=0.4432 smmd=0.2603 ct=7.4434 rec=1.3086 | train/val/test=0.839/0.843/0.827 | c=0.998347
[Epoch 0035] loss=18.3293 cls=0.4434 smmd=0.2579 ct=7.4366 rec=1.3103 | train/val/test=0.840/0.844/0.825 | c=0.998347
[Epoch 0036] loss=18.0918 cls=0.4467 smmd=0.2332 ct=7.4414 rec=1.3080 | train/val/test=0.844/0.850/0.830 | c=0.998347
[Epoch 0037] loss=18.0841 cls=0.4512 smmd=0.2300 ct=7.4514 rec=1.3107 | train/val/test=0.837/0.840/0.824 | c=0.998347
[Epoch 0038] loss=17.9672 cls=0.4499 smmd=0.2199 ct=7.4438 rec=1.3122 | train/val/test=0.841/0.843/0.827 | c=0.998347
[Epoch 0039] loss=17.8756 cls=0.4504 smmd=0.2125 ct=7.4346 rec=1.3120 | train/val/test=0.842/0.847/0.829 | c=0.998347
[Epoch 0040] loss=17.7762 cls=0.4521 smmd=0.2006 ct=7.4435 rec=1.3152 | train/val/test=0.844/0.849/0.831 | c=0.998347
[Epoch 0041] loss=17.6146 cls=0.4539 smmd=0.1860 ct=7.4350 rec=1.3141 | train/val/test=0.843/0.848/0.831 | c=0.998347
[Epoch 0042] loss=17.5639 cls=0.4548 smmd=0.1805 ct=7.4369 rec=1.3144 | train/val/test=0.837/0.841/0.824 | c=0.998347
[Epoch 0043] loss=17.5405 cls=0.4583 smmd=0.1766 ct=7.4436 rec=1.3167 | train/val/test=0.845/0.851/0.834 | c=0.998347
[Epoch 0044] loss=17.3987 cls=0.4563 smmd=0.1668 ct=7.4223 rec=1.3166 | train/val/test=0.842/0.848/0.829 | c=0.998347
[Epoch 0045] loss=17.3709 cls=0.4566 smmd=0.1633 ct=7.4260 rec=1.3162 | train/val/test=0.843/0.849/0.830 | c=0.998347
[Epoch 0046] loss=17.2697 cls=0.4581 smmd=0.1517 ct=7.4327 rec=1.3167 | train/val/test=0.841/0.846/0.828 | c=0.998347
[Epoch 0047] loss=17.2214 cls=0.4594 smmd=0.1483 ct=7.4248 rec=1.3185 | train/val/test=0.845/0.850/0.831 | c=0.998347
[Epoch 0048] loss=17.1753 cls=0.4627 smmd=0.1441 ct=7.4218 rec=1.3181 | train/val/test=0.842/0.850/0.828 | c=0.998347
[Epoch 0049] loss=17.1955 cls=0.4627 smmd=0.1469 ct=7.4173 rec=1.3201 | train/val/test=0.846/0.851/0.833 | c=0.998347
[Epoch 0050] loss=17.1295 cls=0.4665 smmd=0.1390 ct=7.4229 rec=1.3204 | train/val/test=0.841/0.843/0.825 | c=0.998347
[Epoch 0051] loss=17.0801 cls=0.4691 smmd=0.1351 ct=7.4166 rec=1.3220 | train/val/test=0.846/0.851/0.834 | c=0.998347
[Epoch 0052] loss=17.0303 cls=0.4686 smmd=0.1311 ct=7.4117 rec=1.3230 | train/val/test=0.839/0.841/0.825 | c=0.998347
[Epoch 0053] loss=16.9724 cls=0.4739 smmd=0.1255 ct=7.4094 rec=1.3232 | train/val/test=0.842/0.851/0.827 | c=0.998347
[Epoch 0054] loss=17.0221 cls=0.4791 smmd=0.1295 ct=7.4125 rec=1.3258 | train/val/test=0.828/0.829/0.809 | c=0.998347
[Epoch 0055] loss=17.0711 cls=0.4897 smmd=0.1348 ct=7.4065 rec=1.3300 | train/val/test=0.833/0.843/0.819 | c=0.998347
[Epoch 0056] loss=17.0689 cls=0.4942 smmd=0.1333 ct=7.4118 rec=1.3301 | train/val/test=0.817/0.817/0.802 | c=0.998347
[Epoch 0057] loss=17.1058 cls=0.5037 smmd=0.1391 ct=7.3977 rec=1.3348 | train/val/test=0.822/0.834/0.812 | c=0.998347
[Epoch 0058] loss=17.1349 cls=0.5070 smmd=0.1385 ct=7.4149 rec=1.3333 | train/val/test=0.811/0.811/0.799 | c=0.998347
[Epoch 0059] loss=17.1861 cls=0.5121 smmd=0.1480 ct=7.3910 rec=1.3365 | train/val/test=0.821/0.835/0.811 | c=0.998347
[Epoch 0060] loss=17.2205 cls=0.5046 smmd=0.1486 ct=7.4079 rec=1.3327 | train/val/test=0.819/0.820/0.804 | c=0.998347
[Epoch 0061] loss=17.1023 cls=0.4987 smmd=0.1421 ct=7.3829 rec=1.3320 | train/val/test=0.837/0.846/0.825 | c=0.998347
[Epoch 0062] loss=16.8525 cls=0.4781 smmd=0.1165 ct=7.3928 rec=1.3258 | train/val/test=0.837/0.841/0.824 | c=0.998347
[Epoch 0063] loss=16.8392 cls=0.4731 smmd=0.1196 ct=7.3722 rec=1.3241 | train/val/test=0.844/0.847/0.833 | c=0.998347
[Epoch 0064] loss=16.8098 cls=0.4688 smmd=0.1154 ct=7.3795 rec=1.3251 | train/val/test=0.846/0.848/0.833 | c=0.998347
[Epoch 0065] loss=16.7509 cls=0.4758 smmd=0.1095 ct=7.3772 rec=1.3265 | train/val/test=0.837/0.838/0.822 | c=0.998347
[Epoch 0066] loss=16.7657 cls=0.4850 smmd=0.1114 ct=7.3718 rec=1.3318 | train/val/test=0.841/0.851/0.829 | c=0.998347
[Epoch 0067] loss=16.8252 cls=0.4896 smmd=0.1143 ct=7.3858 rec=1.3323 | train/val/test=0.829/0.833/0.817 | c=0.998347
[Epoch 0068] loss=16.8711 cls=0.4975 smmd=0.1214 ct=7.3699 rec=1.3364 | train/val/test=0.838/0.847/0.823 | c=0.998347
[Epoch 0069] loss=16.8874 cls=0.4967 smmd=0.1205 ct=7.3836 rec=1.3335 | train/val/test=0.826/0.826/0.809 | c=0.998347
[Epoch 0070] loss=16.9051 cls=0.4989 smmd=0.1233 ct=7.3774 rec=1.3368 | train/val/test=0.830/0.841/0.818 | c=0.998347
[Epoch 0071] loss=16.9618 cls=0.5030 smmd=0.1292 ct=7.3762 rec=1.3327 | train/val/test=0.815/0.814/0.799 | c=0.998347
[Epoch 0072] loss=16.9478 cls=0.5079 smmd=0.1267 ct=7.3796 rec=1.3361 | train/val/test=0.819/0.828/0.806 | c=0.998347
[Epoch 0073] loss=16.9328 cls=0.5086 smmd=0.1245 ct=7.3833 rec=1.3334 | train/val/test=0.817/0.819/0.803 | c=0.998347
[Epoch 0074] loss=16.8891 cls=0.5008 smmd=0.1249 ct=7.3624 rec=1.3289 | train/val/test=0.833/0.845/0.823 | c=0.998347
[Epoch 0075] loss=16.8319 cls=0.4836 smmd=0.1159 ct=7.3838 rec=1.3272 | train/val/test=0.830/0.834/0.815 | c=0.998347
[Epoch 0076] loss=16.7909 cls=0.4832 smmd=0.1181 ct=7.3532 rec=1.3231 | train/val/test=0.840/0.849/0.827 | c=0.998347
[Epoch 0077] loss=16.7038 cls=0.4736 smmd=0.1064 ct=7.3701 rec=1.3256 | train/val/test=0.838/0.844/0.822 | c=0.998347
[Epoch 0078] loss=16.6564 cls=0.4840 smmd=0.1031 ct=7.3598 rec=1.3270 | train/val/test=0.840/0.847/0.829 | c=0.998347
[Epoch 0079] loss=16.7315 cls=0.4843 smmd=0.1093 ct=7.3651 rec=1.3315 | train/val/test=0.841/0.847/0.830 | c=0.998347
[Epoch 0080] loss=16.7789 cls=0.4909 smmd=0.1138 ct=7.3649 rec=1.3322 | train/val/test=0.837/0.844/0.829 | c=0.998347
[Epoch 0081] loss=16.7905 cls=0.4897 smmd=0.1149 ct=7.3647 rec=1.3351 | train/val/test=0.843/0.849/0.829 | c=0.998347
[Epoch 0082] loss=16.7851 cls=0.4876 smmd=0.1150 ct=7.3630 rec=1.3301 | train/val/test=0.833/0.836/0.819 | c=0.998347
[Epoch 0083] loss=16.8365 cls=0.4844 smmd=0.1194 ct=7.3667 rec=1.3340 | train/val/test=0.839/0.848/0.828 | c=0.998347
[Epoch 0084] loss=16.8283 cls=0.4880 smmd=0.1196 ct=7.3630 rec=1.3251 | train/val/test=0.825/0.828/0.810 | c=0.998347
[Epoch 0085] loss=16.8136 cls=0.4824 smmd=0.1179 ct=7.3638 rec=1.3313 | train/val/test=0.835/0.843/0.820 | c=0.998347
[Epoch 0086] loss=16.8357 cls=0.4950 smmd=0.1185 ct=7.3705 rec=1.3243 | train/val/test=0.813/0.815/0.798 | c=0.998347
[Epoch 0087] loss=16.9852 cls=0.4979 smmd=0.1345 ct=7.3620 rec=1.3341 | train/val/test=0.814/0.827/0.804 | c=0.998347
[Epoch 0088] loss=16.9990 cls=0.5201 smmd=0.1305 ct=7.3840 rec=1.3325 | train/val/test=0.791/0.791/0.778 | c=0.998347
[Epoch 0089] loss=17.1279 cls=0.5394 smmd=0.1445 ct=7.3705 rec=1.3452 | train/val/test=0.779/0.784/0.770 | c=0.998347
[Epoch 0090] loss=17.2759 cls=0.5568 smmd=0.1521 ct=7.4016 rec=1.3465 | train/val/test=0.780/0.782/0.768 | c=0.998347
[Epoch 0091] loss=17.3370 cls=0.5664 smmd=0.1629 ct=7.3752 rec=1.3487 | train/val/test=0.793/0.805/0.781 | c=0.998347
[Epoch 0092] loss=17.1875 cls=0.5367 smmd=0.1449 ct=7.3989 rec=1.3454 | train/val/test=0.814/0.815/0.800 | c=0.998347
[Epoch 0093] loss=17.0174 cls=0.5094 smmd=0.1391 ct=7.3543 rec=1.3270 | train/val/test=0.838/0.850/0.825 | c=0.998347
[Epoch 0094] loss=16.7770 cls=0.4726 smmd=0.1148 ct=7.3650 rec=1.3260 | train/val/test=0.839/0.844/0.825 | c=0.998347
[Epoch 0095] loss=16.7099 cls=0.4786 smmd=0.1111 ct=7.3498 rec=1.3198 | train/val/test=0.828/0.832/0.812 | c=0.998347
[Epoch 0096] loss=16.7672 cls=0.4867 smmd=0.1143 ct=7.3575 rec=1.3321 | train/val/test=0.832/0.845/0.818 | c=0.998347
[Epoch 0097] loss=16.8879 cls=0.5118 smmd=0.1238 ct=7.3635 rec=1.3329 | train/val/test=0.807/0.804/0.787 | c=0.998347
[Epoch 0098] loss=16.9884 cls=0.5296 smmd=0.1294 ct=7.3777 rec=1.3490 | train/val/test=0.819/0.829/0.807 | c=0.998347
[Epoch 0099] loss=17.0716 cls=0.5386 smmd=0.1386 ct=7.3732 rec=1.3400 | train/val/test=0.799/0.798/0.781 | c=0.998347
=== Best @ epoch 43: val=0.8511, test=0.8337 ===
[2025-09-20 04:44:32] END attempt 1: exit_code=0
