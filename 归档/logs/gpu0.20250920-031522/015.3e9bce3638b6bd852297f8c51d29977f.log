[2025-09-20 04:03:42] START attempt 1: python main.py --is_transfer True --test_dataset Cora --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth --few True --shot 10 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1449 cls=1.9425 smmd=4.1745 ct=9.2501 rec=1.3889 | train/val/test=0.431/0.208/0.215 | c=0.998437
[Epoch 0001] loss=16.6195 cls=1.8873 smmd=2.7725 ct=9.1812 rec=1.3892 | train/val/test=0.638/0.270/0.306 | c=0.998437
[Epoch 0002] loss=15.0111 cls=1.7393 smmd=1.4548 ct=9.0399 rec=1.3885 | train/val/test=0.500/0.286/0.281 | c=0.998437
[Epoch 0003] loss=14.9435 cls=1.5623 smmd=1.4953 ct=9.1148 rec=1.3855 | train/val/test=0.776/0.342/0.358 | c=0.998437
[Epoch 0004] loss=14.7195 cls=1.2208 smmd=1.7759 ct=8.9670 rec=1.3778 | train/val/test=0.914/0.480/0.475 | c=0.998437
[Epoch 0005] loss=14.1633 cls=0.9084 smmd=1.6433 ct=8.8925 rec=1.3596 | train/val/test=0.914/0.588/0.596 | c=0.998437
[Epoch 0006] loss=13.5710 cls=0.6632 smmd=1.3857 ct=8.8530 rec=1.3346 | train/val/test=0.966/0.564/0.585 | c=0.998437
[Epoch 0007] loss=12.9443 cls=0.4433 smmd=1.0818 ct=8.8121 rec=1.3035 | train/val/test=0.966/0.556/0.568 | c=0.998437
[Epoch 0008] loss=13.3602 cls=0.2848 smmd=1.0085 ct=9.5158 rec=1.2756 | train/val/test=0.983/0.608/0.605 | c=0.998437
[Epoch 0009] loss=13.2606 cls=0.1805 smmd=1.1401 ct=9.4386 rec=1.2507 | train/val/test=0.983/0.626/0.638 | c=0.998437
[Epoch 0010] loss=13.1725 cls=0.1215 smmd=1.2384 ct=9.3565 rec=1.2280 | train/val/test=0.966/0.632/0.643 | c=0.998437
[Epoch 0011] loss=13.0380 cls=0.0912 smmd=1.2006 ct=9.3208 rec=1.2127 | train/val/test=0.983/0.634/0.633 | c=0.998437
[Epoch 0012] loss=12.8535 cls=0.0583 smmd=1.0717 ct=9.3197 rec=1.2019 | train/val/test=1.000/0.632/0.634 | c=0.998437
[Epoch 0013] loss=12.6275 cls=0.0323 smmd=0.8490 ct=9.3547 rec=1.1958 | train/val/test=1.000/0.654/0.648 | c=0.998437
[Epoch 0014] loss=12.5551 cls=0.0198 smmd=0.7459 ct=9.4079 rec=1.1907 | train/val/test=1.000/0.676/0.674 | c=0.998437
[Epoch 0015] loss=12.5335 cls=0.0136 smmd=0.7046 ct=9.4431 rec=1.1861 | train/val/test=1.000/0.688/0.676 | c=0.998437
[Epoch 0016] loss=12.4823 cls=0.0107 smmd=0.6658 ct=9.4390 rec=1.1834 | train/val/test=1.000/0.676/0.667 | c=0.998437
[Epoch 0017] loss=12.4068 cls=0.0100 smmd=0.6253 ct=9.4051 rec=1.1832 | train/val/test=1.000/0.664/0.670 | c=0.998437
[Epoch 0018] loss=12.2927 cls=0.0100 smmd=0.5393 ct=9.3741 rec=1.1846 | train/val/test=1.000/0.682/0.684 | c=0.998437
[Epoch 0019] loss=12.2533 cls=0.0106 smmd=0.5019 ct=9.3691 rec=1.1858 | train/val/test=1.000/0.690/0.695 | c=0.998437
[Epoch 0020] loss=12.2102 cls=0.0133 smmd=0.4368 ct=9.3850 rec=1.1876 | train/val/test=1.000/0.690/0.693 | c=0.998437
[Epoch 0021] loss=12.1820 cls=0.0165 smmd=0.3967 ct=9.3881 rec=1.1904 | train/val/test=1.000/0.674/0.688 | c=0.998437
[Epoch 0022] loss=12.1203 cls=0.0202 smmd=0.3268 ct=9.3854 rec=1.1940 | train/val/test=1.000/0.680/0.702 | c=0.998437
[Epoch 0023] loss=12.0914 cls=0.0255 smmd=0.2857 ct=9.3906 rec=1.1949 | train/val/test=1.000/0.688/0.708 | c=0.998437
[Epoch 0024] loss=12.0526 cls=0.0296 smmd=0.2399 ct=9.3940 rec=1.1946 | train/val/test=1.000/0.684/0.703 | c=0.998437
[Epoch 0025] loss=12.0458 cls=0.0327 smmd=0.2356 ct=9.3911 rec=1.1932 | train/val/test=1.000/0.708/0.712 | c=0.998437
[Epoch 0026] loss=12.0069 cls=0.0353 smmd=0.2062 ct=9.3861 rec=1.1897 | train/val/test=1.000/0.706/0.710 | c=0.998437
[Epoch 0027] loss=11.9866 cls=0.0340 smmd=0.2023 ct=9.3793 rec=1.1855 | train/val/test=1.000/0.712/0.718 | c=0.998437
[Epoch 0028] loss=11.9507 cls=0.0345 smmd=0.1720 ct=9.3820 rec=1.1811 | train/val/test=1.000/0.716/0.721 | c=0.998437
[Epoch 0029] loss=11.9114 cls=0.0324 smmd=0.1436 ct=9.3801 rec=1.1777 | train/val/test=1.000/0.716/0.720 | c=0.998437
[Epoch 0030] loss=11.9023 cls=0.0311 smmd=0.1496 ct=9.3739 rec=1.1739 | train/val/test=1.000/0.720/0.726 | c=0.998437
[Epoch 0031] loss=11.8725 cls=0.0280 smmd=0.1386 ct=9.3627 rec=1.1716 | train/val/test=1.000/0.722/0.727 | c=0.998437
[Epoch 0032] loss=11.8454 cls=0.0256 smmd=0.1198 ct=9.3606 rec=1.1697 | train/val/test=1.000/0.728/0.726 | c=0.998437
[Epoch 0033] loss=11.8343 cls=0.0230 smmd=0.1135 ct=9.3608 rec=1.1685 | train/val/test=1.000/0.726/0.728 | c=0.998437
[Epoch 0034] loss=11.8099 cls=0.0207 smmd=0.0929 ct=9.3603 rec=1.1680 | train/val/test=1.000/0.722/0.727 | c=0.998437
[Epoch 0035] loss=11.7991 cls=0.0195 smmd=0.0873 ct=9.3562 rec=1.1681 | train/val/test=1.000/0.720/0.725 | c=0.998437
[Epoch 0036] loss=11.7903 cls=0.0189 smmd=0.0869 ct=9.3468 rec=1.1689 | train/val/test=1.000/0.722/0.728 | c=0.998437
[Epoch 0037] loss=11.7731 cls=0.0188 smmd=0.0717 ct=9.3430 rec=1.1698 | train/val/test=1.000/0.720/0.727 | c=0.998437
[Epoch 0038] loss=11.7663 cls=0.0193 smmd=0.0626 ct=9.3432 rec=1.1707 | train/val/test=1.000/0.724/0.731 | c=0.998437
[Epoch 0039] loss=11.7671 cls=0.0197 smmd=0.0657 ct=9.3393 rec=1.1712 | train/val/test=1.000/0.728/0.732 | c=0.998437
[Epoch 0040] loss=11.7532 cls=0.0208 smmd=0.0474 ct=9.3430 rec=1.1710 | train/val/test=1.000/0.726/0.730 | c=0.998437
[Epoch 0041] loss=11.7474 cls=0.0207 smmd=0.0488 ct=9.3353 rec=1.1713 | train/val/test=1.000/0.732/0.734 | c=0.998437
[Epoch 0042] loss=11.7462 cls=0.0230 smmd=0.0456 ct=9.3365 rec=1.1705 | train/val/test=1.000/0.726/0.724 | c=0.998437
[Epoch 0043] loss=11.7532 cls=0.0220 smmd=0.0572 ct=9.3289 rec=1.1725 | train/val/test=1.000/0.748/0.741 | c=0.998437
[Epoch 0044] loss=11.7882 cls=0.0325 smmd=0.0577 ct=9.3506 rec=1.1737 | train/val/test=1.000/0.700/0.701 | c=0.998437
[Epoch 0045] loss=11.8557 cls=0.0345 smmd=0.0984 ct=9.3530 rec=1.1849 | train/val/test=1.000/0.746/0.742 | c=0.998437
[Epoch 0046] loss=11.8697 cls=0.0434 smmd=0.0899 ct=9.3803 rec=1.1781 | train/val/test=1.000/0.730/0.724 | c=0.998437
[Epoch 0047] loss=11.7301 cls=0.0155 smmd=0.0607 ct=9.3240 rec=1.1649 | train/val/test=1.000/0.714/0.718 | c=0.998437
[Epoch 0048] loss=11.7364 cls=0.0164 smmd=0.0646 ct=9.3220 rec=1.1667 | train/val/test=1.000/0.754/0.741 | c=0.998437
[Epoch 0049] loss=11.7696 cls=0.0180 smmd=0.0730 ct=9.3484 rec=1.1651 | train/val/test=1.000/0.740/0.737 | c=0.998437
[Epoch 0050] loss=11.7020 cls=0.0124 smmd=0.0459 ct=9.3210 rec=1.1613 | train/val/test=1.000/0.710/0.716 | c=0.998437
[Epoch 0051] loss=11.7563 cls=0.0172 smmd=0.0775 ct=9.3239 rec=1.1689 | train/val/test=1.000/0.738/0.731 | c=0.998437
[Epoch 0052] loss=11.7076 cls=0.0138 smmd=0.0387 ct=9.3289 rec=1.1631 | train/val/test=1.000/0.754/0.744 | c=0.998437
[Epoch 0053] loss=11.7146 cls=0.0159 smmd=0.0401 ct=9.3270 rec=1.1658 | train/val/test=1.000/0.712/0.717 | c=0.998437
[Epoch 0054] loss=11.7257 cls=0.0180 smmd=0.0543 ct=9.3137 rec=1.1699 | train/val/test=1.000/0.728/0.730 | c=0.998437
[Epoch 0055] loss=11.6951 cls=0.0177 smmd=0.0313 ct=9.3116 rec=1.1672 | train/val/test=1.000/0.754/0.744 | c=0.998437
[Epoch 0056] loss=11.7156 cls=0.0220 smmd=0.0337 ct=9.3170 rec=1.1714 | train/val/test=1.000/0.720/0.721 | c=0.998437
[Epoch 0057] loss=11.7061 cls=0.0219 smmd=0.0305 ct=9.3092 rec=1.1722 | train/val/test=1.000/0.744/0.733 | c=0.998437
[Epoch 0058] loss=11.7022 cls=0.0247 smmd=0.0244 ct=9.3102 rec=1.1715 | train/val/test=1.000/0.742/0.736 | c=0.998437
[Epoch 0059] loss=11.6990 cls=0.0248 smmd=0.0228 ct=9.3077 rec=1.1719 | train/val/test=1.000/0.738/0.729 | c=0.998437
[Epoch 0060] loss=11.6870 cls=0.0233 smmd=0.0235 ct=9.2991 rec=1.1705 | train/val/test=1.000/0.740/0.734 | c=0.998437
[Epoch 0061] loss=11.6904 cls=0.0278 smmd=0.0162 ct=9.3056 rec=1.1704 | train/val/test=1.000/0.732/0.734 | c=0.998437
[Epoch 0062] loss=11.6929 cls=0.0240 smmd=0.0248 ct=9.3029 rec=1.1706 | train/val/test=1.000/0.740/0.739 | c=0.998437
[Epoch 0063] loss=11.6658 cls=0.0224 smmd=0.0163 ct=9.2914 rec=1.1679 | train/val/test=1.000/0.742/0.738 | c=0.998437
[Epoch 0064] loss=11.6646 cls=0.0218 smmd=0.0151 ct=9.2923 rec=1.1678 | train/val/test=1.000/0.730/0.734 | c=0.998437
[Epoch 0065] loss=11.6738 cls=0.0221 smmd=0.0167 ct=9.2957 rec=1.1696 | train/val/test=1.000/0.738/0.739 | c=0.998437
[Epoch 0066] loss=11.6537 cls=0.0208 smmd=0.0077 ct=9.2901 rec=1.1675 | train/val/test=1.000/0.742/0.736 | c=0.998437
[Epoch 0067] loss=11.6490 cls=0.0201 smmd=0.0102 ct=9.2831 rec=1.1678 | train/val/test=1.000/0.728/0.734 | c=0.998437
[Epoch 0068] loss=11.6562 cls=0.0210 smmd=0.0099 ct=9.2867 rec=1.1693 | train/val/test=1.000/0.738/0.735 | c=0.998437
[Epoch 0069] loss=11.6498 cls=0.0211 smmd=0.0071 ct=9.2845 rec=1.1685 | train/val/test=1.000/0.734/0.731 | c=0.998437
[Epoch 0070] loss=11.6457 cls=0.0205 smmd=0.0093 ct=9.2768 rec=1.1695 | train/val/test=1.000/0.730/0.737 | c=0.998437
[Epoch 0071] loss=11.6412 cls=0.0211 smmd=-0.0004 ct=9.2818 rec=1.1693 | train/val/test=1.000/0.734/0.736 | c=0.998437
[Epoch 0072] loss=11.6396 cls=0.0213 smmd=0.0026 ct=9.2772 rec=1.1692 | train/val/test=1.000/0.734/0.729 | c=0.998437
[Epoch 0073] loss=11.6400 cls=0.0213 smmd=0.0037 ct=9.2740 rec=1.1705 | train/val/test=1.000/0.734/0.737 | c=0.998437
[Epoch 0074] loss=11.6435 cls=0.0234 smmd=0.0029 ct=9.2760 rec=1.1706 | train/val/test=1.000/0.730/0.732 | c=0.998437
[Epoch 0075] loss=11.6362 cls=0.0216 smmd=0.0042 ct=9.2699 rec=1.1703 | train/val/test=1.000/0.732/0.733 | c=0.998437
[Epoch 0076] loss=11.6252 cls=0.0221 smmd=-0.0086 ct=9.2721 rec=1.1698 | train/val/test=1.000/0.734/0.733 | c=0.998437
[Epoch 0077] loss=11.6290 cls=0.0224 smmd=-0.0023 ct=9.2687 rec=1.1701 | train/val/test=1.000/0.728/0.730 | c=0.998437
[Epoch 0078] loss=11.6242 cls=0.0220 smmd=-0.0028 ct=9.2648 rec=1.1701 | train/val/test=1.000/0.734/0.733 | c=0.998437
[Epoch 0079] loss=11.6239 cls=0.0218 smmd=-0.0014 ct=9.2646 rec=1.1694 | train/val/test=1.000/0.732/0.733 | c=0.998437
[Epoch 0080] loss=11.6229 cls=0.0218 smmd=-0.0064 ct=9.2692 rec=1.1692 | train/val/test=1.000/0.732/0.733 | c=0.998437
[Epoch 0081] loss=11.6147 cls=0.0212 smmd=-0.0057 ct=9.2612 rec=1.1690 | train/val/test=1.000/0.734/0.733 | c=0.998437
[Epoch 0082] loss=11.6143 cls=0.0212 smmd=-0.0036 ct=9.2602 rec=1.1682 | train/val/test=1.000/0.732/0.733 | c=0.998437
[Epoch 0083] loss=11.6126 cls=0.0208 smmd=-0.0045 ct=9.2595 rec=1.1684 | train/val/test=1.000/0.734/0.732 | c=0.998437
[Epoch 0084] loss=11.6071 cls=0.0212 smmd=-0.0143 ct=9.2634 rec=1.1684 | train/val/test=1.000/0.734/0.733 | c=0.998437
[Epoch 0085] loss=11.6095 cls=0.0211 smmd=-0.0089 ct=9.2599 rec=1.1687 | train/val/test=1.000/0.732/0.734 | c=0.998437
[Epoch 0086] loss=11.6054 cls=0.0213 smmd=-0.0078 ct=9.2555 rec=1.1682 | train/val/test=1.000/0.730/0.731 | c=0.998437
[Epoch 0087] loss=11.6020 cls=0.0208 smmd=-0.0088 ct=9.2528 rec=1.1686 | train/val/test=1.000/0.734/0.733 | c=0.998437
[Epoch 0088] loss=11.6062 cls=0.0218 smmd=-0.0130 ct=9.2602 rec=1.1685 | train/val/test=1.000/0.732/0.731 | c=0.998437
[Epoch 0089] loss=11.6080 cls=0.0214 smmd=-0.0073 ct=9.2544 rec=1.1698 | train/val/test=1.000/0.732/0.732 | c=0.998437
[Epoch 0090] loss=11.6247 cls=0.0243 smmd=-0.0067 ct=9.2656 rec=1.1707 | train/val/test=1.000/0.722/0.728 | c=0.998437
[Epoch 0091] loss=11.6588 cls=0.0254 smmd=0.0251 ct=9.2581 rec=1.1752 | train/val/test=1.000/0.732/0.715 | c=0.998437
[Epoch 0092] loss=11.7098 cls=0.0341 smmd=0.0240 ct=9.2958 rec=1.1779 | train/val/test=1.000/0.710/0.723 | c=0.998437
[Epoch 0093] loss=11.7247 cls=0.0286 smmd=0.0577 ct=9.2801 rec=1.1792 | train/val/test=1.000/0.736/0.731 | c=0.998437
[Epoch 0094] loss=11.6257 cls=0.0200 smmd=0.0023 ct=9.2744 rec=1.1645 | train/val/test=1.000/0.734/0.725 | c=0.998437
[Epoch 0095] loss=11.6070 cls=0.0154 smmd=0.0060 ct=9.2634 rec=1.1611 | train/val/test=1.000/0.726/0.734 | c=0.998437
[Epoch 0096] loss=11.6574 cls=0.0155 smmd=0.0434 ct=9.2668 rec=1.1659 | train/val/test=1.000/0.736/0.734 | c=0.998437
[Epoch 0097] loss=11.6061 cls=0.0152 smmd=-0.0003 ct=9.2685 rec=1.1613 | train/val/test=1.000/0.732/0.724 | c=0.998437
[Epoch 0098] loss=11.6041 cls=0.0158 smmd=-0.0059 ct=9.2666 rec=1.1638 | train/val/test=1.000/0.726/0.738 | c=0.998437
[Epoch 0099] loss=11.6412 cls=0.0183 smmd=0.0232 ct=9.2600 rec=1.1699 | train/val/test=1.000/0.734/0.729 | c=0.998437
=== Best @ epoch 48: val=0.7540, test=0.7410 ===
[2025-09-20 04:04:16] END attempt 1: exit_code=0
