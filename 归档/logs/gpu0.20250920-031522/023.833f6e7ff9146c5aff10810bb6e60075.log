[2025-09-20 04:44:37] START attempt 1: python main.py --is_transfer True --test_dataset PubMed --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth --few True --shot 5 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.7026 cls=1.1047 smmd=5.6637 ct=11.2841 rec=1.4138 | train/val/test=0.615/0.498/0.499 | c=0.998347
[Epoch 0001] loss=22.5129 cls=1.0899 smmd=4.0076 ct=11.2422 rec=1.4135 | train/val/test=0.538/0.348/0.340 | c=0.998347
[Epoch 0002] loss=24.8491 cls=1.0825 smmd=4.9376 ct=11.2572 rec=1.4135 | train/val/test=0.769/0.610/0.613 | c=0.998347
[Epoch 0003] loss=23.7755 cls=1.0581 smmd=4.5503 ct=11.1640 rec=1.4134 | train/val/test=0.692/0.634/0.646 | c=0.998347
[Epoch 0004] loss=18.9233 cls=1.0179 smmd=2.4265 ct=11.6416 rec=1.4129 | train/val/test=0.692/0.570/0.597 | c=0.998347
[Epoch 0005] loss=20.5804 cls=0.9798 smmd=3.1926 ct=11.4030 rec=1.4120 | train/val/test=0.769/0.622/0.648 | c=0.998347
[Epoch 0006] loss=21.4013 cls=0.9384 smmd=3.5378 ct=11.3822 rec=1.4108 | train/val/test=0.769/0.652/0.656 | c=0.998347
[Epoch 0007] loss=19.7184 cls=0.8980 smmd=2.8898 ct=11.3404 rec=1.4090 | train/val/test=0.846/0.646/0.653 | c=0.998347
[Epoch 0008] loss=17.3851 cls=0.8683 smmd=1.9464 ct=11.3816 rec=1.4064 | train/val/test=0.923/0.662/0.664 | c=0.998347
[Epoch 0009] loss=18.8956 cls=0.8560 smmd=2.4636 ct=11.6060 rec=1.4053 | train/val/test=0.923/0.670/0.661 | c=0.998347
[Epoch 0010] loss=19.5947 cls=0.8392 smmd=2.7515 ct=11.5933 rec=1.4059 | train/val/test=0.923/0.684/0.679 | c=0.998347
[Epoch 0011] loss=17.1223 cls=0.8121 smmd=1.8203 ct=11.4627 rec=1.4055 | train/val/test=0.923/0.682/0.678 | c=0.998347
[Epoch 0012] loss=18.4439 cls=0.7906 smmd=2.3171 ct=11.5542 rec=1.4034 | train/val/test=0.923/0.660/0.676 | c=0.998347
[Epoch 0013] loss=18.4079 cls=0.7499 smmd=2.3715 ct=11.4031 rec=1.4019 | train/val/test=0.923/0.690/0.695 | c=0.998347
[Epoch 0014] loss=17.0469 cls=0.6984 smmd=1.8484 ct=11.3782 rec=1.3970 | train/val/test=1.000/0.696/0.693 | c=0.998347
[Epoch 0015] loss=16.5385 cls=0.6544 smmd=1.6033 ct=11.5081 rec=1.3898 | train/val/test=1.000/0.690/0.693 | c=0.998347
[Epoch 0016] loss=16.4881 cls=0.6139 smmd=1.5771 ct=11.5465 rec=1.3839 | train/val/test=0.923/0.706/0.700 | c=0.998347
[Epoch 0017] loss=16.3397 cls=0.5752 smmd=1.5474 ct=11.4933 rec=1.3803 | train/val/test=0.923/0.702/0.705 | c=0.998347
[Epoch 0018] loss=15.7862 cls=0.5523 smmd=1.3494 ct=11.4469 rec=1.3794 | train/val/test=0.923/0.712/0.708 | c=0.998347
[Epoch 0019] loss=15.6757 cls=0.5466 smmd=1.3074 ct=11.4435 rec=1.3807 | train/val/test=1.000/0.714/0.708 | c=0.998347
[Epoch 0020] loss=15.8335 cls=0.5474 smmd=1.3615 ct=11.4640 rec=1.3842 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0021] loss=15.7213 cls=0.5458 smmd=1.2965 ct=11.5134 rec=1.3873 | train/val/test=1.000/0.712/0.714 | c=0.998347
[Epoch 0022] loss=15.7056 cls=0.5238 smmd=1.2853 ct=11.5368 rec=1.3873 | train/val/test=1.000/0.708/0.715 | c=0.998347
[Epoch 0023] loss=15.6393 cls=0.4845 smmd=1.2745 ct=11.5189 rec=1.3839 | train/val/test=1.000/0.726/0.712 | c=0.998347
[Epoch 0024] loss=15.0857 cls=0.4333 smmd=1.0752 ct=11.4933 rec=1.3755 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0025] loss=14.9623 cls=0.3897 smmd=1.0445 ct=11.4708 rec=1.3708 | train/val/test=1.000/0.724/0.716 | c=0.998347
[Epoch 0026] loss=14.6972 cls=0.3541 smmd=0.9451 ct=11.4755 rec=1.3637 | train/val/test=1.000/0.730/0.720 | c=0.998347
[Epoch 0027] loss=14.6225 cls=0.3361 smmd=0.9182 ct=11.4784 rec=1.3611 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0028] loss=14.4193 cls=0.3209 smmd=0.8431 ct=11.4707 rec=1.3608 | train/val/test=1.000/0.724/0.724 | c=0.998347
[Epoch 0029] loss=14.5381 cls=0.3209 smmd=0.8789 ct=11.4984 rec=1.3637 | train/val/test=1.000/0.726/0.729 | c=0.998347
[Epoch 0030] loss=14.4824 cls=0.3230 smmd=0.8546 ct=11.5015 rec=1.3660 | train/val/test=1.000/0.736/0.737 | c=0.998347
[Epoch 0031] loss=14.8910 cls=0.3249 smmd=1.0083 ct=11.5246 rec=1.3661 | train/val/test=1.000/0.714/0.724 | c=0.998347
[Epoch 0032] loss=14.5895 cls=0.3019 smmd=0.9155 ct=11.4671 rec=1.3654 | train/val/test=1.000/0.742/0.730 | c=0.998347
[Epoch 0033] loss=14.3528 cls=0.2808 smmd=0.8129 ct=11.5023 rec=1.3556 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0034] loss=14.3520 cls=0.2484 smmd=0.8201 ct=11.5030 rec=1.3488 | train/val/test=1.000/0.724/0.722 | c=0.998347
[Epoch 0035] loss=13.9349 cls=0.2293 smmd=0.6756 ct=11.4597 rec=1.3430 | train/val/test=1.000/0.730/0.723 | c=0.998347
[Epoch 0036] loss=13.9949 cls=0.2225 smmd=0.6898 ct=11.4885 rec=1.3416 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0037] loss=13.9775 cls=0.2228 smmd=0.6857 ct=11.4803 rec=1.3430 | train/val/test=1.000/0.722/0.730 | c=0.998347
[Epoch 0038] loss=13.9426 cls=0.2250 smmd=0.6689 ct=11.4844 rec=1.3473 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0039] loss=14.1561 cls=0.2362 smmd=0.7456 ct=11.4988 rec=1.3503 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0040] loss=14.3634 cls=0.2331 smmd=0.8232 ct=11.5120 rec=1.3537 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0041] loss=14.1967 cls=0.2233 smmd=0.7525 ct=11.5293 rec=1.3488 | train/val/test=1.000/0.722/0.728 | c=0.998347
[Epoch 0042] loss=14.0295 cls=0.2066 smmd=0.7103 ct=11.4768 rec=1.3472 | train/val/test=1.000/0.690/0.692 | c=0.998347
[Epoch 0043] loss=13.7752 cls=0.1841 smmd=0.6039 ct=11.5048 rec=1.3374 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0044] loss=13.7950 cls=0.1658 smmd=0.6218 ct=11.4909 rec=1.3333 | train/val/test=1.000/0.708/0.702 | c=0.998347
[Epoch 0045] loss=13.6692 cls=0.1599 smmd=0.5756 ct=11.4846 rec=1.3313 | train/val/test=1.000/0.720/0.730 | c=0.998347
[Epoch 0046] loss=13.7249 cls=0.1602 smmd=0.5965 ct=11.4869 rec=1.3334 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0047] loss=13.7584 cls=0.1680 smmd=0.5999 ct=11.5064 rec=1.3365 | train/val/test=1.000/0.722/0.729 | c=0.998347
[Epoch 0048] loss=14.0949 cls=0.1758 smmd=0.7375 ct=11.4922 rec=1.3421 | train/val/test=1.000/0.690/0.690 | c=0.998347
[Epoch 0049] loss=14.0615 cls=0.1769 smmd=0.7066 ct=11.5358 rec=1.3414 | train/val/test=1.000/0.734/0.738 | c=0.998347
[Epoch 0050] loss=14.0170 cls=0.1662 smmd=0.7010 ct=11.5132 rec=1.3366 | train/val/test=1.000/0.704/0.688 | c=0.998347
[Epoch 0051] loss=13.7160 cls=0.1468 smmd=0.5935 ct=11.4947 rec=1.3284 | train/val/test=1.000/0.734/0.727 | c=0.998347
[Epoch 0052] loss=13.6271 cls=0.1291 smmd=0.5666 ct=11.4852 rec=1.3215 | train/val/test=1.000/0.732/0.709 | c=0.998347
[Epoch 0053] loss=13.5056 cls=0.1242 smmd=0.5205 ct=11.4835 rec=1.3178 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0054] loss=13.4668 cls=0.1236 smmd=0.5119 ct=11.4653 rec=1.3200 | train/val/test=1.000/0.738/0.713 | c=0.998347
[Epoch 0055] loss=13.5227 cls=0.1392 smmd=0.5150 ct=11.5029 rec=1.3253 | train/val/test=1.000/0.724/0.732 | c=0.998347
[Epoch 0056] loss=13.8486 cls=0.1525 smmd=0.6474 ct=11.4870 rec=1.3337 | train/val/test=1.000/0.724/0.698 | c=0.998347
[Epoch 0057] loss=14.0651 cls=0.1626 smmd=0.7152 ct=11.5280 rec=1.3358 | train/val/test=1.000/0.710/0.726 | c=0.998347
[Epoch 0058] loss=13.9764 cls=0.1715 smmd=0.6834 ct=11.5121 rec=1.3400 | train/val/test=1.000/0.644/0.630 | c=0.998347
[Epoch 0059] loss=13.7624 cls=0.1664 smmd=0.5834 ct=11.5521 rec=1.3372 | train/val/test=1.000/0.700/0.726 | c=0.998347
[Epoch 0060] loss=13.7661 cls=0.1571 smmd=0.6075 ct=11.5035 rec=1.3308 | train/val/test=1.000/0.720/0.690 | c=0.998347
[Epoch 0061] loss=13.4306 cls=0.1098 smmd=0.4860 ct=11.5036 rec=1.3143 | train/val/test=1.000/0.732/0.726 | c=0.998347
[Epoch 0062] loss=13.3321 cls=0.0872 smmd=0.4729 ct=11.4528 rec=1.3070 | train/val/test=1.000/0.734/0.737 | c=0.998347
[Epoch 0063] loss=13.3086 cls=0.0922 smmd=0.4542 ct=11.4713 rec=1.3112 | train/val/test=1.000/0.750/0.720 | c=0.998347
[Epoch 0064] loss=13.5185 cls=0.1081 smmd=0.5167 ct=11.5135 rec=1.3187 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0065] loss=13.8327 cls=0.1208 smmd=0.6477 ct=11.4881 rec=1.3298 | train/val/test=1.000/0.730/0.715 | c=0.998347
[Epoch 0066] loss=14.0518 cls=0.1353 smmd=0.7172 ct=11.5261 rec=1.3300 | train/val/test=1.000/0.706/0.719 | c=0.998347
[Epoch 0067] loss=14.0208 cls=0.1384 smmd=0.7086 ct=11.5107 rec=1.3389 | train/val/test=1.000/0.692/0.664 | c=0.998347
[Epoch 0068] loss=13.5837 cls=0.1330 smmd=0.5313 ct=11.5263 rec=1.3253 | train/val/test=1.000/0.704/0.725 | c=0.998347
[Epoch 0069] loss=13.5752 cls=0.1185 smmd=0.5571 ct=11.4637 rec=1.3189 | train/val/test=1.000/0.724/0.701 | c=0.998347
[Epoch 0070] loss=13.3064 cls=0.0842 smmd=0.4486 ct=11.4906 rec=1.3046 | train/val/test=1.000/0.738/0.723 | c=0.998347
[Epoch 0071] loss=13.1752 cls=0.0787 smmd=0.4110 ct=11.4571 rec=1.3025 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0072] loss=13.3520 cls=0.0877 smmd=0.4786 ct=11.4573 rec=1.3087 | train/val/test=1.000/0.744/0.725 | c=0.998347
[Epoch 0073] loss=13.4069 cls=0.1059 smmd=0.4718 ct=11.5153 rec=1.3186 | train/val/test=1.000/0.732/0.735 | c=0.998347
[Epoch 0074] loss=13.7359 cls=0.1277 smmd=0.6084 ct=11.4855 rec=1.3313 | train/val/test=1.000/0.704/0.666 | c=0.998347
[Epoch 0075] loss=14.4584 cls=0.1563 smmd=0.8547 ct=11.5740 rec=1.3393 | train/val/test=1.000/0.698/0.724 | c=0.998347
[Epoch 0076] loss=14.1852 cls=0.1735 smmd=0.7480 ct=11.5529 rec=1.3512 | train/val/test=1.000/0.582/0.557 | c=0.998347
[Epoch 0077] loss=13.6224 cls=0.1875 smmd=0.5223 ct=11.5561 rec=1.3334 | train/val/test=1.000/0.710/0.726 | c=0.998347
[Epoch 0078] loss=13.6832 cls=0.0776 smmd=0.6049 ct=11.4764 rec=1.3116 | train/val/test=1.000/0.748/0.734 | c=0.998347
[Epoch 0079] loss=13.1150 cls=0.0492 smmd=0.3942 ct=11.4591 rec=1.2917 | train/val/test=1.000/0.714/0.678 | c=0.998347
[Epoch 0080] loss=13.3532 cls=0.0737 smmd=0.4765 ct=11.4732 rec=1.3037 | train/val/test=1.000/0.726/0.731 | c=0.998347
[Epoch 0081] loss=13.3815 cls=0.0717 smmd=0.4889 ct=11.4695 rec=1.3077 | train/val/test=1.000/0.760/0.724 | c=0.998347
[Epoch 0082] loss=13.3383 cls=0.0752 smmd=0.4484 ct=11.5248 rec=1.3097 | train/val/test=1.000/0.744/0.750 | c=0.998347
[Epoch 0083] loss=14.0602 cls=0.1007 smmd=0.7499 ct=11.4740 rec=1.3218 | train/val/test=1.000/0.700/0.696 | c=0.998347
[Epoch 0084] loss=14.3218 cls=0.1146 smmd=0.8075 ct=11.5791 rec=1.3332 | train/val/test=0.923/0.722/0.734 | c=0.998347
[Epoch 0085] loss=14.2256 cls=0.2000 smmd=0.7601 ct=11.5551 rec=1.3403 | train/val/test=1.000/0.540/0.523 | c=0.998347
[Epoch 0086] loss=13.9307 cls=0.1676 smmd=0.6251 ct=11.6025 rec=1.3632 | train/val/test=1.000/0.720/0.714 | c=0.998347
[Epoch 0087] loss=13.6407 cls=0.1133 smmd=0.5676 ct=11.5089 rec=1.3125 | train/val/test=1.000/0.738/0.739 | c=0.998347
[Epoch 0088] loss=13.1511 cls=0.0430 smmd=0.4111 ct=11.4576 rec=1.2887 | train/val/test=1.000/0.626/0.599 | c=0.998347
[Epoch 0089] loss=13.4583 cls=0.1016 smmd=0.4990 ct=11.5038 rec=1.3126 | train/val/test=1.000/0.728/0.726 | c=0.998347
[Epoch 0090] loss=13.3499 cls=0.0571 smmd=0.4789 ct=11.4719 rec=1.3041 | train/val/test=1.000/0.748/0.732 | c=0.998347
[Epoch 0091] loss=13.3002 cls=0.0647 smmd=0.4436 ct=11.5070 rec=1.3035 | train/val/test=1.000/0.702/0.691 | c=0.998347
[Epoch 0092] loss=13.5913 cls=0.0932 smmd=0.5465 ct=11.5183 rec=1.3205 | train/val/test=1.000/0.766/0.745 | c=0.998347
[Epoch 0093] loss=14.0548 cls=0.1159 smmd=0.7169 ct=11.5383 rec=1.3329 | train/val/test=1.000/0.678/0.676 | c=0.998347
[Epoch 0094] loss=14.2284 cls=0.1129 smmd=0.7750 ct=11.5673 rec=1.3343 | train/val/test=1.000/0.746/0.734 | c=0.998347
[Epoch 0095] loss=13.8905 cls=0.1081 smmd=0.6518 ct=11.5458 rec=1.3223 | train/val/test=1.000/0.690/0.676 | c=0.998347
[Epoch 0096] loss=13.4686 cls=0.0607 smmd=0.5334 ct=11.4548 rec=1.3000 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0097] loss=13.2914 cls=0.0670 smmd=0.4465 ct=11.4941 rec=1.2950 | train/val/test=1.000/0.718/0.707 | c=0.998347
[Epoch 0098] loss=13.1710 cls=0.0573 smmd=0.4083 ct=11.4749 rec=1.2934 | train/val/test=1.000/0.716/0.684 | c=0.998347
[Epoch 0099] loss=13.2294 cls=0.0574 smmd=0.4494 ct=11.4297 rec=1.2950 | train/val/test=1.000/0.730/0.730 | c=0.998347
=== Best @ epoch 92: val=0.7660, test=0.7450 ===
[2025-09-20 04:47:49] END attempt 1: exit_code=0
