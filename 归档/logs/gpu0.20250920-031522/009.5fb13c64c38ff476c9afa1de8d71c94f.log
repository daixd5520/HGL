[2025-09-20 03:55:19] START attempt 1: python main.py --is_transfer True --test_dataset PubMed --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth --few True --shot 10 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.3550 cls=1.1004 smmd=5.6759 ct=11.2838 rec=1.4136 | train/val/test=0.423/0.300/0.299 | c=0.998437
[Epoch 0001] loss=29.4488 cls=1.0807 smmd=3.5051 ct=11.2414 rec=1.4139 | train/val/test=0.385/0.422/0.415 | c=0.998437
[Epoch 0002] loss=36.3759 cls=1.0566 smmd=4.8913 ct=11.2498 rec=1.4138 | train/val/test=0.654/0.518/0.529 | c=0.998437
[Epoch 0003] loss=35.8649 cls=0.9736 smmd=4.8055 ct=11.2094 rec=1.4122 | train/val/test=0.615/0.494/0.504 | c=0.998437
[Epoch 0004] loss=27.1312 cls=0.8580 smmd=3.0985 ct=11.0688 rec=1.4074 | train/val/test=0.769/0.574/0.552 | c=0.998437
[Epoch 0005] loss=25.0319 cls=0.7497 smmd=2.7074 ct=10.9808 rec=1.3920 | train/val/test=0.769/0.600/0.596 | c=0.998437
[Epoch 0006] loss=27.6475 cls=0.6504 smmd=3.2384 ct=10.9928 rec=1.3733 | train/val/test=0.731/0.604/0.597 | c=0.998437
[Epoch 0007] loss=26.6892 cls=0.5649 smmd=3.0679 ct=10.9318 rec=1.3535 | train/val/test=0.769/0.606/0.597 | c=0.998437
[Epoch 0008] loss=22.5465 cls=0.4973 smmd=2.2538 ct=10.8950 rec=1.3383 | train/val/test=0.885/0.624/0.620 | c=0.998437
[Epoch 0009] loss=22.0052 cls=0.4398 smmd=2.1456 ct=10.9238 rec=1.3333 | train/val/test=0.846/0.664/0.657 | c=0.998437
[Epoch 0010] loss=23.8252 cls=0.3648 smmd=2.5164 ct=10.9275 rec=1.3322 | train/val/test=0.923/0.686/0.677 | c=0.998437
[Epoch 0011] loss=22.9757 cls=0.3277 smmd=2.3486 ct=10.9352 rec=1.3381 | train/val/test=0.962/0.682/0.672 | c=0.998437
[Epoch 0012] loss=19.8335 cls=0.2792 smmd=1.7316 ct=10.9018 rec=1.3388 | train/val/test=1.000/0.728/0.721 | c=0.998437
[Epoch 0013] loss=22.2998 cls=0.2492 smmd=2.2238 ct=10.9221 rec=1.3417 | train/val/test=1.000/0.756/0.729 | c=0.998437
[Epoch 0014] loss=21.6255 cls=0.1984 smmd=2.0979 ct=10.9032 rec=1.3344 | train/val/test=1.000/0.766/0.736 | c=0.998437
[Epoch 0015] loss=19.0288 cls=0.1566 smmd=1.5874 ct=10.8813 rec=1.3220 | train/val/test=1.000/0.782/0.758 | c=0.998437
[Epoch 0016] loss=19.1778 cls=0.1279 smmd=1.6246 ct=10.8591 rec=1.3147 | train/val/test=1.000/0.780/0.749 | c=0.998437
[Epoch 0017] loss=19.4942 cls=0.1092 smmd=1.6925 ct=10.8462 rec=1.3095 | train/val/test=1.000/0.782/0.748 | c=0.998437
[Epoch 0018] loss=17.9480 cls=0.0962 smmd=1.3857 ct=10.8406 rec=1.3067 | train/val/test=1.000/0.790/0.759 | c=0.998437
[Epoch 0019] loss=17.5670 cls=0.0867 smmd=1.3091 ct=10.8471 rec=1.3084 | train/val/test=1.000/0.798/0.765 | c=0.998437
[Epoch 0020] loss=17.7064 cls=0.0891 smmd=1.3348 ct=10.8570 rec=1.3110 | train/val/test=1.000/0.792/0.769 | c=0.998437
[Epoch 0021] loss=17.3110 cls=0.0989 smmd=1.2500 ct=10.8799 rec=1.3164 | train/val/test=1.000/0.792/0.763 | c=0.998437
[Epoch 0022] loss=17.5270 cls=0.0994 smmd=1.1444 ct=11.6239 rec=1.3133 | train/val/test=1.000/0.782/0.739 | c=0.998437
[Epoch 0023] loss=18.1142 cls=0.1140 smmd=1.2778 ct=11.5369 rec=1.3136 | train/val/test=1.000/0.786/0.772 | c=0.998437
[Epoch 0024] loss=17.2073 cls=0.0977 smmd=1.1000 ct=11.5273 rec=1.3130 | train/val/test=1.000/0.790/0.761 | c=0.998437
[Epoch 0025] loss=17.1403 cls=0.0935 smmd=1.0754 ct=11.5859 rec=1.3068 | train/val/test=1.000/0.796/0.762 | c=0.998437
[Epoch 0026] loss=16.7009 cls=0.0827 smmd=1.0013 ct=11.5227 rec=1.3031 | train/val/test=1.000/0.792/0.756 | c=0.998437
[Epoch 0027] loss=16.3116 cls=0.0774 smmd=0.9208 ct=11.5391 rec=1.2989 | train/val/test=1.000/0.792/0.749 | c=0.998437
[Epoch 0028] loss=16.2080 cls=0.0758 smmd=0.8918 ct=11.5816 rec=1.2970 | train/val/test=1.000/0.798/0.767 | c=0.998437
[Epoch 0029] loss=15.7737 cls=0.0701 smmd=0.8197 ct=11.5104 rec=1.2976 | train/val/test=1.000/0.798/0.772 | c=0.998437
[Epoch 0030] loss=15.9180 cls=0.0752 smmd=0.8446 ct=11.5275 rec=1.3010 | train/val/test=1.000/0.786/0.761 | c=0.998437
[Epoch 0031] loss=15.7800 cls=0.0795 smmd=0.8144 ct=11.5384 rec=1.3005 | train/val/test=1.000/0.802/0.779 | c=0.998437
[Epoch 0032] loss=15.9054 cls=0.0777 smmd=0.8370 ct=11.5510 rec=1.3036 | train/val/test=1.000/0.800/0.773 | c=0.998437
[Epoch 0033] loss=15.9672 cls=0.0791 smmd=0.8526 ct=11.5344 rec=1.3032 | train/val/test=1.000/0.806/0.773 | c=0.998437
[Epoch 0034] loss=15.3110 cls=0.0729 smmd=0.7244 ct=11.5224 rec=1.3008 | train/val/test=1.000/0.804/0.776 | c=0.998437
[Epoch 0035] loss=15.2286 cls=0.0704 smmd=0.7030 ct=11.5484 rec=1.2988 | train/val/test=1.000/0.814/0.778 | c=0.998437
[Epoch 0036] loss=15.1572 cls=0.0713 smmd=0.7034 ct=11.4742 rec=1.3021 | train/val/test=1.000/0.812/0.773 | c=0.998437
[Epoch 0037] loss=14.6937 cls=0.0709 smmd=0.6006 ct=11.5253 rec=1.2981 | train/val/test=1.000/0.810/0.781 | c=0.998437
[Epoch 0038] loss=14.8846 cls=0.0732 smmd=0.6374 ct=11.5307 rec=1.3017 | train/val/test=1.000/0.812/0.777 | c=0.998437
[Epoch 0039] loss=14.7940 cls=0.0796 smmd=0.6259 ct=11.4939 rec=1.3064 | train/val/test=1.000/0.810/0.784 | c=0.998437
[Epoch 0040] loss=14.9799 cls=0.0858 smmd=0.6502 ct=11.5554 rec=1.3076 | train/val/test=1.000/0.818/0.783 | c=0.998437
[Epoch 0041] loss=15.0439 cls=0.0903 smmd=0.6736 ct=11.4997 rec=1.3107 | train/val/test=1.000/0.812/0.784 | c=0.998437
[Epoch 0042] loss=15.0536 cls=0.0915 smmd=0.6698 ct=11.5281 rec=1.3093 | train/val/test=1.000/0.816/0.774 | c=0.998437
[Epoch 0043] loss=14.6857 cls=0.0897 smmd=0.5937 ct=11.5414 rec=1.3117 | train/val/test=1.000/0.810/0.785 | c=0.998437
[Epoch 0044] loss=14.5326 cls=0.0861 smmd=0.5700 ct=11.5091 rec=1.3056 | train/val/test=1.000/0.814/0.783 | c=0.998437
[Epoch 0045] loss=14.4599 cls=0.0826 smmd=0.5586 ct=11.4949 rec=1.3062 | train/val/test=1.000/0.814/0.781 | c=0.998437
[Epoch 0046] loss=14.3006 cls=0.0828 smmd=0.5169 ct=11.5443 rec=1.3057 | train/val/test=1.000/0.812/0.780 | c=0.998437
[Epoch 0047] loss=14.1873 cls=0.0847 smmd=0.5033 ct=11.4977 rec=1.3079 | train/val/test=1.000/0.812/0.783 | c=0.998437
[Epoch 0048] loss=14.1972 cls=0.0901 smmd=0.5042 ct=11.5002 rec=1.3096 | train/val/test=1.000/0.820/0.775 | c=0.998437
[Epoch 0049] loss=14.5344 cls=0.0960 smmd=0.5594 ct=11.5578 rec=1.3149 | train/val/test=1.000/0.802/0.771 | c=0.998437
[Epoch 0050] loss=14.5449 cls=0.1028 smmd=0.5736 ct=11.4941 rec=1.3160 | train/val/test=1.000/0.820/0.770 | c=0.998437
[Epoch 0051] loss=14.4383 cls=0.1022 smmd=0.5408 ct=11.5510 rec=1.3204 | train/val/test=1.000/0.812/0.772 | c=0.998437
[Epoch 0052] loss=14.3849 cls=0.1000 smmd=0.5393 ct=11.5070 rec=1.3147 | train/val/test=1.000/0.812/0.779 | c=0.998437
[Epoch 0053] loss=14.1821 cls=0.0977 smmd=0.4988 ct=11.5078 rec=1.3161 | train/val/test=1.000/0.818/0.756 | c=0.998437
[Epoch 0054] loss=13.9681 cls=0.0995 smmd=0.4525 ct=11.5243 rec=1.3153 | train/val/test=1.000/0.808/0.780 | c=0.998437
[Epoch 0055] loss=13.8366 cls=0.0958 smmd=0.4304 ct=11.5051 rec=1.3149 | train/val/test=1.000/0.814/0.770 | c=0.998437
[Epoch 0056] loss=13.9553 cls=0.0968 smmd=0.4545 ct=11.5027 rec=1.3155 | train/val/test=1.000/0.812/0.762 | c=0.998437
[Epoch 0057] loss=14.0307 cls=0.1019 smmd=0.4648 ct=11.5238 rec=1.3205 | train/val/test=1.000/0.796/0.759 | c=0.998437
[Epoch 0058] loss=14.2171 cls=0.1111 smmd=0.5043 ct=11.5080 rec=1.3235 | train/val/test=1.000/0.782/0.739 | c=0.998437
[Epoch 0059] loss=14.3745 cls=0.1153 smmd=0.5288 ct=11.5401 rec=1.3292 | train/val/test=1.000/0.778/0.748 | c=0.998437
[Epoch 0060] loss=14.4755 cls=0.1149 smmd=0.5545 ct=11.5127 rec=1.3268 | train/val/test=1.000/0.786/0.742 | c=0.998437
[Epoch 0061] loss=13.9503 cls=0.1056 smmd=0.4502 ct=11.5139 rec=1.3268 | train/val/test=1.000/0.794/0.749 | c=0.998437
[Epoch 0062] loss=13.8188 cls=0.1006 smmd=0.4250 ct=11.5115 rec=1.3195 | train/val/test=1.000/0.808/0.761 | c=0.998437
[Epoch 0063] loss=13.9014 cls=0.0999 smmd=0.4422 ct=11.5077 rec=1.3255 | train/val/test=1.000/0.790/0.742 | c=0.998437
[Epoch 0064] loss=13.7591 cls=0.1039 smmd=0.4129 ct=11.5104 rec=1.3238 | train/val/test=1.000/0.810/0.756 | c=0.998437
[Epoch 0065] loss=13.6724 cls=0.0983 smmd=0.3932 ct=11.5244 rec=1.3259 | train/val/test=1.000/0.798/0.754 | c=0.998437
[Epoch 0066] loss=14.1305 cls=0.1013 smmd=0.4908 ct=11.4933 rec=1.3264 | train/val/test=1.000/0.800/0.751 | c=0.998437
[Epoch 0067] loss=14.2457 cls=0.1040 smmd=0.5046 ct=11.5377 rec=1.3301 | train/val/test=1.000/0.786/0.749 | c=0.998437
[Epoch 0068] loss=14.1145 cls=0.1068 smmd=0.4850 ct=11.5032 rec=1.3310 | train/val/test=1.000/0.792/0.743 | c=0.998437
[Epoch 0069] loss=14.1939 cls=0.1076 smmd=0.4944 ct=11.5349 rec=1.3337 | train/val/test=1.000/0.764/0.731 | c=0.998437
[Epoch 0070] loss=14.0579 cls=0.1120 smmd=0.4731 ct=11.5029 rec=1.3332 | train/val/test=1.000/0.786/0.743 | c=0.998437
[Epoch 0071] loss=13.8787 cls=0.1010 smmd=0.4351 ct=11.5197 rec=1.3315 | train/val/test=1.000/0.774/0.741 | c=0.998437
[Epoch 0072] loss=13.6815 cls=0.1005 smmd=0.4017 ct=11.4901 rec=1.3279 | train/val/test=1.000/0.806/0.762 | c=0.998437
[Epoch 0073] loss=13.7925 cls=0.0941 smmd=0.4228 ct=11.4985 rec=1.3273 | train/val/test=1.000/0.778/0.738 | c=0.998437
[Epoch 0074] loss=13.9380 cls=0.1028 smmd=0.4494 ct=11.5067 rec=1.3306 | train/val/test=1.000/0.800/0.760 | c=0.998437
[Epoch 0075] loss=13.9024 cls=0.1033 smmd=0.4429 ct=11.5027 rec=1.3330 | train/val/test=1.000/0.770/0.729 | c=0.998437
[Epoch 0076] loss=14.1302 cls=0.1112 smmd=0.4856 ct=11.5128 rec=1.3362 | train/val/test=1.000/0.800/0.755 | c=0.998437
[Epoch 0077] loss=14.2129 cls=0.1118 smmd=0.4991 ct=11.5277 rec=1.3375 | train/val/test=1.000/0.742/0.710 | c=0.998437
[Epoch 0078] loss=14.1420 cls=0.1172 smmd=0.4893 ct=11.5028 rec=1.3394 | train/val/test=1.000/0.800/0.751 | c=0.998437
[Epoch 0079] loss=13.9725 cls=0.1060 smmd=0.4509 ct=11.5316 rec=1.3349 | train/val/test=1.000/0.746/0.721 | c=0.998437
[Epoch 0080] loss=13.8633 cls=0.1019 smmd=0.4397 ct=11.4806 rec=1.3321 | train/val/test=1.000/0.804/0.761 | c=0.998437
[Epoch 0081] loss=13.7284 cls=0.0867 smmd=0.4109 ct=11.4981 rec=1.3261 | train/val/test=1.000/0.794/0.751 | c=0.998437
[Epoch 0082] loss=13.4229 cls=0.0877 smmd=0.3544 ct=11.4743 rec=1.3266 | train/val/test=1.000/0.800/0.754 | c=0.998437
[Epoch 0083] loss=13.6666 cls=0.0934 smmd=0.3996 ct=11.4887 rec=1.3303 | train/val/test=1.000/0.776/0.737 | c=0.998437
[Epoch 0084] loss=14.1147 cls=0.1054 smmd=0.4845 ct=11.5058 rec=1.3371 | train/val/test=1.000/0.772/0.734 | c=0.998437
[Epoch 0085] loss=14.3367 cls=0.1210 smmd=0.5252 ct=11.5158 rec=1.3446 | train/val/test=1.000/0.726/0.701 | c=0.998437
[Epoch 0086] loss=14.3515 cls=0.1320 smmd=0.5248 ct=11.5267 rec=1.3473 | train/val/test=1.000/0.704/0.669 | c=0.998437
[Epoch 0087] loss=14.2323 cls=0.1563 smmd=0.4921 ct=11.5572 rec=1.3647 | train/val/test=1.000/0.694/0.678 | c=0.998437
[Epoch 0088] loss=13.9053 cls=0.1377 smmd=0.4371 ct=11.5164 rec=1.3463 | train/val/test=1.000/0.762/0.732 | c=0.998437
[Epoch 0089] loss=13.5307 cls=0.0944 smmd=0.3678 ct=11.5110 rec=1.3336 | train/val/test=1.000/0.806/0.754 | c=0.998437
[Epoch 0090] loss=13.3356 cls=0.0733 smmd=0.3419 ct=11.4576 rec=1.3184 | train/val/test=1.000/0.792/0.754 | c=0.998437
[Epoch 0091] loss=13.3962 cls=0.0796 smmd=0.3515 ct=11.4668 rec=1.3233 | train/val/test=1.000/0.788/0.743 | c=0.998437
[Epoch 0092] loss=13.4928 cls=0.0902 smmd=0.3594 ct=11.5177 rec=1.3308 | train/val/test=1.000/0.790/0.752 | c=0.998437
[Epoch 0093] loss=13.7121 cls=0.0991 smmd=0.4082 ct=11.4880 rec=1.3351 | train/val/test=1.000/0.792/0.763 | c=0.998437
[Epoch 0094] loss=14.4171 cls=0.1065 smmd=0.5369 ct=11.5453 rec=1.3394 | train/val/test=1.000/0.798/0.756 | c=0.998437
[Epoch 0095] loss=14.9214 cls=0.1085 smmd=0.6446 ct=11.5104 rec=1.3398 | train/val/test=1.000/0.786/0.752 | c=0.998437
[Epoch 0096] loss=14.2020 cls=0.1081 smmd=0.4996 ct=11.5163 rec=1.3384 | train/val/test=1.000/0.800/0.755 | c=0.998437
[Epoch 0097] loss=13.5518 cls=0.0980 smmd=0.3785 ct=11.4770 rec=1.3343 | train/val/test=1.000/0.762/0.733 | c=0.998437
[Epoch 0098] loss=13.5526 cls=0.1081 smmd=0.3769 ct=11.4804 rec=1.3346 | train/val/test=1.000/0.800/0.763 | c=0.998437
[Epoch 0099] loss=13.3265 cls=0.0976 smmd=0.3363 ct=11.4631 rec=1.3330 | train/val/test=1.000/0.786/0.740 | c=0.998437
=== Best @ epoch 48: val=0.8200, test=0.7750 ===
[2025-09-20 03:58:30] END attempt 1: exit_code=0
