[2025-09-20 04:47:54] START attempt 1: python main.py --is_transfer True --test_dataset PubMed --pretrained_model_name /mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth --few True --shot 10 --gpu_id 0
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=39.8700 cls=1.1028 smmd=5.5797 ct=11.2785 rec=1.4138 | train/val/test=0.346/0.388/0.413 | c=0.998347
[Epoch 0001] loss=30.4760 cls=1.0796 smmd=3.7103 ct=11.2432 rec=1.4139 | train/val/test=0.577/0.472/0.469 | c=0.998347
[Epoch 0002] loss=34.7221 cls=1.0527 smmd=4.5591 ct=11.2587 rec=1.4135 | train/val/test=0.500/0.288/0.256 | c=0.998347
[Epoch 0003] loss=33.2620 cls=0.9881 smmd=4.2863 ct=11.1948 rec=1.4137 | train/val/test=0.731/0.694/0.682 | c=0.998347
[Epoch 0004] loss=23.9302 cls=0.8695 smmd=2.4706 ct=11.0012 rec=1.4105 | train/val/test=0.808/0.578/0.533 | c=0.998347
[Epoch 0005] loss=26.6707 cls=0.7433 smmd=3.0360 ct=10.9793 rec=1.4001 | train/val/test=0.885/0.708/0.683 | c=0.998347
[Epoch 0006] loss=28.4158 cls=0.6077 smmd=3.4089 ct=10.9288 rec=1.3880 | train/val/test=0.885/0.746/0.711 | c=0.998347
[Epoch 0007] loss=25.4246 cls=0.4683 smmd=2.8357 ct=10.8756 rec=1.3661 | train/val/test=0.885/0.700/0.663 | c=0.998347
[Epoch 0008] loss=20.6694 cls=0.3685 smmd=1.9007 ct=10.8471 rec=1.3449 | train/val/test=0.962/0.750/0.687 | c=0.998347
[Epoch 0009] loss=22.7768 cls=0.2965 smmd=2.3254 ct=10.8679 rec=1.3346 | train/val/test=0.962/0.784/0.755 | c=0.998347
[Epoch 0010] loss=24.3637 cls=0.2298 smmd=2.6490 ct=10.8713 rec=1.3271 | train/val/test=0.962/0.792/0.771 | c=0.998347
[Epoch 0011] loss=21.0462 cls=0.1946 smmd=1.9894 ct=10.8695 rec=1.3242 | train/val/test=1.000/0.784/0.761 | c=0.998347
[Epoch 0012] loss=20.4183 cls=0.1596 smmd=1.8677 ct=10.8677 rec=1.3215 | train/val/test=1.000/0.756/0.738 | c=0.998347
[Epoch 0013] loss=22.1981 cls=0.1382 smmd=2.2221 ct=10.8865 rec=1.3216 | train/val/test=1.000/0.816/0.761 | c=0.998347
[Epoch 0014] loss=21.2088 cls=0.1053 smmd=1.8711 ct=11.6699 rec=1.3096 | train/val/test=1.000/0.778/0.758 | c=0.998347
[Epoch 0015] loss=19.2475 cls=0.0935 smmd=1.5143 ct=11.4995 rec=1.2991 | train/val/test=1.000/0.790/0.753 | c=0.998347
[Epoch 0016] loss=20.1087 cls=0.0725 smmd=1.7030 ct=11.4284 rec=1.2925 | train/val/test=1.000/0.790/0.751 | c=0.998347
[Epoch 0017] loss=19.9602 cls=0.0630 smmd=1.6596 ct=11.5012 rec=1.2925 | train/val/test=1.000/0.768/0.747 | c=0.998347
[Epoch 0018] loss=18.1645 cls=0.0693 smmd=1.2824 ct=11.5882 rec=1.2942 | train/val/test=1.000/0.782/0.750 | c=0.998347
[Epoch 0019] loss=18.4397 cls=0.0748 smmd=1.3479 ct=11.5330 rec=1.3001 | train/val/test=1.000/0.776/0.734 | c=0.998347
[Epoch 0020] loss=18.5476 cls=0.0809 smmd=1.3750 ct=11.5021 rec=1.3020 | train/val/test=1.000/0.776/0.735 | c=0.998347
[Epoch 0021] loss=17.8964 cls=0.0845 smmd=1.2291 ct=11.5784 rec=1.3012 | train/val/test=1.000/0.776/0.742 | c=0.998347
[Epoch 0022] loss=17.7971 cls=0.0843 smmd=1.2065 ct=11.5923 rec=1.3001 | train/val/test=1.000/0.768/0.745 | c=0.998347
[Epoch 0023] loss=17.7531 cls=0.0858 smmd=1.2140 ct=11.5099 rec=1.3022 | train/val/test=1.000/0.784/0.739 | c=0.998347
[Epoch 0024] loss=17.0221 cls=0.0774 smmd=1.0670 ct=11.5184 rec=1.2989 | train/val/test=1.000/0.770/0.749 | c=0.998347
[Epoch 0025] loss=17.0010 cls=0.0756 smmd=1.0523 ct=11.5723 rec=1.2919 | train/val/test=1.000/0.796/0.749 | c=0.998347
[Epoch 0026] loss=16.3542 cls=0.0628 smmd=0.9310 ct=11.5388 rec=1.2926 | train/val/test=1.000/0.796/0.758 | c=0.998347
[Epoch 0027] loss=16.3839 cls=0.0582 smmd=0.9474 ct=11.4889 rec=1.2871 | train/val/test=1.000/0.766/0.744 | c=0.998347
[Epoch 0028] loss=15.9644 cls=0.0627 smmd=0.8528 ct=11.5404 rec=1.2847 | train/val/test=1.000/0.782/0.755 | c=0.998347
[Epoch 0029] loss=15.8984 cls=0.0604 smmd=0.8354 ct=11.5624 rec=1.2888 | train/val/test=1.000/0.812/0.758 | c=0.998347
[Epoch 0030] loss=16.0821 cls=0.0630 smmd=0.8835 ct=11.5035 rec=1.2948 | train/val/test=1.000/0.794/0.756 | c=0.998347
[Epoch 0031] loss=16.0251 cls=0.0609 smmd=0.8625 ct=11.5528 rec=1.2943 | train/val/test=1.000/0.802/0.756 | c=0.998347
[Epoch 0032] loss=16.0800 cls=0.0615 smmd=0.8764 ct=11.5374 rec=1.2965 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0033] loss=15.9669 cls=0.0583 smmd=0.8545 ct=11.5360 rec=1.2921 | train/val/test=1.000/0.808/0.754 | c=0.998347
[Epoch 0034] loss=15.2249 cls=0.0552 smmd=0.7077 ct=11.5292 rec=1.2970 | train/val/test=1.000/0.792/0.759 | c=0.998347
[Epoch 0035] loss=15.4020 cls=0.0520 smmd=0.7479 ct=11.5079 rec=1.2889 | train/val/test=1.000/0.794/0.765 | c=0.998347
[Epoch 0036] loss=14.9702 cls=0.0530 smmd=0.6619 ct=11.5054 rec=1.2905 | train/val/test=1.000/0.802/0.764 | c=0.998347
[Epoch 0037] loss=14.8161 cls=0.0572 smmd=0.6261 ct=11.5273 rec=1.2956 | train/val/test=1.000/0.792/0.765 | c=0.998347
[Epoch 0038] loss=14.9980 cls=0.0631 smmd=0.6641 ct=11.5167 rec=1.2939 | train/val/test=1.000/0.816/0.761 | c=0.998347
[Epoch 0039] loss=15.0237 cls=0.0691 smmd=0.6678 ct=11.5201 rec=1.2993 | train/val/test=1.000/0.818/0.760 | c=0.998347
[Epoch 0040] loss=15.2304 cls=0.0763 smmd=0.7023 ct=11.5502 rec=1.3056 | train/val/test=1.000/0.792/0.768 | c=0.998347
[Epoch 0041] loss=15.4325 cls=0.0814 smmd=0.7454 ct=11.5345 rec=1.3021 | train/val/test=1.000/0.816/0.772 | c=0.998347
[Epoch 0042] loss=15.1685 cls=0.0781 smmd=0.6941 ct=11.5286 rec=1.3054 | train/val/test=1.000/0.786/0.765 | c=0.998347
[Epoch 0043] loss=14.9157 cls=0.0807 smmd=0.6458 ct=11.5161 rec=1.3016 | train/val/test=1.000/0.786/0.756 | c=0.998347
[Epoch 0044] loss=14.6286 cls=0.0829 smmd=0.5876 ct=11.5181 rec=1.3098 | train/val/test=1.000/0.784/0.764 | c=0.998347
[Epoch 0045] loss=14.6311 cls=0.0767 smmd=0.5886 ct=11.5196 rec=1.2995 | train/val/test=1.000/0.820/0.773 | c=0.998347
[Epoch 0046] loss=14.3139 cls=0.0685 smmd=0.5287 ct=11.5057 rec=1.3023 | train/val/test=1.000/0.818/0.767 | c=0.998347
[Epoch 0047] loss=14.4038 cls=0.0699 smmd=0.5488 ct=11.4949 rec=1.3017 | train/val/test=1.000/0.818/0.765 | c=0.998347
[Epoch 0048] loss=14.4811 cls=0.0761 smmd=0.5537 ct=11.5436 rec=1.3073 | train/val/test=1.000/0.808/0.765 | c=0.998347
[Epoch 0049] loss=14.6502 cls=0.0854 smmd=0.5937 ct=11.5076 rec=1.3123 | train/val/test=1.000/0.784/0.740 | c=0.998347
[Epoch 0050] loss=15.0726 cls=0.1000 smmd=0.6701 ct=11.5396 rec=1.3246 | train/val/test=1.000/0.732/0.723 | c=0.998347
[Epoch 0051] loss=15.1413 cls=0.1246 smmd=0.6751 ct=11.5698 rec=1.3346 | train/val/test=1.000/0.726/0.687 | c=0.998347
[Epoch 0052] loss=14.6177 cls=0.1205 smmd=0.5801 ct=11.5233 rec=1.3370 | train/val/test=1.000/0.750/0.736 | c=0.998347
[Epoch 0053] loss=14.4056 cls=0.0988 smmd=0.5395 ct=11.5267 rec=1.3181 | train/val/test=1.000/0.814/0.764 | c=0.998347
[Epoch 0054] loss=14.0827 cls=0.0702 smmd=0.4807 ct=11.5133 rec=1.3096 | train/val/test=1.000/0.810/0.767 | c=0.998347
[Epoch 0055] loss=13.9651 cls=0.0646 smmd=0.4662 ct=11.4718 rec=1.3001 | train/val/test=1.000/0.814/0.773 | c=0.998347
[Epoch 0056] loss=14.0976 cls=0.0679 smmd=0.4828 ct=11.5192 rec=1.3041 | train/val/test=1.000/0.810/0.767 | c=0.998347
[Epoch 0057] loss=14.0738 cls=0.0761 smmd=0.4762 ct=11.5240 rec=1.3089 | train/val/test=1.000/0.812/0.765 | c=0.998347
[Epoch 0058] loss=14.4856 cls=0.0883 smmd=0.5584 ct=11.5172 rec=1.3203 | train/val/test=1.000/0.766/0.747 | c=0.998347
[Epoch 0059] loss=15.0390 cls=0.1072 smmd=0.6613 ct=11.5460 rec=1.3287 | train/val/test=1.000/0.748/0.713 | c=0.998347
[Epoch 0060] loss=14.8592 cls=0.1152 smmd=0.6205 ct=11.5655 rec=1.3366 | train/val/test=1.000/0.740/0.733 | c=0.998347
[Epoch 0061] loss=14.4880 cls=0.1116 smmd=0.5554 ct=11.5217 rec=1.3336 | train/val/test=1.000/0.730/0.691 | c=0.998347
[Epoch 0062] loss=14.1817 cls=0.1088 smmd=0.4946 ct=11.5212 rec=1.3329 | train/val/test=1.000/0.768/0.740 | c=0.998347
[Epoch 0063] loss=13.9596 cls=0.0906 smmd=0.4533 ct=11.5158 rec=1.3216 | train/val/test=1.000/0.786/0.734 | c=0.998347
[Epoch 0064] loss=13.8822 cls=0.0811 smmd=0.4452 ct=11.4836 rec=1.3186 | train/val/test=1.000/0.800/0.767 | c=0.998347
[Epoch 0065] loss=13.6833 cls=0.0742 smmd=0.4031 ct=11.4992 rec=1.3127 | train/val/test=1.000/0.814/0.764 | c=0.998347
[Epoch 0066] loss=13.9660 cls=0.0799 smmd=0.4556 ct=11.5163 rec=1.3185 | train/val/test=1.000/0.790/0.761 | c=0.998347
[Epoch 0067] loss=14.2575 cls=0.0914 smmd=0.5135 ct=11.5119 rec=1.3240 | train/val/test=1.000/0.792/0.744 | c=0.998347
[Epoch 0068] loss=14.7556 cls=0.1020 smmd=0.6008 ct=11.5674 rec=1.3335 | train/val/test=1.000/0.730/0.725 | c=0.998347
[Epoch 0069] loss=15.0654 cls=0.1206 smmd=0.6678 ct=11.5320 rec=1.3407 | train/val/test=1.000/0.720/0.675 | c=0.998347
[Epoch 0070] loss=14.3242 cls=0.1191 smmd=0.5172 ct=11.5445 rec=1.3433 | train/val/test=1.000/0.736/0.723 | c=0.998347
[Epoch 0071] loss=14.1002 cls=0.1090 smmd=0.4765 ct=11.5300 rec=1.3336 | train/val/test=1.000/0.746/0.721 | c=0.998347
[Epoch 0072] loss=13.8004 cls=0.0824 smmd=0.4270 ct=11.4920 rec=1.3237 | train/val/test=1.000/0.806/0.770 | c=0.998347
[Epoch 0073] loss=13.6350 cls=0.0633 smmd=0.3997 ct=11.4740 rec=1.3063 | train/val/test=1.000/0.816/0.775 | c=0.998347
[Epoch 0074] loss=13.6106 cls=0.0628 smmd=0.3894 ct=11.5012 rec=1.3089 | train/val/test=1.000/0.804/0.762 | c=0.998347
[Epoch 0075] loss=13.6040 cls=0.0729 smmd=0.3877 ct=11.4972 rec=1.3173 | train/val/test=1.000/0.806/0.774 | c=0.998347
[Epoch 0076] loss=14.0609 cls=0.0852 smmd=0.4731 ct=11.5202 rec=1.3245 | train/val/test=1.000/0.810/0.769 | c=0.998347
[Epoch 0077] loss=14.9731 cls=0.0944 smmd=0.6492 ct=11.5470 rec=1.3313 | train/val/test=1.000/0.802/0.771 | c=0.998347
[Epoch 0078] loss=15.0771 cls=0.0943 smmd=0.6736 ct=11.5286 rec=1.3315 | train/val/test=1.000/0.802/0.771 | c=0.998347
[Epoch 0079] loss=14.1311 cls=0.0863 smmd=0.4905 ct=11.5027 rec=1.3273 | train/val/test=1.000/0.800/0.757 | c=0.998347
[Epoch 0080] loss=13.8291 cls=0.0820 smmd=0.4325 ct=11.4932 rec=1.3240 | train/val/test=1.000/0.804/0.769 | c=0.998347
[Epoch 0081] loss=13.6485 cls=0.0755 smmd=0.3987 ct=11.4851 rec=1.3192 | train/val/test=1.000/0.808/0.759 | c=0.998347
[Epoch 0082] loss=13.3340 cls=0.0770 smmd=0.3390 ct=11.4683 rec=1.3208 | train/val/test=1.000/0.804/0.765 | c=0.998347
[Epoch 0083] loss=13.4681 cls=0.0854 smmd=0.3615 ct=11.4855 rec=1.3229 | train/val/test=1.000/0.806/0.762 | c=0.998347
[Epoch 0084] loss=13.6254 cls=0.0942 smmd=0.3848 ct=11.5214 rec=1.3300 | train/val/test=1.000/0.778/0.749 | c=0.998347
[Epoch 0085] loss=13.9467 cls=0.1131 smmd=0.4488 ct=11.5123 rec=1.3370 | train/val/test=1.000/0.690/0.668 | c=0.998347
[Epoch 0086] loss=15.1026 cls=0.1529 smmd=0.6599 ct=11.5908 rec=1.3564 | train/val/test=0.885/0.610/0.608 | c=0.998347
[Epoch 0087] loss=15.6529 cls=0.2752 smmd=0.7508 ct=11.6212 rec=1.4021 | train/val/test=0.846/0.560/0.549 | c=0.998347
[Epoch 0088] loss=15.1252 cls=0.3112 smmd=0.6375 ct=11.6439 rec=1.3837 | train/val/test=0.923/0.686/0.690 | c=0.998347
[Epoch 0089] loss=14.4577 cls=0.1723 smmd=0.5354 ct=11.5582 rec=1.3630 | train/val/test=1.000/0.814/0.764 | c=0.998347
[Epoch 0090] loss=13.6998 cls=0.0395 smmd=0.4159 ct=11.4708 rec=1.2966 | train/val/test=1.000/0.750/0.710 | c=0.998347
[Epoch 0091] loss=13.7940 cls=0.0531 smmd=0.4290 ct=11.4909 rec=1.3138 | train/val/test=1.000/0.784/0.762 | c=0.998347
[Epoch 0092] loss=13.5655 cls=0.0465 smmd=0.3845 ct=11.4899 rec=1.2961 | train/val/test=1.000/0.812/0.781 | c=0.998347
[Epoch 0093] loss=13.6354 cls=0.0394 smmd=0.3964 ct=11.5037 rec=1.2982 | train/val/test=1.000/0.784/0.732 | c=0.998347
[Epoch 0094] loss=13.9836 cls=0.0599 smmd=0.4600 ct=11.5213 rec=1.3215 | train/val/test=1.000/0.800/0.767 | c=0.998347
[Epoch 0095] loss=14.4738 cls=0.0791 smmd=0.5437 ct=11.5824 rec=1.3324 | train/val/test=1.000/0.718/0.691 | c=0.998347
[Epoch 0096] loss=15.6108 cls=0.1158 smmd=0.7631 ct=11.6020 rec=1.3516 | train/val/test=1.000/0.602/0.596 | c=0.998347
[Epoch 0097] loss=15.9101 cls=0.1768 smmd=0.8153 ct=11.6062 rec=1.3886 | train/val/test=0.962/0.654/0.625 | c=0.998347
[Epoch 0098] loss=14.4361 cls=0.1521 smmd=0.5208 ct=11.6200 rec=1.3588 | train/val/test=1.000/0.688/0.652 | c=0.998347
[Epoch 0099] loss=14.1039 cls=0.1053 smmd=0.4859 ct=11.4876 rec=1.3394 | train/val/test=1.000/0.784/0.744 | c=0.998347
=== Best @ epoch 45: val=0.8200, test=0.7730 ===
[2025-09-20 04:50:45] END attempt 1: exit_code=0
