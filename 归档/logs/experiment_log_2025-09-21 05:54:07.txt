Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 - 2025-09-21 05:54:07:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.6863 cls=1.1071 smmd=5.6580 ct=11.2809 rec=1.4138 | train/val/test=0.385/0.206/0.188 | c=0.998347
[Epoch 0001] loss=22.5109 cls=1.0943 smmd=3.9989 ct=11.2598 rec=1.4136 | train/val/test=0.385/0.428/0.424 | c=0.998347
[Epoch 0002] loss=24.7557 cls=1.0886 smmd=4.8983 ct=11.2587 rec=1.4137 | train/val/test=0.615/0.400/0.391 | c=0.998347
[Epoch 0003] loss=23.6475 cls=1.0599 smmd=4.5074 ct=11.1424 rec=1.4134 | train/val/test=0.769/0.668/0.647 | c=0.998347
[Epoch 0004] loss=18.3952 cls=1.0093 smmd=2.4678 ct=11.0147 rec=1.4128 | train/val/test=0.846/0.622/0.634 | c=0.998347
[Epoch 0005] loss=19.7586 cls=0.9685 smmd=3.0432 ct=10.9609 rec=1.4108 | train/val/test=0.846/0.626/0.634 | c=0.998347
[Epoch 0006] loss=20.3505 cls=0.9177 smmd=3.3103 ct=10.9116 rec=1.4084 | train/val/test=0.923/0.664/0.657 | c=0.998347
[Epoch 0007] loss=18.3431 cls=0.8634 smmd=2.5371 ct=10.8661 rec=1.4052 | train/val/test=0.846/0.652/0.658 | c=0.998347
[Epoch 0008] loss=17.2000 cls=0.8279 smmd=1.8172 ct=11.5417 rec=1.4024 | train/val/test=0.846/0.628/0.639 | c=0.998347
[Epoch 0009] loss=18.8402 cls=0.8167 smmd=2.5104 ct=11.4551 rec=1.4015 | train/val/test=0.923/0.666/0.666 | c=0.998347
[Epoch 0010] loss=18.9363 cls=0.7878 smmd=2.5605 ct=11.4404 rec=1.4014 | train/val/test=0.923/0.690/0.681 | c=0.998347
[Epoch 0011] loss=16.6940 cls=0.7620 smmd=1.6391 ct=11.5141 rec=1.4024 | train/val/test=0.923/0.682/0.680 | c=0.998347
[Epoch 0012] loss=18.3825 cls=0.7513 smmd=2.2948 ct=11.5685 rec=1.4025 | train/val/test=0.923/0.708/0.695 | c=0.998347
[Epoch 0013] loss=17.9201 cls=0.7116 smmd=2.1474 ct=11.4960 rec=1.3995 | train/val/test=0.923/0.692/0.686 | c=0.998347
[Epoch 0014] loss=16.5750 cls=0.6467 smmd=1.6475 ct=11.4359 rec=1.3941 | train/val/test=0.923/0.706/0.700 | c=0.998347
[Epoch 0015] loss=16.4936 cls=0.5870 smmd=1.6115 ct=11.4768 rec=1.3891 | train/val/test=0.923/0.696/0.701 | c=0.998347
[Epoch 0016] loss=16.3029 cls=0.5424 smmd=1.5375 ct=11.4960 rec=1.3840 | train/val/test=0.923/0.704/0.704 | c=0.998347
[Epoch 0017] loss=15.9256 cls=0.5153 smmd=1.3968 ct=11.4858 rec=1.3807 | train/val/test=1.000/0.718/0.702 | c=0.998347
[Epoch 0018] loss=15.5810 cls=0.4996 smmd=1.2633 ct=11.4829 rec=1.3803 | train/val/test=1.000/0.706/0.717 | c=0.998347
[Epoch 0019] loss=15.5709 cls=0.4866 smmd=1.2756 ct=11.4481 rec=1.3811 | train/val/test=1.000/0.722/0.716 | c=0.998347
[Epoch 0020] loss=15.7193 cls=0.4781 smmd=1.3277 ct=11.4691 rec=1.3837 | train/val/test=1.000/0.728/0.721 | c=0.998347
[Epoch 0021] loss=15.2918 cls=0.4712 smmd=1.1380 ct=11.5186 rec=1.3853 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0022] loss=15.7805 cls=0.4539 smmd=1.3353 ct=11.5227 rec=1.3851 | train/val/test=1.000/0.740/0.735 | c=0.998347
[Epoch 0023] loss=15.2209 cls=0.4138 smmd=1.1411 ct=11.4713 rec=1.3799 | train/val/test=1.000/0.724/0.733 | c=0.998347
[Epoch 0024] loss=15.1268 cls=0.3682 smmd=1.1022 ct=11.4998 rec=1.3746 | train/val/test=1.000/0.736/0.729 | c=0.998347
[Epoch 0025] loss=14.7482 cls=0.3293 smmd=0.9777 ct=11.4561 rec=1.3667 | train/val/test=1.000/0.740/0.717 | c=0.998347
[Epoch 0026] loss=14.6197 cls=0.3012 smmd=0.9322 ct=11.4581 rec=1.3609 | train/val/test=1.000/0.730/0.732 | c=0.998347
[Epoch 0027] loss=14.4455 cls=0.2760 smmd=0.8612 ct=11.4756 rec=1.3579 | train/val/test=1.000/0.732/0.734 | c=0.998347
[Epoch 0028] loss=14.4243 cls=0.2672 smmd=0.8505 ct=11.4858 rec=1.3571 | train/val/test=1.000/0.732/0.731 | c=0.998347
[Epoch 0029] loss=14.3405 cls=0.2687 smmd=0.8189 ct=11.4801 rec=1.3578 | train/val/test=1.000/0.730/0.728 | c=0.998347
[Epoch 0030] loss=14.8265 cls=0.2729 smmd=1.0091 ct=11.4875 rec=1.3595 | train/val/test=1.000/0.722/0.732 | c=0.998347
[Epoch 0031] loss=14.3779 cls=0.2631 smmd=0.8276 ct=11.4978 rec=1.3593 | train/val/test=1.000/0.710/0.713 | c=0.998347
[Epoch 0032] loss=14.9414 cls=0.2584 smmd=1.0526 ct=11.5034 rec=1.3545 | train/val/test=1.000/0.712/0.725 | c=0.998347
[Epoch 0033] loss=14.1982 cls=0.2245 smmd=0.7678 ct=11.4927 rec=1.3474 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0034] loss=14.1932 cls=0.2092 smmd=0.7768 ct=11.4764 rec=1.3403 | train/val/test=1.000/0.722/0.722 | c=0.998347
[Epoch 0035] loss=14.0807 cls=0.1878 smmd=0.7421 ct=11.4643 rec=1.3344 | train/val/test=1.000/0.720/0.716 | c=0.998347
[Epoch 0036] loss=13.8211 cls=0.1829 smmd=0.6361 ct=11.4729 rec=1.3332 | train/val/test=1.000/0.726/0.711 | c=0.998347
[Epoch 0037] loss=13.9285 cls=0.1911 smmd=0.6783 ct=11.4689 rec=1.3367 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0038] loss=14.0318 cls=0.1955 smmd=0.7155 ct=11.4750 rec=1.3406 | train/val/test=1.000/0.726/0.724 | c=0.998347
[Epoch 0039] loss=14.0180 cls=0.2050 smmd=0.6963 ct=11.5020 rec=1.3453 | train/val/test=1.000/0.730/0.726 | c=0.998347
[Epoch 0040] loss=14.2858 cls=0.2071 smmd=0.8037 ct=11.5003 rec=1.3453 | train/val/test=1.000/0.722/0.731 | c=0.998347
[Epoch 0041] loss=14.2502 cls=0.1877 smmd=0.8044 ct=11.4743 rec=1.3421 | train/val/test=1.000/0.730/0.715 | c=0.998347
[Epoch 0042] loss=13.8642 cls=0.1725 smmd=0.6494 ct=11.4871 rec=1.3347 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0043] loss=13.7407 cls=0.1489 smmd=0.6057 ct=11.4883 rec=1.3276 | train/val/test=1.000/0.724/0.705 | c=0.998347
[Epoch 0044] loss=13.6482 cls=0.1430 smmd=0.5807 ct=11.4625 rec=1.3247 | train/val/test=1.000/0.722/0.721 | c=0.998347
[Epoch 0045] loss=13.5564 cls=0.1359 smmd=0.5384 ct=11.4808 rec=1.3235 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0046] loss=13.7261 cls=0.1459 smmd=0.6052 ct=11.4765 rec=1.3273 | train/val/test=1.000/0.708/0.725 | c=0.998347
[Epoch 0047] loss=13.8450 cls=0.1514 smmd=0.6425 ct=11.4966 rec=1.3330 | train/val/test=1.000/0.712/0.706 | c=0.998347
[Epoch 0048] loss=14.0252 cls=0.1629 smmd=0.7084 ct=11.5052 rec=1.3352 | train/val/test=1.000/0.704/0.716 | c=0.998347
[Epoch 0049] loss=14.1001 cls=0.1577 smmd=0.7340 ct=11.5177 rec=1.3370 | train/val/test=1.000/0.690/0.671 | c=0.998347
[Epoch 0050] loss=13.8273 cls=0.1523 smmd=0.6382 ct=11.4895 rec=1.3322 | train/val/test=1.000/0.704/0.714 | c=0.998347
[Epoch 0051] loss=13.8376 cls=0.1278 smmd=0.6349 ct=11.5244 rec=1.3240 | train/val/test=1.000/0.698/0.682 | c=0.998347
[Epoch 0052] loss=13.5427 cls=0.1165 smmd=0.5413 ct=11.4725 rec=1.3171 | train/val/test=1.000/0.716/0.719 | c=0.998347
[Epoch 0053] loss=13.5322 cls=0.0965 smmd=0.5451 ct=11.4654 rec=1.3118 | train/val/test=1.000/0.724/0.711 | c=0.998347
[Epoch 0054] loss=13.4303 cls=0.1063 smmd=0.4955 ct=11.4812 rec=1.3145 | train/val/test=1.000/0.726/0.723 | c=0.998347
[Epoch 0055] loss=13.6011 cls=0.1106 smmd=0.5602 ct=11.4854 rec=1.3198 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0056] loss=13.7703 cls=0.1266 smmd=0.6249 ct=11.4823 rec=1.3249 | train/val/test=1.000/0.718/0.724 | c=0.998347
[Epoch 0057] loss=13.9969 cls=0.1322 smmd=0.7001 ct=11.5154 rec=1.3302 | train/val/test=1.000/0.708/0.694 | c=0.998347
[Epoch 0058] loss=13.9246 cls=0.1409 smmd=0.6778 ct=11.4957 rec=1.3278 | train/val/test=1.000/0.696/0.719 | c=0.998347
[Epoch 0059] loss=13.7821 cls=0.1361 smmd=0.6157 ct=11.5104 rec=1.3290 | train/val/test=1.000/0.668/0.656 | c=0.998347
[Epoch 0060] loss=13.6679 cls=0.1397 smmd=0.5734 ct=11.5027 rec=1.3236 | train/val/test=1.000/0.700/0.718 | c=0.998347
[Epoch 0061] loss=13.5222 cls=0.1100 smmd=0.5231 ct=11.5019 rec=1.3153 | train/val/test=1.000/0.708/0.688 | c=0.998347
[Epoch 0062] loss=13.3308 cls=0.0912 smmd=0.4678 ct=11.4625 rec=1.3066 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0063] loss=13.3249 cls=0.0805 smmd=0.4614 ct=11.4790 rec=1.3045 | train/val/test=1.000/0.728/0.715 | c=0.998347
[Epoch 0064] loss=13.4350 cls=0.0891 smmd=0.5029 ct=11.4779 rec=1.3109 | train/val/test=1.000/0.728/0.716 | c=0.998347
[Epoch 0065] loss=13.6483 cls=0.1031 smmd=0.5763 ct=11.4969 rec=1.3181 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0066] loss=13.9642 cls=0.1104 smmd=0.6940 ct=11.5118 rec=1.3245 | train/val/test=1.000/0.726/0.710 | c=0.998347
[Epoch 0067] loss=13.9473 cls=0.1090 smmd=0.6989 ct=11.4858 rec=1.3194 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0068] loss=13.5804 cls=0.0925 smmd=0.5517 ct=11.4972 rec=1.3153 | train/val/test=1.000/0.704/0.700 | c=0.998347
[Epoch 0069] loss=13.3781 cls=0.0999 smmd=0.4763 ct=11.4824 rec=1.3102 | train/val/test=1.000/0.710/0.720 | c=0.998347
[Epoch 0070] loss=13.3137 cls=0.0833 smmd=0.4568 ct=11.4764 rec=1.3072 | train/val/test=1.000/0.702/0.688 | c=0.998347
[Epoch 0071] loss=13.2466 cls=0.0984 smmd=0.4295 ct=11.4696 rec=1.3083 | train/val/test=1.000/0.706/0.716 | c=0.998347
[Epoch 0072] loss=13.3745 cls=0.0965 smmd=0.4658 ct=11.5054 rec=1.3128 | train/val/test=1.000/0.682/0.670 | c=0.998347
[Epoch 0073] loss=13.5079 cls=0.1210 smmd=0.5155 ct=11.4980 rec=1.3213 | train/val/test=1.000/0.680/0.699 | c=0.998347
[Epoch 0074] loss=13.9235 cls=0.1665 smmd=0.6402 ct=11.5633 rec=1.3526 | train/val/test=1.000/0.528/0.496 | c=0.998347
[Epoch 0075] loss=14.2749 cls=0.2365 smmd=0.7497 ct=11.6040 rec=1.3568 | train/val/test=0.923/0.664/0.683 | c=0.998347
[Epoch 0076] loss=14.0600 cls=0.1835 smmd=0.6808 ct=11.5850 rec=1.3627 | train/val/test=1.000/0.708/0.673 | c=0.998347
[Epoch 0077] loss=13.2047 cls=0.0492 smmd=0.4175 ct=11.4896 rec=1.2937 | train/val/test=1.000/0.684/0.660 | c=0.998347
[Epoch 0078] loss=13.5129 cls=0.0527 smmd=0.5427 ct=11.4821 rec=1.2957 | train/val/test=1.000/0.714/0.724 | c=0.998347
[Epoch 0079] loss=13.0686 cls=0.0304 smmd=0.3708 ct=11.4823 rec=1.2884 | train/val/test=1.000/0.718/0.714 | c=0.998347
[Epoch 0080] loss=13.2481 cls=0.0333 smmd=0.4308 ct=11.5100 rec=1.2893 | train/val/test=1.000/0.680/0.670 | c=0.998347
[Epoch 0081] loss=13.5455 cls=0.0754 smmd=0.5393 ct=11.5038 rec=1.3112 | train/val/test=1.000/0.712/0.724 | c=0.998347
[Epoch 0082] loss=13.4014 cls=0.0676 smmd=0.4756 ct=11.5215 rec=1.3143 | train/val/test=1.000/0.720/0.716 | c=0.998347
[Epoch 0083] loss=13.9128 cls=0.0887 smmd=0.6638 ct=11.5482 rec=1.3213 | train/val/test=1.000/0.660/0.669 | c=0.998347
[Epoch 0084] loss=14.2762 cls=0.1316 smmd=0.8082 ct=11.5270 rec=1.3260 | train/val/test=1.000/0.682/0.703 | c=0.998347
[Epoch 0085] loss=13.7584 cls=0.1559 smmd=0.5755 ct=11.5665 rec=1.3502 | train/val/test=0.923/0.474/0.457 | c=0.998347
[Epoch 0086] loss=14.0139 cls=0.2791 smmd=0.6594 ct=11.5548 rec=1.3420 | train/val/test=0.923/0.656/0.682 | c=0.998347
[Epoch 0087] loss=13.9090 cls=0.2006 smmd=0.6178 ct=11.5857 rec=1.3573 | train/val/test=1.000/0.658/0.669 | c=0.998347
[Epoch 0088] loss=13.2797 cls=0.0635 smmd=0.4614 ct=11.4435 rec=1.3018 | train/val/test=1.000/0.672/0.683 | c=0.998347
[Epoch 0089] loss=13.1843 cls=0.0525 smmd=0.4194 ct=11.4592 rec=1.3006 | train/val/test=1.000/0.688/0.703 | c=0.998347
[Epoch 0090] loss=13.4646 cls=0.0799 smmd=0.4830 ct=11.5524 rec=1.3292 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0091] loss=13.6995 cls=0.0776 smmd=0.5893 ct=11.5273 rec=1.3199 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0092] loss=13.8412 cls=0.0547 smmd=0.6592 ct=11.5107 rec=1.3099 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0093] loss=13.7379 cls=0.0560 smmd=0.6112 ct=11.5301 rec=1.3037 | train/val/test=1.000/0.720/0.718 | c=0.998347
[Epoch 0094] loss=13.6965 cls=0.0476 smmd=0.6094 ct=11.4999 rec=1.2988 | train/val/test=1.000/0.714/0.694 | c=0.998347
[Epoch 0095] loss=13.5491 cls=0.0495 smmd=0.5489 ct=11.5039 rec=1.2965 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0096] loss=13.2542 cls=0.0432 smmd=0.4389 ct=11.4906 rec=1.2895 | train/val/test=1.000/0.708/0.685 | c=0.998347
[Epoch 0097] loss=13.2817 cls=0.0561 smmd=0.4533 ct=11.4755 rec=1.2901 | train/val/test=1.000/0.712/0.707 | c=0.998347
[Epoch 0098] loss=13.2441 cls=0.0618 smmd=0.4320 ct=11.4858 rec=1.2950 | train/val/test=1.000/0.698/0.690 | c=0.998347
[Epoch 0099] loss=13.2309 cls=0.1018 smmd=0.4143 ct=11.4917 rec=1.3052 | train/val/test=1.000/0.702/0.705 | c=0.998347
=== Best @ epoch 22: val=0.7400, test=0.7350 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 - 2025-09-21 05:54:07:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.6863 cls=1.1071 smmd=5.6580 ct=11.2809 rec=1.4138 | train/val/test=0.385/0.206/0.188 | c=0.998347
[Epoch 0001] loss=22.5109 cls=1.0943 smmd=3.9989 ct=11.2598 rec=1.4136 | train/val/test=0.385/0.428/0.424 | c=0.998347
[Epoch 0002] loss=24.7557 cls=1.0886 smmd=4.8983 ct=11.2587 rec=1.4137 | train/val/test=0.615/0.400/0.391 | c=0.998347
[Epoch 0003] loss=23.6475 cls=1.0599 smmd=4.5074 ct=11.1424 rec=1.4134 | train/val/test=0.769/0.668/0.647 | c=0.998347
[Epoch 0004] loss=18.3952 cls=1.0093 smmd=2.4678 ct=11.0147 rec=1.4128 | train/val/test=0.846/0.622/0.634 | c=0.998347
[Epoch 0005] loss=19.7586 cls=0.9685 smmd=3.0432 ct=10.9609 rec=1.4108 | train/val/test=0.846/0.626/0.634 | c=0.998347
[Epoch 0006] loss=20.3505 cls=0.9177 smmd=3.3103 ct=10.9116 rec=1.4084 | train/val/test=0.923/0.664/0.657 | c=0.998347
[Epoch 0007] loss=18.3431 cls=0.8634 smmd=2.5371 ct=10.8661 rec=1.4052 | train/val/test=0.846/0.652/0.658 | c=0.998347
[Epoch 0008] loss=17.2000 cls=0.8279 smmd=1.8172 ct=11.5417 rec=1.4024 | train/val/test=0.846/0.628/0.639 | c=0.998347
[Epoch 0009] loss=18.8402 cls=0.8167 smmd=2.5104 ct=11.4551 rec=1.4015 | train/val/test=0.923/0.666/0.666 | c=0.998347
[Epoch 0010] loss=18.9363 cls=0.7878 smmd=2.5605 ct=11.4404 rec=1.4014 | train/val/test=0.923/0.690/0.681 | c=0.998347
[Epoch 0011] loss=16.6940 cls=0.7620 smmd=1.6391 ct=11.5141 rec=1.4024 | train/val/test=0.923/0.682/0.680 | c=0.998347
[Epoch 0012] loss=18.3825 cls=0.7513 smmd=2.2948 ct=11.5685 rec=1.4025 | train/val/test=0.923/0.708/0.695 | c=0.998347
[Epoch 0013] loss=17.9201 cls=0.7116 smmd=2.1474 ct=11.4960 rec=1.3995 | train/val/test=0.923/0.692/0.686 | c=0.998347
[Epoch 0014] loss=16.5750 cls=0.6467 smmd=1.6475 ct=11.4359 rec=1.3941 | train/val/test=0.923/0.706/0.700 | c=0.998347
[Epoch 0015] loss=16.4936 cls=0.5870 smmd=1.6115 ct=11.4768 rec=1.3891 | train/val/test=0.923/0.696/0.701 | c=0.998347
[Epoch 0016] loss=16.3029 cls=0.5424 smmd=1.5375 ct=11.4960 rec=1.3840 | train/val/test=0.923/0.704/0.704 | c=0.998347
[Epoch 0017] loss=15.9256 cls=0.5153 smmd=1.3968 ct=11.4858 rec=1.3807 | train/val/test=1.000/0.718/0.702 | c=0.998347
[Epoch 0018] loss=15.5810 cls=0.4996 smmd=1.2633 ct=11.4829 rec=1.3803 | train/val/test=1.000/0.706/0.717 | c=0.998347
[Epoch 0019] loss=15.5709 cls=0.4866 smmd=1.2756 ct=11.4481 rec=1.3811 | train/val/test=1.000/0.722/0.716 | c=0.998347
[Epoch 0020] loss=15.7193 cls=0.4781 smmd=1.3277 ct=11.4691 rec=1.3837 | train/val/test=1.000/0.728/0.721 | c=0.998347
[Epoch 0021] loss=15.2918 cls=0.4712 smmd=1.1380 ct=11.5186 rec=1.3853 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0022] loss=15.7805 cls=0.4539 smmd=1.3353 ct=11.5227 rec=1.3851 | train/val/test=1.000/0.740/0.735 | c=0.998347
[Epoch 0023] loss=15.2209 cls=0.4138 smmd=1.1411 ct=11.4713 rec=1.3799 | train/val/test=1.000/0.724/0.733 | c=0.998347
[Epoch 0024] loss=15.1268 cls=0.3682 smmd=1.1022 ct=11.4998 rec=1.3746 | train/val/test=1.000/0.736/0.729 | c=0.998347
[Epoch 0025] loss=14.7482 cls=0.3293 smmd=0.9777 ct=11.4561 rec=1.3667 | train/val/test=1.000/0.740/0.717 | c=0.998347
[Epoch 0026] loss=14.6197 cls=0.3012 smmd=0.9322 ct=11.4581 rec=1.3609 | train/val/test=1.000/0.730/0.732 | c=0.998347
[Epoch 0027] loss=14.4455 cls=0.2760 smmd=0.8612 ct=11.4756 rec=1.3579 | train/val/test=1.000/0.732/0.734 | c=0.998347
[Epoch 0028] loss=14.4243 cls=0.2672 smmd=0.8505 ct=11.4858 rec=1.3571 | train/val/test=1.000/0.732/0.731 | c=0.998347
[Epoch 0029] loss=14.3405 cls=0.2687 smmd=0.8189 ct=11.4801 rec=1.3578 | train/val/test=1.000/0.730/0.728 | c=0.998347
[Epoch 0030] loss=14.8265 cls=0.2729 smmd=1.0091 ct=11.4875 rec=1.3595 | train/val/test=1.000/0.722/0.732 | c=0.998347
[Epoch 0031] loss=14.3779 cls=0.2631 smmd=0.8276 ct=11.4978 rec=1.3593 | train/val/test=1.000/0.710/0.713 | c=0.998347
[Epoch 0032] loss=14.9414 cls=0.2584 smmd=1.0526 ct=11.5034 rec=1.3545 | train/val/test=1.000/0.712/0.725 | c=0.998347
[Epoch 0033] loss=14.1982 cls=0.2245 smmd=0.7678 ct=11.4927 rec=1.3474 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0034] loss=14.1932 cls=0.2092 smmd=0.7768 ct=11.4764 rec=1.3403 | train/val/test=1.000/0.722/0.722 | c=0.998347
[Epoch 0035] loss=14.0807 cls=0.1878 smmd=0.7421 ct=11.4643 rec=1.3344 | train/val/test=1.000/0.720/0.716 | c=0.998347
[Epoch 0036] loss=13.8211 cls=0.1829 smmd=0.6361 ct=11.4729 rec=1.3332 | train/val/test=1.000/0.726/0.711 | c=0.998347
[Epoch 0037] loss=13.9285 cls=0.1911 smmd=0.6783 ct=11.4689 rec=1.3367 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0038] loss=14.0318 cls=0.1955 smmd=0.7155 ct=11.4750 rec=1.3406 | train/val/test=1.000/0.726/0.724 | c=0.998347
[Epoch 0039] loss=14.0180 cls=0.2050 smmd=0.6963 ct=11.5020 rec=1.3453 | train/val/test=1.000/0.730/0.726 | c=0.998347
[Epoch 0040] loss=14.2858 cls=0.2071 smmd=0.8037 ct=11.5003 rec=1.3453 | train/val/test=1.000/0.722/0.731 | c=0.998347
[Epoch 0041] loss=14.2502 cls=0.1877 smmd=0.8044 ct=11.4743 rec=1.3421 | train/val/test=1.000/0.730/0.715 | c=0.998347
[Epoch 0042] loss=13.8642 cls=0.1725 smmd=0.6494 ct=11.4871 rec=1.3347 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0043] loss=13.7407 cls=0.1489 smmd=0.6057 ct=11.4883 rec=1.3276 | train/val/test=1.000/0.724/0.705 | c=0.998347
[Epoch 0044] loss=13.6482 cls=0.1430 smmd=0.5807 ct=11.4625 rec=1.3247 | train/val/test=1.000/0.722/0.721 | c=0.998347
[Epoch 0045] loss=13.5564 cls=0.1359 smmd=0.5384 ct=11.4808 rec=1.3235 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0046] loss=13.7261 cls=0.1459 smmd=0.6052 ct=11.4765 rec=1.3273 | train/val/test=1.000/0.708/0.725 | c=0.998347
[Epoch 0047] loss=13.8450 cls=0.1514 smmd=0.6425 ct=11.4966 rec=1.3330 | train/val/test=1.000/0.712/0.706 | c=0.998347
[Epoch 0048] loss=14.0252 cls=0.1629 smmd=0.7084 ct=11.5052 rec=1.3352 | train/val/test=1.000/0.704/0.716 | c=0.998347
[Epoch 0049] loss=14.1001 cls=0.1577 smmd=0.7340 ct=11.5177 rec=1.3370 | train/val/test=1.000/0.690/0.671 | c=0.998347
[Epoch 0050] loss=13.8273 cls=0.1523 smmd=0.6382 ct=11.4895 rec=1.3322 | train/val/test=1.000/0.704/0.714 | c=0.998347
[Epoch 0051] loss=13.8376 cls=0.1278 smmd=0.6349 ct=11.5244 rec=1.3240 | train/val/test=1.000/0.698/0.682 | c=0.998347
[Epoch 0052] loss=13.5427 cls=0.1165 smmd=0.5413 ct=11.4725 rec=1.3171 | train/val/test=1.000/0.716/0.719 | c=0.998347
[Epoch 0053] loss=13.5322 cls=0.0965 smmd=0.5451 ct=11.4654 rec=1.3118 | train/val/test=1.000/0.724/0.711 | c=0.998347
[Epoch 0054] loss=13.4303 cls=0.1063 smmd=0.4955 ct=11.4812 rec=1.3145 | train/val/test=1.000/0.726/0.723 | c=0.998347
[Epoch 0055] loss=13.6011 cls=0.1106 smmd=0.5602 ct=11.4854 rec=1.3198 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0056] loss=13.7703 cls=0.1266 smmd=0.6249 ct=11.4823 rec=1.3249 | train/val/test=1.000/0.718/0.724 | c=0.998347
[Epoch 0057] loss=13.9969 cls=0.1322 smmd=0.7001 ct=11.5154 rec=1.3302 | train/val/test=1.000/0.708/0.694 | c=0.998347
[Epoch 0058] loss=13.9246 cls=0.1409 smmd=0.6778 ct=11.4957 rec=1.3278 | train/val/test=1.000/0.696/0.719 | c=0.998347
[Epoch 0059] loss=13.7821 cls=0.1361 smmd=0.6157 ct=11.5104 rec=1.3290 | train/val/test=1.000/0.668/0.656 | c=0.998347
[Epoch 0060] loss=13.6679 cls=0.1397 smmd=0.5734 ct=11.5027 rec=1.3236 | train/val/test=1.000/0.700/0.718 | c=0.998347
[Epoch 0061] loss=13.5222 cls=0.1100 smmd=0.5231 ct=11.5019 rec=1.3153 | train/val/test=1.000/0.708/0.688 | c=0.998347
[Epoch 0062] loss=13.3308 cls=0.0912 smmd=0.4678 ct=11.4625 rec=1.3066 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0063] loss=13.3249 cls=0.0805 smmd=0.4614 ct=11.4790 rec=1.3045 | train/val/test=1.000/0.728/0.715 | c=0.998347
[Epoch 0064] loss=13.4350 cls=0.0891 smmd=0.5029 ct=11.4779 rec=1.3109 | train/val/test=1.000/0.728/0.716 | c=0.998347
[Epoch 0065] loss=13.6483 cls=0.1031 smmd=0.5763 ct=11.4969 rec=1.3181 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0066] loss=13.9642 cls=0.1104 smmd=0.6940 ct=11.5118 rec=1.3245 | train/val/test=1.000/0.726/0.710 | c=0.998347
[Epoch 0067] loss=13.9473 cls=0.1090 smmd=0.6989 ct=11.4858 rec=1.3194 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0068] loss=13.5804 cls=0.0925 smmd=0.5517 ct=11.4972 rec=1.3153 | train/val/test=1.000/0.704/0.700 | c=0.998347
[Epoch 0069] loss=13.3781 cls=0.0999 smmd=0.4763 ct=11.4824 rec=1.3102 | train/val/test=1.000/0.710/0.720 | c=0.998347
[Epoch 0070] loss=13.3137 cls=0.0833 smmd=0.4568 ct=11.4764 rec=1.3072 | train/val/test=1.000/0.702/0.688 | c=0.998347
[Epoch 0071] loss=13.2466 cls=0.0984 smmd=0.4295 ct=11.4696 rec=1.3083 | train/val/test=1.000/0.706/0.716 | c=0.998347
[Epoch 0072] loss=13.3745 cls=0.0965 smmd=0.4658 ct=11.5054 rec=1.3128 | train/val/test=1.000/0.682/0.670 | c=0.998347
[Epoch 0073] loss=13.5079 cls=0.1210 smmd=0.5155 ct=11.4980 rec=1.3213 | train/val/test=1.000/0.680/0.699 | c=0.998347
[Epoch 0074] loss=13.9235 cls=0.1665 smmd=0.6402 ct=11.5633 rec=1.3526 | train/val/test=1.000/0.528/0.496 | c=0.998347
[Epoch 0075] loss=14.2749 cls=0.2365 smmd=0.7497 ct=11.6040 rec=1.3568 | train/val/test=0.923/0.664/0.683 | c=0.998347
[Epoch 0076] loss=14.0600 cls=0.1835 smmd=0.6808 ct=11.5850 rec=1.3627 | train/val/test=1.000/0.708/0.673 | c=0.998347
[Epoch 0077] loss=13.2047 cls=0.0492 smmd=0.4175 ct=11.4896 rec=1.2937 | train/val/test=1.000/0.684/0.660 | c=0.998347
[Epoch 0078] loss=13.5129 cls=0.0527 smmd=0.5427 ct=11.4821 rec=1.2957 | train/val/test=1.000/0.714/0.724 | c=0.998347
[Epoch 0079] loss=13.0686 cls=0.0304 smmd=0.3708 ct=11.4823 rec=1.2884 | train/val/test=1.000/0.718/0.714 | c=0.998347
[Epoch 0080] loss=13.2481 cls=0.0333 smmd=0.4308 ct=11.5100 rec=1.2893 | train/val/test=1.000/0.680/0.670 | c=0.998347
[Epoch 0081] loss=13.5455 cls=0.0754 smmd=0.5393 ct=11.5038 rec=1.3112 | train/val/test=1.000/0.712/0.724 | c=0.998347
[Epoch 0082] loss=13.4014 cls=0.0676 smmd=0.4756 ct=11.5215 rec=1.3143 | train/val/test=1.000/0.720/0.716 | c=0.998347
[Epoch 0083] loss=13.9128 cls=0.0887 smmd=0.6638 ct=11.5482 rec=1.3213 | train/val/test=1.000/0.660/0.669 | c=0.998347
[Epoch 0084] loss=14.2762 cls=0.1316 smmd=0.8082 ct=11.5270 rec=1.3260 | train/val/test=1.000/0.682/0.703 | c=0.998347
[Epoch 0085] loss=13.7584 cls=0.1559 smmd=0.5755 ct=11.5665 rec=1.3502 | train/val/test=0.923/0.474/0.457 | c=0.998347
[Epoch 0086] loss=14.0139 cls=0.2791 smmd=0.6594 ct=11.5548 rec=1.3420 | train/val/test=0.923/0.656/0.682 | c=0.998347
[Epoch 0087] loss=13.9090 cls=0.2006 smmd=0.6178 ct=11.5857 rec=1.3573 | train/val/test=1.000/0.658/0.669 | c=0.998347
[Epoch 0088] loss=13.2797 cls=0.0635 smmd=0.4614 ct=11.4435 rec=1.3018 | train/val/test=1.000/0.672/0.683 | c=0.998347
[Epoch 0089] loss=13.1843 cls=0.0525 smmd=0.4194 ct=11.4592 rec=1.3006 | train/val/test=1.000/0.688/0.703 | c=0.998347
[Epoch 0090] loss=13.4646 cls=0.0799 smmd=0.4830 ct=11.5524 rec=1.3292 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0091] loss=13.6995 cls=0.0776 smmd=0.5893 ct=11.5273 rec=1.3199 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0092] loss=13.8412 cls=0.0547 smmd=0.6592 ct=11.5107 rec=1.3099 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0093] loss=13.7379 cls=0.0560 smmd=0.6112 ct=11.5301 rec=1.3037 | train/val/test=1.000/0.720/0.718 | c=0.998347
[Epoch 0094] loss=13.6965 cls=0.0476 smmd=0.6094 ct=11.4999 rec=1.2988 | train/val/test=1.000/0.714/0.694 | c=0.998347
[Epoch 0095] loss=13.5491 cls=0.0495 smmd=0.5489 ct=11.5039 rec=1.2965 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0096] loss=13.2542 cls=0.0432 smmd=0.4389 ct=11.4906 rec=1.2895 | train/val/test=1.000/0.708/0.685 | c=0.998347
[Epoch 0097] loss=13.2817 cls=0.0561 smmd=0.4533 ct=11.4755 rec=1.2901 | train/val/test=1.000/0.712/0.707 | c=0.998347
[Epoch 0098] loss=13.2441 cls=0.0618 smmd=0.4320 ct=11.4858 rec=1.2950 | train/val/test=1.000/0.698/0.690 | c=0.998347
[Epoch 0099] loss=13.2309 cls=0.1018 smmd=0.4143 ct=11.4917 rec=1.3052 | train/val/test=1.000/0.702/0.705 | c=0.998347
=== Best @ epoch 22: val=0.7400, test=0.7350 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 completed in 192.51 seconds.
==================================================
