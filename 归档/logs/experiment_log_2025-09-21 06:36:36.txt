Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 - 2025-09-21 06:36:36:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1088 cls=1.9493 smmd=4.1224 ct=9.2593 rec=1.3889 | train/val/test=0.362/0.232/0.262 | c=0.998347
[Epoch 0001] loss=16.6288 cls=1.9077 smmd=2.7462 ct=9.1961 rec=1.3894 | train/val/test=0.638/0.352/0.352 | c=0.998347
[Epoch 0002] loss=15.1255 cls=1.7804 smmd=1.4824 ct=9.0855 rec=1.3886 | train/val/test=0.724/0.398/0.431 | c=0.998347
[Epoch 0003] loss=14.8427 cls=1.5403 smmd=1.5120 ct=9.0172 rec=1.3866 | train/val/test=0.879/0.554/0.561 | c=0.998347
[Epoch 0004] loss=14.5556 cls=1.2290 smmd=1.6621 ct=8.9089 rec=1.3778 | train/val/test=0.810/0.472/0.502 | c=0.998347
[Epoch 0005] loss=14.0730 cls=0.8834 smmd=1.5939 ct=8.8766 rec=1.3596 | train/val/test=0.931/0.634/0.650 | c=0.998347
[Epoch 0006] loss=13.4370 cls=0.5791 smmd=1.3769 ct=8.8287 rec=1.3261 | train/val/test=0.931/0.644/0.655 | c=0.998347
[Epoch 0007] loss=12.8387 cls=0.3895 smmd=1.0500 ct=8.8136 rec=1.2928 | train/val/test=0.966/0.618/0.637 | c=0.998347
[Epoch 0008] loss=12.5961 cls=0.2591 smmd=1.0063 ct=8.8008 rec=1.2649 | train/val/test=0.983/0.662/0.673 | c=0.998347
[Epoch 0009] loss=12.5514 cls=0.1675 smmd=1.1024 ct=8.7944 rec=1.2436 | train/val/test=1.000/0.718/0.712 | c=0.998347
[Epoch 0010] loss=12.4390 cls=0.1087 smmd=1.0991 ct=8.7869 rec=1.2221 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0011] loss=12.9430 cls=0.0729 smmd=0.9915 ct=9.4670 rec=1.2058 | train/val/test=1.000/0.708/0.693 | c=0.998347
[Epoch 0012] loss=12.7029 cls=0.0465 smmd=0.8666 ct=9.3976 rec=1.1961 | train/val/test=1.000/0.684/0.677 | c=0.998347
[Epoch 0013] loss=12.5918 cls=0.0315 smmd=0.8316 ct=9.3455 rec=1.1916 | train/val/test=1.000/0.696/0.682 | c=0.998347
[Epoch 0014] loss=12.5580 cls=0.0226 smmd=0.8294 ct=9.3343 rec=1.1859 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0015] loss=12.4839 cls=0.0190 smmd=0.7414 ct=9.3629 rec=1.1803 | train/val/test=1.000/0.732/0.715 | c=0.998347
[Epoch 0016] loss=12.4380 cls=0.0178 smmd=0.6616 ct=9.4025 rec=1.1780 | train/val/test=1.000/0.732/0.695 | c=0.998347
[Epoch 0017] loss=12.3240 cls=0.0159 smmd=0.5475 ct=9.4033 rec=1.1787 | train/val/test=1.000/0.716/0.675 | c=0.998347
[Epoch 0018] loss=12.2344 cls=0.0165 smmd=0.4756 ct=9.3760 rec=1.1832 | train/val/test=1.000/0.720/0.680 | c=0.998347
[Epoch 0019] loss=12.1958 cls=0.0174 smmd=0.4515 ct=9.3561 rec=1.1854 | train/val/test=1.000/0.728/0.692 | c=0.998347
[Epoch 0020] loss=12.1824 cls=0.0173 smmd=0.4329 ct=9.3601 rec=1.1860 | train/val/test=1.000/0.724/0.689 | c=0.998347
[Epoch 0021] loss=12.1196 cls=0.0197 smmd=0.3510 ct=9.3694 rec=1.1897 | train/val/test=1.000/0.716/0.686 | c=0.998347
[Epoch 0022] loss=12.0694 cls=0.0241 smmd=0.2858 ct=9.3727 rec=1.1934 | train/val/test=1.000/0.730/0.691 | c=0.998347
[Epoch 0023] loss=12.0274 cls=0.0283 smmd=0.2377 ct=9.3756 rec=1.1929 | train/val/test=1.000/0.730/0.696 | c=0.998347
[Epoch 0024] loss=12.0151 cls=0.0304 smmd=0.2342 ct=9.3705 rec=1.1900 | train/val/test=1.000/0.734/0.694 | c=0.998347
[Epoch 0025] loss=11.9851 cls=0.0371 smmd=0.2151 ct=9.3585 rec=1.1872 | train/val/test=1.000/0.740/0.699 | c=0.998347
[Epoch 0026] loss=11.9507 cls=0.0345 smmd=0.1818 ct=9.3664 rec=1.1840 | train/val/test=1.000/0.730/0.697 | c=0.998347
[Epoch 0027] loss=11.9698 cls=0.0450 smmd=0.2010 ct=9.3594 rec=1.1822 | train/val/test=1.000/0.740/0.703 | c=0.998347
[Epoch 0028] loss=11.9262 cls=0.0343 smmd=0.1571 ct=9.3791 rec=1.1779 | train/val/test=1.000/0.736/0.704 | c=0.998347
[Epoch 0029] loss=11.8826 cls=0.0310 smmd=0.1545 ct=9.3515 rec=1.1728 | train/val/test=1.000/0.730/0.705 | c=0.998347
[Epoch 0030] loss=11.8461 cls=0.0257 smmd=0.1416 ct=9.3420 rec=1.1684 | train/val/test=1.000/0.742/0.715 | c=0.998347
[Epoch 0031] loss=11.8396 cls=0.0252 smmd=0.1333 ct=9.3464 rec=1.1673 | train/val/test=1.000/0.728/0.707 | c=0.998347
[Epoch 0032] loss=11.8094 cls=0.0212 smmd=0.1194 ct=9.3372 rec=1.1658 | train/val/test=1.000/0.738/0.706 | c=0.998347
[Epoch 0033] loss=11.7995 cls=0.0195 smmd=0.1193 ct=9.3326 rec=1.1641 | train/val/test=1.000/0.744/0.716 | c=0.998347
[Epoch 0034] loss=11.7794 cls=0.0210 smmd=0.0912 ct=9.3366 rec=1.1653 | train/val/test=1.000/0.736/0.710 | c=0.998347
[Epoch 0035] loss=11.7696 cls=0.0199 smmd=0.0901 ct=9.3281 rec=1.1658 | train/val/test=1.000/0.736/0.707 | c=0.998347
[Epoch 0036] loss=11.7526 cls=0.0198 smmd=0.0726 ct=9.3275 rec=1.1663 | train/val/test=1.000/0.748/0.712 | c=0.998347
[Epoch 0037] loss=11.7413 cls=0.0207 smmd=0.0594 ct=9.3254 rec=1.1679 | train/val/test=1.000/0.742/0.711 | c=0.998347
[Epoch 0038] loss=11.7358 cls=0.0218 smmd=0.0649 ct=9.3109 rec=1.1691 | train/val/test=1.000/0.748/0.708 | c=0.998347
[Epoch 0039] loss=11.7317 cls=0.0203 smmd=0.0527 ct=9.3215 rec=1.1686 | train/val/test=1.000/0.746/0.720 | c=0.998347
[Epoch 0040] loss=11.7162 cls=0.0209 smmd=0.0391 ct=9.3208 rec=1.1677 | train/val/test=1.000/0.750/0.711 | c=0.998347
[Epoch 0041] loss=11.7114 cls=0.0208 smmd=0.0427 ct=9.3138 rec=1.1671 | train/val/test=1.000/0.750/0.717 | c=0.998347
[Epoch 0042] loss=11.7018 cls=0.0202 smmd=0.0364 ct=9.3123 rec=1.1665 | train/val/test=1.000/0.748/0.718 | c=0.998347
[Epoch 0043] loss=11.6936 cls=0.0206 smmd=0.0404 ct=9.3020 rec=1.1653 | train/val/test=1.000/0.752/0.722 | c=0.998347
[Epoch 0044] loss=11.6816 cls=0.0198 smmd=0.0244 ct=9.3085 rec=1.1644 | train/val/test=1.000/0.754/0.721 | c=0.998347
[Epoch 0045] loss=11.6752 cls=0.0199 smmd=0.0180 ct=9.3089 rec=1.1642 | train/val/test=1.000/0.750/0.718 | c=0.998347
[Epoch 0046] loss=11.6780 cls=0.0200 smmd=0.0310 ct=9.2990 rec=1.1640 | train/val/test=1.000/0.752/0.720 | c=0.998347
[Epoch 0047] loss=11.6711 cls=0.0203 smmd=0.0247 ct=9.2979 rec=1.1640 | train/val/test=1.000/0.748/0.717 | c=0.998347
[Epoch 0048] loss=11.6611 cls=0.0201 smmd=0.0216 ct=9.2917 rec=1.1639 | train/val/test=1.000/0.748/0.716 | c=0.998347
[Epoch 0049] loss=11.6570 cls=0.0205 smmd=0.0134 ct=9.2944 rec=1.1643 | train/val/test=1.000/0.748/0.719 | c=0.998347
[Epoch 0050] loss=11.6576 cls=0.0214 smmd=0.0114 ct=9.2948 rec=1.1650 | train/val/test=1.000/0.748/0.714 | c=0.998347
[Epoch 0051] loss=11.6575 cls=0.0215 smmd=0.0221 ct=9.2819 rec=1.1660 | train/val/test=1.000/0.746/0.721 | c=0.998347
[Epoch 0052] loss=11.6528 cls=0.0221 smmd=0.0119 ct=9.2868 rec=1.1660 | train/val/test=1.000/0.752/0.713 | c=0.998347
[Epoch 0053] loss=11.6554 cls=0.0221 smmd=0.0130 ct=9.2865 rec=1.1669 | train/val/test=1.000/0.752/0.723 | c=0.998347
[Epoch 0054] loss=11.6573 cls=0.0230 smmd=0.0133 ct=9.2875 rec=1.1668 | train/val/test=1.000/0.750/0.712 | c=0.998347
[Epoch 0055] loss=11.6703 cls=0.0241 smmd=0.0213 ct=9.2877 rec=1.1686 | train/val/test=1.000/0.754/0.717 | c=0.998347
[Epoch 0056] loss=11.6940 cls=0.0276 smmd=0.0393 ct=9.2876 rec=1.1697 | train/val/test=1.000/0.756/0.707 | c=0.998347
[Epoch 0057] loss=11.7126 cls=0.0273 smmd=0.0307 ct=9.3090 rec=1.1728 | train/val/test=1.000/0.756/0.714 | c=0.998347
[Epoch 0058] loss=11.7075 cls=0.0295 smmd=0.0527 ct=9.2859 rec=1.1697 | train/val/test=1.000/0.758/0.715 | c=0.998347
[Epoch 0059] loss=11.6711 cls=0.0189 smmd=0.0211 ct=9.3009 rec=1.1651 | train/val/test=1.000/0.748/0.715 | c=0.998347
[Epoch 0060] loss=11.6336 cls=0.0159 smmd=0.0232 ct=9.2722 rec=1.1611 | train/val/test=1.000/0.756/0.717 | c=0.998347
[Epoch 0061] loss=11.6486 cls=0.0190 smmd=0.0269 ct=9.2769 rec=1.1629 | train/val/test=1.000/0.756/0.715 | c=0.998347
[Epoch 0062] loss=11.6530 cls=0.0176 smmd=0.0135 ct=9.2932 rec=1.1644 | train/val/test=1.000/0.746/0.713 | c=0.998347
[Epoch 0063] loss=11.6321 cls=0.0178 smmd=0.0232 ct=9.2632 rec=1.1640 | train/val/test=1.000/0.760/0.720 | c=0.998347
[Epoch 0064] loss=11.6378 cls=0.0203 smmd=0.0117 ct=9.2749 rec=1.1654 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0065] loss=11.6539 cls=0.0218 smmd=0.0057 ct=9.2869 rec=1.1697 | train/val/test=1.000/0.746/0.712 | c=0.998347
[Epoch 0066] loss=11.6489 cls=0.0251 smmd=0.0237 ct=9.2594 rec=1.1703 | train/val/test=1.000/0.754/0.717 | c=0.998347
[Epoch 0067] loss=11.6377 cls=0.0218 smmd=0.0088 ct=9.2704 rec=1.1683 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0068] loss=11.6185 cls=0.0209 smmd=0.0007 ct=9.2647 rec=1.1661 | train/val/test=1.000/0.752/0.715 | c=0.998347
[Epoch 0069] loss=11.6124 cls=0.0224 smmd=-0.0038 ct=9.2609 rec=1.1664 | train/val/test=1.000/0.758/0.717 | c=0.998347
[Epoch 0070] loss=11.6351 cls=0.0244 smmd=-0.0035 ct=9.2743 rec=1.1699 | train/val/test=1.000/0.750/0.712 | c=0.998347
[Epoch 0071] loss=11.6400 cls=0.0259 smmd=0.0222 ct=9.2554 rec=1.1683 | train/val/test=1.000/0.756/0.713 | c=0.998347
[Epoch 0072] loss=11.6173 cls=0.0224 smmd=-0.0080 ct=9.2677 rec=1.1676 | train/val/test=1.000/0.756/0.715 | c=0.998347
[Epoch 0073] loss=11.6063 cls=0.0223 smmd=-0.0036 ct=9.2564 rec=1.1656 | train/val/test=1.000/0.752/0.711 | c=0.998347
[Epoch 0074] loss=11.6072 cls=0.0221 smmd=-0.0002 ct=9.2533 rec=1.1660 | train/val/test=1.000/0.754/0.711 | c=0.998347
[Epoch 0075] loss=11.6066 cls=0.0216 smmd=-0.0099 ct=9.2610 rec=1.1670 | train/val/test=1.000/0.750/0.717 | c=0.998347
[Epoch 0076] loss=11.6090 cls=0.0238 smmd=0.0005 ct=9.2497 rec=1.1675 | train/val/test=1.000/0.752/0.708 | c=0.998347
[Epoch 0077] loss=11.6101 cls=0.0228 smmd=-0.0089 ct=9.2587 rec=1.1687 | train/val/test=1.000/0.754/0.713 | c=0.998347
[Epoch 0078] loss=11.5977 cls=0.0228 smmd=-0.0077 ct=9.2478 rec=1.1674 | train/val/test=1.000/0.752/0.708 | c=0.998347
[Epoch 0079] loss=11.5892 cls=0.0219 smmd=-0.0153 ct=9.2489 rec=1.1668 | train/val/test=1.000/0.748/0.707 | c=0.998347
[Epoch 0080] loss=11.5902 cls=0.0221 smmd=-0.0163 ct=9.2504 rec=1.1670 | train/val/test=1.000/0.752/0.716 | c=0.998347
[Epoch 0081] loss=11.5928 cls=0.0226 smmd=-0.0068 ct=9.2439 rec=1.1666 | train/val/test=1.000/0.752/0.710 | c=0.998347
[Epoch 0082] loss=11.5899 cls=0.0217 smmd=-0.0155 ct=9.2508 rec=1.1664 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0083] loss=11.5850 cls=0.0223 smmd=-0.0105 ct=9.2417 rec=1.1658 | train/val/test=1.000/0.754/0.713 | c=0.998347
[Epoch 0084] loss=11.5804 cls=0.0214 smmd=-0.0212 ct=9.2489 rec=1.1656 | train/val/test=1.000/0.752/0.713 | c=0.998347
[Epoch 0085] loss=11.5794 cls=0.0216 smmd=-0.0135 ct=9.2404 rec=1.1654 | train/val/test=1.000/0.750/0.711 | c=0.998347
[Epoch 0086] loss=11.5766 cls=0.0215 smmd=-0.0192 ct=9.2421 rec=1.1661 | train/val/test=1.000/0.752/0.712 | c=0.998347
[Epoch 0087] loss=11.5813 cls=0.0223 smmd=-0.0153 ct=9.2410 rec=1.1666 | train/val/test=1.000/0.746/0.710 | c=0.998347
[Epoch 0088] loss=11.5772 cls=0.0229 smmd=-0.0179 ct=9.2364 rec=1.1679 | train/val/test=1.000/0.750/0.710 | c=0.998347
[Epoch 0089] loss=11.5754 cls=0.0230 smmd=-0.0254 ct=9.2411 rec=1.1684 | train/val/test=1.000/0.750/0.713 | c=0.998347
[Epoch 0090] loss=11.5749 cls=0.0235 smmd=-0.0189 ct=9.2331 rec=1.1686 | train/val/test=1.000/0.750/0.713 | c=0.998347
[Epoch 0091] loss=11.5795 cls=0.0230 smmd=-0.0227 ct=9.2421 rec=1.1686 | train/val/test=1.000/0.746/0.711 | c=0.998347
[Epoch 0092] loss=11.5851 cls=0.0241 smmd=-0.0102 ct=9.2339 rec=1.1687 | train/val/test=1.000/0.752/0.716 | c=0.998347
[Epoch 0093] loss=11.5977 cls=0.0238 smmd=-0.0203 ct=9.2545 rec=1.1699 | train/val/test=1.000/0.750/0.708 | c=0.998347
[Epoch 0094] loss=11.6247 cls=0.0303 smmd=0.0147 ct=9.2336 rec=1.1731 | train/val/test=1.000/0.748/0.703 | c=0.998347
[Epoch 0095] loss=11.6961 cls=0.0346 smmd=0.0152 ct=9.2862 rec=1.1800 | train/val/test=1.000/0.732/0.702 | c=0.998347
[Epoch 0096] loss=11.7192 cls=0.0432 smmd=0.0629 ct=9.2498 rec=1.1816 | train/val/test=1.000/0.750/0.701 | c=0.998347
[Epoch 0097] loss=11.6916 cls=0.0271 smmd=0.0275 ct=9.2889 rec=1.1741 | train/val/test=1.000/0.754/0.722 | c=0.998347
[Epoch 0098] loss=11.5859 cls=0.0140 smmd=0.0046 ct=9.2491 rec=1.1591 | train/val/test=1.000/0.748/0.706 | c=0.998347
[Epoch 0099] loss=11.6401 cls=0.0186 smmd=0.0542 ct=9.2377 rec=1.1648 | train/val/test=1.000/0.752/0.709 | c=0.998347
=== Best @ epoch 63: val=0.7600, test=0.7200 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 - 2025-09-21 06:36:36:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1088 cls=1.9493 smmd=4.1224 ct=9.2593 rec=1.3889 | train/val/test=0.362/0.232/0.262 | c=0.998347
[Epoch 0001] loss=16.6288 cls=1.9077 smmd=2.7462 ct=9.1961 rec=1.3894 | train/val/test=0.638/0.352/0.352 | c=0.998347
[Epoch 0002] loss=15.1255 cls=1.7804 smmd=1.4824 ct=9.0855 rec=1.3886 | train/val/test=0.724/0.398/0.431 | c=0.998347
[Epoch 0003] loss=14.8427 cls=1.5403 smmd=1.5120 ct=9.0172 rec=1.3866 | train/val/test=0.879/0.554/0.561 | c=0.998347
[Epoch 0004] loss=14.5556 cls=1.2290 smmd=1.6621 ct=8.9089 rec=1.3778 | train/val/test=0.810/0.472/0.502 | c=0.998347
[Epoch 0005] loss=14.0730 cls=0.8834 smmd=1.5939 ct=8.8766 rec=1.3596 | train/val/test=0.931/0.634/0.650 | c=0.998347
[Epoch 0006] loss=13.4370 cls=0.5791 smmd=1.3769 ct=8.8287 rec=1.3261 | train/val/test=0.931/0.644/0.655 | c=0.998347
[Epoch 0007] loss=12.8387 cls=0.3895 smmd=1.0500 ct=8.8136 rec=1.2928 | train/val/test=0.966/0.618/0.637 | c=0.998347
[Epoch 0008] loss=12.5961 cls=0.2591 smmd=1.0063 ct=8.8008 rec=1.2649 | train/val/test=0.983/0.662/0.673 | c=0.998347
[Epoch 0009] loss=12.5514 cls=0.1675 smmd=1.1024 ct=8.7944 rec=1.2436 | train/val/test=1.000/0.718/0.712 | c=0.998347
[Epoch 0010] loss=12.4390 cls=0.1087 smmd=1.0991 ct=8.7869 rec=1.2221 | train/val/test=1.000/0.728/0.713 | c=0.998347
[Epoch 0011] loss=12.9430 cls=0.0729 smmd=0.9915 ct=9.4670 rec=1.2058 | train/val/test=1.000/0.708/0.693 | c=0.998347
[Epoch 0012] loss=12.7029 cls=0.0465 smmd=0.8666 ct=9.3976 rec=1.1961 | train/val/test=1.000/0.684/0.677 | c=0.998347
[Epoch 0013] loss=12.5918 cls=0.0315 smmd=0.8316 ct=9.3455 rec=1.1916 | train/val/test=1.000/0.696/0.682 | c=0.998347
[Epoch 0014] loss=12.5580 cls=0.0226 smmd=0.8294 ct=9.3343 rec=1.1859 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0015] loss=12.4839 cls=0.0190 smmd=0.7414 ct=9.3629 rec=1.1803 | train/val/test=1.000/0.732/0.715 | c=0.998347
[Epoch 0016] loss=12.4380 cls=0.0178 smmd=0.6616 ct=9.4025 rec=1.1780 | train/val/test=1.000/0.732/0.695 | c=0.998347
[Epoch 0017] loss=12.3240 cls=0.0159 smmd=0.5475 ct=9.4033 rec=1.1787 | train/val/test=1.000/0.716/0.675 | c=0.998347
[Epoch 0018] loss=12.2344 cls=0.0165 smmd=0.4756 ct=9.3760 rec=1.1832 | train/val/test=1.000/0.720/0.680 | c=0.998347
[Epoch 0019] loss=12.1958 cls=0.0174 smmd=0.4515 ct=9.3561 rec=1.1854 | train/val/test=1.000/0.728/0.692 | c=0.998347
[Epoch 0020] loss=12.1824 cls=0.0173 smmd=0.4329 ct=9.3601 rec=1.1860 | train/val/test=1.000/0.724/0.689 | c=0.998347
[Epoch 0021] loss=12.1196 cls=0.0197 smmd=0.3510 ct=9.3694 rec=1.1897 | train/val/test=1.000/0.716/0.686 | c=0.998347
[Epoch 0022] loss=12.0694 cls=0.0241 smmd=0.2858 ct=9.3727 rec=1.1934 | train/val/test=1.000/0.730/0.691 | c=0.998347
[Epoch 0023] loss=12.0274 cls=0.0283 smmd=0.2377 ct=9.3756 rec=1.1929 | train/val/test=1.000/0.730/0.696 | c=0.998347
[Epoch 0024] loss=12.0151 cls=0.0304 smmd=0.2342 ct=9.3705 rec=1.1900 | train/val/test=1.000/0.734/0.694 | c=0.998347
[Epoch 0025] loss=11.9851 cls=0.0371 smmd=0.2151 ct=9.3585 rec=1.1872 | train/val/test=1.000/0.740/0.699 | c=0.998347
[Epoch 0026] loss=11.9507 cls=0.0345 smmd=0.1818 ct=9.3664 rec=1.1840 | train/val/test=1.000/0.730/0.697 | c=0.998347
[Epoch 0027] loss=11.9698 cls=0.0450 smmd=0.2010 ct=9.3594 rec=1.1822 | train/val/test=1.000/0.740/0.703 | c=0.998347
[Epoch 0028] loss=11.9262 cls=0.0343 smmd=0.1571 ct=9.3791 rec=1.1779 | train/val/test=1.000/0.736/0.704 | c=0.998347
[Epoch 0029] loss=11.8826 cls=0.0310 smmd=0.1545 ct=9.3515 rec=1.1728 | train/val/test=1.000/0.730/0.705 | c=0.998347
[Epoch 0030] loss=11.8461 cls=0.0257 smmd=0.1416 ct=9.3420 rec=1.1684 | train/val/test=1.000/0.742/0.715 | c=0.998347
[Epoch 0031] loss=11.8396 cls=0.0252 smmd=0.1333 ct=9.3464 rec=1.1673 | train/val/test=1.000/0.728/0.707 | c=0.998347
[Epoch 0032] loss=11.8094 cls=0.0212 smmd=0.1194 ct=9.3372 rec=1.1658 | train/val/test=1.000/0.738/0.706 | c=0.998347
[Epoch 0033] loss=11.7995 cls=0.0195 smmd=0.1193 ct=9.3326 rec=1.1641 | train/val/test=1.000/0.744/0.716 | c=0.998347
[Epoch 0034] loss=11.7794 cls=0.0210 smmd=0.0912 ct=9.3366 rec=1.1653 | train/val/test=1.000/0.736/0.710 | c=0.998347
[Epoch 0035] loss=11.7696 cls=0.0199 smmd=0.0901 ct=9.3281 rec=1.1658 | train/val/test=1.000/0.736/0.707 | c=0.998347
[Epoch 0036] loss=11.7526 cls=0.0198 smmd=0.0726 ct=9.3275 rec=1.1663 | train/val/test=1.000/0.748/0.712 | c=0.998347
[Epoch 0037] loss=11.7413 cls=0.0207 smmd=0.0594 ct=9.3254 rec=1.1679 | train/val/test=1.000/0.742/0.711 | c=0.998347
[Epoch 0038] loss=11.7358 cls=0.0218 smmd=0.0649 ct=9.3109 rec=1.1691 | train/val/test=1.000/0.748/0.708 | c=0.998347
[Epoch 0039] loss=11.7317 cls=0.0203 smmd=0.0527 ct=9.3215 rec=1.1686 | train/val/test=1.000/0.746/0.720 | c=0.998347
[Epoch 0040] loss=11.7162 cls=0.0209 smmd=0.0391 ct=9.3208 rec=1.1677 | train/val/test=1.000/0.750/0.711 | c=0.998347
[Epoch 0041] loss=11.7114 cls=0.0208 smmd=0.0427 ct=9.3138 rec=1.1671 | train/val/test=1.000/0.750/0.717 | c=0.998347
[Epoch 0042] loss=11.7018 cls=0.0202 smmd=0.0364 ct=9.3123 rec=1.1665 | train/val/test=1.000/0.748/0.718 | c=0.998347
[Epoch 0043] loss=11.6936 cls=0.0206 smmd=0.0404 ct=9.3020 rec=1.1653 | train/val/test=1.000/0.752/0.722 | c=0.998347
[Epoch 0044] loss=11.6816 cls=0.0198 smmd=0.0244 ct=9.3085 rec=1.1644 | train/val/test=1.000/0.754/0.721 | c=0.998347
[Epoch 0045] loss=11.6752 cls=0.0199 smmd=0.0180 ct=9.3089 rec=1.1642 | train/val/test=1.000/0.750/0.718 | c=0.998347
[Epoch 0046] loss=11.6780 cls=0.0200 smmd=0.0310 ct=9.2990 rec=1.1640 | train/val/test=1.000/0.752/0.720 | c=0.998347
[Epoch 0047] loss=11.6711 cls=0.0203 smmd=0.0247 ct=9.2979 rec=1.1640 | train/val/test=1.000/0.748/0.717 | c=0.998347
[Epoch 0048] loss=11.6611 cls=0.0201 smmd=0.0216 ct=9.2917 rec=1.1639 | train/val/test=1.000/0.748/0.716 | c=0.998347
[Epoch 0049] loss=11.6570 cls=0.0205 smmd=0.0134 ct=9.2944 rec=1.1643 | train/val/test=1.000/0.748/0.719 | c=0.998347
[Epoch 0050] loss=11.6576 cls=0.0214 smmd=0.0114 ct=9.2948 rec=1.1650 | train/val/test=1.000/0.748/0.714 | c=0.998347
[Epoch 0051] loss=11.6575 cls=0.0215 smmd=0.0221 ct=9.2819 rec=1.1660 | train/val/test=1.000/0.746/0.721 | c=0.998347
[Epoch 0052] loss=11.6528 cls=0.0221 smmd=0.0119 ct=9.2868 rec=1.1660 | train/val/test=1.000/0.752/0.713 | c=0.998347
[Epoch 0053] loss=11.6554 cls=0.0221 smmd=0.0130 ct=9.2865 rec=1.1669 | train/val/test=1.000/0.752/0.723 | c=0.998347
[Epoch 0054] loss=11.6573 cls=0.0230 smmd=0.0133 ct=9.2875 rec=1.1668 | train/val/test=1.000/0.750/0.712 | c=0.998347
[Epoch 0055] loss=11.6703 cls=0.0241 smmd=0.0213 ct=9.2877 rec=1.1686 | train/val/test=1.000/0.754/0.717 | c=0.998347
[Epoch 0056] loss=11.6940 cls=0.0276 smmd=0.0393 ct=9.2876 rec=1.1697 | train/val/test=1.000/0.756/0.707 | c=0.998347
[Epoch 0057] loss=11.7126 cls=0.0273 smmd=0.0307 ct=9.3090 rec=1.1728 | train/val/test=1.000/0.756/0.714 | c=0.998347
[Epoch 0058] loss=11.7075 cls=0.0295 smmd=0.0527 ct=9.2859 rec=1.1697 | train/val/test=1.000/0.758/0.715 | c=0.998347
[Epoch 0059] loss=11.6711 cls=0.0189 smmd=0.0211 ct=9.3009 rec=1.1651 | train/val/test=1.000/0.748/0.715 | c=0.998347
[Epoch 0060] loss=11.6336 cls=0.0159 smmd=0.0232 ct=9.2722 rec=1.1611 | train/val/test=1.000/0.756/0.717 | c=0.998347
[Epoch 0061] loss=11.6486 cls=0.0190 smmd=0.0269 ct=9.2769 rec=1.1629 | train/val/test=1.000/0.756/0.715 | c=0.998347
[Epoch 0062] loss=11.6530 cls=0.0176 smmd=0.0135 ct=9.2932 rec=1.1644 | train/val/test=1.000/0.746/0.713 | c=0.998347
[Epoch 0063] loss=11.6321 cls=0.0178 smmd=0.0232 ct=9.2632 rec=1.1640 | train/val/test=1.000/0.760/0.720 | c=0.998347
[Epoch 0064] loss=11.6378 cls=0.0203 smmd=0.0117 ct=9.2749 rec=1.1654 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0065] loss=11.6539 cls=0.0218 smmd=0.0057 ct=9.2869 rec=1.1697 | train/val/test=1.000/0.746/0.712 | c=0.998347
[Epoch 0066] loss=11.6489 cls=0.0251 smmd=0.0237 ct=9.2594 rec=1.1703 | train/val/test=1.000/0.754/0.717 | c=0.998347
[Epoch 0067] loss=11.6377 cls=0.0218 smmd=0.0088 ct=9.2704 rec=1.1683 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0068] loss=11.6185 cls=0.0209 smmd=0.0007 ct=9.2647 rec=1.1661 | train/val/test=1.000/0.752/0.715 | c=0.998347
[Epoch 0069] loss=11.6124 cls=0.0224 smmd=-0.0038 ct=9.2609 rec=1.1664 | train/val/test=1.000/0.758/0.717 | c=0.998347
[Epoch 0070] loss=11.6351 cls=0.0244 smmd=-0.0035 ct=9.2743 rec=1.1699 | train/val/test=1.000/0.750/0.712 | c=0.998347
[Epoch 0071] loss=11.6400 cls=0.0259 smmd=0.0222 ct=9.2554 rec=1.1683 | train/val/test=1.000/0.756/0.713 | c=0.998347
[Epoch 0072] loss=11.6173 cls=0.0224 smmd=-0.0080 ct=9.2677 rec=1.1676 | train/val/test=1.000/0.756/0.715 | c=0.998347
[Epoch 0073] loss=11.6063 cls=0.0223 smmd=-0.0036 ct=9.2564 rec=1.1656 | train/val/test=1.000/0.752/0.711 | c=0.998347
[Epoch 0074] loss=11.6072 cls=0.0221 smmd=-0.0002 ct=9.2533 rec=1.1660 | train/val/test=1.000/0.754/0.711 | c=0.998347
[Epoch 0075] loss=11.6066 cls=0.0216 smmd=-0.0099 ct=9.2610 rec=1.1670 | train/val/test=1.000/0.750/0.717 | c=0.998347
[Epoch 0076] loss=11.6090 cls=0.0238 smmd=0.0005 ct=9.2497 rec=1.1675 | train/val/test=1.000/0.752/0.708 | c=0.998347
[Epoch 0077] loss=11.6101 cls=0.0228 smmd=-0.0089 ct=9.2587 rec=1.1687 | train/val/test=1.000/0.754/0.713 | c=0.998347
[Epoch 0078] loss=11.5977 cls=0.0228 smmd=-0.0077 ct=9.2478 rec=1.1674 | train/val/test=1.000/0.752/0.708 | c=0.998347
[Epoch 0079] loss=11.5892 cls=0.0219 smmd=-0.0153 ct=9.2489 rec=1.1668 | train/val/test=1.000/0.748/0.707 | c=0.998347
[Epoch 0080] loss=11.5902 cls=0.0221 smmd=-0.0163 ct=9.2504 rec=1.1670 | train/val/test=1.000/0.752/0.716 | c=0.998347
[Epoch 0081] loss=11.5928 cls=0.0226 smmd=-0.0068 ct=9.2439 rec=1.1666 | train/val/test=1.000/0.752/0.710 | c=0.998347
[Epoch 0082] loss=11.5899 cls=0.0217 smmd=-0.0155 ct=9.2508 rec=1.1664 | train/val/test=1.000/0.754/0.712 | c=0.998347
[Epoch 0083] loss=11.5850 cls=0.0223 smmd=-0.0105 ct=9.2417 rec=1.1658 | train/val/test=1.000/0.754/0.713 | c=0.998347
[Epoch 0084] loss=11.5804 cls=0.0214 smmd=-0.0212 ct=9.2489 rec=1.1656 | train/val/test=1.000/0.752/0.713 | c=0.998347
[Epoch 0085] loss=11.5794 cls=0.0216 smmd=-0.0135 ct=9.2404 rec=1.1654 | train/val/test=1.000/0.750/0.711 | c=0.998347
[Epoch 0086] loss=11.5766 cls=0.0215 smmd=-0.0192 ct=9.2421 rec=1.1661 | train/val/test=1.000/0.752/0.712 | c=0.998347
[Epoch 0087] loss=11.5813 cls=0.0223 smmd=-0.0153 ct=9.2410 rec=1.1666 | train/val/test=1.000/0.746/0.710 | c=0.998347
[Epoch 0088] loss=11.5772 cls=0.0229 smmd=-0.0179 ct=9.2364 rec=1.1679 | train/val/test=1.000/0.750/0.710 | c=0.998347
[Epoch 0089] loss=11.5754 cls=0.0230 smmd=-0.0254 ct=9.2411 rec=1.1684 | train/val/test=1.000/0.750/0.713 | c=0.998347
[Epoch 0090] loss=11.5749 cls=0.0235 smmd=-0.0189 ct=9.2331 rec=1.1686 | train/val/test=1.000/0.750/0.713 | c=0.998347
[Epoch 0091] loss=11.5795 cls=0.0230 smmd=-0.0227 ct=9.2421 rec=1.1686 | train/val/test=1.000/0.746/0.711 | c=0.998347
[Epoch 0092] loss=11.5851 cls=0.0241 smmd=-0.0102 ct=9.2339 rec=1.1687 | train/val/test=1.000/0.752/0.716 | c=0.998347
[Epoch 0093] loss=11.5977 cls=0.0238 smmd=-0.0203 ct=9.2545 rec=1.1699 | train/val/test=1.000/0.750/0.708 | c=0.998347
[Epoch 0094] loss=11.6247 cls=0.0303 smmd=0.0147 ct=9.2336 rec=1.1731 | train/val/test=1.000/0.748/0.703 | c=0.998347
[Epoch 0095] loss=11.6961 cls=0.0346 smmd=0.0152 ct=9.2862 rec=1.1800 | train/val/test=1.000/0.732/0.702 | c=0.998347
[Epoch 0096] loss=11.7192 cls=0.0432 smmd=0.0629 ct=9.2498 rec=1.1816 | train/val/test=1.000/0.750/0.701 | c=0.998347
[Epoch 0097] loss=11.6916 cls=0.0271 smmd=0.0275 ct=9.2889 rec=1.1741 | train/val/test=1.000/0.754/0.722 | c=0.998347
[Epoch 0098] loss=11.5859 cls=0.0140 smmd=0.0046 ct=9.2491 rec=1.1591 | train/val/test=1.000/0.748/0.706 | c=0.998347
[Epoch 0099] loss=11.6401 cls=0.0186 smmd=0.0542 ct=9.2377 rec=1.1648 | train/val/test=1.000/0.752/0.709 | c=0.998347
=== Best @ epoch 63: val=0.7600, test=0.7200 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 completed in 22.70 seconds.
==================================================
