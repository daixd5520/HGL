Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2 - 2025-09-21 05:30:17:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=71.7577 cls=1.0994 smmd=5.5983 ct=7.2590 rec=1.4136 | train/val/test=0.399/0.403/0.396 | c=0.998347
[Epoch 0001] loss=51.7945 cls=1.0628 smmd=3.6128 ct=7.2137 rec=1.4154 | train/val/test=0.341/0.327/0.334 | c=0.998347
[Epoch 0002] loss=38.8849 cls=1.0939 smmd=2.3321 ct=7.1550 rec=1.4134 | train/val/test=0.546/0.539/0.549 | c=0.998347
[Epoch 0003] loss=40.2826 cls=1.0520 smmd=2.4849 ct=7.1006 rec=1.4133 | train/val/test=0.563/0.573/0.565 | c=0.998347
[Epoch 0004] loss=39.6974 cls=1.0005 smmd=2.4369 ct=7.0602 rec=1.4158 | train/val/test=0.556/0.565/0.558 | c=0.998347
[Epoch 0005] loss=35.8543 cls=0.9607 smmd=2.0703 ct=6.9813 rec=1.4166 | train/val/test=0.555/0.562/0.555 | c=0.998347
[Epoch 0006] loss=30.7605 cls=0.9181 smmd=1.5743 ct=6.9257 rec=1.4133 | train/val/test=0.557/0.564/0.557 | c=0.998347
[Epoch 0007] loss=34.5303 cls=0.8740 smmd=1.8207 ct=7.5919 rec=1.4054 | train/val/test=0.557/0.555/0.561 | c=0.998347
[Epoch 0008] loss=35.5443 cls=0.8335 smmd=1.9342 ct=7.5437 rec=1.3960 | train/val/test=0.608/0.602/0.618 | c=0.998347
[Epoch 0009] loss=29.7990 cls=0.7972 smmd=1.3702 ct=7.5029 rec=1.3862 | train/val/test=0.654/0.652/0.659 | c=0.998347
[Epoch 0010] loss=27.7570 cls=0.7706 smmd=1.1642 ct=7.5203 rec=1.3792 | train/val/test=0.667/0.669/0.671 | c=0.998347
[Epoch 0011] loss=30.2970 cls=0.7519 smmd=1.4176 ct=7.5286 rec=1.3754 | train/val/test=0.685/0.688/0.689 | c=0.998347
[Epoch 0012] loss=27.4045 cls=0.7207 smmd=1.1331 ct=7.5144 rec=1.3679 | train/val/test=0.689/0.699/0.699 | c=0.998347
[Epoch 0013] loss=26.5442 cls=0.7038 smmd=1.0401 ct=7.5550 rec=1.3623 | train/val/test=0.716/0.726/0.727 | c=0.998347
[Epoch 0014] loss=25.7748 cls=0.6728 smmd=0.9683 ct=7.5388 rec=1.3546 | train/val/test=0.728/0.735/0.735 | c=0.998347
[Epoch 0015] loss=25.3038 cls=0.6643 smmd=0.9250 ct=7.5221 rec=1.3547 | train/val/test=0.748/0.753/0.756 | c=0.998347
[Epoch 0016] loss=25.0261 cls=0.6376 smmd=0.8948 ct=7.5431 rec=1.3470 | train/val/test=0.764/0.774/0.774 | c=0.998347
[Epoch 0017] loss=23.2242 cls=0.6092 smmd=0.7128 ct=7.5612 rec=1.3391 | train/val/test=0.768/0.770/0.771 | c=0.998347
[Epoch 0018] loss=23.7181 cls=0.6011 smmd=0.7675 ct=7.5369 rec=1.3372 | train/val/test=0.785/0.790/0.788 | c=0.998347
[Epoch 0019] loss=22.4810 cls=0.5767 smmd=0.6495 ct=7.5165 rec=1.3299 | train/val/test=0.780/0.779/0.787 | c=0.998347
[Epoch 0020] loss=22.4392 cls=0.5651 smmd=0.6414 ct=7.5406 rec=1.3230 | train/val/test=0.787/0.786/0.797 | c=0.998347
[Epoch 0021] loss=21.8306 cls=0.5515 smmd=0.5825 ct=7.5353 rec=1.3193 | train/val/test=0.806/0.801/0.805 | c=0.998347
[Epoch 0022] loss=21.4894 cls=0.5276 smmd=0.5515 ct=7.5261 rec=1.3167 | train/val/test=0.811/0.806/0.809 | c=0.998347
[Epoch 0023] loss=21.1547 cls=0.5172 smmd=0.5184 ct=7.5273 rec=1.3145 | train/val/test=0.812/0.811/0.812 | c=0.998347
[Epoch 0024] loss=20.6115 cls=0.5108 smmd=0.4648 ct=7.5264 rec=1.3114 | train/val/test=0.809/0.806/0.817 | c=0.998347
[Epoch 0025] loss=20.4919 cls=0.5025 smmd=0.4535 ct=7.5250 rec=1.3102 | train/val/test=0.813/0.812/0.822 | c=0.998347
[Epoch 0026] loss=20.0061 cls=0.4922 smmd=0.4060 ct=7.5228 rec=1.3093 | train/val/test=0.822/0.822/0.823 | c=0.998347
[Epoch 0027] loss=20.0335 cls=0.4859 smmd=0.4102 ct=7.5167 rec=1.3106 | train/val/test=0.824/0.823/0.825 | c=0.998347
[Epoch 0028] loss=19.5902 cls=0.4729 smmd=0.3672 ct=7.5139 rec=1.3079 | train/val/test=0.824/0.826/0.830 | c=0.998347
[Epoch 0029] loss=19.4829 cls=0.4682 smmd=0.3557 ct=7.5191 rec=1.3064 | train/val/test=0.829/0.833/0.832 | c=0.998347
[Epoch 0030] loss=19.2071 cls=0.4625 smmd=0.3279 ct=7.5218 rec=1.3062 | train/val/test=0.828/0.830/0.831 | c=0.998347
[Epoch 0031] loss=19.0377 cls=0.4558 smmd=0.3159 ct=7.4986 rec=1.3069 | train/val/test=0.832/0.838/0.836 | c=0.998347
[Epoch 0032] loss=18.8300 cls=0.4558 smmd=0.2957 ct=7.4959 rec=1.3063 | train/val/test=0.834/0.838/0.837 | c=0.998347
[Epoch 0033] loss=18.6853 cls=0.4531 smmd=0.2779 ct=7.5135 rec=1.3060 | train/val/test=0.829/0.830/0.832 | c=0.998347
[Epoch 0034] loss=18.5335 cls=0.4504 smmd=0.2653 ct=7.5007 rec=1.3071 | train/val/test=0.838/0.837/0.840 | c=0.998347
[Epoch 0035] loss=18.4365 cls=0.4493 smmd=0.2567 ct=7.4955 rec=1.3082 | train/val/test=0.838/0.839/0.841 | c=0.998347
[Epoch 0036] loss=18.2229 cls=0.4505 smmd=0.2351 ct=7.4961 rec=1.3082 | train/val/test=0.839/0.840/0.842 | c=0.998347
[Epoch 0037] loss=18.1761 cls=0.4500 smmd=0.2302 ct=7.4967 rec=1.3107 | train/val/test=0.835/0.837/0.837 | c=0.998347
[Epoch 0038] loss=17.9813 cls=0.4506 smmd=0.2091 ct=7.5048 rec=1.3098 | train/val/test=0.834/0.835/0.836 | c=0.998347
[Epoch 0039] loss=17.9739 cls=0.4515 smmd=0.2116 ct=7.4882 rec=1.3105 | train/val/test=0.843/0.840/0.843 | c=0.998347
[Epoch 0040] loss=17.8165 cls=0.4514 smmd=0.1959 ct=7.4877 rec=1.3133 | train/val/test=0.836/0.836/0.837 | c=0.998347
[Epoch 0041] loss=17.8753 cls=0.4525 smmd=0.1990 ct=7.5017 rec=1.3115 | train/val/test=0.839/0.838/0.841 | c=0.998347
[Epoch 0042] loss=17.6683 cls=0.4518 smmd=0.1806 ct=7.4900 rec=1.3120 | train/val/test=0.833/0.832/0.836 | c=0.998347
[Epoch 0043] loss=17.5915 cls=0.4544 smmd=0.1740 ct=7.4843 rec=1.3122 | train/val/test=0.838/0.839/0.842 | c=0.998347
[Epoch 0044] loss=17.5557 cls=0.4527 smmd=0.1706 ct=7.4837 rec=1.3120 | train/val/test=0.836/0.834/0.837 | c=0.998347
[Epoch 0045] loss=17.4752 cls=0.4535 smmd=0.1623 ct=7.4847 rec=1.3126 | train/val/test=0.838/0.840/0.841 | c=0.998347
[Epoch 0046] loss=17.3793 cls=0.4537 smmd=0.1520 ct=7.4883 rec=1.3125 | train/val/test=0.832/0.833/0.834 | c=0.998347
[Epoch 0047] loss=17.3708 cls=0.4569 smmd=0.1528 ct=7.4787 rec=1.3132 | train/val/test=0.841/0.841/0.843 | c=0.998347
[Epoch 0048] loss=17.2818 cls=0.4562 smmd=0.1443 ct=7.4768 rec=1.3139 | train/val/test=0.836/0.834/0.837 | c=0.998347
[Epoch 0049] loss=17.3544 cls=0.4592 smmd=0.1504 ct=7.4820 rec=1.3138 | train/val/test=0.842/0.844/0.843 | c=0.998347
[Epoch 0050] loss=17.2838 cls=0.4594 smmd=0.1437 ct=7.4796 rec=1.3166 | train/val/test=0.834/0.830/0.836 | c=0.998347
[Epoch 0051] loss=17.3008 cls=0.4625 smmd=0.1451 ct=7.4800 rec=1.3163 | train/val/test=0.842/0.845/0.846 | c=0.998347
[Epoch 0052] loss=17.1460 cls=0.4622 smmd=0.1309 ct=7.4739 rec=1.3167 | train/val/test=0.838/0.838/0.838 | c=0.998347
[Epoch 0053] loss=17.0964 cls=0.4610 smmd=0.1271 ct=7.4683 rec=1.3162 | train/val/test=0.843/0.845/0.845 | c=0.998347
[Epoch 0054] loss=17.1322 cls=0.4624 smmd=0.1285 ct=7.4786 rec=1.3171 | train/val/test=0.838/0.836/0.838 | c=0.998347
[Epoch 0055] loss=17.1305 cls=0.4620 smmd=0.1314 ct=7.4634 rec=1.3181 | train/val/test=0.844/0.845/0.846 | c=0.998347
[Epoch 0056] loss=17.0948 cls=0.4654 smmd=0.1266 ct=7.4683 rec=1.3187 | train/val/test=0.834/0.834/0.838 | c=0.998347
[Epoch 0057] loss=17.0580 cls=0.4682 smmd=0.1243 ct=7.4605 rec=1.3197 | train/val/test=0.840/0.836/0.844 | c=0.998347
[Epoch 0058] loss=17.0801 cls=0.4723 smmd=0.1255 ct=7.4638 rec=1.3227 | train/val/test=0.823/0.821/0.828 | c=0.998347
[Epoch 0059] loss=17.1657 cls=0.4840 smmd=0.1339 ct=7.4612 rec=1.3251 | train/val/test=0.824/0.822/0.827 | c=0.998347
[Epoch 0060] loss=17.2792 cls=0.4953 smmd=0.1443 ct=7.4618 rec=1.3304 | train/val/test=0.796/0.786/0.799 | c=0.998347
[Epoch 0061] loss=17.3969 cls=0.5270 smmd=0.1539 ct=7.4626 rec=1.3390 | train/val/test=0.799/0.798/0.798 | c=0.998347
[Epoch 0062] loss=17.4263 cls=0.5270 smmd=0.1540 ct=7.4770 rec=1.3381 | train/val/test=0.796/0.788/0.801 | c=0.998347
[Epoch 0063] loss=17.4646 cls=0.5250 smmd=0.1644 ct=7.4446 rec=1.3372 | train/val/test=0.822/0.821/0.824 | c=0.998347
[Epoch 0064] loss=17.3726 cls=0.4965 smmd=0.1521 ct=7.4697 rec=1.3273 | train/val/test=0.821/0.816/0.826 | c=0.998347
[Epoch 0065] loss=17.2280 cls=0.4781 smmd=0.1465 ct=7.4315 rec=1.3227 | train/val/test=0.840/0.844/0.842 | c=0.998347
[Epoch 0066] loss=16.9847 cls=0.4578 smmd=0.1223 ct=7.4377 rec=1.3150 | train/val/test=0.839/0.843/0.843 | c=0.998347
[Epoch 0067] loss=16.9545 cls=0.4575 smmd=0.1197 ct=7.4352 rec=1.3172 | train/val/test=0.829/0.828/0.834 | c=0.998347
[Epoch 0068] loss=17.0172 cls=0.4740 smmd=0.1258 ct=7.4306 rec=1.3225 | train/val/test=0.839/0.839/0.841 | c=0.998347
[Epoch 0069] loss=17.0073 cls=0.4760 smmd=0.1229 ct=7.4389 rec=1.3256 | train/val/test=0.829/0.827/0.833 | c=0.998347
[Epoch 0070] loss=16.9722 cls=0.4849 smmd=0.1196 ct=7.4345 rec=1.3299 | train/val/test=0.841/0.840/0.842 | c=0.998347
[Epoch 0071] loss=16.9935 cls=0.4834 smmd=0.1219 ct=7.4340 rec=1.3303 | train/val/test=0.833/0.832/0.836 | c=0.998347
[Epoch 0072] loss=17.1275 cls=0.4899 smmd=0.1354 ct=7.4309 rec=1.3329 | train/val/test=0.841/0.841/0.842 | c=0.998347
[Epoch 0073] loss=17.1577 cls=0.4857 smmd=0.1373 ct=7.4378 rec=1.3319 | train/val/test=0.831/0.830/0.835 | c=0.998347
[Epoch 0074] loss=17.1485 cls=0.4886 smmd=0.1396 ct=7.4208 rec=1.3325 | train/val/test=0.835/0.838/0.840 | c=0.998347
[Epoch 0075] loss=17.0664 cls=0.4883 smmd=0.1282 ct=7.4377 rec=1.3302 | train/val/test=0.824/0.820/0.830 | c=0.998347
[Epoch 0076] loss=17.0568 cls=0.4878 smmd=0.1306 ct=7.4208 rec=1.3314 | train/val/test=0.831/0.834/0.834 | c=0.998347
[Epoch 0077] loss=17.0560 cls=0.4896 smmd=0.1294 ct=7.4269 rec=1.3273 | train/val/test=0.824/0.818/0.828 | c=0.998347
[Epoch 0078] loss=17.0209 cls=0.4812 smmd=0.1265 ct=7.4257 rec=1.3275 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0079] loss=16.8963 cls=0.4774 smmd=0.1159 ct=7.4185 rec=1.3233 | train/val/test=0.829/0.830/0.833 | c=0.998347
[Epoch 0080] loss=16.9148 cls=0.4714 smmd=0.1188 ct=7.4146 rec=1.3230 | train/val/test=0.841/0.844/0.842 | c=0.998347
[Epoch 0081] loss=16.9257 cls=0.4699 smmd=0.1182 ct=7.4234 rec=1.3230 | train/val/test=0.830/0.832/0.836 | c=0.998347
[Epoch 0082] loss=16.9107 cls=0.4755 smmd=0.1186 ct=7.4122 rec=1.3261 | train/val/test=0.841/0.841/0.840 | c=0.998347
[Epoch 0083] loss=16.8356 cls=0.4774 smmd=0.1088 ct=7.4226 rec=1.3279 | train/val/test=0.822/0.819/0.825 | c=0.998347
[Epoch 0084] loss=16.9398 cls=0.4925 smmd=0.1190 ct=7.4184 rec=1.3340 | train/val/test=0.836/0.834/0.836 | c=0.998347
[Epoch 0085] loss=16.9896 cls=0.4902 smmd=0.1224 ct=7.4269 rec=1.3324 | train/val/test=0.809/0.804/0.810 | c=0.998347
[Epoch 0086] loss=17.0964 cls=0.5147 smmd=0.1319 ct=7.4247 rec=1.3408 | train/val/test=0.807/0.806/0.805 | c=0.998347
[Epoch 0087] loss=17.2469 cls=0.5198 smmd=0.1440 ct=7.4383 rec=1.3402 | train/val/test=0.781/0.778/0.785 | c=0.998347
[Epoch 0088] loss=17.4096 cls=0.5573 smmd=0.1579 ct=7.4391 rec=1.3487 | train/val/test=0.781/0.785/0.781 | c=0.998347
[Epoch 0089] loss=17.3859 cls=0.5423 smmd=0.1553 ct=7.4455 rec=1.3420 | train/val/test=0.801/0.795/0.807 | c=0.998347
[Epoch 0090] loss=17.2422 cls=0.5146 smmd=0.1482 ct=7.4190 rec=1.3289 | train/val/test=0.837/0.838/0.840 | c=0.998347
[Epoch 0091] loss=16.8553 cls=0.4628 smmd=0.1133 ct=7.4167 rec=1.3155 | train/val/test=0.839/0.840/0.839 | c=0.998347
[Epoch 0092] loss=16.7420 cls=0.4550 smmd=0.1059 ct=7.4001 rec=1.3111 | train/val/test=0.824/0.817/0.828 | c=0.998347
[Epoch 0093] loss=16.8734 cls=0.4758 smmd=0.1157 ct=7.4090 rec=1.3201 | train/val/test=0.828/0.827/0.832 | c=0.998347
[Epoch 0094] loss=16.9690 cls=0.4956 smmd=0.1205 ct=7.4262 rec=1.3283 | train/val/test=0.809/0.804/0.813 | c=0.998347
[Epoch 0095] loss=17.0811 cls=0.5168 smmd=0.1312 ct=7.4203 rec=1.3403 | train/val/test=0.827/0.824/0.831 | c=0.998347
[Epoch 0096] loss=17.1517 cls=0.5146 smmd=0.1363 ct=7.4314 rec=1.3372 | train/val/test=0.807/0.800/0.808 | c=0.998347
[Epoch 0097] loss=17.2462 cls=0.5251 smmd=0.1458 ct=7.4262 rec=1.3471 | train/val/test=0.825/0.826/0.830 | c=0.998347
[Epoch 0098] loss=17.3084 cls=0.5198 smmd=0.1525 ct=7.4271 rec=1.3378 | train/val/test=0.812/0.805/0.814 | c=0.998347
[Epoch 0099] loss=17.2609 cls=0.5110 smmd=0.1497 ct=7.4185 rec=1.3431 | train/val/test=0.831/0.836/0.834 | c=0.998347
=== Best @ epoch 51: val=0.8453, test=0.8456 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2 - 2025-09-21 05:30:17:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=71.7577 cls=1.0994 smmd=5.5983 ct=7.2590 rec=1.4136 | train/val/test=0.399/0.403/0.396 | c=0.998347
[Epoch 0001] loss=51.7945 cls=1.0628 smmd=3.6128 ct=7.2137 rec=1.4154 | train/val/test=0.341/0.327/0.334 | c=0.998347
[Epoch 0002] loss=38.8849 cls=1.0939 smmd=2.3321 ct=7.1550 rec=1.4134 | train/val/test=0.546/0.539/0.549 | c=0.998347
[Epoch 0003] loss=40.2826 cls=1.0520 smmd=2.4849 ct=7.1006 rec=1.4133 | train/val/test=0.563/0.573/0.565 | c=0.998347
[Epoch 0004] loss=39.6974 cls=1.0005 smmd=2.4369 ct=7.0602 rec=1.4158 | train/val/test=0.556/0.565/0.558 | c=0.998347
[Epoch 0005] loss=35.8543 cls=0.9607 smmd=2.0703 ct=6.9813 rec=1.4166 | train/val/test=0.555/0.562/0.555 | c=0.998347
[Epoch 0006] loss=30.7605 cls=0.9181 smmd=1.5743 ct=6.9257 rec=1.4133 | train/val/test=0.557/0.564/0.557 | c=0.998347
[Epoch 0007] loss=34.5303 cls=0.8740 smmd=1.8207 ct=7.5919 rec=1.4054 | train/val/test=0.557/0.555/0.561 | c=0.998347
[Epoch 0008] loss=35.5443 cls=0.8335 smmd=1.9342 ct=7.5437 rec=1.3960 | train/val/test=0.608/0.602/0.618 | c=0.998347
[Epoch 0009] loss=29.7990 cls=0.7972 smmd=1.3702 ct=7.5029 rec=1.3862 | train/val/test=0.654/0.652/0.659 | c=0.998347
[Epoch 0010] loss=27.7570 cls=0.7706 smmd=1.1642 ct=7.5203 rec=1.3792 | train/val/test=0.667/0.669/0.671 | c=0.998347
[Epoch 0011] loss=30.2970 cls=0.7519 smmd=1.4176 ct=7.5286 rec=1.3754 | train/val/test=0.685/0.688/0.689 | c=0.998347
[Epoch 0012] loss=27.4045 cls=0.7207 smmd=1.1331 ct=7.5144 rec=1.3679 | train/val/test=0.689/0.699/0.699 | c=0.998347
[Epoch 0013] loss=26.5442 cls=0.7038 smmd=1.0401 ct=7.5550 rec=1.3623 | train/val/test=0.716/0.726/0.727 | c=0.998347
[Epoch 0014] loss=25.7748 cls=0.6728 smmd=0.9683 ct=7.5388 rec=1.3546 | train/val/test=0.728/0.735/0.735 | c=0.998347
[Epoch 0015] loss=25.3038 cls=0.6643 smmd=0.9250 ct=7.5221 rec=1.3547 | train/val/test=0.748/0.753/0.756 | c=0.998347
[Epoch 0016] loss=25.0261 cls=0.6376 smmd=0.8948 ct=7.5431 rec=1.3470 | train/val/test=0.764/0.774/0.774 | c=0.998347
[Epoch 0017] loss=23.2242 cls=0.6092 smmd=0.7128 ct=7.5612 rec=1.3391 | train/val/test=0.768/0.770/0.771 | c=0.998347
[Epoch 0018] loss=23.7181 cls=0.6011 smmd=0.7675 ct=7.5369 rec=1.3372 | train/val/test=0.785/0.790/0.788 | c=0.998347
[Epoch 0019] loss=22.4810 cls=0.5767 smmd=0.6495 ct=7.5165 rec=1.3299 | train/val/test=0.780/0.779/0.787 | c=0.998347
[Epoch 0020] loss=22.4392 cls=0.5651 smmd=0.6414 ct=7.5406 rec=1.3230 | train/val/test=0.787/0.786/0.797 | c=0.998347
[Epoch 0021] loss=21.8306 cls=0.5515 smmd=0.5825 ct=7.5353 rec=1.3193 | train/val/test=0.806/0.801/0.805 | c=0.998347
[Epoch 0022] loss=21.4894 cls=0.5276 smmd=0.5515 ct=7.5261 rec=1.3167 | train/val/test=0.811/0.806/0.809 | c=0.998347
[Epoch 0023] loss=21.1547 cls=0.5172 smmd=0.5184 ct=7.5273 rec=1.3145 | train/val/test=0.812/0.811/0.812 | c=0.998347
[Epoch 0024] loss=20.6115 cls=0.5108 smmd=0.4648 ct=7.5264 rec=1.3114 | train/val/test=0.809/0.806/0.817 | c=0.998347
[Epoch 0025] loss=20.4919 cls=0.5025 smmd=0.4535 ct=7.5250 rec=1.3102 | train/val/test=0.813/0.812/0.822 | c=0.998347
[Epoch 0026] loss=20.0061 cls=0.4922 smmd=0.4060 ct=7.5228 rec=1.3093 | train/val/test=0.822/0.822/0.823 | c=0.998347
[Epoch 0027] loss=20.0335 cls=0.4859 smmd=0.4102 ct=7.5167 rec=1.3106 | train/val/test=0.824/0.823/0.825 | c=0.998347
[Epoch 0028] loss=19.5902 cls=0.4729 smmd=0.3672 ct=7.5139 rec=1.3079 | train/val/test=0.824/0.826/0.830 | c=0.998347
[Epoch 0029] loss=19.4829 cls=0.4682 smmd=0.3557 ct=7.5191 rec=1.3064 | train/val/test=0.829/0.833/0.832 | c=0.998347
[Epoch 0030] loss=19.2071 cls=0.4625 smmd=0.3279 ct=7.5218 rec=1.3062 | train/val/test=0.828/0.830/0.831 | c=0.998347
[Epoch 0031] loss=19.0377 cls=0.4558 smmd=0.3159 ct=7.4986 rec=1.3069 | train/val/test=0.832/0.838/0.836 | c=0.998347
[Epoch 0032] loss=18.8300 cls=0.4558 smmd=0.2957 ct=7.4959 rec=1.3063 | train/val/test=0.834/0.838/0.837 | c=0.998347
[Epoch 0033] loss=18.6853 cls=0.4531 smmd=0.2779 ct=7.5135 rec=1.3060 | train/val/test=0.829/0.830/0.832 | c=0.998347
[Epoch 0034] loss=18.5335 cls=0.4504 smmd=0.2653 ct=7.5007 rec=1.3071 | train/val/test=0.838/0.837/0.840 | c=0.998347
[Epoch 0035] loss=18.4365 cls=0.4493 smmd=0.2567 ct=7.4955 rec=1.3082 | train/val/test=0.838/0.839/0.841 | c=0.998347
[Epoch 0036] loss=18.2229 cls=0.4505 smmd=0.2351 ct=7.4961 rec=1.3082 | train/val/test=0.839/0.840/0.842 | c=0.998347
[Epoch 0037] loss=18.1761 cls=0.4500 smmd=0.2302 ct=7.4967 rec=1.3107 | train/val/test=0.835/0.837/0.837 | c=0.998347
[Epoch 0038] loss=17.9813 cls=0.4506 smmd=0.2091 ct=7.5048 rec=1.3098 | train/val/test=0.834/0.835/0.836 | c=0.998347
[Epoch 0039] loss=17.9739 cls=0.4515 smmd=0.2116 ct=7.4882 rec=1.3105 | train/val/test=0.843/0.840/0.843 | c=0.998347
[Epoch 0040] loss=17.8165 cls=0.4514 smmd=0.1959 ct=7.4877 rec=1.3133 | train/val/test=0.836/0.836/0.837 | c=0.998347
[Epoch 0041] loss=17.8753 cls=0.4525 smmd=0.1990 ct=7.5017 rec=1.3115 | train/val/test=0.839/0.838/0.841 | c=0.998347
[Epoch 0042] loss=17.6683 cls=0.4518 smmd=0.1806 ct=7.4900 rec=1.3120 | train/val/test=0.833/0.832/0.836 | c=0.998347
[Epoch 0043] loss=17.5915 cls=0.4544 smmd=0.1740 ct=7.4843 rec=1.3122 | train/val/test=0.838/0.839/0.842 | c=0.998347
[Epoch 0044] loss=17.5557 cls=0.4527 smmd=0.1706 ct=7.4837 rec=1.3120 | train/val/test=0.836/0.834/0.837 | c=0.998347
[Epoch 0045] loss=17.4752 cls=0.4535 smmd=0.1623 ct=7.4847 rec=1.3126 | train/val/test=0.838/0.840/0.841 | c=0.998347
[Epoch 0046] loss=17.3793 cls=0.4537 smmd=0.1520 ct=7.4883 rec=1.3125 | train/val/test=0.832/0.833/0.834 | c=0.998347
[Epoch 0047] loss=17.3708 cls=0.4569 smmd=0.1528 ct=7.4787 rec=1.3132 | train/val/test=0.841/0.841/0.843 | c=0.998347
[Epoch 0048] loss=17.2818 cls=0.4562 smmd=0.1443 ct=7.4768 rec=1.3139 | train/val/test=0.836/0.834/0.837 | c=0.998347
[Epoch 0049] loss=17.3544 cls=0.4592 smmd=0.1504 ct=7.4820 rec=1.3138 | train/val/test=0.842/0.844/0.843 | c=0.998347
[Epoch 0050] loss=17.2838 cls=0.4594 smmd=0.1437 ct=7.4796 rec=1.3166 | train/val/test=0.834/0.830/0.836 | c=0.998347
[Epoch 0051] loss=17.3008 cls=0.4625 smmd=0.1451 ct=7.4800 rec=1.3163 | train/val/test=0.842/0.845/0.846 | c=0.998347
[Epoch 0052] loss=17.1460 cls=0.4622 smmd=0.1309 ct=7.4739 rec=1.3167 | train/val/test=0.838/0.838/0.838 | c=0.998347
[Epoch 0053] loss=17.0964 cls=0.4610 smmd=0.1271 ct=7.4683 rec=1.3162 | train/val/test=0.843/0.845/0.845 | c=0.998347
[Epoch 0054] loss=17.1322 cls=0.4624 smmd=0.1285 ct=7.4786 rec=1.3171 | train/val/test=0.838/0.836/0.838 | c=0.998347
[Epoch 0055] loss=17.1305 cls=0.4620 smmd=0.1314 ct=7.4634 rec=1.3181 | train/val/test=0.844/0.845/0.846 | c=0.998347
[Epoch 0056] loss=17.0948 cls=0.4654 smmd=0.1266 ct=7.4683 rec=1.3187 | train/val/test=0.834/0.834/0.838 | c=0.998347
[Epoch 0057] loss=17.0580 cls=0.4682 smmd=0.1243 ct=7.4605 rec=1.3197 | train/val/test=0.840/0.836/0.844 | c=0.998347
[Epoch 0058] loss=17.0801 cls=0.4723 smmd=0.1255 ct=7.4638 rec=1.3227 | train/val/test=0.823/0.821/0.828 | c=0.998347
[Epoch 0059] loss=17.1657 cls=0.4840 smmd=0.1339 ct=7.4612 rec=1.3251 | train/val/test=0.824/0.822/0.827 | c=0.998347
[Epoch 0060] loss=17.2792 cls=0.4953 smmd=0.1443 ct=7.4618 rec=1.3304 | train/val/test=0.796/0.786/0.799 | c=0.998347
[Epoch 0061] loss=17.3969 cls=0.5270 smmd=0.1539 ct=7.4626 rec=1.3390 | train/val/test=0.799/0.798/0.798 | c=0.998347
[Epoch 0062] loss=17.4263 cls=0.5270 smmd=0.1540 ct=7.4770 rec=1.3381 | train/val/test=0.796/0.788/0.801 | c=0.998347
[Epoch 0063] loss=17.4646 cls=0.5250 smmd=0.1644 ct=7.4446 rec=1.3372 | train/val/test=0.822/0.821/0.824 | c=0.998347
[Epoch 0064] loss=17.3726 cls=0.4965 smmd=0.1521 ct=7.4697 rec=1.3273 | train/val/test=0.821/0.816/0.826 | c=0.998347
[Epoch 0065] loss=17.2280 cls=0.4781 smmd=0.1465 ct=7.4315 rec=1.3227 | train/val/test=0.840/0.844/0.842 | c=0.998347
[Epoch 0066] loss=16.9847 cls=0.4578 smmd=0.1223 ct=7.4377 rec=1.3150 | train/val/test=0.839/0.843/0.843 | c=0.998347
[Epoch 0067] loss=16.9545 cls=0.4575 smmd=0.1197 ct=7.4352 rec=1.3172 | train/val/test=0.829/0.828/0.834 | c=0.998347
[Epoch 0068] loss=17.0172 cls=0.4740 smmd=0.1258 ct=7.4306 rec=1.3225 | train/val/test=0.839/0.839/0.841 | c=0.998347
[Epoch 0069] loss=17.0073 cls=0.4760 smmd=0.1229 ct=7.4389 rec=1.3256 | train/val/test=0.829/0.827/0.833 | c=0.998347
[Epoch 0070] loss=16.9722 cls=0.4849 smmd=0.1196 ct=7.4345 rec=1.3299 | train/val/test=0.841/0.840/0.842 | c=0.998347
[Epoch 0071] loss=16.9935 cls=0.4834 smmd=0.1219 ct=7.4340 rec=1.3303 | train/val/test=0.833/0.832/0.836 | c=0.998347
[Epoch 0072] loss=17.1275 cls=0.4899 smmd=0.1354 ct=7.4309 rec=1.3329 | train/val/test=0.841/0.841/0.842 | c=0.998347
[Epoch 0073] loss=17.1577 cls=0.4857 smmd=0.1373 ct=7.4378 rec=1.3319 | train/val/test=0.831/0.830/0.835 | c=0.998347
[Epoch 0074] loss=17.1485 cls=0.4886 smmd=0.1396 ct=7.4208 rec=1.3325 | train/val/test=0.835/0.838/0.840 | c=0.998347
[Epoch 0075] loss=17.0664 cls=0.4883 smmd=0.1282 ct=7.4377 rec=1.3302 | train/val/test=0.824/0.820/0.830 | c=0.998347
[Epoch 0076] loss=17.0568 cls=0.4878 smmd=0.1306 ct=7.4208 rec=1.3314 | train/val/test=0.831/0.834/0.834 | c=0.998347
[Epoch 0077] loss=17.0560 cls=0.4896 smmd=0.1294 ct=7.4269 rec=1.3273 | train/val/test=0.824/0.818/0.828 | c=0.998347
[Epoch 0078] loss=17.0209 cls=0.4812 smmd=0.1265 ct=7.4257 rec=1.3275 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0079] loss=16.8963 cls=0.4774 smmd=0.1159 ct=7.4185 rec=1.3233 | train/val/test=0.829/0.830/0.833 | c=0.998347
[Epoch 0080] loss=16.9148 cls=0.4714 smmd=0.1188 ct=7.4146 rec=1.3230 | train/val/test=0.841/0.844/0.842 | c=0.998347
[Epoch 0081] loss=16.9257 cls=0.4699 smmd=0.1182 ct=7.4234 rec=1.3230 | train/val/test=0.830/0.832/0.836 | c=0.998347
[Epoch 0082] loss=16.9107 cls=0.4755 smmd=0.1186 ct=7.4122 rec=1.3261 | train/val/test=0.841/0.841/0.840 | c=0.998347
[Epoch 0083] loss=16.8356 cls=0.4774 smmd=0.1088 ct=7.4226 rec=1.3279 | train/val/test=0.822/0.819/0.825 | c=0.998347
[Epoch 0084] loss=16.9398 cls=0.4925 smmd=0.1190 ct=7.4184 rec=1.3340 | train/val/test=0.836/0.834/0.836 | c=0.998347
[Epoch 0085] loss=16.9896 cls=0.4902 smmd=0.1224 ct=7.4269 rec=1.3324 | train/val/test=0.809/0.804/0.810 | c=0.998347
[Epoch 0086] loss=17.0964 cls=0.5147 smmd=0.1319 ct=7.4247 rec=1.3408 | train/val/test=0.807/0.806/0.805 | c=0.998347
[Epoch 0087] loss=17.2469 cls=0.5198 smmd=0.1440 ct=7.4383 rec=1.3402 | train/val/test=0.781/0.778/0.785 | c=0.998347
[Epoch 0088] loss=17.4096 cls=0.5573 smmd=0.1579 ct=7.4391 rec=1.3487 | train/val/test=0.781/0.785/0.781 | c=0.998347
[Epoch 0089] loss=17.3859 cls=0.5423 smmd=0.1553 ct=7.4455 rec=1.3420 | train/val/test=0.801/0.795/0.807 | c=0.998347
[Epoch 0090] loss=17.2422 cls=0.5146 smmd=0.1482 ct=7.4190 rec=1.3289 | train/val/test=0.837/0.838/0.840 | c=0.998347
[Epoch 0091] loss=16.8553 cls=0.4628 smmd=0.1133 ct=7.4167 rec=1.3155 | train/val/test=0.839/0.840/0.839 | c=0.998347
[Epoch 0092] loss=16.7420 cls=0.4550 smmd=0.1059 ct=7.4001 rec=1.3111 | train/val/test=0.824/0.817/0.828 | c=0.998347
[Epoch 0093] loss=16.8734 cls=0.4758 smmd=0.1157 ct=7.4090 rec=1.3201 | train/val/test=0.828/0.827/0.832 | c=0.998347
[Epoch 0094] loss=16.9690 cls=0.4956 smmd=0.1205 ct=7.4262 rec=1.3283 | train/val/test=0.809/0.804/0.813 | c=0.998347
[Epoch 0095] loss=17.0811 cls=0.5168 smmd=0.1312 ct=7.4203 rec=1.3403 | train/val/test=0.827/0.824/0.831 | c=0.998347
[Epoch 0096] loss=17.1517 cls=0.5146 smmd=0.1363 ct=7.4314 rec=1.3372 | train/val/test=0.807/0.800/0.808 | c=0.998347
[Epoch 0097] loss=17.2462 cls=0.5251 smmd=0.1458 ct=7.4262 rec=1.3471 | train/val/test=0.825/0.826/0.830 | c=0.998347
[Epoch 0098] loss=17.3084 cls=0.5198 smmd=0.1525 ct=7.4271 rec=1.3378 | train/val/test=0.812/0.805/0.814 | c=0.998347
[Epoch 0099] loss=17.2609 cls=0.5110 smmd=0.1497 ct=7.4185 rec=1.3431 | train/val/test=0.831/0.836/0.834 | c=0.998347
=== Best @ epoch 51: val=0.8453, test=0.8456 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-2 completed in 142.15 seconds.
==================================================
