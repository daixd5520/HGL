Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 - 2025-09-21 06:34:18:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1884 cls=1.9447 smmd=4.1912 ct=9.2747 rec=1.3889 | train/val/test=0.345/0.346/0.357 | c=0.998347
[Epoch 0001] loss=16.6874 cls=1.8939 smmd=2.7887 ct=9.2251 rec=1.3898 | train/val/test=0.724/0.462/0.446 | c=0.998347
[Epoch 0002] loss=15.1552 cls=1.7454 smmd=1.5215 ct=9.1108 rec=1.3887 | train/val/test=0.759/0.200/0.232 | c=0.998347
[Epoch 0003] loss=14.7596 cls=1.4730 smmd=1.4810 ct=9.0314 rec=1.3871 | train/val/test=0.897/0.424/0.404 | c=0.998347
[Epoch 0004] loss=14.6039 cls=1.1683 smmd=1.7202 ct=8.9533 rec=1.3810 | train/val/test=0.828/0.380/0.406 | c=0.998347
[Epoch 0005] loss=14.0541 cls=0.7825 smmd=1.6507 ct=8.8925 rec=1.3642 | train/val/test=0.966/0.436/0.439 | c=0.998347
[Epoch 0006] loss=13.4312 cls=0.5247 smmd=1.3767 ct=8.8458 rec=1.3420 | train/val/test=0.966/0.442/0.482 | c=0.998347
[Epoch 0007] loss=12.8587 cls=0.3224 smmd=1.0813 ct=8.8296 rec=1.3127 | train/val/test=0.966/0.452/0.482 | c=0.998347
[Epoch 0008] loss=12.5726 cls=0.2037 smmd=0.9748 ct=8.8243 rec=1.2849 | train/val/test=1.000/0.480/0.511 | c=0.998347
[Epoch 0009] loss=12.5357 cls=0.1146 smmd=1.0846 ct=8.8122 rec=1.2621 | train/val/test=1.000/0.492/0.529 | c=0.998347
[Epoch 0010] loss=12.4775 cls=0.0650 smmd=1.1150 ct=8.8079 rec=1.2448 | train/val/test=1.000/0.512/0.542 | c=0.998347
[Epoch 0011] loss=12.9990 cls=0.0395 smmd=1.0208 ct=9.4806 rec=1.2290 | train/val/test=1.000/0.506/0.532 | c=0.998347
[Epoch 0012] loss=12.7196 cls=0.0243 smmd=0.8483 ct=9.4169 rec=1.2151 | train/val/test=1.000/0.510/0.539 | c=0.998347
[Epoch 0013] loss=12.5887 cls=0.0138 smmd=0.7994 ct=9.3620 rec=1.2067 | train/val/test=1.000/0.536/0.552 | c=0.998347
[Epoch 0014] loss=12.5438 cls=0.0084 smmd=0.7876 ct=9.3451 rec=1.2014 | train/val/test=1.000/0.558/0.577 | c=0.998347
[Epoch 0015] loss=12.5020 cls=0.0071 smmd=0.7333 ct=9.3658 rec=1.1979 | train/val/test=1.000/0.564/0.592 | c=0.998347
[Epoch 0016] loss=12.4880 cls=0.0061 smmd=0.6863 ct=9.4030 rec=1.1963 | train/val/test=1.000/0.564/0.585 | c=0.998347
[Epoch 0017] loss=12.3651 cls=0.0049 smmd=0.5477 ct=9.4226 rec=1.1949 | train/val/test=1.000/0.572/0.598 | c=0.998347
[Epoch 0018] loss=12.2438 cls=0.0047 smmd=0.4361 ct=9.4134 rec=1.1948 | train/val/test=1.000/0.578/0.608 | c=0.998347
[Epoch 0019] loss=12.2196 cls=0.0054 smmd=0.4250 ct=9.3962 rec=1.1965 | train/val/test=1.000/0.580/0.615 | c=0.998347
[Epoch 0020] loss=12.2106 cls=0.0064 smmd=0.4160 ct=9.3910 rec=1.1986 | train/val/test=1.000/0.576/0.615 | c=0.998347
[Epoch 0021] loss=12.1591 cls=0.0081 smmd=0.3550 ct=9.3973 rec=1.1994 | train/val/test=1.000/0.578/0.614 | c=0.998347
[Epoch 0022] loss=12.0738 cls=0.0114 smmd=0.2604 ct=9.4034 rec=1.1993 | train/val/test=1.000/0.584/0.622 | c=0.998347
[Epoch 0023] loss=12.0597 cls=0.0151 smmd=0.2482 ct=9.4005 rec=1.1979 | train/val/test=1.000/0.576/0.621 | c=0.998347
[Epoch 0024] loss=12.0365 cls=0.0196 smmd=0.2337 ct=9.3917 rec=1.1958 | train/val/test=1.000/0.582/0.634 | c=0.998347
[Epoch 0025] loss=12.0071 cls=0.0240 smmd=0.2156 ct=9.3837 rec=1.1919 | train/val/test=1.000/0.586/0.630 | c=0.998347
[Epoch 0026] loss=11.9756 cls=0.0282 smmd=0.1889 ct=9.3821 rec=1.1882 | train/val/test=1.000/0.588/0.628 | c=0.998347
[Epoch 0027] loss=11.9477 cls=0.0301 smmd=0.1611 ct=9.3892 rec=1.1836 | train/val/test=1.000/0.586/0.640 | c=0.998347
[Epoch 0028] loss=11.9354 cls=0.0295 smmd=0.1513 ct=9.3945 rec=1.1800 | train/val/test=1.000/0.598/0.640 | c=0.998347
[Epoch 0029] loss=11.8989 cls=0.0264 smmd=0.1363 ct=9.3860 rec=1.1751 | train/val/test=1.000/0.604/0.642 | c=0.998347
[Epoch 0030] loss=11.8704 cls=0.0234 smmd=0.1320 ct=9.3719 rec=1.1715 | train/val/test=1.000/0.608/0.647 | c=0.998347
[Epoch 0031] loss=11.8394 cls=0.0208 smmd=0.1151 ct=9.3657 rec=1.1689 | train/val/test=1.000/0.610/0.645 | c=0.998347
[Epoch 0032] loss=11.8307 cls=0.0185 smmd=0.1155 ct=9.3627 rec=1.1670 | train/val/test=1.000/0.612/0.646 | c=0.998347
[Epoch 0033] loss=11.8083 cls=0.0164 smmd=0.0975 ct=9.3624 rec=1.1660 | train/val/test=1.000/0.608/0.641 | c=0.998347
[Epoch 0034] loss=11.7844 cls=0.0139 smmd=0.0754 ct=9.3638 rec=1.1657 | train/val/test=1.000/0.614/0.642 | c=0.998347
[Epoch 0035] loss=11.7754 cls=0.0123 smmd=0.0695 ct=9.3618 rec=1.1659 | train/val/test=1.000/0.612/0.641 | c=0.998347
[Epoch 0036] loss=11.7673 cls=0.0120 smmd=0.0652 ct=9.3575 rec=1.1663 | train/val/test=1.000/0.620/0.640 | c=0.998347
[Epoch 0037] loss=11.7546 cls=0.0113 smmd=0.0554 ct=9.3540 rec=1.1669 | train/val/test=1.000/0.608/0.637 | c=0.998347
[Epoch 0038] loss=11.7460 cls=0.0123 smmd=0.0463 ct=9.3519 rec=1.1678 | train/val/test=1.000/0.622/0.645 | c=0.998347
[Epoch 0039] loss=11.7436 cls=0.0113 smmd=0.0440 ct=9.3528 rec=1.1677 | train/val/test=1.000/0.608/0.634 | c=0.998347
[Epoch 0040] loss=11.7472 cls=0.0128 smmd=0.0460 ct=9.3517 rec=1.1683 | train/val/test=1.000/0.624/0.649 | c=0.998347
[Epoch 0041] loss=11.7508 cls=0.0132 smmd=0.0464 ct=9.3534 rec=1.1689 | train/val/test=1.000/0.596/0.626 | c=0.998347
[Epoch 0042] loss=11.7815 cls=0.0166 smmd=0.0623 ct=9.3577 rec=1.1724 | train/val/test=1.000/0.626/0.652 | c=0.998347
[Epoch 0043] loss=11.8006 cls=0.0192 smmd=0.0684 ct=9.3684 rec=1.1723 | train/val/test=1.000/0.604/0.628 | c=0.998347
[Epoch 0044] loss=11.7589 cls=0.0150 smmd=0.0530 ct=9.3563 rec=1.1673 | train/val/test=1.000/0.616/0.648 | c=0.998347
[Epoch 0045] loss=11.7023 cls=0.0109 smmd=0.0325 ct=9.3403 rec=1.1593 | train/val/test=1.000/0.620/0.650 | c=0.998347
[Epoch 0046] loss=11.7258 cls=0.0124 smmd=0.0483 ct=9.3419 rec=1.1616 | train/val/test=1.000/0.612/0.633 | c=0.998347
[Epoch 0047] loss=11.7282 cls=0.0126 smmd=0.0477 ct=9.3449 rec=1.1615 | train/val/test=1.000/0.616/0.649 | c=0.998347
[Epoch 0048] loss=11.6835 cls=0.0107 smmd=0.0173 ct=9.3388 rec=1.1583 | train/val/test=1.000/0.618/0.642 | c=0.998347
[Epoch 0049] loss=11.7058 cls=0.0118 smmd=0.0313 ct=9.3394 rec=1.1616 | train/val/test=1.000/0.606/0.636 | c=0.998347
[Epoch 0050] loss=11.7196 cls=0.0142 smmd=0.0349 ct=9.3417 rec=1.1644 | train/val/test=1.000/0.616/0.644 | c=0.998347
[Epoch 0051] loss=11.6900 cls=0.0123 smmd=0.0199 ct=9.3312 rec=1.1633 | train/val/test=1.000/0.620/0.647 | c=0.998347
[Epoch 0052] loss=11.6803 cls=0.0126 smmd=0.0095 ct=9.3302 rec=1.1640 | train/val/test=1.000/0.606/0.635 | c=0.998347
[Epoch 0053] loss=11.7054 cls=0.0157 smmd=0.0193 ct=9.3352 rec=1.1675 | train/val/test=1.000/0.622/0.647 | c=0.998347
[Epoch 0054] loss=11.7053 cls=0.0148 smmd=0.0162 ct=9.3345 rec=1.1699 | train/val/test=1.000/0.606/0.642 | c=0.998347
[Epoch 0055] loss=11.6920 cls=0.0139 smmd=0.0104 ct=9.3338 rec=1.1669 | train/val/test=1.000/0.616/0.643 | c=0.998347
[Epoch 0056] loss=11.6675 cls=0.0127 smmd=0.0046 ct=9.3220 rec=1.1641 | train/val/test=1.000/0.626/0.652 | c=0.998347
[Epoch 0057] loss=11.6699 cls=0.0130 smmd=0.0035 ct=9.3251 rec=1.1642 | train/val/test=1.000/0.612/0.642 | c=0.998347
[Epoch 0058] loss=11.6781 cls=0.0136 smmd=0.0112 ct=9.3246 rec=1.1643 | train/val/test=1.000/0.630/0.655 | c=0.998347
[Epoch 0059] loss=11.6614 cls=0.0128 smmd=0.0032 ct=9.3206 rec=1.1624 | train/val/test=1.000/0.622/0.648 | c=0.998347
[Epoch 0060] loss=11.6549 cls=0.0120 smmd=0.0032 ct=9.3174 rec=1.1612 | train/val/test=1.000/0.624/0.653 | c=0.998347
[Epoch 0061] loss=11.6567 cls=0.0128 smmd=-0.0001 ct=9.3194 rec=1.1623 | train/val/test=1.000/0.634/0.655 | c=0.998347
[Epoch 0062] loss=11.6559 cls=0.0130 smmd=0.0016 ct=9.3159 rec=1.1627 | train/val/test=1.000/0.630/0.652 | c=0.998347
[Epoch 0063] loss=11.6462 cls=0.0131 smmd=-0.0057 ct=9.3141 rec=1.1624 | train/val/test=1.000/0.632/0.650 | c=0.998347
[Epoch 0064] loss=11.6469 cls=0.0135 smmd=-0.0017 ct=9.3095 rec=1.1628 | train/val/test=1.000/0.632/0.650 | c=0.998347
[Epoch 0065] loss=11.6470 cls=0.0141 smmd=-0.0067 ct=9.3103 rec=1.1646 | train/val/test=1.000/0.630/0.651 | c=0.998347
[Epoch 0066] loss=11.6540 cls=0.0153 smmd=-0.0044 ct=9.3117 rec=1.1657 | train/val/test=1.000/0.634/0.650 | c=0.998347
[Epoch 0067] loss=11.6431 cls=0.0145 smmd=-0.0101 ct=9.3070 rec=1.1658 | train/val/test=1.000/0.636/0.651 | c=0.998347
[Epoch 0068] loss=11.6410 cls=0.0141 smmd=-0.0089 ct=9.3054 rec=1.1652 | train/val/test=1.000/0.638/0.649 | c=0.998347
[Epoch 0069] loss=11.6377 cls=0.0138 smmd=-0.0094 ct=9.3033 rec=1.1650 | train/val/test=1.000/0.640/0.651 | c=0.998347
[Epoch 0070] loss=11.6353 cls=0.0135 smmd=-0.0105 ct=9.3021 rec=1.1651 | train/val/test=1.000/0.638/0.651 | c=0.998347
[Epoch 0071] loss=11.6361 cls=0.0134 smmd=-0.0093 ct=9.3027 rec=1.1647 | train/val/test=1.000/0.636/0.649 | c=0.998347
[Epoch 0072] loss=11.6308 cls=0.0133 smmd=-0.0102 ct=9.2988 rec=1.1645 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0073] loss=11.6310 cls=0.0131 smmd=-0.0092 ct=9.2985 rec=1.1643 | train/val/test=1.000/0.634/0.646 | c=0.998347
[Epoch 0074] loss=11.6297 cls=0.0141 smmd=-0.0096 ct=9.2954 rec=1.1649 | train/val/test=1.000/0.642/0.660 | c=0.998347
[Epoch 0075] loss=11.6406 cls=0.0150 smmd=-0.0109 ct=9.3025 rec=1.1670 | train/val/test=1.000/0.622/0.636 | c=0.998347
[Epoch 0076] loss=11.6755 cls=0.0197 smmd=0.0132 ct=9.3013 rec=1.1707 | train/val/test=1.000/0.654/0.661 | c=0.998347
[Epoch 0077] loss=11.7345 cls=0.0243 smmd=0.0321 ct=9.3216 rec=1.1782 | train/val/test=1.000/0.604/0.607 | c=0.998347
[Epoch 0078] loss=11.7826 cls=0.0312 smmd=0.0677 ct=9.3203 rec=1.1817 | train/val/test=1.000/0.646/0.663 | c=0.998347
[Epoch 0079] loss=11.7192 cls=0.0168 smmd=0.0313 ct=9.3253 rec=1.1729 | train/val/test=1.000/0.646/0.652 | c=0.998347
[Epoch 0080] loss=11.6145 cls=0.0081 smmd=-0.0057 ct=9.2931 rec=1.1595 | train/val/test=1.000/0.634/0.634 | c=0.998347
[Epoch 0081] loss=11.6776 cls=0.0107 smmd=0.0373 ct=9.3004 rec=1.1646 | train/val/test=1.000/0.646/0.661 | c=0.998347
[Epoch 0082] loss=11.6465 cls=0.0085 smmd=0.0093 ct=9.3059 rec=1.1614 | train/val/test=1.000/0.648/0.661 | c=0.998347
[Epoch 0083] loss=11.6232 cls=0.0076 smmd=-0.0039 ct=9.2997 rec=1.1599 | train/val/test=1.000/0.632/0.630 | c=0.998347
[Epoch 0084] loss=11.6682 cls=0.0110 smmd=0.0289 ct=9.2987 rec=1.1648 | train/val/test=1.000/0.646/0.653 | c=0.998347
[Epoch 0085] loss=11.6176 cls=0.0088 smmd=-0.0086 ct=9.2932 rec=1.1621 | train/val/test=1.000/0.648/0.662 | c=0.998347
[Epoch 0086] loss=11.6421 cls=0.0107 smmd=-0.0025 ct=9.3015 rec=1.1662 | train/val/test=1.000/0.618/0.628 | c=0.998347
[Epoch 0087] loss=11.6696 cls=0.0176 smmd=0.0131 ct=9.2966 rec=1.1712 | train/val/test=1.000/0.640/0.654 | c=0.998347
[Epoch 0088] loss=11.6294 cls=0.0135 smmd=-0.0116 ct=9.2910 rec=1.1683 | train/val/test=1.000/0.650/0.657 | c=0.998347
[Epoch 0089] loss=11.6292 cls=0.0144 smmd=-0.0169 ct=9.2941 rec=1.1688 | train/val/test=1.000/0.616/0.624 | c=0.998347
[Epoch 0090] loss=11.6706 cls=0.0232 smmd=0.0077 ct=9.2923 rec=1.1737 | train/val/test=1.000/0.646/0.666 | c=0.998347
[Epoch 0091] loss=11.6493 cls=0.0177 smmd=-0.0093 ct=9.2987 rec=1.1711 | train/val/test=1.000/0.638/0.649 | c=0.998347
[Epoch 0092] loss=11.6107 cls=0.0146 smmd=-0.0205 ct=9.2862 rec=1.1652 | train/val/test=1.000/0.630/0.639 | c=0.998347
[Epoch 0093] loss=11.6202 cls=0.0157 smmd=-0.0112 ct=9.2839 rec=1.1659 | train/val/test=1.000/0.648/0.662 | c=0.998347
[Epoch 0094] loss=11.6265 cls=0.0154 smmd=-0.0127 ct=9.2917 rec=1.1660 | train/val/test=1.000/0.638/0.648 | c=0.998347
[Epoch 0095] loss=11.6083 cls=0.0130 smmd=-0.0143 ct=9.2837 rec=1.1630 | train/val/test=1.000/0.638/0.643 | c=0.998347
[Epoch 0096] loss=11.6131 cls=0.0133 smmd=-0.0083 ct=9.2801 rec=1.1640 | train/val/test=1.000/0.648/0.659 | c=0.998347
[Epoch 0097] loss=11.6150 cls=0.0131 smmd=-0.0173 ct=9.2889 rec=1.1652 | train/val/test=1.000/0.640/0.645 | c=0.998347
[Epoch 0098] loss=11.6015 cls=0.0135 smmd=-0.0211 ct=9.2799 rec=1.1646 | train/val/test=1.000/0.640/0.643 | c=0.998347
[Epoch 0099] loss=11.6038 cls=0.0139 smmd=-0.0191 ct=9.2761 rec=1.1664 | train/val/test=1.000/0.644/0.655 | c=0.998347
=== Best @ epoch 76: val=0.6540, test=0.6610 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 - 2025-09-21 06:34:18:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1884 cls=1.9447 smmd=4.1912 ct=9.2747 rec=1.3889 | train/val/test=0.345/0.346/0.357 | c=0.998347
[Epoch 0001] loss=16.6874 cls=1.8939 smmd=2.7887 ct=9.2251 rec=1.3898 | train/val/test=0.724/0.462/0.446 | c=0.998347
[Epoch 0002] loss=15.1552 cls=1.7454 smmd=1.5215 ct=9.1108 rec=1.3887 | train/val/test=0.759/0.200/0.232 | c=0.998347
[Epoch 0003] loss=14.7596 cls=1.4730 smmd=1.4810 ct=9.0314 rec=1.3871 | train/val/test=0.897/0.424/0.404 | c=0.998347
[Epoch 0004] loss=14.6039 cls=1.1683 smmd=1.7202 ct=8.9533 rec=1.3810 | train/val/test=0.828/0.380/0.406 | c=0.998347
[Epoch 0005] loss=14.0541 cls=0.7825 smmd=1.6507 ct=8.8925 rec=1.3642 | train/val/test=0.966/0.436/0.439 | c=0.998347
[Epoch 0006] loss=13.4312 cls=0.5247 smmd=1.3767 ct=8.8458 rec=1.3420 | train/val/test=0.966/0.442/0.482 | c=0.998347
[Epoch 0007] loss=12.8587 cls=0.3224 smmd=1.0813 ct=8.8296 rec=1.3127 | train/val/test=0.966/0.452/0.482 | c=0.998347
[Epoch 0008] loss=12.5726 cls=0.2037 smmd=0.9748 ct=8.8243 rec=1.2849 | train/val/test=1.000/0.480/0.511 | c=0.998347
[Epoch 0009] loss=12.5357 cls=0.1146 smmd=1.0846 ct=8.8122 rec=1.2621 | train/val/test=1.000/0.492/0.529 | c=0.998347
[Epoch 0010] loss=12.4775 cls=0.0650 smmd=1.1150 ct=8.8079 rec=1.2448 | train/val/test=1.000/0.512/0.542 | c=0.998347
[Epoch 0011] loss=12.9990 cls=0.0395 smmd=1.0208 ct=9.4806 rec=1.2290 | train/val/test=1.000/0.506/0.532 | c=0.998347
[Epoch 0012] loss=12.7196 cls=0.0243 smmd=0.8483 ct=9.4169 rec=1.2151 | train/val/test=1.000/0.510/0.539 | c=0.998347
[Epoch 0013] loss=12.5887 cls=0.0138 smmd=0.7994 ct=9.3620 rec=1.2067 | train/val/test=1.000/0.536/0.552 | c=0.998347
[Epoch 0014] loss=12.5438 cls=0.0084 smmd=0.7876 ct=9.3451 rec=1.2014 | train/val/test=1.000/0.558/0.577 | c=0.998347
[Epoch 0015] loss=12.5020 cls=0.0071 smmd=0.7333 ct=9.3658 rec=1.1979 | train/val/test=1.000/0.564/0.592 | c=0.998347
[Epoch 0016] loss=12.4880 cls=0.0061 smmd=0.6863 ct=9.4030 rec=1.1963 | train/val/test=1.000/0.564/0.585 | c=0.998347
[Epoch 0017] loss=12.3651 cls=0.0049 smmd=0.5477 ct=9.4226 rec=1.1949 | train/val/test=1.000/0.572/0.598 | c=0.998347
[Epoch 0018] loss=12.2438 cls=0.0047 smmd=0.4361 ct=9.4134 rec=1.1948 | train/val/test=1.000/0.578/0.608 | c=0.998347
[Epoch 0019] loss=12.2196 cls=0.0054 smmd=0.4250 ct=9.3962 rec=1.1965 | train/val/test=1.000/0.580/0.615 | c=0.998347
[Epoch 0020] loss=12.2106 cls=0.0064 smmd=0.4160 ct=9.3910 rec=1.1986 | train/val/test=1.000/0.576/0.615 | c=0.998347
[Epoch 0021] loss=12.1591 cls=0.0081 smmd=0.3550 ct=9.3973 rec=1.1994 | train/val/test=1.000/0.578/0.614 | c=0.998347
[Epoch 0022] loss=12.0738 cls=0.0114 smmd=0.2604 ct=9.4034 rec=1.1993 | train/val/test=1.000/0.584/0.622 | c=0.998347
[Epoch 0023] loss=12.0597 cls=0.0151 smmd=0.2482 ct=9.4005 rec=1.1979 | train/val/test=1.000/0.576/0.621 | c=0.998347
[Epoch 0024] loss=12.0365 cls=0.0196 smmd=0.2337 ct=9.3917 rec=1.1958 | train/val/test=1.000/0.582/0.634 | c=0.998347
[Epoch 0025] loss=12.0071 cls=0.0240 smmd=0.2156 ct=9.3837 rec=1.1919 | train/val/test=1.000/0.586/0.630 | c=0.998347
[Epoch 0026] loss=11.9756 cls=0.0282 smmd=0.1889 ct=9.3821 rec=1.1882 | train/val/test=1.000/0.588/0.628 | c=0.998347
[Epoch 0027] loss=11.9477 cls=0.0301 smmd=0.1611 ct=9.3892 rec=1.1836 | train/val/test=1.000/0.586/0.640 | c=0.998347
[Epoch 0028] loss=11.9354 cls=0.0295 smmd=0.1513 ct=9.3945 rec=1.1800 | train/val/test=1.000/0.598/0.640 | c=0.998347
[Epoch 0029] loss=11.8989 cls=0.0264 smmd=0.1363 ct=9.3860 rec=1.1751 | train/val/test=1.000/0.604/0.642 | c=0.998347
[Epoch 0030] loss=11.8704 cls=0.0234 smmd=0.1320 ct=9.3719 rec=1.1715 | train/val/test=1.000/0.608/0.647 | c=0.998347
[Epoch 0031] loss=11.8394 cls=0.0208 smmd=0.1151 ct=9.3657 rec=1.1689 | train/val/test=1.000/0.610/0.645 | c=0.998347
[Epoch 0032] loss=11.8307 cls=0.0185 smmd=0.1155 ct=9.3627 rec=1.1670 | train/val/test=1.000/0.612/0.646 | c=0.998347
[Epoch 0033] loss=11.8083 cls=0.0164 smmd=0.0975 ct=9.3624 rec=1.1660 | train/val/test=1.000/0.608/0.641 | c=0.998347
[Epoch 0034] loss=11.7844 cls=0.0139 smmd=0.0754 ct=9.3638 rec=1.1657 | train/val/test=1.000/0.614/0.642 | c=0.998347
[Epoch 0035] loss=11.7754 cls=0.0123 smmd=0.0695 ct=9.3618 rec=1.1659 | train/val/test=1.000/0.612/0.641 | c=0.998347
[Epoch 0036] loss=11.7673 cls=0.0120 smmd=0.0652 ct=9.3575 rec=1.1663 | train/val/test=1.000/0.620/0.640 | c=0.998347
[Epoch 0037] loss=11.7546 cls=0.0113 smmd=0.0554 ct=9.3540 rec=1.1669 | train/val/test=1.000/0.608/0.637 | c=0.998347
[Epoch 0038] loss=11.7460 cls=0.0123 smmd=0.0463 ct=9.3519 rec=1.1678 | train/val/test=1.000/0.622/0.645 | c=0.998347
[Epoch 0039] loss=11.7436 cls=0.0113 smmd=0.0440 ct=9.3528 rec=1.1677 | train/val/test=1.000/0.608/0.634 | c=0.998347
[Epoch 0040] loss=11.7472 cls=0.0128 smmd=0.0460 ct=9.3517 rec=1.1683 | train/val/test=1.000/0.624/0.649 | c=0.998347
[Epoch 0041] loss=11.7508 cls=0.0132 smmd=0.0464 ct=9.3534 rec=1.1689 | train/val/test=1.000/0.596/0.626 | c=0.998347
[Epoch 0042] loss=11.7815 cls=0.0166 smmd=0.0623 ct=9.3577 rec=1.1724 | train/val/test=1.000/0.626/0.652 | c=0.998347
[Epoch 0043] loss=11.8006 cls=0.0192 smmd=0.0684 ct=9.3684 rec=1.1723 | train/val/test=1.000/0.604/0.628 | c=0.998347
[Epoch 0044] loss=11.7589 cls=0.0150 smmd=0.0530 ct=9.3563 rec=1.1673 | train/val/test=1.000/0.616/0.648 | c=0.998347
[Epoch 0045] loss=11.7023 cls=0.0109 smmd=0.0325 ct=9.3403 rec=1.1593 | train/val/test=1.000/0.620/0.650 | c=0.998347
[Epoch 0046] loss=11.7258 cls=0.0124 smmd=0.0483 ct=9.3419 rec=1.1616 | train/val/test=1.000/0.612/0.633 | c=0.998347
[Epoch 0047] loss=11.7282 cls=0.0126 smmd=0.0477 ct=9.3449 rec=1.1615 | train/val/test=1.000/0.616/0.649 | c=0.998347
[Epoch 0048] loss=11.6835 cls=0.0107 smmd=0.0173 ct=9.3388 rec=1.1583 | train/val/test=1.000/0.618/0.642 | c=0.998347
[Epoch 0049] loss=11.7058 cls=0.0118 smmd=0.0313 ct=9.3394 rec=1.1616 | train/val/test=1.000/0.606/0.636 | c=0.998347
[Epoch 0050] loss=11.7196 cls=0.0142 smmd=0.0349 ct=9.3417 rec=1.1644 | train/val/test=1.000/0.616/0.644 | c=0.998347
[Epoch 0051] loss=11.6900 cls=0.0123 smmd=0.0199 ct=9.3312 rec=1.1633 | train/val/test=1.000/0.620/0.647 | c=0.998347
[Epoch 0052] loss=11.6803 cls=0.0126 smmd=0.0095 ct=9.3302 rec=1.1640 | train/val/test=1.000/0.606/0.635 | c=0.998347
[Epoch 0053] loss=11.7054 cls=0.0157 smmd=0.0193 ct=9.3352 rec=1.1675 | train/val/test=1.000/0.622/0.647 | c=0.998347
[Epoch 0054] loss=11.7053 cls=0.0148 smmd=0.0162 ct=9.3345 rec=1.1699 | train/val/test=1.000/0.606/0.642 | c=0.998347
[Epoch 0055] loss=11.6920 cls=0.0139 smmd=0.0104 ct=9.3338 rec=1.1669 | train/val/test=1.000/0.616/0.643 | c=0.998347
[Epoch 0056] loss=11.6675 cls=0.0127 smmd=0.0046 ct=9.3220 rec=1.1641 | train/val/test=1.000/0.626/0.652 | c=0.998347
[Epoch 0057] loss=11.6699 cls=0.0130 smmd=0.0035 ct=9.3251 rec=1.1642 | train/val/test=1.000/0.612/0.642 | c=0.998347
[Epoch 0058] loss=11.6781 cls=0.0136 smmd=0.0112 ct=9.3246 rec=1.1643 | train/val/test=1.000/0.630/0.655 | c=0.998347
[Epoch 0059] loss=11.6614 cls=0.0128 smmd=0.0032 ct=9.3206 rec=1.1624 | train/val/test=1.000/0.622/0.648 | c=0.998347
[Epoch 0060] loss=11.6549 cls=0.0120 smmd=0.0032 ct=9.3174 rec=1.1612 | train/val/test=1.000/0.624/0.653 | c=0.998347
[Epoch 0061] loss=11.6567 cls=0.0128 smmd=-0.0001 ct=9.3194 rec=1.1623 | train/val/test=1.000/0.634/0.655 | c=0.998347
[Epoch 0062] loss=11.6559 cls=0.0130 smmd=0.0016 ct=9.3159 rec=1.1627 | train/val/test=1.000/0.630/0.652 | c=0.998347
[Epoch 0063] loss=11.6462 cls=0.0131 smmd=-0.0057 ct=9.3141 rec=1.1624 | train/val/test=1.000/0.632/0.650 | c=0.998347
[Epoch 0064] loss=11.6469 cls=0.0135 smmd=-0.0017 ct=9.3095 rec=1.1628 | train/val/test=1.000/0.632/0.650 | c=0.998347
[Epoch 0065] loss=11.6470 cls=0.0141 smmd=-0.0067 ct=9.3103 rec=1.1646 | train/val/test=1.000/0.630/0.651 | c=0.998347
[Epoch 0066] loss=11.6540 cls=0.0153 smmd=-0.0044 ct=9.3117 rec=1.1657 | train/val/test=1.000/0.634/0.650 | c=0.998347
[Epoch 0067] loss=11.6431 cls=0.0145 smmd=-0.0101 ct=9.3070 rec=1.1658 | train/val/test=1.000/0.636/0.651 | c=0.998347
[Epoch 0068] loss=11.6410 cls=0.0141 smmd=-0.0089 ct=9.3054 rec=1.1652 | train/val/test=1.000/0.638/0.649 | c=0.998347
[Epoch 0069] loss=11.6377 cls=0.0138 smmd=-0.0094 ct=9.3033 rec=1.1650 | train/val/test=1.000/0.640/0.651 | c=0.998347
[Epoch 0070] loss=11.6353 cls=0.0135 smmd=-0.0105 ct=9.3021 rec=1.1651 | train/val/test=1.000/0.638/0.651 | c=0.998347
[Epoch 0071] loss=11.6361 cls=0.0134 smmd=-0.0093 ct=9.3027 rec=1.1647 | train/val/test=1.000/0.636/0.649 | c=0.998347
[Epoch 0072] loss=11.6308 cls=0.0133 smmd=-0.0102 ct=9.2988 rec=1.1645 | train/val/test=1.000/0.636/0.655 | c=0.998347
[Epoch 0073] loss=11.6310 cls=0.0131 smmd=-0.0092 ct=9.2985 rec=1.1643 | train/val/test=1.000/0.634/0.646 | c=0.998347
[Epoch 0074] loss=11.6297 cls=0.0141 smmd=-0.0096 ct=9.2954 rec=1.1649 | train/val/test=1.000/0.642/0.660 | c=0.998347
[Epoch 0075] loss=11.6406 cls=0.0150 smmd=-0.0109 ct=9.3025 rec=1.1670 | train/val/test=1.000/0.622/0.636 | c=0.998347
[Epoch 0076] loss=11.6755 cls=0.0197 smmd=0.0132 ct=9.3013 rec=1.1707 | train/val/test=1.000/0.654/0.661 | c=0.998347
[Epoch 0077] loss=11.7345 cls=0.0243 smmd=0.0321 ct=9.3216 rec=1.1782 | train/val/test=1.000/0.604/0.607 | c=0.998347
[Epoch 0078] loss=11.7826 cls=0.0312 smmd=0.0677 ct=9.3203 rec=1.1817 | train/val/test=1.000/0.646/0.663 | c=0.998347
[Epoch 0079] loss=11.7192 cls=0.0168 smmd=0.0313 ct=9.3253 rec=1.1729 | train/val/test=1.000/0.646/0.652 | c=0.998347
[Epoch 0080] loss=11.6145 cls=0.0081 smmd=-0.0057 ct=9.2931 rec=1.1595 | train/val/test=1.000/0.634/0.634 | c=0.998347
[Epoch 0081] loss=11.6776 cls=0.0107 smmd=0.0373 ct=9.3004 rec=1.1646 | train/val/test=1.000/0.646/0.661 | c=0.998347
[Epoch 0082] loss=11.6465 cls=0.0085 smmd=0.0093 ct=9.3059 rec=1.1614 | train/val/test=1.000/0.648/0.661 | c=0.998347
[Epoch 0083] loss=11.6232 cls=0.0076 smmd=-0.0039 ct=9.2997 rec=1.1599 | train/val/test=1.000/0.632/0.630 | c=0.998347
[Epoch 0084] loss=11.6682 cls=0.0110 smmd=0.0289 ct=9.2987 rec=1.1648 | train/val/test=1.000/0.646/0.653 | c=0.998347
[Epoch 0085] loss=11.6176 cls=0.0088 smmd=-0.0086 ct=9.2932 rec=1.1621 | train/val/test=1.000/0.648/0.662 | c=0.998347
[Epoch 0086] loss=11.6421 cls=0.0107 smmd=-0.0025 ct=9.3015 rec=1.1662 | train/val/test=1.000/0.618/0.628 | c=0.998347
[Epoch 0087] loss=11.6696 cls=0.0176 smmd=0.0131 ct=9.2966 rec=1.1712 | train/val/test=1.000/0.640/0.654 | c=0.998347
[Epoch 0088] loss=11.6294 cls=0.0135 smmd=-0.0116 ct=9.2910 rec=1.1683 | train/val/test=1.000/0.650/0.657 | c=0.998347
[Epoch 0089] loss=11.6292 cls=0.0144 smmd=-0.0169 ct=9.2941 rec=1.1688 | train/val/test=1.000/0.616/0.624 | c=0.998347
[Epoch 0090] loss=11.6706 cls=0.0232 smmd=0.0077 ct=9.2923 rec=1.1737 | train/val/test=1.000/0.646/0.666 | c=0.998347
[Epoch 0091] loss=11.6493 cls=0.0177 smmd=-0.0093 ct=9.2987 rec=1.1711 | train/val/test=1.000/0.638/0.649 | c=0.998347
[Epoch 0092] loss=11.6107 cls=0.0146 smmd=-0.0205 ct=9.2862 rec=1.1652 | train/val/test=1.000/0.630/0.639 | c=0.998347
[Epoch 0093] loss=11.6202 cls=0.0157 smmd=-0.0112 ct=9.2839 rec=1.1659 | train/val/test=1.000/0.648/0.662 | c=0.998347
[Epoch 0094] loss=11.6265 cls=0.0154 smmd=-0.0127 ct=9.2917 rec=1.1660 | train/val/test=1.000/0.638/0.648 | c=0.998347
[Epoch 0095] loss=11.6083 cls=0.0130 smmd=-0.0143 ct=9.2837 rec=1.1630 | train/val/test=1.000/0.638/0.643 | c=0.998347
[Epoch 0096] loss=11.6131 cls=0.0133 smmd=-0.0083 ct=9.2801 rec=1.1640 | train/val/test=1.000/0.648/0.659 | c=0.998347
[Epoch 0097] loss=11.6150 cls=0.0131 smmd=-0.0173 ct=9.2889 rec=1.1652 | train/val/test=1.000/0.640/0.645 | c=0.998347
[Epoch 0098] loss=11.6015 cls=0.0135 smmd=-0.0211 ct=9.2799 rec=1.1646 | train/val/test=1.000/0.640/0.643 | c=0.998347
[Epoch 0099] loss=11.6038 cls=0.0139 smmd=-0.0191 ct=9.2761 rec=1.1664 | train/val/test=1.000/0.644/0.655 | c=0.998347
=== Best @ epoch 76: val=0.6540, test=0.6610 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 completed in 21.47 seconds.
==================================================
