Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 - 2025-09-21 05:47:44:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5643 cls=1.1025 smmd=5.6068 ct=11.2891 rec=1.4136 | train/val/test=0.385/0.388/0.413 | c=0.998347
[Epoch 0001] loss=22.7488 cls=1.0918 smmd=4.0944 ct=11.2601 rec=1.4136 | train/val/test=0.538/0.494/0.519 | c=0.998347
[Epoch 0002] loss=24.4059 cls=1.0813 smmd=4.7532 ct=11.2754 rec=1.4136 | train/val/test=0.538/0.540/0.570 | c=0.998347
[Epoch 0003] loss=23.0664 cls=1.0530 smmd=4.2634 ct=11.1747 rec=1.4135 | train/val/test=0.538/0.514/0.552 | c=0.998347
[Epoch 0004] loss=18.1588 cls=1.0041 smmd=2.3614 ct=11.0465 rec=1.4134 | train/val/test=0.692/0.530/0.564 | c=0.998347
[Epoch 0005] loss=19.8845 cls=0.9624 smmd=3.0837 ct=10.9887 rec=1.4107 | train/val/test=0.692/0.538/0.564 | c=0.998347
[Epoch 0006] loss=19.7783 cls=0.9029 smmd=3.0841 ct=10.9129 rec=1.4075 | train/val/test=0.692/0.538/0.564 | c=0.998347
[Epoch 0007] loss=17.5133 cls=0.8474 smmd=2.2024 ct=10.8819 rec=1.4035 | train/val/test=0.769/0.564/0.592 | c=0.998347
[Epoch 0008] loss=17.7467 cls=0.8167 smmd=2.0330 ct=11.5561 rec=1.3993 | train/val/test=0.923/0.646/0.646 | c=0.998347
[Epoch 0009] loss=18.8133 cls=0.7908 smmd=2.5218 ct=11.4142 rec=1.3982 | train/val/test=0.846/0.640/0.642 | c=0.998347
[Epoch 0010] loss=17.9219 cls=0.7717 smmd=2.1852 ct=11.3737 rec=1.3987 | train/val/test=0.769/0.580/0.597 | c=0.998347
[Epoch 0011] loss=16.8694 cls=0.7587 smmd=1.7115 ct=11.5118 rec=1.3991 | train/val/test=0.769/0.570/0.602 | c=0.998347
[Epoch 0012] loss=18.0538 cls=0.7487 smmd=2.1330 ct=11.6476 rec=1.3987 | train/val/test=0.846/0.648/0.660 | c=0.998347
[Epoch 0013] loss=16.8014 cls=0.7050 smmd=1.7137 ct=11.4670 rec=1.3951 | train/val/test=0.923/0.680/0.682 | c=0.998347
[Epoch 0014] loss=16.5633 cls=0.6556 smmd=1.6470 ct=11.4237 rec=1.3886 | train/val/test=0.923/0.694/0.688 | c=0.998347
[Epoch 0015] loss=16.4329 cls=0.6052 smmd=1.5796 ct=11.4892 rec=1.3844 | train/val/test=0.923/0.684/0.686 | c=0.998347
[Epoch 0016] loss=15.8170 cls=0.5695 smmd=1.3400 ct=11.4938 rec=1.3769 | train/val/test=0.923/0.672/0.685 | c=0.998347
[Epoch 0017] loss=15.5656 cls=0.5673 smmd=1.2396 ct=11.4954 rec=1.3751 | train/val/test=0.923/0.688/0.683 | c=0.998347
[Epoch 0018] loss=15.6289 cls=0.5378 smmd=1.2760 ct=11.4832 rec=1.3735 | train/val/test=0.923/0.714/0.702 | c=0.998347
[Epoch 0019] loss=15.2382 cls=0.5224 smmd=1.1340 ct=11.4540 rec=1.3759 | train/val/test=0.923/0.692/0.705 | c=0.998347
[Epoch 0020] loss=15.3849 cls=0.5229 smmd=1.1848 ct=11.4722 rec=1.3785 | train/val/test=0.923/0.714/0.716 | c=0.998347
[Epoch 0021] loss=15.2883 cls=0.5241 smmd=1.1327 ct=11.5040 rec=1.3814 | train/val/test=0.923/0.716/0.724 | c=0.998347
[Epoch 0022] loss=15.4423 cls=0.5049 smmd=1.2125 ct=11.4669 rec=1.3834 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0023] loss=15.0310 cls=0.4733 smmd=1.0520 ct=11.4747 rec=1.3793 | train/val/test=1.000/0.716/0.723 | c=0.998347
[Epoch 0024] loss=14.9082 cls=0.4280 smmd=1.0123 ct=11.4753 rec=1.3764 | train/val/test=1.000/0.726/0.743 | c=0.998347
[Epoch 0025] loss=14.5592 cls=0.3941 smmd=0.9013 ct=11.4247 rec=1.3683 | train/val/test=1.000/0.726/0.732 | c=0.998347
[Epoch 0026] loss=14.4339 cls=0.3605 smmd=0.8535 ct=11.4376 rec=1.3648 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0027] loss=14.3478 cls=0.3429 smmd=0.8085 ct=11.4740 rec=1.3624 | train/val/test=1.000/0.724/0.734 | c=0.998347
[Epoch 0028] loss=14.2571 cls=0.3301 smmd=0.7875 ct=11.4422 rec=1.3620 | train/val/test=1.000/0.724/0.731 | c=0.998347
[Epoch 0029] loss=14.2840 cls=0.3254 smmd=0.7983 ct=11.4445 rec=1.3624 | train/val/test=1.000/0.724/0.722 | c=0.998347
[Epoch 0030] loss=14.5821 cls=0.3191 smmd=0.9066 ct=11.4745 rec=1.3630 | train/val/test=1.000/0.726/0.720 | c=0.998347
[Epoch 0031] loss=14.4389 cls=0.3068 smmd=0.8533 ct=11.4723 rec=1.3599 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0032] loss=14.4249 cls=0.2819 smmd=0.8604 ct=11.4549 rec=1.3560 | train/val/test=1.000/0.724/0.717 | c=0.998347
[Epoch 0033] loss=14.1435 cls=0.2618 smmd=0.7467 ct=11.4718 rec=1.3480 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0034] loss=14.0373 cls=0.2351 smmd=0.7238 ct=11.4390 rec=1.3426 | train/val/test=1.000/0.728/0.720 | c=0.998347
[Epoch 0035] loss=13.8520 cls=0.2287 smmd=0.6507 ct=11.4415 rec=1.3389 | train/val/test=1.000/0.714/0.718 | c=0.998347
[Epoch 0036] loss=13.7960 cls=0.2173 smmd=0.6275 ct=11.4493 rec=1.3385 | train/val/test=1.000/0.730/0.719 | c=0.998347
[Epoch 0037] loss=13.8047 cls=0.2246 smmd=0.6302 ct=11.4468 rec=1.3404 | train/val/test=1.000/0.722/0.720 | c=0.998347
[Epoch 0038] loss=13.8069 cls=0.2283 smmd=0.6294 ct=11.4464 rec=1.3456 | train/val/test=1.000/0.738/0.722 | c=0.998347
[Epoch 0039] loss=14.0291 cls=0.2370 smmd=0.7046 ct=11.4753 rec=1.3472 | train/val/test=1.000/0.718/0.720 | c=0.998347
[Epoch 0040] loss=14.1122 cls=0.2306 smmd=0.7481 ct=11.4508 rec=1.3517 | train/val/test=1.000/0.736/0.730 | c=0.998347
[Epoch 0041] loss=14.1014 cls=0.2246 smmd=0.7333 ct=11.4841 rec=1.3438 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0042] loss=13.9174 cls=0.1976 smmd=0.6757 ct=11.4564 rec=1.3460 | train/val/test=1.000/0.714/0.733 | c=0.998347
[Epoch 0043] loss=13.7190 cls=0.1973 smmd=0.5802 ct=11.5019 rec=1.3361 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0044] loss=13.7836 cls=0.1548 smmd=0.6405 ct=11.4411 rec=1.3279 | train/val/test=1.000/0.720/0.720 | c=0.998347
[Epoch 0045] loss=13.4293 cls=0.1469 smmd=0.5005 ct=11.4445 rec=1.3201 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0046] loss=13.5239 cls=0.1450 smmd=0.5363 ct=11.4497 rec=1.3217 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0047] loss=13.7255 cls=0.1515 smmd=0.6114 ct=11.4574 rec=1.3276 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0048] loss=13.7649 cls=0.1617 smmd=0.6199 ct=11.4674 rec=1.3337 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0049] loss=13.9631 cls=0.1708 smmd=0.7011 ct=11.4569 rec=1.3361 | train/val/test=1.000/0.720/0.710 | c=0.998347
[Epoch 0050] loss=14.0077 cls=0.1703 smmd=0.7101 ct=11.4758 rec=1.3432 | train/val/test=1.000/0.706/0.711 | c=0.998347
[Epoch 0051] loss=13.6641 cls=0.1704 smmd=0.5726 ct=11.4812 rec=1.3324 | train/val/test=1.000/0.718/0.706 | c=0.998347
[Epoch 0052] loss=13.5723 cls=0.1458 smmd=0.5541 ct=11.4477 rec=1.3328 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0053] loss=13.4447 cls=0.1386 smmd=0.4942 ct=11.4798 rec=1.3202 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0054] loss=13.4636 cls=0.1163 smmd=0.5298 ct=11.4226 rec=1.3167 | train/val/test=1.000/0.710/0.707 | c=0.998347
[Epoch 0055] loss=13.3169 cls=0.1243 smmd=0.4547 ct=11.4593 rec=1.3175 | train/val/test=1.000/0.722/0.719 | c=0.998347
[Epoch 0056] loss=13.4999 cls=0.1277 smmd=0.5248 ct=11.4612 rec=1.3258 | train/val/test=1.000/0.730/0.715 | c=0.998347
[Epoch 0057] loss=13.7538 cls=0.1443 smmd=0.6211 ct=11.4647 rec=1.3287 | train/val/test=1.000/0.704/0.698 | c=0.998347
[Epoch 0058] loss=14.0502 cls=0.1503 smmd=0.7282 ct=11.4830 rec=1.3432 | train/val/test=1.000/0.714/0.711 | c=0.998347
[Epoch 0059] loss=13.9408 cls=0.1622 smmd=0.6723 ct=11.5126 rec=1.3330 | train/val/test=1.000/0.690/0.697 | c=0.998347
[Epoch 0060] loss=13.7372 cls=0.1405 smmd=0.6126 ct=11.4656 rec=1.3398 | train/val/test=1.000/0.686/0.690 | c=0.998347
[Epoch 0061] loss=13.3585 cls=0.1137 smmd=0.4641 ct=11.4853 rec=1.3124 | train/val/test=1.000/0.714/0.713 | c=0.998347
[Epoch 0062] loss=13.3728 cls=0.0806 smmd=0.4998 ct=11.4333 rec=1.2993 | train/val/test=1.000/0.716/0.714 | c=0.998347
[Epoch 0063] loss=13.1593 cls=0.0801 smmd=0.4192 ct=11.4212 rec=1.3002 | train/val/test=1.000/0.728/0.717 | c=0.998347
[Epoch 0064] loss=13.2277 cls=0.0928 smmd=0.4231 ct=11.4696 rec=1.3080 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0065] loss=13.5068 cls=0.1105 smmd=0.5335 ct=11.4569 rec=1.3220 | train/val/test=1.000/0.744/0.726 | c=0.998347
[Epoch 0066] loss=13.9388 cls=0.1395 smmd=0.6963 ct=11.4630 rec=1.3308 | train/val/test=1.000/0.656/0.663 | c=0.998347
[Epoch 0067] loss=14.2461 cls=0.1829 smmd=0.7759 ct=11.5296 rec=1.3704 | train/val/test=0.923/0.578/0.580 | c=0.998347
[Epoch 0068] loss=14.4591 cls=0.2925 smmd=0.8083 ct=11.6115 rec=1.3611 | train/val/test=0.923/0.630/0.648 | c=0.998347
[Epoch 0069] loss=14.1260 cls=0.2097 smmd=0.7190 ct=11.5311 rec=1.3851 | train/val/test=1.000/0.718/0.712 | c=0.998347
[Epoch 0070] loss=13.2213 cls=0.0554 smmd=0.4385 ct=11.4520 rec=1.2906 | train/val/test=1.000/0.666/0.687 | c=0.998347
[Epoch 0071] loss=13.7314 cls=0.1099 smmd=0.5975 ct=11.5192 rec=1.3269 | train/val/test=1.000/0.716/0.719 | c=0.998347
[Epoch 0072] loss=13.2910 cls=0.0584 smmd=0.4762 ct=11.4253 rec=1.2919 | train/val/test=1.000/0.720/0.722 | c=0.998347
[Epoch 0073] loss=13.4495 cls=0.0559 smmd=0.5171 ct=11.4822 rec=1.2930 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0074] loss=13.3001 cls=0.0788 smmd=0.4297 ct=11.5302 rec=1.3125 | train/val/test=1.000/0.722/0.723 | c=0.998347
[Epoch 0075] loss=13.8155 cls=0.0794 smmd=0.6769 ct=11.4253 rec=1.3164 | train/val/test=1.000/0.722/0.731 | c=0.998347
[Epoch 0076] loss=14.2480 cls=0.1020 smmd=0.7871 ct=11.5671 rec=1.3244 | train/val/test=1.000/0.650/0.640 | c=0.998347
[Epoch 0077] loss=14.3548 cls=0.1556 smmd=0.8422 ct=11.4957 rec=1.3516 | train/val/test=1.000/0.736/0.742 | c=0.998347
[Epoch 0078] loss=14.3328 cls=0.1512 smmd=0.8119 ct=11.5603 rec=1.3343 | train/val/test=1.000/0.700/0.678 | c=0.998347
[Epoch 0079] loss=13.4961 cls=0.1217 smmd=0.5237 ct=11.4638 rec=1.3244 | train/val/test=1.000/0.736/0.723 | c=0.998347
[Epoch 0080] loss=13.4494 cls=0.0875 smmd=0.5311 ct=11.4273 rec=1.3010 | train/val/test=1.000/0.784/0.769 | c=0.998347
[Epoch 0081] loss=13.4074 cls=0.0703 smmd=0.5150 ct=11.4344 rec=1.3004 | train/val/test=1.000/0.742/0.734 | c=0.998347
[Epoch 0082] loss=13.2316 cls=0.0786 smmd=0.4503 ct=11.4155 rec=1.3020 | train/val/test=1.000/0.738/0.731 | c=0.998347
[Epoch 0083] loss=13.3802 cls=0.0873 smmd=0.4853 ct=11.4685 rec=1.3096 | train/val/test=1.000/0.758/0.759 | c=0.998347
[Epoch 0084] loss=13.3572 cls=0.0984 smmd=0.4812 ct=11.4464 rec=1.3173 | train/val/test=1.000/0.756/0.752 | c=0.998347
[Epoch 0085] loss=13.8723 cls=0.1178 smmd=0.6642 ct=11.4907 rec=1.3248 | train/val/test=1.000/0.688/0.690 | c=0.998347
[Epoch 0086] loss=14.2851 cls=0.1398 smmd=0.8105 ct=11.5183 rec=1.3414 | train/val/test=1.000/0.694/0.725 | c=0.998347
[Epoch 0087] loss=14.0742 cls=0.1413 smmd=0.7112 ct=11.5568 rec=1.3377 | train/val/test=0.923/0.676/0.677 | c=0.998347
[Epoch 0088] loss=13.8619 cls=0.1488 smmd=0.6564 ct=11.4767 rec=1.3395 | train/val/test=1.000/0.726/0.721 | c=0.998347
[Epoch 0089] loss=13.3636 cls=0.0643 smmd=0.4777 ct=11.4846 rec=1.3052 | train/val/test=1.000/0.742/0.725 | c=0.998347
[Epoch 0090] loss=13.3025 cls=0.0458 smmd=0.4736 ct=11.4499 rec=1.2912 | train/val/test=1.000/0.710/0.721 | c=0.998347
[Epoch 0091] loss=13.2388 cls=0.0444 smmd=0.4577 ct=11.4249 rec=1.2952 | train/val/test=1.000/0.756/0.735 | c=0.998347
[Epoch 0092] loss=13.2680 cls=0.0533 smmd=0.4510 ct=11.4639 rec=1.3001 | train/val/test=1.000/0.742/0.723 | c=0.998347
[Epoch 0093] loss=13.2681 cls=0.0581 smmd=0.4487 ct=11.4637 rec=1.3071 | train/val/test=1.000/0.760/0.738 | c=0.998347
[Epoch 0094] loss=13.6678 cls=0.0847 smmd=0.6031 ct=11.4577 rec=1.3197 | train/val/test=1.000/0.756/0.722 | c=0.998347
[Epoch 0095] loss=14.1239 cls=0.1113 smmd=0.7621 ct=11.4959 rec=1.3344 | train/val/test=1.000/0.800/0.779 | c=0.998347
[Epoch 0096] loss=14.2318 cls=0.1151 smmd=0.8082 ct=11.4929 rec=1.3216 | train/val/test=1.000/0.684/0.660 | c=0.998347
[Epoch 0097] loss=13.6046 cls=0.1169 smmd=0.5627 ct=11.4660 rec=1.3468 | train/val/test=0.923/0.646/0.644 | c=0.998347
[Epoch 0098] loss=13.7018 cls=0.2154 smmd=0.5537 ct=11.5395 rec=1.3409 | train/val/test=1.000/0.718/0.700 | c=0.998347
[Epoch 0099] loss=13.5240 cls=0.0865 smmd=0.5503 ct=11.4476 rec=1.3147 | train/val/test=1.000/0.744/0.732 | c=0.998347
=== Best @ epoch 95: val=0.8000, test=0.7790 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 - 2025-09-21 05:47:44:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5643 cls=1.1025 smmd=5.6068 ct=11.2891 rec=1.4136 | train/val/test=0.385/0.388/0.413 | c=0.998347
[Epoch 0001] loss=22.7488 cls=1.0918 smmd=4.0944 ct=11.2601 rec=1.4136 | train/val/test=0.538/0.494/0.519 | c=0.998347
[Epoch 0002] loss=24.4059 cls=1.0813 smmd=4.7532 ct=11.2754 rec=1.4136 | train/val/test=0.538/0.540/0.570 | c=0.998347
[Epoch 0003] loss=23.0664 cls=1.0530 smmd=4.2634 ct=11.1747 rec=1.4135 | train/val/test=0.538/0.514/0.552 | c=0.998347
[Epoch 0004] loss=18.1588 cls=1.0041 smmd=2.3614 ct=11.0465 rec=1.4134 | train/val/test=0.692/0.530/0.564 | c=0.998347
[Epoch 0005] loss=19.8845 cls=0.9624 smmd=3.0837 ct=10.9887 rec=1.4107 | train/val/test=0.692/0.538/0.564 | c=0.998347
[Epoch 0006] loss=19.7783 cls=0.9029 smmd=3.0841 ct=10.9129 rec=1.4075 | train/val/test=0.692/0.538/0.564 | c=0.998347
[Epoch 0007] loss=17.5133 cls=0.8474 smmd=2.2024 ct=10.8819 rec=1.4035 | train/val/test=0.769/0.564/0.592 | c=0.998347
[Epoch 0008] loss=17.7467 cls=0.8167 smmd=2.0330 ct=11.5561 rec=1.3993 | train/val/test=0.923/0.646/0.646 | c=0.998347
[Epoch 0009] loss=18.8133 cls=0.7908 smmd=2.5218 ct=11.4142 rec=1.3982 | train/val/test=0.846/0.640/0.642 | c=0.998347
[Epoch 0010] loss=17.9219 cls=0.7717 smmd=2.1852 ct=11.3737 rec=1.3987 | train/val/test=0.769/0.580/0.597 | c=0.998347
[Epoch 0011] loss=16.8694 cls=0.7587 smmd=1.7115 ct=11.5118 rec=1.3991 | train/val/test=0.769/0.570/0.602 | c=0.998347
[Epoch 0012] loss=18.0538 cls=0.7487 smmd=2.1330 ct=11.6476 rec=1.3987 | train/val/test=0.846/0.648/0.660 | c=0.998347
[Epoch 0013] loss=16.8014 cls=0.7050 smmd=1.7137 ct=11.4670 rec=1.3951 | train/val/test=0.923/0.680/0.682 | c=0.998347
[Epoch 0014] loss=16.5633 cls=0.6556 smmd=1.6470 ct=11.4237 rec=1.3886 | train/val/test=0.923/0.694/0.688 | c=0.998347
[Epoch 0015] loss=16.4329 cls=0.6052 smmd=1.5796 ct=11.4892 rec=1.3844 | train/val/test=0.923/0.684/0.686 | c=0.998347
[Epoch 0016] loss=15.8170 cls=0.5695 smmd=1.3400 ct=11.4938 rec=1.3769 | train/val/test=0.923/0.672/0.685 | c=0.998347
[Epoch 0017] loss=15.5656 cls=0.5673 smmd=1.2396 ct=11.4954 rec=1.3751 | train/val/test=0.923/0.688/0.683 | c=0.998347
[Epoch 0018] loss=15.6289 cls=0.5378 smmd=1.2760 ct=11.4832 rec=1.3735 | train/val/test=0.923/0.714/0.702 | c=0.998347
[Epoch 0019] loss=15.2382 cls=0.5224 smmd=1.1340 ct=11.4540 rec=1.3759 | train/val/test=0.923/0.692/0.705 | c=0.998347
[Epoch 0020] loss=15.3849 cls=0.5229 smmd=1.1848 ct=11.4722 rec=1.3785 | train/val/test=0.923/0.714/0.716 | c=0.998347
[Epoch 0021] loss=15.2883 cls=0.5241 smmd=1.1327 ct=11.5040 rec=1.3814 | train/val/test=0.923/0.716/0.724 | c=0.998347
[Epoch 0022] loss=15.4423 cls=0.5049 smmd=1.2125 ct=11.4669 rec=1.3834 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0023] loss=15.0310 cls=0.4733 smmd=1.0520 ct=11.4747 rec=1.3793 | train/val/test=1.000/0.716/0.723 | c=0.998347
[Epoch 0024] loss=14.9082 cls=0.4280 smmd=1.0123 ct=11.4753 rec=1.3764 | train/val/test=1.000/0.726/0.743 | c=0.998347
[Epoch 0025] loss=14.5592 cls=0.3941 smmd=0.9013 ct=11.4247 rec=1.3683 | train/val/test=1.000/0.726/0.732 | c=0.998347
[Epoch 0026] loss=14.4339 cls=0.3605 smmd=0.8535 ct=11.4376 rec=1.3648 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0027] loss=14.3478 cls=0.3429 smmd=0.8085 ct=11.4740 rec=1.3624 | train/val/test=1.000/0.724/0.734 | c=0.998347
[Epoch 0028] loss=14.2571 cls=0.3301 smmd=0.7875 ct=11.4422 rec=1.3620 | train/val/test=1.000/0.724/0.731 | c=0.998347
[Epoch 0029] loss=14.2840 cls=0.3254 smmd=0.7983 ct=11.4445 rec=1.3624 | train/val/test=1.000/0.724/0.722 | c=0.998347
[Epoch 0030] loss=14.5821 cls=0.3191 smmd=0.9066 ct=11.4745 rec=1.3630 | train/val/test=1.000/0.726/0.720 | c=0.998347
[Epoch 0031] loss=14.4389 cls=0.3068 smmd=0.8533 ct=11.4723 rec=1.3599 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0032] loss=14.4249 cls=0.2819 smmd=0.8604 ct=11.4549 rec=1.3560 | train/val/test=1.000/0.724/0.717 | c=0.998347
[Epoch 0033] loss=14.1435 cls=0.2618 smmd=0.7467 ct=11.4718 rec=1.3480 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0034] loss=14.0373 cls=0.2351 smmd=0.7238 ct=11.4390 rec=1.3426 | train/val/test=1.000/0.728/0.720 | c=0.998347
[Epoch 0035] loss=13.8520 cls=0.2287 smmd=0.6507 ct=11.4415 rec=1.3389 | train/val/test=1.000/0.714/0.718 | c=0.998347
[Epoch 0036] loss=13.7960 cls=0.2173 smmd=0.6275 ct=11.4493 rec=1.3385 | train/val/test=1.000/0.730/0.719 | c=0.998347
[Epoch 0037] loss=13.8047 cls=0.2246 smmd=0.6302 ct=11.4468 rec=1.3404 | train/val/test=1.000/0.722/0.720 | c=0.998347
[Epoch 0038] loss=13.8069 cls=0.2283 smmd=0.6294 ct=11.4464 rec=1.3456 | train/val/test=1.000/0.738/0.722 | c=0.998347
[Epoch 0039] loss=14.0291 cls=0.2370 smmd=0.7046 ct=11.4753 rec=1.3472 | train/val/test=1.000/0.718/0.720 | c=0.998347
[Epoch 0040] loss=14.1122 cls=0.2306 smmd=0.7481 ct=11.4508 rec=1.3517 | train/val/test=1.000/0.736/0.730 | c=0.998347
[Epoch 0041] loss=14.1014 cls=0.2246 smmd=0.7333 ct=11.4841 rec=1.3438 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0042] loss=13.9174 cls=0.1976 smmd=0.6757 ct=11.4564 rec=1.3460 | train/val/test=1.000/0.714/0.733 | c=0.998347
[Epoch 0043] loss=13.7190 cls=0.1973 smmd=0.5802 ct=11.5019 rec=1.3361 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0044] loss=13.7836 cls=0.1548 smmd=0.6405 ct=11.4411 rec=1.3279 | train/val/test=1.000/0.720/0.720 | c=0.998347
[Epoch 0045] loss=13.4293 cls=0.1469 smmd=0.5005 ct=11.4445 rec=1.3201 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0046] loss=13.5239 cls=0.1450 smmd=0.5363 ct=11.4497 rec=1.3217 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0047] loss=13.7255 cls=0.1515 smmd=0.6114 ct=11.4574 rec=1.3276 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0048] loss=13.7649 cls=0.1617 smmd=0.6199 ct=11.4674 rec=1.3337 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0049] loss=13.9631 cls=0.1708 smmd=0.7011 ct=11.4569 rec=1.3361 | train/val/test=1.000/0.720/0.710 | c=0.998347
[Epoch 0050] loss=14.0077 cls=0.1703 smmd=0.7101 ct=11.4758 rec=1.3432 | train/val/test=1.000/0.706/0.711 | c=0.998347
[Epoch 0051] loss=13.6641 cls=0.1704 smmd=0.5726 ct=11.4812 rec=1.3324 | train/val/test=1.000/0.718/0.706 | c=0.998347
[Epoch 0052] loss=13.5723 cls=0.1458 smmd=0.5541 ct=11.4477 rec=1.3328 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0053] loss=13.4447 cls=0.1386 smmd=0.4942 ct=11.4798 rec=1.3202 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0054] loss=13.4636 cls=0.1163 smmd=0.5298 ct=11.4226 rec=1.3167 | train/val/test=1.000/0.710/0.707 | c=0.998347
[Epoch 0055] loss=13.3169 cls=0.1243 smmd=0.4547 ct=11.4593 rec=1.3175 | train/val/test=1.000/0.722/0.719 | c=0.998347
[Epoch 0056] loss=13.4999 cls=0.1277 smmd=0.5248 ct=11.4612 rec=1.3258 | train/val/test=1.000/0.730/0.715 | c=0.998347
[Epoch 0057] loss=13.7538 cls=0.1443 smmd=0.6211 ct=11.4647 rec=1.3287 | train/val/test=1.000/0.704/0.698 | c=0.998347
[Epoch 0058] loss=14.0502 cls=0.1503 smmd=0.7282 ct=11.4830 rec=1.3432 | train/val/test=1.000/0.714/0.711 | c=0.998347
[Epoch 0059] loss=13.9408 cls=0.1622 smmd=0.6723 ct=11.5126 rec=1.3330 | train/val/test=1.000/0.690/0.697 | c=0.998347
[Epoch 0060] loss=13.7372 cls=0.1405 smmd=0.6126 ct=11.4656 rec=1.3398 | train/val/test=1.000/0.686/0.690 | c=0.998347
[Epoch 0061] loss=13.3585 cls=0.1137 smmd=0.4641 ct=11.4853 rec=1.3124 | train/val/test=1.000/0.714/0.713 | c=0.998347
[Epoch 0062] loss=13.3728 cls=0.0806 smmd=0.4998 ct=11.4333 rec=1.2993 | train/val/test=1.000/0.716/0.714 | c=0.998347
[Epoch 0063] loss=13.1593 cls=0.0801 smmd=0.4192 ct=11.4212 rec=1.3002 | train/val/test=1.000/0.728/0.717 | c=0.998347
[Epoch 0064] loss=13.2277 cls=0.0928 smmd=0.4231 ct=11.4696 rec=1.3080 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0065] loss=13.5068 cls=0.1105 smmd=0.5335 ct=11.4569 rec=1.3220 | train/val/test=1.000/0.744/0.726 | c=0.998347
[Epoch 0066] loss=13.9388 cls=0.1395 smmd=0.6963 ct=11.4630 rec=1.3308 | train/val/test=1.000/0.656/0.663 | c=0.998347
[Epoch 0067] loss=14.2461 cls=0.1829 smmd=0.7759 ct=11.5296 rec=1.3704 | train/val/test=0.923/0.578/0.580 | c=0.998347
[Epoch 0068] loss=14.4591 cls=0.2925 smmd=0.8083 ct=11.6115 rec=1.3611 | train/val/test=0.923/0.630/0.648 | c=0.998347
[Epoch 0069] loss=14.1260 cls=0.2097 smmd=0.7190 ct=11.5311 rec=1.3851 | train/val/test=1.000/0.718/0.712 | c=0.998347
[Epoch 0070] loss=13.2213 cls=0.0554 smmd=0.4385 ct=11.4520 rec=1.2906 | train/val/test=1.000/0.666/0.687 | c=0.998347
[Epoch 0071] loss=13.7314 cls=0.1099 smmd=0.5975 ct=11.5192 rec=1.3269 | train/val/test=1.000/0.716/0.719 | c=0.998347
[Epoch 0072] loss=13.2910 cls=0.0584 smmd=0.4762 ct=11.4253 rec=1.2919 | train/val/test=1.000/0.720/0.722 | c=0.998347
[Epoch 0073] loss=13.4495 cls=0.0559 smmd=0.5171 ct=11.4822 rec=1.2930 | train/val/test=1.000/0.724/0.730 | c=0.998347
[Epoch 0074] loss=13.3001 cls=0.0788 smmd=0.4297 ct=11.5302 rec=1.3125 | train/val/test=1.000/0.722/0.723 | c=0.998347
[Epoch 0075] loss=13.8155 cls=0.0794 smmd=0.6769 ct=11.4253 rec=1.3164 | train/val/test=1.000/0.722/0.731 | c=0.998347
[Epoch 0076] loss=14.2480 cls=0.1020 smmd=0.7871 ct=11.5671 rec=1.3244 | train/val/test=1.000/0.650/0.640 | c=0.998347
[Epoch 0077] loss=14.3548 cls=0.1556 smmd=0.8422 ct=11.4957 rec=1.3516 | train/val/test=1.000/0.736/0.742 | c=0.998347
[Epoch 0078] loss=14.3328 cls=0.1512 smmd=0.8119 ct=11.5603 rec=1.3343 | train/val/test=1.000/0.700/0.678 | c=0.998347
[Epoch 0079] loss=13.4961 cls=0.1217 smmd=0.5237 ct=11.4638 rec=1.3244 | train/val/test=1.000/0.736/0.723 | c=0.998347
[Epoch 0080] loss=13.4494 cls=0.0875 smmd=0.5311 ct=11.4273 rec=1.3010 | train/val/test=1.000/0.784/0.769 | c=0.998347
[Epoch 0081] loss=13.4074 cls=0.0703 smmd=0.5150 ct=11.4344 rec=1.3004 | train/val/test=1.000/0.742/0.734 | c=0.998347
[Epoch 0082] loss=13.2316 cls=0.0786 smmd=0.4503 ct=11.4155 rec=1.3020 | train/val/test=1.000/0.738/0.731 | c=0.998347
[Epoch 0083] loss=13.3802 cls=0.0873 smmd=0.4853 ct=11.4685 rec=1.3096 | train/val/test=1.000/0.758/0.759 | c=0.998347
[Epoch 0084] loss=13.3572 cls=0.0984 smmd=0.4812 ct=11.4464 rec=1.3173 | train/val/test=1.000/0.756/0.752 | c=0.998347
[Epoch 0085] loss=13.8723 cls=0.1178 smmd=0.6642 ct=11.4907 rec=1.3248 | train/val/test=1.000/0.688/0.690 | c=0.998347
[Epoch 0086] loss=14.2851 cls=0.1398 smmd=0.8105 ct=11.5183 rec=1.3414 | train/val/test=1.000/0.694/0.725 | c=0.998347
[Epoch 0087] loss=14.0742 cls=0.1413 smmd=0.7112 ct=11.5568 rec=1.3377 | train/val/test=0.923/0.676/0.677 | c=0.998347
[Epoch 0088] loss=13.8619 cls=0.1488 smmd=0.6564 ct=11.4767 rec=1.3395 | train/val/test=1.000/0.726/0.721 | c=0.998347
[Epoch 0089] loss=13.3636 cls=0.0643 smmd=0.4777 ct=11.4846 rec=1.3052 | train/val/test=1.000/0.742/0.725 | c=0.998347
[Epoch 0090] loss=13.3025 cls=0.0458 smmd=0.4736 ct=11.4499 rec=1.2912 | train/val/test=1.000/0.710/0.721 | c=0.998347
[Epoch 0091] loss=13.2388 cls=0.0444 smmd=0.4577 ct=11.4249 rec=1.2952 | train/val/test=1.000/0.756/0.735 | c=0.998347
[Epoch 0092] loss=13.2680 cls=0.0533 smmd=0.4510 ct=11.4639 rec=1.3001 | train/val/test=1.000/0.742/0.723 | c=0.998347
[Epoch 0093] loss=13.2681 cls=0.0581 smmd=0.4487 ct=11.4637 rec=1.3071 | train/val/test=1.000/0.760/0.738 | c=0.998347
[Epoch 0094] loss=13.6678 cls=0.0847 smmd=0.6031 ct=11.4577 rec=1.3197 | train/val/test=1.000/0.756/0.722 | c=0.998347
[Epoch 0095] loss=14.1239 cls=0.1113 smmd=0.7621 ct=11.4959 rec=1.3344 | train/val/test=1.000/0.800/0.779 | c=0.998347
[Epoch 0096] loss=14.2318 cls=0.1151 smmd=0.8082 ct=11.4929 rec=1.3216 | train/val/test=1.000/0.684/0.660 | c=0.998347
[Epoch 0097] loss=13.6046 cls=0.1169 smmd=0.5627 ct=11.4660 rec=1.3468 | train/val/test=0.923/0.646/0.644 | c=0.998347
[Epoch 0098] loss=13.7018 cls=0.2154 smmd=0.5537 ct=11.5395 rec=1.3409 | train/val/test=1.000/0.718/0.700 | c=0.998347
[Epoch 0099] loss=13.5240 cls=0.0865 smmd=0.5503 ct=11.4476 rec=1.3147 | train/val/test=1.000/0.744/0.732 | c=0.998347
=== Best @ epoch 95: val=0.8000, test=0.7790 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-3 completed in 145.41 seconds.
==================================================
