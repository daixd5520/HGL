Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2 - 2025-09-21 04:27:23:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1962 cls=1.9429 smmd=4.2024 ct=9.2730 rec=1.3889 | train/val/test=0.276/0.094/0.120 | c=0.998437
[Epoch 0001] loss=16.6889 cls=1.8936 smmd=2.7951 ct=9.2209 rec=1.3896 | train/val/test=0.621/0.146/0.164 | c=0.998437
[Epoch 0002] loss=15.1319 cls=1.7817 smmd=1.4630 ct=9.1095 rec=1.3889 | train/val/test=1.000/0.600/0.576 | c=0.998437
[Epoch 0003] loss=14.8706 cls=1.5683 smmd=1.4616 ct=9.0639 rec=1.3884 | train/val/test=0.931/0.468/0.482 | c=0.998437
[Epoch 0004] loss=14.8075 cls=1.3075 smmd=1.7115 ct=9.0172 rec=1.3856 | train/val/test=0.966/0.522/0.529 | c=0.998437
[Epoch 0005] loss=14.2782 cls=0.9385 smmd=1.6398 ct=8.9398 rec=1.3801 | train/val/test=0.931/0.620/0.602 | c=0.998437
[Epoch 0006] loss=13.5435 cls=0.6488 smmd=1.3043 ct=8.8597 rec=1.3653 | train/val/test=0.966/0.552/0.602 | c=0.998437
[Epoch 0007] loss=12.9269 cls=0.4159 smmd=1.0089 ct=8.8216 rec=1.3402 | train/val/test=1.000/0.536/0.570 | c=0.998437
[Epoch 0008] loss=12.6512 cls=0.2335 smmd=0.9936 ct=8.8008 rec=1.3116 | train/val/test=1.000/0.572/0.599 | c=0.998437
[Epoch 0009] loss=12.5461 cls=0.1202 smmd=1.0753 ct=8.7920 rec=1.2794 | train/val/test=1.000/0.592/0.619 | c=0.998437
[Epoch 0010] loss=12.4406 cls=0.0625 smmd=1.0896 ct=8.7875 rec=1.2504 | train/val/test=1.000/0.600/0.633 | c=0.998437
[Epoch 0011] loss=12.2475 cls=0.0308 smmd=0.9714 ct=8.7865 rec=1.2294 | train/val/test=1.000/0.612/0.637 | c=0.998437
[Epoch 0012] loss=12.0561 cls=0.0157 smmd=0.8298 ct=8.7799 rec=1.2154 | train/val/test=1.000/0.614/0.625 | c=0.998437
[Epoch 0013] loss=11.9392 cls=0.0097 smmd=0.7394 ct=8.7768 rec=1.2066 | train/val/test=1.000/0.618/0.632 | c=0.998437
[Epoch 0014] loss=12.5863 cls=0.0066 smmd=0.6970 ct=9.4839 rec=1.1994 | train/val/test=1.000/0.642/0.636 | c=0.998437
[Epoch 0015] loss=12.5088 cls=0.0061 smmd=0.7249 ct=9.3883 rec=1.1947 | train/val/test=1.000/0.644/0.634 | c=0.998437
[Epoch 0016] loss=12.4709 cls=0.0063 smmd=0.7623 ct=9.3157 rec=1.1933 | train/val/test=1.000/0.648/0.636 | c=0.998437
[Epoch 0017] loss=12.3949 cls=0.0050 smmd=0.7047 ct=9.3046 rec=1.1903 | train/val/test=1.000/0.660/0.649 | c=0.998437
[Epoch 0018] loss=12.2711 cls=0.0038 smmd=0.5352 ct=9.3570 rec=1.1875 | train/val/test=1.000/0.662/0.651 | c=0.998437
[Epoch 0019] loss=12.2170 cls=0.0038 smmd=0.4196 ct=9.4192 rec=1.1872 | train/val/test=1.000/0.664/0.652 | c=0.998437
[Epoch 0020] loss=12.1728 cls=0.0050 smmd=0.3696 ct=9.4199 rec=1.1891 | train/val/test=1.000/0.662/0.650 | c=0.998437
[Epoch 0021] loss=12.1376 cls=0.0071 smmd=0.3576 ct=9.3893 rec=1.1918 | train/val/test=1.000/0.666/0.654 | c=0.998437
[Epoch 0022] loss=12.1057 cls=0.0090 smmd=0.3344 ct=9.3781 rec=1.1921 | train/val/test=1.000/0.662/0.655 | c=0.998437
[Epoch 0023] loss=12.0501 cls=0.0125 smmd=0.2660 ct=9.3861 rec=1.1927 | train/val/test=1.000/0.664/0.654 | c=0.998437
[Epoch 0024] loss=12.0139 cls=0.0182 smmd=0.2343 ct=9.3744 rec=1.1935 | train/val/test=1.000/0.662/0.663 | c=0.998437
[Epoch 0025] loss=12.0145 cls=0.0238 smmd=0.2416 ct=9.3648 rec=1.1921 | train/val/test=1.000/0.662/0.656 | c=0.998437
[Epoch 0026] loss=11.9826 cls=0.0297 smmd=0.2097 ct=9.3652 rec=1.1890 | train/val/test=1.000/0.650/0.660 | c=0.998437
[Epoch 0027] loss=11.9442 cls=0.0320 smmd=0.1696 ct=9.3714 rec=1.1857 | train/val/test=1.000/0.668/0.657 | c=0.998437
[Epoch 0028] loss=11.9086 cls=0.0330 smmd=0.1396 ct=9.3724 rec=1.1818 | train/val/test=1.000/0.660/0.667 | c=0.998437
[Epoch 0029] loss=11.8864 cls=0.0297 smmd=0.1342 ct=9.3689 rec=1.1768 | train/val/test=1.000/0.662/0.662 | c=0.998437
[Epoch 0030] loss=11.8493 cls=0.0260 smmd=0.1351 ct=9.3428 rec=1.1727 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0031] loss=11.8269 cls=0.0217 smmd=0.1335 ct=9.3339 rec=1.1689 | train/val/test=1.000/0.662/0.670 | c=0.998437
[Epoch 0032] loss=11.7926 cls=0.0180 smmd=0.1104 ct=9.3314 rec=1.1664 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0033] loss=11.7807 cls=0.0149 smmd=0.1053 ct=9.3309 rec=1.1648 | train/val/test=1.000/0.658/0.673 | c=0.998437
[Epoch 0034] loss=11.7536 cls=0.0127 smmd=0.0866 ct=9.3255 rec=1.1644 | train/val/test=1.000/0.658/0.673 | c=0.998437
[Epoch 0035] loss=11.7446 cls=0.0114 smmd=0.0836 ct=9.3202 rec=1.1647 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0036] loss=11.7345 cls=0.0110 smmd=0.0742 ct=9.3175 rec=1.1659 | train/val/test=1.000/0.660/0.674 | c=0.998437
[Epoch 0037] loss=11.7185 cls=0.0110 smmd=0.0624 ct=9.3104 rec=1.1674 | train/val/test=1.000/0.662/0.671 | c=0.998437
[Epoch 0038] loss=11.7090 cls=0.0112 smmd=0.0567 ct=9.3029 rec=1.1691 | train/val/test=1.000/0.656/0.676 | c=0.998437
[Epoch 0039] loss=11.7017 cls=0.0122 smmd=0.0464 ct=9.3025 rec=1.1703 | train/val/test=1.000/0.652/0.672 | c=0.998437
[Epoch 0040] loss=11.7040 cls=0.0130 smmd=0.0489 ct=9.3005 rec=1.1708 | train/val/test=1.000/0.660/0.677 | c=0.998437
[Epoch 0041] loss=11.6967 cls=0.0145 smmd=0.0356 ct=9.3040 rec=1.1713 | train/val/test=1.000/0.652/0.669 | c=0.998437
[Epoch 0042] loss=11.7031 cls=0.0156 smmd=0.0467 ct=9.2959 rec=1.1724 | train/val/test=1.000/0.658/0.686 | c=0.998437
[Epoch 0043] loss=11.7295 cls=0.0192 smmd=0.0470 ct=9.3147 rec=1.1743 | train/val/test=1.000/0.654/0.648 | c=0.998437
[Epoch 0044] loss=11.7717 cls=0.0235 smmd=0.0836 ct=9.3065 rec=1.1791 | train/val/test=1.000/0.662/0.686 | c=0.998437
[Epoch 0045] loss=11.8192 cls=0.0305 smmd=0.0889 ct=9.3399 rec=1.1800 | train/val/test=1.000/0.660/0.658 | c=0.998437
[Epoch 0046] loss=11.7123 cls=0.0170 smmd=0.0627 ct=9.2981 rec=1.1672 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0047] loss=11.6623 cls=0.0120 smmd=0.0387 ct=9.2877 rec=1.1619 | train/val/test=1.000/0.666/0.690 | c=0.998437
[Epoch 0048] loss=11.7205 cls=0.0121 smmd=0.0647 ct=9.3123 rec=1.1656 | train/val/test=1.000/0.664/0.674 | c=0.998437
[Epoch 0049] loss=11.6473 cls=0.0103 smmd=0.0331 ct=9.2849 rec=1.1595 | train/val/test=1.000/0.656/0.664 | c=0.998437
[Epoch 0050] loss=11.6903 cls=0.0124 smmd=0.0621 ct=9.2890 rec=1.1634 | train/val/test=1.000/0.668/0.689 | c=0.998437
[Epoch 0051] loss=11.6772 cls=0.0102 smmd=0.0384 ct=9.2998 rec=1.1644 | train/val/test=1.000/0.668/0.683 | c=0.998437
[Epoch 0052] loss=11.6469 cls=0.0101 smmd=0.0218 ct=9.2872 rec=1.1639 | train/val/test=1.000/0.658/0.663 | c=0.998437
[Epoch 0053] loss=11.6970 cls=0.0142 smmd=0.0549 ct=9.2866 rec=1.1706 | train/val/test=1.000/0.672/0.687 | c=0.998437
[Epoch 0054] loss=11.6737 cls=0.0134 smmd=0.0263 ct=9.2893 rec=1.1723 | train/val/test=1.000/0.662/0.681 | c=0.998437
[Epoch 0055] loss=11.6416 cls=0.0129 smmd=0.0096 ct=9.2804 rec=1.1694 | train/val/test=1.000/0.658/0.665 | c=0.998437
[Epoch 0056] loss=11.6798 cls=0.0164 smmd=0.0351 ct=9.2808 rec=1.1738 | train/val/test=1.000/0.682/0.687 | c=0.998437
[Epoch 0057] loss=11.6853 cls=0.0189 smmd=0.0218 ct=9.2920 rec=1.1764 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0058] loss=11.6406 cls=0.0153 smmd=0.0097 ct=9.2764 rec=1.1696 | train/val/test=1.000/0.660/0.668 | c=0.998437
[Epoch 0059] loss=11.6437 cls=0.0159 smmd=0.0217 ct=9.2673 rec=1.1694 | train/val/test=1.000/0.672/0.684 | c=0.998437
[Epoch 0060] loss=11.6623 cls=0.0187 smmd=0.0193 ct=9.2839 rec=1.1702 | train/val/test=1.000/0.658/0.669 | c=0.998437
[Epoch 0061] loss=11.6401 cls=0.0156 smmd=0.0112 ct=9.2776 rec=1.1679 | train/val/test=1.000/0.670/0.672 | c=0.998437
[Epoch 0062] loss=11.6285 cls=0.0152 smmd=0.0184 ct=9.2612 rec=1.1668 | train/val/test=1.000/0.666/0.678 | c=0.998437
[Epoch 0063] loss=11.6331 cls=0.0158 smmd=0.0090 ct=9.2720 rec=1.1681 | train/val/test=1.000/0.666/0.670 | c=0.998437
[Epoch 0064] loss=11.6291 cls=0.0158 smmd=0.0047 ct=9.2720 rec=1.1683 | train/val/test=1.000/0.666/0.674 | c=0.998437
[Epoch 0065] loss=11.6165 cls=0.0150 smmd=0.0068 ct=9.2581 rec=1.1683 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0066] loss=11.6188 cls=0.0151 smmd=-0.0008 ct=9.2648 rec=1.1699 | train/val/test=1.000/0.666/0.669 | c=0.998437
[Epoch 0067] loss=11.6228 cls=0.0165 smmd=-0.0008 ct=9.2664 rec=1.1704 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0068] loss=11.6114 cls=0.0150 smmd=0.0012 ct=9.2537 rec=1.1708 | train/val/test=1.000/0.664/0.673 | c=0.998437
[Epoch 0069] loss=11.6061 cls=0.0148 smmd=-0.0054 ct=9.2566 rec=1.1700 | train/val/test=1.000/0.662/0.671 | c=0.998437
[Epoch 0070] loss=11.6107 cls=0.0156 smmd=-0.0081 ct=9.2625 rec=1.1703 | train/val/test=1.000/0.660/0.676 | c=0.998437
[Epoch 0071] loss=11.6103 cls=0.0153 smmd=0.0010 ct=9.2518 rec=1.1711 | train/val/test=1.000/0.658/0.670 | c=0.998437
[Epoch 0072] loss=11.6002 cls=0.0150 smmd=-0.0029 ct=9.2495 rec=1.1693 | train/val/test=1.000/0.658/0.671 | c=0.998437
[Epoch 0073] loss=11.5957 cls=0.0151 smmd=-0.0106 ct=9.2536 rec=1.1688 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0074] loss=11.5955 cls=0.0152 smmd=-0.0064 ct=9.2486 rec=1.1691 | train/val/test=1.000/0.662/0.668 | c=0.998437
[Epoch 0075] loss=11.5939 cls=0.0156 smmd=-0.0089 ct=9.2486 rec=1.1693 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0076] loss=11.5935 cls=0.0159 smmd=-0.0096 ct=9.2486 rec=1.1693 | train/val/test=1.000/0.658/0.670 | c=0.998437
[Epoch 0077] loss=11.5856 cls=0.0159 smmd=-0.0111 ct=9.2417 rec=1.1695 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0078] loss=11.5851 cls=0.0161 smmd=-0.0139 ct=9.2442 rec=1.1694 | train/val/test=1.000/0.660/0.672 | c=0.998437
[Epoch 0079] loss=11.5823 cls=0.0164 smmd=-0.0177 ct=9.2438 rec=1.1699 | train/val/test=1.000/0.660/0.668 | c=0.998437
[Epoch 0080] loss=11.5852 cls=0.0169 smmd=-0.0117 ct=9.2388 rec=1.1706 | train/val/test=1.000/0.654/0.675 | c=0.998437
[Epoch 0081] loss=11.5922 cls=0.0169 smmd=-0.0108 ct=9.2442 rec=1.1710 | train/val/test=1.000/0.656/0.664 | c=0.998437
[Epoch 0082] loss=11.5877 cls=0.0164 smmd=-0.0119 ct=9.2425 rec=1.1703 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0083] loss=11.5797 cls=0.0160 smmd=-0.0141 ct=9.2379 rec=1.1699 | train/val/test=1.000/0.654/0.668 | c=0.998437
[Epoch 0084] loss=11.5783 cls=0.0150 smmd=-0.0122 ct=9.2377 rec=1.1689 | train/val/test=1.000/0.660/0.670 | c=0.998437
[Epoch 0085] loss=11.5749 cls=0.0153 smmd=-0.0147 ct=9.2375 rec=1.1684 | train/val/test=1.000/0.652/0.673 | c=0.998437
[Epoch 0086] loss=11.5760 cls=0.0145 smmd=-0.0119 ct=9.2351 rec=1.1691 | train/val/test=1.000/0.662/0.667 | c=0.998437
[Epoch 0087] loss=11.5783 cls=0.0163 smmd=-0.0171 ct=9.2400 rec=1.1696 | train/val/test=1.000/0.652/0.674 | c=0.998437
[Epoch 0088] loss=11.5787 cls=0.0152 smmd=-0.0137 ct=9.2362 rec=1.1705 | train/val/test=1.000/0.658/0.665 | c=0.998437
[Epoch 0089] loss=11.5786 cls=0.0160 smmd=-0.0087 ct=9.2312 rec=1.1701 | train/val/test=1.000/0.652/0.675 | c=0.998437
[Epoch 0090] loss=11.5754 cls=0.0153 smmd=-0.0173 ct=9.2376 rec=1.1699 | train/val/test=1.000/0.654/0.665 | c=0.998437
[Epoch 0091] loss=11.5685 cls=0.0151 smmd=-0.0175 ct=9.2318 rec=1.1695 | train/val/test=1.000/0.660/0.674 | c=0.998437
[Epoch 0092] loss=11.5715 cls=0.0158 smmd=-0.0159 ct=9.2329 rec=1.1693 | train/val/test=1.000/0.650/0.669 | c=0.998437
[Epoch 0093] loss=11.5683 cls=0.0150 smmd=-0.0203 ct=9.2342 rec=1.1697 | train/val/test=1.000/0.664/0.670 | c=0.998437
[Epoch 0094] loss=11.5729 cls=0.0165 smmd=-0.0119 ct=9.2290 rec=1.1697 | train/val/test=1.000/0.648/0.671 | c=0.998437
[Epoch 0095] loss=11.5771 cls=0.0151 smmd=-0.0148 ct=9.2357 rec=1.1706 | train/val/test=1.000/0.658/0.666 | c=0.998437
[Epoch 0096] loss=11.5826 cls=0.0176 smmd=-0.0140 ct=9.2374 rec=1.1708 | train/val/test=1.000/0.648/0.671 | c=0.998437
[Epoch 0097] loss=11.5933 cls=0.0155 smmd=-0.0023 ct=9.2354 rec=1.1724 | train/val/test=1.000/0.660/0.666 | c=0.998437
[Epoch 0098] loss=11.5896 cls=0.0180 smmd=-0.0094 ct=9.2390 rec=1.1710 | train/val/test=1.000/0.648/0.669 | c=0.998437
[Epoch 0099] loss=11.5782 cls=0.0143 smmd=-0.0099 ct=9.2338 rec=1.1700 | train/val/test=1.000/0.664/0.677 | c=0.998437
=== Best @ epoch 56: val=0.6820, test=0.6870 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2 - 2025-09-21 04:27:23:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1962 cls=1.9429 smmd=4.2024 ct=9.2730 rec=1.3889 | train/val/test=0.276/0.094/0.120 | c=0.998437
[Epoch 0001] loss=16.6889 cls=1.8936 smmd=2.7951 ct=9.2209 rec=1.3896 | train/val/test=0.621/0.146/0.164 | c=0.998437
[Epoch 0002] loss=15.1319 cls=1.7817 smmd=1.4630 ct=9.1095 rec=1.3889 | train/val/test=1.000/0.600/0.576 | c=0.998437
[Epoch 0003] loss=14.8706 cls=1.5683 smmd=1.4616 ct=9.0639 rec=1.3884 | train/val/test=0.931/0.468/0.482 | c=0.998437
[Epoch 0004] loss=14.8075 cls=1.3075 smmd=1.7115 ct=9.0172 rec=1.3856 | train/val/test=0.966/0.522/0.529 | c=0.998437
[Epoch 0005] loss=14.2782 cls=0.9385 smmd=1.6398 ct=8.9398 rec=1.3801 | train/val/test=0.931/0.620/0.602 | c=0.998437
[Epoch 0006] loss=13.5435 cls=0.6488 smmd=1.3043 ct=8.8597 rec=1.3653 | train/val/test=0.966/0.552/0.602 | c=0.998437
[Epoch 0007] loss=12.9269 cls=0.4159 smmd=1.0089 ct=8.8216 rec=1.3402 | train/val/test=1.000/0.536/0.570 | c=0.998437
[Epoch 0008] loss=12.6512 cls=0.2335 smmd=0.9936 ct=8.8008 rec=1.3116 | train/val/test=1.000/0.572/0.599 | c=0.998437
[Epoch 0009] loss=12.5461 cls=0.1202 smmd=1.0753 ct=8.7920 rec=1.2794 | train/val/test=1.000/0.592/0.619 | c=0.998437
[Epoch 0010] loss=12.4406 cls=0.0625 smmd=1.0896 ct=8.7875 rec=1.2504 | train/val/test=1.000/0.600/0.633 | c=0.998437
[Epoch 0011] loss=12.2475 cls=0.0308 smmd=0.9714 ct=8.7865 rec=1.2294 | train/val/test=1.000/0.612/0.637 | c=0.998437
[Epoch 0012] loss=12.0561 cls=0.0157 smmd=0.8298 ct=8.7799 rec=1.2154 | train/val/test=1.000/0.614/0.625 | c=0.998437
[Epoch 0013] loss=11.9392 cls=0.0097 smmd=0.7394 ct=8.7768 rec=1.2066 | train/val/test=1.000/0.618/0.632 | c=0.998437
[Epoch 0014] loss=12.5863 cls=0.0066 smmd=0.6970 ct=9.4839 rec=1.1994 | train/val/test=1.000/0.642/0.636 | c=0.998437
[Epoch 0015] loss=12.5088 cls=0.0061 smmd=0.7249 ct=9.3883 rec=1.1947 | train/val/test=1.000/0.644/0.634 | c=0.998437
[Epoch 0016] loss=12.4709 cls=0.0063 smmd=0.7623 ct=9.3157 rec=1.1933 | train/val/test=1.000/0.648/0.636 | c=0.998437
[Epoch 0017] loss=12.3949 cls=0.0050 smmd=0.7047 ct=9.3046 rec=1.1903 | train/val/test=1.000/0.660/0.649 | c=0.998437
[Epoch 0018] loss=12.2711 cls=0.0038 smmd=0.5352 ct=9.3570 rec=1.1875 | train/val/test=1.000/0.662/0.651 | c=0.998437
[Epoch 0019] loss=12.2170 cls=0.0038 smmd=0.4196 ct=9.4192 rec=1.1872 | train/val/test=1.000/0.664/0.652 | c=0.998437
[Epoch 0020] loss=12.1728 cls=0.0050 smmd=0.3696 ct=9.4199 rec=1.1891 | train/val/test=1.000/0.662/0.650 | c=0.998437
[Epoch 0021] loss=12.1376 cls=0.0071 smmd=0.3576 ct=9.3893 rec=1.1918 | train/val/test=1.000/0.666/0.654 | c=0.998437
[Epoch 0022] loss=12.1057 cls=0.0090 smmd=0.3344 ct=9.3781 rec=1.1921 | train/val/test=1.000/0.662/0.655 | c=0.998437
[Epoch 0023] loss=12.0501 cls=0.0125 smmd=0.2660 ct=9.3861 rec=1.1927 | train/val/test=1.000/0.664/0.654 | c=0.998437
[Epoch 0024] loss=12.0139 cls=0.0182 smmd=0.2343 ct=9.3744 rec=1.1935 | train/val/test=1.000/0.662/0.663 | c=0.998437
[Epoch 0025] loss=12.0145 cls=0.0238 smmd=0.2416 ct=9.3648 rec=1.1921 | train/val/test=1.000/0.662/0.656 | c=0.998437
[Epoch 0026] loss=11.9826 cls=0.0297 smmd=0.2097 ct=9.3652 rec=1.1890 | train/val/test=1.000/0.650/0.660 | c=0.998437
[Epoch 0027] loss=11.9442 cls=0.0320 smmd=0.1696 ct=9.3714 rec=1.1857 | train/val/test=1.000/0.668/0.657 | c=0.998437
[Epoch 0028] loss=11.9086 cls=0.0330 smmd=0.1396 ct=9.3724 rec=1.1818 | train/val/test=1.000/0.660/0.667 | c=0.998437
[Epoch 0029] loss=11.8864 cls=0.0297 smmd=0.1342 ct=9.3689 rec=1.1768 | train/val/test=1.000/0.662/0.662 | c=0.998437
[Epoch 0030] loss=11.8493 cls=0.0260 smmd=0.1351 ct=9.3428 rec=1.1727 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0031] loss=11.8269 cls=0.0217 smmd=0.1335 ct=9.3339 rec=1.1689 | train/val/test=1.000/0.662/0.670 | c=0.998437
[Epoch 0032] loss=11.7926 cls=0.0180 smmd=0.1104 ct=9.3314 rec=1.1664 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0033] loss=11.7807 cls=0.0149 smmd=0.1053 ct=9.3309 rec=1.1648 | train/val/test=1.000/0.658/0.673 | c=0.998437
[Epoch 0034] loss=11.7536 cls=0.0127 smmd=0.0866 ct=9.3255 rec=1.1644 | train/val/test=1.000/0.658/0.673 | c=0.998437
[Epoch 0035] loss=11.7446 cls=0.0114 smmd=0.0836 ct=9.3202 rec=1.1647 | train/val/test=1.000/0.660/0.673 | c=0.998437
[Epoch 0036] loss=11.7345 cls=0.0110 smmd=0.0742 ct=9.3175 rec=1.1659 | train/val/test=1.000/0.660/0.674 | c=0.998437
[Epoch 0037] loss=11.7185 cls=0.0110 smmd=0.0624 ct=9.3104 rec=1.1674 | train/val/test=1.000/0.662/0.671 | c=0.998437
[Epoch 0038] loss=11.7090 cls=0.0112 smmd=0.0567 ct=9.3029 rec=1.1691 | train/val/test=1.000/0.656/0.676 | c=0.998437
[Epoch 0039] loss=11.7017 cls=0.0122 smmd=0.0464 ct=9.3025 rec=1.1703 | train/val/test=1.000/0.652/0.672 | c=0.998437
[Epoch 0040] loss=11.7040 cls=0.0130 smmd=0.0489 ct=9.3005 rec=1.1708 | train/val/test=1.000/0.660/0.677 | c=0.998437
[Epoch 0041] loss=11.6967 cls=0.0145 smmd=0.0356 ct=9.3040 rec=1.1713 | train/val/test=1.000/0.652/0.669 | c=0.998437
[Epoch 0042] loss=11.7031 cls=0.0156 smmd=0.0467 ct=9.2959 rec=1.1724 | train/val/test=1.000/0.658/0.686 | c=0.998437
[Epoch 0043] loss=11.7295 cls=0.0192 smmd=0.0470 ct=9.3147 rec=1.1743 | train/val/test=1.000/0.654/0.648 | c=0.998437
[Epoch 0044] loss=11.7717 cls=0.0235 smmd=0.0836 ct=9.3065 rec=1.1791 | train/val/test=1.000/0.662/0.686 | c=0.998437
[Epoch 0045] loss=11.8192 cls=0.0305 smmd=0.0889 ct=9.3399 rec=1.1800 | train/val/test=1.000/0.660/0.658 | c=0.998437
[Epoch 0046] loss=11.7123 cls=0.0170 smmd=0.0627 ct=9.2981 rec=1.1672 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0047] loss=11.6623 cls=0.0120 smmd=0.0387 ct=9.2877 rec=1.1619 | train/val/test=1.000/0.666/0.690 | c=0.998437
[Epoch 0048] loss=11.7205 cls=0.0121 smmd=0.0647 ct=9.3123 rec=1.1656 | train/val/test=1.000/0.664/0.674 | c=0.998437
[Epoch 0049] loss=11.6473 cls=0.0103 smmd=0.0331 ct=9.2849 rec=1.1595 | train/val/test=1.000/0.656/0.664 | c=0.998437
[Epoch 0050] loss=11.6903 cls=0.0124 smmd=0.0621 ct=9.2890 rec=1.1634 | train/val/test=1.000/0.668/0.689 | c=0.998437
[Epoch 0051] loss=11.6772 cls=0.0102 smmd=0.0384 ct=9.2998 rec=1.1644 | train/val/test=1.000/0.668/0.683 | c=0.998437
[Epoch 0052] loss=11.6469 cls=0.0101 smmd=0.0218 ct=9.2872 rec=1.1639 | train/val/test=1.000/0.658/0.663 | c=0.998437
[Epoch 0053] loss=11.6970 cls=0.0142 smmd=0.0549 ct=9.2866 rec=1.1706 | train/val/test=1.000/0.672/0.687 | c=0.998437
[Epoch 0054] loss=11.6737 cls=0.0134 smmd=0.0263 ct=9.2893 rec=1.1723 | train/val/test=1.000/0.662/0.681 | c=0.998437
[Epoch 0055] loss=11.6416 cls=0.0129 smmd=0.0096 ct=9.2804 rec=1.1694 | train/val/test=1.000/0.658/0.665 | c=0.998437
[Epoch 0056] loss=11.6798 cls=0.0164 smmd=0.0351 ct=9.2808 rec=1.1738 | train/val/test=1.000/0.682/0.687 | c=0.998437
[Epoch 0057] loss=11.6853 cls=0.0189 smmd=0.0218 ct=9.2920 rec=1.1764 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0058] loss=11.6406 cls=0.0153 smmd=0.0097 ct=9.2764 rec=1.1696 | train/val/test=1.000/0.660/0.668 | c=0.998437
[Epoch 0059] loss=11.6437 cls=0.0159 smmd=0.0217 ct=9.2673 rec=1.1694 | train/val/test=1.000/0.672/0.684 | c=0.998437
[Epoch 0060] loss=11.6623 cls=0.0187 smmd=0.0193 ct=9.2839 rec=1.1702 | train/val/test=1.000/0.658/0.669 | c=0.998437
[Epoch 0061] loss=11.6401 cls=0.0156 smmd=0.0112 ct=9.2776 rec=1.1679 | train/val/test=1.000/0.670/0.672 | c=0.998437
[Epoch 0062] loss=11.6285 cls=0.0152 smmd=0.0184 ct=9.2612 rec=1.1668 | train/val/test=1.000/0.666/0.678 | c=0.998437
[Epoch 0063] loss=11.6331 cls=0.0158 smmd=0.0090 ct=9.2720 rec=1.1681 | train/val/test=1.000/0.666/0.670 | c=0.998437
[Epoch 0064] loss=11.6291 cls=0.0158 smmd=0.0047 ct=9.2720 rec=1.1683 | train/val/test=1.000/0.666/0.674 | c=0.998437
[Epoch 0065] loss=11.6165 cls=0.0150 smmd=0.0068 ct=9.2581 rec=1.1683 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0066] loss=11.6188 cls=0.0151 smmd=-0.0008 ct=9.2648 rec=1.1699 | train/val/test=1.000/0.666/0.669 | c=0.998437
[Epoch 0067] loss=11.6228 cls=0.0165 smmd=-0.0008 ct=9.2664 rec=1.1704 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0068] loss=11.6114 cls=0.0150 smmd=0.0012 ct=9.2537 rec=1.1708 | train/val/test=1.000/0.664/0.673 | c=0.998437
[Epoch 0069] loss=11.6061 cls=0.0148 smmd=-0.0054 ct=9.2566 rec=1.1700 | train/val/test=1.000/0.662/0.671 | c=0.998437
[Epoch 0070] loss=11.6107 cls=0.0156 smmd=-0.0081 ct=9.2625 rec=1.1703 | train/val/test=1.000/0.660/0.676 | c=0.998437
[Epoch 0071] loss=11.6103 cls=0.0153 smmd=0.0010 ct=9.2518 rec=1.1711 | train/val/test=1.000/0.658/0.670 | c=0.998437
[Epoch 0072] loss=11.6002 cls=0.0150 smmd=-0.0029 ct=9.2495 rec=1.1693 | train/val/test=1.000/0.658/0.671 | c=0.998437
[Epoch 0073] loss=11.5957 cls=0.0151 smmd=-0.0106 ct=9.2536 rec=1.1688 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0074] loss=11.5955 cls=0.0152 smmd=-0.0064 ct=9.2486 rec=1.1691 | train/val/test=1.000/0.662/0.668 | c=0.998437
[Epoch 0075] loss=11.5939 cls=0.0156 smmd=-0.0089 ct=9.2486 rec=1.1693 | train/val/test=1.000/0.658/0.674 | c=0.998437
[Epoch 0076] loss=11.5935 cls=0.0159 smmd=-0.0096 ct=9.2486 rec=1.1693 | train/val/test=1.000/0.658/0.670 | c=0.998437
[Epoch 0077] loss=11.5856 cls=0.0159 smmd=-0.0111 ct=9.2417 rec=1.1695 | train/val/test=1.000/0.658/0.672 | c=0.998437
[Epoch 0078] loss=11.5851 cls=0.0161 smmd=-0.0139 ct=9.2442 rec=1.1694 | train/val/test=1.000/0.660/0.672 | c=0.998437
[Epoch 0079] loss=11.5823 cls=0.0164 smmd=-0.0177 ct=9.2438 rec=1.1699 | train/val/test=1.000/0.660/0.668 | c=0.998437
[Epoch 0080] loss=11.5852 cls=0.0169 smmd=-0.0117 ct=9.2388 rec=1.1706 | train/val/test=1.000/0.654/0.675 | c=0.998437
[Epoch 0081] loss=11.5922 cls=0.0169 smmd=-0.0108 ct=9.2442 rec=1.1710 | train/val/test=1.000/0.656/0.664 | c=0.998437
[Epoch 0082] loss=11.5877 cls=0.0164 smmd=-0.0119 ct=9.2425 rec=1.1703 | train/val/test=1.000/0.662/0.675 | c=0.998437
[Epoch 0083] loss=11.5797 cls=0.0160 smmd=-0.0141 ct=9.2379 rec=1.1699 | train/val/test=1.000/0.654/0.668 | c=0.998437
[Epoch 0084] loss=11.5783 cls=0.0150 smmd=-0.0122 ct=9.2377 rec=1.1689 | train/val/test=1.000/0.660/0.670 | c=0.998437
[Epoch 0085] loss=11.5749 cls=0.0153 smmd=-0.0147 ct=9.2375 rec=1.1684 | train/val/test=1.000/0.652/0.673 | c=0.998437
[Epoch 0086] loss=11.5760 cls=0.0145 smmd=-0.0119 ct=9.2351 rec=1.1691 | train/val/test=1.000/0.662/0.667 | c=0.998437
[Epoch 0087] loss=11.5783 cls=0.0163 smmd=-0.0171 ct=9.2400 rec=1.1696 | train/val/test=1.000/0.652/0.674 | c=0.998437
[Epoch 0088] loss=11.5787 cls=0.0152 smmd=-0.0137 ct=9.2362 rec=1.1705 | train/val/test=1.000/0.658/0.665 | c=0.998437
[Epoch 0089] loss=11.5786 cls=0.0160 smmd=-0.0087 ct=9.2312 rec=1.1701 | train/val/test=1.000/0.652/0.675 | c=0.998437
[Epoch 0090] loss=11.5754 cls=0.0153 smmd=-0.0173 ct=9.2376 rec=1.1699 | train/val/test=1.000/0.654/0.665 | c=0.998437
[Epoch 0091] loss=11.5685 cls=0.0151 smmd=-0.0175 ct=9.2318 rec=1.1695 | train/val/test=1.000/0.660/0.674 | c=0.998437
[Epoch 0092] loss=11.5715 cls=0.0158 smmd=-0.0159 ct=9.2329 rec=1.1693 | train/val/test=1.000/0.650/0.669 | c=0.998437
[Epoch 0093] loss=11.5683 cls=0.0150 smmd=-0.0203 ct=9.2342 rec=1.1697 | train/val/test=1.000/0.664/0.670 | c=0.998437
[Epoch 0094] loss=11.5729 cls=0.0165 smmd=-0.0119 ct=9.2290 rec=1.1697 | train/val/test=1.000/0.648/0.671 | c=0.998437
[Epoch 0095] loss=11.5771 cls=0.0151 smmd=-0.0148 ct=9.2357 rec=1.1706 | train/val/test=1.000/0.658/0.666 | c=0.998437
[Epoch 0096] loss=11.5826 cls=0.0176 smmd=-0.0140 ct=9.2374 rec=1.1708 | train/val/test=1.000/0.648/0.671 | c=0.998437
[Epoch 0097] loss=11.5933 cls=0.0155 smmd=-0.0023 ct=9.2354 rec=1.1724 | train/val/test=1.000/0.660/0.666 | c=0.998437
[Epoch 0098] loss=11.5896 cls=0.0180 smmd=-0.0094 ct=9.2390 rec=1.1710 | train/val/test=1.000/0.648/0.669 | c=0.998437
[Epoch 0099] loss=11.5782 cls=0.0143 smmd=-0.0099 ct=9.2338 rec=1.1700 | train/val/test=1.000/0.664/0.677 | c=0.998437
=== Best @ epoch 56: val=0.6820, test=0.6870 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-2 completed in 21.53 seconds.
==================================================
