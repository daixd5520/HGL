Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4 - 2025-09-21 04:28:11:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.3107 cls=1.9427 smmd=4.3226 ct=9.2676 rec=1.3889 | train/val/test=0.241/0.104/0.121 | c=0.998437
[Epoch 0001] loss=16.7970 cls=1.8885 smmd=2.9246 ct=9.2046 rec=1.3897 | train/val/test=0.517/0.248/0.265 | c=0.998437
[Epoch 0002] loss=15.1814 cls=1.7539 smmd=1.5819 ct=9.0678 rec=1.3889 | train/val/test=0.759/0.262/0.282 | c=0.998437
[Epoch 0003] loss=14.7561 cls=1.5075 smmd=1.4131 ct=9.0595 rec=1.3880 | train/val/test=0.966/0.518/0.536 | c=0.998437
[Epoch 0004] loss=14.6675 cls=1.1713 smmd=1.7664 ct=8.9635 rec=1.3832 | train/val/test=1.000/0.486/0.521 | c=0.998437
[Epoch 0005] loss=14.0828 cls=0.7795 smmd=1.6745 ct=8.8861 rec=1.3714 | train/val/test=0.966/0.488/0.526 | c=0.998437
[Epoch 0006] loss=13.4558 cls=0.4880 smmd=1.3997 ct=8.8682 rec=1.3500 | train/val/test=0.966/0.564/0.577 | c=0.998437
[Epoch 0007] loss=12.8179 cls=0.2897 smmd=1.0592 ct=8.8263 rec=1.3213 | train/val/test=0.966/0.582/0.592 | c=0.998437
[Epoch 0008] loss=12.5716 cls=0.1681 smmd=1.0037 ct=8.8173 rec=1.2912 | train/val/test=1.000/0.592/0.592 | c=0.998437
[Epoch 0009] loss=12.5469 cls=0.0911 smmd=1.1127 ct=8.8112 rec=1.2659 | train/val/test=1.000/0.600/0.606 | c=0.998437
[Epoch 0010] loss=12.4980 cls=0.0467 smmd=1.1569 ct=8.8031 rec=1.2456 | train/val/test=1.000/0.614/0.630 | c=0.998437
[Epoch 0011] loss=12.3444 cls=0.0238 smmd=1.0638 ct=8.7983 rec=1.2292 | train/val/test=1.000/0.634/0.648 | c=0.998437
[Epoch 0012] loss=12.1171 cls=0.0119 smmd=0.8839 ct=8.7898 rec=1.2157 | train/val/test=1.000/0.646/0.655 | c=0.998437
[Epoch 0013] loss=11.9483 cls=0.0066 smmd=0.7462 ct=8.7842 rec=1.2057 | train/val/test=1.000/0.660/0.669 | c=0.998437
[Epoch 0014] loss=11.8772 cls=0.0043 smmd=0.6931 ct=8.7836 rec=1.1981 | train/val/test=1.000/0.684/0.692 | c=0.998437
[Epoch 0015] loss=12.5741 cls=0.0032 smmd=0.6992 ct=9.4874 rec=1.1922 | train/val/test=1.000/0.676/0.687 | c=0.998437
[Epoch 0016] loss=12.5067 cls=0.0030 smmd=0.7103 ct=9.4138 rec=1.1898 | train/val/test=1.000/0.684/0.688 | c=0.998437
[Epoch 0017] loss=12.3905 cls=0.0032 smmd=0.6559 ct=9.3536 rec=1.1889 | train/val/test=1.000/0.692/0.700 | c=0.998437
[Epoch 0018] loss=12.2911 cls=0.0036 smmd=0.5624 ct=9.3512 rec=1.1869 | train/val/test=1.000/0.696/0.705 | c=0.998437
[Epoch 0019] loss=12.2657 cls=0.0046 smmd=0.4896 ct=9.3976 rec=1.1869 | train/val/test=1.000/0.688/0.703 | c=0.998437
[Epoch 0020] loss=12.2463 cls=0.0067 smmd=0.4185 ct=9.4414 rec=1.1898 | train/val/test=1.000/0.694/0.700 | c=0.998437
[Epoch 0021] loss=12.1764 cls=0.0101 smmd=0.3336 ct=9.4494 rec=1.1917 | train/val/test=1.000/0.700/0.708 | c=0.998437
[Epoch 0022] loss=12.1365 cls=0.0145 smmd=0.3011 ct=9.4355 rec=1.1927 | train/val/test=1.000/0.700/0.704 | c=0.998437
[Epoch 0023] loss=12.0839 cls=0.0190 smmd=0.2604 ct=9.4186 rec=1.1930 | train/val/test=1.000/0.698/0.709 | c=0.998437
[Epoch 0024] loss=12.0801 cls=0.0237 smmd=0.2623 ct=9.4123 rec=1.1909 | train/val/test=1.000/0.702/0.710 | c=0.998437
[Epoch 0025] loss=12.0557 cls=0.0289 smmd=0.2379 ct=9.4105 rec=1.1893 | train/val/test=1.000/0.704/0.717 | c=0.998437
[Epoch 0026] loss=12.0268 cls=0.0312 smmd=0.2110 ct=9.4133 rec=1.1857 | train/val/test=1.000/0.702/0.714 | c=0.998437
[Epoch 0027] loss=11.9779 cls=0.0317 smmd=0.1668 ct=9.4155 rec=1.1820 | train/val/test=1.000/0.704/0.715 | c=0.998437
[Epoch 0028] loss=11.9584 cls=0.0294 smmd=0.1584 ct=9.4146 rec=1.1780 | train/val/test=1.000/0.694/0.716 | c=0.998437
[Epoch 0029] loss=11.9374 cls=0.0267 smmd=0.1579 ct=9.4043 rec=1.1743 | train/val/test=1.000/0.694/0.721 | c=0.998437
[Epoch 0030] loss=11.8929 cls=0.0247 smmd=0.1286 ct=9.3972 rec=1.1712 | train/val/test=1.000/0.694/0.715 | c=0.998437
[Epoch 0031] loss=11.8717 cls=0.0197 smmd=0.1230 ct=9.3911 rec=1.1689 | train/val/test=1.000/0.694/0.717 | c=0.998437
[Epoch 0032] loss=11.8643 cls=0.0170 smmd=0.1248 ct=9.3882 rec=1.1671 | train/val/test=1.000/0.694/0.716 | c=0.998437
[Epoch 0033] loss=11.8370 cls=0.0146 smmd=0.1090 ct=9.3806 rec=1.1664 | train/val/test=1.000/0.700/0.717 | c=0.998437
[Epoch 0034] loss=11.8157 cls=0.0136 smmd=0.0927 ct=9.3763 rec=1.1665 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0035] loss=11.8091 cls=0.0134 smmd=0.0837 ct=9.3780 rec=1.1670 | train/val/test=1.000/0.696/0.720 | c=0.998437
[Epoch 0036] loss=11.8037 cls=0.0135 smmd=0.0767 ct=9.3763 rec=1.1686 | train/val/test=1.000/0.700/0.723 | c=0.998437
[Epoch 0037] loss=11.7836 cls=0.0136 smmd=0.0556 ct=9.3761 rec=1.1692 | train/val/test=1.000/0.694/0.722 | c=0.998437
[Epoch 0038] loss=11.7826 cls=0.0133 smmd=0.0579 ct=9.3695 rec=1.1710 | train/val/test=1.000/0.696/0.724 | c=0.998437
[Epoch 0039] loss=11.7950 cls=0.0146 smmd=0.0614 ct=9.3761 rec=1.1715 | train/val/test=1.000/0.696/0.722 | c=0.998437
[Epoch 0040] loss=11.8105 cls=0.0150 smmd=0.0679 ct=9.3757 rec=1.1759 | train/val/test=1.000/0.686/0.721 | c=0.998437
[Epoch 0041] loss=11.8378 cls=0.0190 smmd=0.0730 ct=9.3882 rec=1.1788 | train/val/test=1.000/0.698/0.725 | c=0.998437
[Epoch 0042] loss=11.8332 cls=0.0186 smmd=0.0846 ct=9.3799 rec=1.1750 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0043] loss=11.7638 cls=0.0126 smmd=0.0520 ct=9.3672 rec=1.1660 | train/val/test=1.000/0.708/0.724 | c=0.998437
[Epoch 0044] loss=11.7449 cls=0.0126 smmd=0.0364 ct=9.3667 rec=1.1646 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0045] loss=11.7792 cls=0.0145 smmd=0.0658 ct=9.3683 rec=1.1653 | train/val/test=1.000/0.702/0.727 | c=0.998437
[Epoch 0046] loss=11.7407 cls=0.0116 smmd=0.0439 ct=9.3597 rec=1.1628 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0047] loss=11.7270 cls=0.0125 smmd=0.0257 ct=9.3617 rec=1.1635 | train/val/test=1.000/0.698/0.725 | c=0.998437
[Epoch 0048] loss=11.7490 cls=0.0140 smmd=0.0422 ct=9.3608 rec=1.1660 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0049] loss=11.7271 cls=0.0135 smmd=0.0279 ct=9.3553 rec=1.1652 | train/val/test=1.000/0.694/0.724 | c=0.998437
[Epoch 0050] loss=11.7171 cls=0.0142 smmd=0.0167 ct=9.3543 rec=1.1659 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0051] loss=11.7402 cls=0.0152 smmd=0.0280 ct=9.3565 rec=1.1702 | train/val/test=1.000/0.696/0.727 | c=0.998437
[Epoch 0052] loss=11.7435 cls=0.0174 smmd=0.0260 ct=9.3602 rec=1.1699 | train/val/test=1.000/0.698/0.724 | c=0.998437
[Epoch 0053] loss=11.7158 cls=0.0150 smmd=0.0118 ct=9.3508 rec=1.1691 | train/val/test=1.000/0.702/0.725 | c=0.998437
[Epoch 0054] loss=11.6974 cls=0.0144 smmd=0.0037 ct=9.3448 rec=1.1672 | train/val/test=1.000/0.698/0.724 | c=0.998437
[Epoch 0055] loss=11.7087 cls=0.0151 smmd=0.0103 ct=9.3481 rec=1.1676 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0056] loss=11.7157 cls=0.0152 smmd=0.0124 ct=9.3509 rec=1.1686 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0057] loss=11.7023 cls=0.0139 smmd=0.0115 ct=9.3446 rec=1.1661 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0058] loss=11.6888 cls=0.0132 smmd=0.0080 ct=9.3385 rec=1.1646 | train/val/test=1.000/0.706/0.726 | c=0.998437
[Epoch 0059] loss=11.6945 cls=0.0137 smmd=0.0103 ct=9.3403 rec=1.1650 | train/val/test=1.000/0.704/0.725 | c=0.998437
[Epoch 0060] loss=11.6928 cls=0.0137 smmd=0.0067 ct=9.3407 rec=1.1659 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0061] loss=11.6838 cls=0.0139 smmd=-0.0004 ct=9.3387 rec=1.1658 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0062] loss=11.6790 cls=0.0139 smmd=-0.0011 ct=9.3339 rec=1.1661 | train/val/test=1.000/0.702/0.722 | c=0.998437
[Epoch 0063] loss=11.6847 cls=0.0148 smmd=0.0025 ct=9.3325 rec=1.1675 | train/val/test=1.000/0.700/0.727 | c=0.998437
[Epoch 0064] loss=11.6885 cls=0.0151 smmd=0.0003 ct=9.3342 rec=1.1694 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0065] loss=11.6835 cls=0.0159 smmd=-0.0016 ct=9.3313 rec=1.1689 | train/val/test=1.000/0.704/0.725 | c=0.998437
[Epoch 0066] loss=11.6737 cls=0.0148 smmd=-0.0095 ct=9.3302 rec=1.1691 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0067] loss=11.6765 cls=0.0149 smmd=-0.0011 ct=9.3271 rec=1.1678 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0068] loss=11.6678 cls=0.0143 smmd=-0.0059 ct=9.3248 rec=1.1673 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0069] loss=11.6615 cls=0.0140 smmd=-0.0099 ct=9.3236 rec=1.1669 | train/val/test=1.000/0.700/0.722 | c=0.998437
[Epoch 0070] loss=11.6637 cls=0.0141 smmd=-0.0056 ct=9.3215 rec=1.1668 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0071] loss=11.6633 cls=0.0140 smmd=-0.0057 ct=9.3210 rec=1.1670 | train/val/test=1.000/0.700/0.722 | c=0.998437
[Epoch 0072] loss=11.6602 cls=0.0141 smmd=-0.0085 ct=9.3206 rec=1.1670 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0073] loss=11.6560 cls=0.0142 smmd=-0.0121 ct=9.3195 rec=1.1672 | train/val/test=1.000/0.702/0.725 | c=0.998437
[Epoch 0074] loss=11.6564 cls=0.0145 smmd=-0.0102 ct=9.3171 rec=1.1675 | train/val/test=1.000/0.696/0.723 | c=0.998437
[Epoch 0075] loss=11.6551 cls=0.0151 smmd=-0.0131 ct=9.3169 rec=1.1681 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0076] loss=11.6561 cls=0.0156 smmd=-0.0125 ct=9.3151 rec=1.1690 | train/val/test=1.000/0.694/0.725 | c=0.998437
[Epoch 0077] loss=11.6606 cls=0.0162 smmd=-0.0116 ct=9.3171 rec=1.1695 | train/val/test=1.000/0.700/0.723 | c=0.998437
[Epoch 0078] loss=11.6644 cls=0.0163 smmd=-0.0096 ct=9.3164 rec=1.1707 | train/val/test=1.000/0.692/0.723 | c=0.998437
[Epoch 0079] loss=11.6713 cls=0.0171 smmd=-0.0072 ct=9.3198 rec=1.1708 | train/val/test=1.000/0.698/0.722 | c=0.998437
[Epoch 0080] loss=11.6772 cls=0.0164 smmd=-0.0025 ct=9.3204 rec=1.1715 | train/val/test=1.000/0.694/0.727 | c=0.998437
[Epoch 0081] loss=11.6809 cls=0.0163 smmd=0.0012 ct=9.3234 rec=1.1700 | train/val/test=1.000/0.694/0.722 | c=0.998437
[Epoch 0082] loss=11.6667 cls=0.0141 smmd=0.0023 ct=9.3155 rec=1.1674 | train/val/test=1.000/0.692/0.725 | c=0.998437
[Epoch 0083] loss=11.6440 cls=0.0124 smmd=-0.0097 ct=9.3129 rec=1.1643 | train/val/test=1.000/0.698/0.723 | c=0.998437
[Epoch 0084] loss=11.6365 cls=0.0118 smmd=-0.0111 ct=9.3091 rec=1.1634 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0085] loss=11.6468 cls=0.0124 smmd=-0.0077 ct=9.3119 rec=1.1651 | train/val/test=1.000/0.696/0.724 | c=0.998437
[Epoch 0086] loss=11.6507 cls=0.0134 smmd=-0.0092 ct=9.3131 rec=1.1667 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0087] loss=11.6454 cls=0.0136 smmd=-0.0120 ct=9.3090 rec=1.1674 | train/val/test=1.000/0.700/0.724 | c=0.998437
[Epoch 0088] loss=11.6394 cls=0.0140 smmd=-0.0132 ct=9.3037 rec=1.1675 | train/val/test=1.000/0.706/0.725 | c=0.998437
[Epoch 0089] loss=11.6344 cls=0.0144 smmd=-0.0209 ct=9.3047 rec=1.1681 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0090] loss=11.6355 cls=0.0151 smmd=-0.0203 ct=9.3036 rec=1.1685 | train/val/test=1.000/0.696/0.728 | c=0.998437
[Epoch 0091] loss=11.6377 cls=0.0158 smmd=-0.0178 ct=9.3015 rec=1.1691 | train/val/test=1.000/0.706/0.724 | c=0.998437
[Epoch 0092] loss=11.6413 cls=0.0162 smmd=-0.0174 ct=9.3037 rec=1.1694 | train/val/test=1.000/0.696/0.729 | c=0.998437
[Epoch 0093] loss=11.6525 cls=0.0171 smmd=-0.0120 ct=9.3077 rec=1.1699 | train/val/test=1.000/0.698/0.723 | c=0.998437
[Epoch 0094] loss=11.6646 cls=0.0175 smmd=-0.0032 ct=9.3082 rec=1.1711 | train/val/test=1.000/0.694/0.735 | c=0.998437
[Epoch 0095] loss=11.6785 cls=0.0184 smmd=-0.0017 ct=9.3185 rec=1.1716 | train/val/test=1.000/0.702/0.723 | c=0.998437
[Epoch 0096] loss=11.6792 cls=0.0171 smmd=0.0163 ct=9.3056 rec=1.1701 | train/val/test=1.000/0.696/0.729 | c=0.998437
[Epoch 0097] loss=11.6481 cls=0.0133 smmd=-0.0124 ct=9.3162 rec=1.1655 | train/val/test=1.000/0.690/0.721 | c=0.998437
[Epoch 0098] loss=11.6294 cls=0.0119 smmd=-0.0108 ct=9.3022 rec=1.1630 | train/val/test=1.000/0.704/0.728 | c=0.998437
[Epoch 0099] loss=11.6538 cls=0.0125 smmd=0.0000 ct=9.3085 rec=1.1664 | train/val/test=1.000/0.692/0.728 | c=0.998437
=== Best @ epoch 43: val=0.7080, test=0.7240 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4 - 2025-09-21 04:28:11:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.3107 cls=1.9427 smmd=4.3226 ct=9.2676 rec=1.3889 | train/val/test=0.241/0.104/0.121 | c=0.998437
[Epoch 0001] loss=16.7970 cls=1.8885 smmd=2.9246 ct=9.2046 rec=1.3897 | train/val/test=0.517/0.248/0.265 | c=0.998437
[Epoch 0002] loss=15.1814 cls=1.7539 smmd=1.5819 ct=9.0678 rec=1.3889 | train/val/test=0.759/0.262/0.282 | c=0.998437
[Epoch 0003] loss=14.7561 cls=1.5075 smmd=1.4131 ct=9.0595 rec=1.3880 | train/val/test=0.966/0.518/0.536 | c=0.998437
[Epoch 0004] loss=14.6675 cls=1.1713 smmd=1.7664 ct=8.9635 rec=1.3832 | train/val/test=1.000/0.486/0.521 | c=0.998437
[Epoch 0005] loss=14.0828 cls=0.7795 smmd=1.6745 ct=8.8861 rec=1.3714 | train/val/test=0.966/0.488/0.526 | c=0.998437
[Epoch 0006] loss=13.4558 cls=0.4880 smmd=1.3997 ct=8.8682 rec=1.3500 | train/val/test=0.966/0.564/0.577 | c=0.998437
[Epoch 0007] loss=12.8179 cls=0.2897 smmd=1.0592 ct=8.8263 rec=1.3213 | train/val/test=0.966/0.582/0.592 | c=0.998437
[Epoch 0008] loss=12.5716 cls=0.1681 smmd=1.0037 ct=8.8173 rec=1.2912 | train/val/test=1.000/0.592/0.592 | c=0.998437
[Epoch 0009] loss=12.5469 cls=0.0911 smmd=1.1127 ct=8.8112 rec=1.2659 | train/val/test=1.000/0.600/0.606 | c=0.998437
[Epoch 0010] loss=12.4980 cls=0.0467 smmd=1.1569 ct=8.8031 rec=1.2456 | train/val/test=1.000/0.614/0.630 | c=0.998437
[Epoch 0011] loss=12.3444 cls=0.0238 smmd=1.0638 ct=8.7983 rec=1.2292 | train/val/test=1.000/0.634/0.648 | c=0.998437
[Epoch 0012] loss=12.1171 cls=0.0119 smmd=0.8839 ct=8.7898 rec=1.2157 | train/val/test=1.000/0.646/0.655 | c=0.998437
[Epoch 0013] loss=11.9483 cls=0.0066 smmd=0.7462 ct=8.7842 rec=1.2057 | train/val/test=1.000/0.660/0.669 | c=0.998437
[Epoch 0014] loss=11.8772 cls=0.0043 smmd=0.6931 ct=8.7836 rec=1.1981 | train/val/test=1.000/0.684/0.692 | c=0.998437
[Epoch 0015] loss=12.5741 cls=0.0032 smmd=0.6992 ct=9.4874 rec=1.1922 | train/val/test=1.000/0.676/0.687 | c=0.998437
[Epoch 0016] loss=12.5067 cls=0.0030 smmd=0.7103 ct=9.4138 rec=1.1898 | train/val/test=1.000/0.684/0.688 | c=0.998437
[Epoch 0017] loss=12.3905 cls=0.0032 smmd=0.6559 ct=9.3536 rec=1.1889 | train/val/test=1.000/0.692/0.700 | c=0.998437
[Epoch 0018] loss=12.2911 cls=0.0036 smmd=0.5624 ct=9.3512 rec=1.1869 | train/val/test=1.000/0.696/0.705 | c=0.998437
[Epoch 0019] loss=12.2657 cls=0.0046 smmd=0.4896 ct=9.3976 rec=1.1869 | train/val/test=1.000/0.688/0.703 | c=0.998437
[Epoch 0020] loss=12.2463 cls=0.0067 smmd=0.4185 ct=9.4414 rec=1.1898 | train/val/test=1.000/0.694/0.700 | c=0.998437
[Epoch 0021] loss=12.1764 cls=0.0101 smmd=0.3336 ct=9.4494 rec=1.1917 | train/val/test=1.000/0.700/0.708 | c=0.998437
[Epoch 0022] loss=12.1365 cls=0.0145 smmd=0.3011 ct=9.4355 rec=1.1927 | train/val/test=1.000/0.700/0.704 | c=0.998437
[Epoch 0023] loss=12.0839 cls=0.0190 smmd=0.2604 ct=9.4186 rec=1.1930 | train/val/test=1.000/0.698/0.709 | c=0.998437
[Epoch 0024] loss=12.0801 cls=0.0237 smmd=0.2623 ct=9.4123 rec=1.1909 | train/val/test=1.000/0.702/0.710 | c=0.998437
[Epoch 0025] loss=12.0557 cls=0.0289 smmd=0.2379 ct=9.4105 rec=1.1893 | train/val/test=1.000/0.704/0.717 | c=0.998437
[Epoch 0026] loss=12.0268 cls=0.0312 smmd=0.2110 ct=9.4133 rec=1.1857 | train/val/test=1.000/0.702/0.714 | c=0.998437
[Epoch 0027] loss=11.9779 cls=0.0317 smmd=0.1668 ct=9.4155 rec=1.1820 | train/val/test=1.000/0.704/0.715 | c=0.998437
[Epoch 0028] loss=11.9584 cls=0.0294 smmd=0.1584 ct=9.4146 rec=1.1780 | train/val/test=1.000/0.694/0.716 | c=0.998437
[Epoch 0029] loss=11.9374 cls=0.0267 smmd=0.1579 ct=9.4043 rec=1.1743 | train/val/test=1.000/0.694/0.721 | c=0.998437
[Epoch 0030] loss=11.8929 cls=0.0247 smmd=0.1286 ct=9.3972 rec=1.1712 | train/val/test=1.000/0.694/0.715 | c=0.998437
[Epoch 0031] loss=11.8717 cls=0.0197 smmd=0.1230 ct=9.3911 rec=1.1689 | train/val/test=1.000/0.694/0.717 | c=0.998437
[Epoch 0032] loss=11.8643 cls=0.0170 smmd=0.1248 ct=9.3882 rec=1.1671 | train/val/test=1.000/0.694/0.716 | c=0.998437
[Epoch 0033] loss=11.8370 cls=0.0146 smmd=0.1090 ct=9.3806 rec=1.1664 | train/val/test=1.000/0.700/0.717 | c=0.998437
[Epoch 0034] loss=11.8157 cls=0.0136 smmd=0.0927 ct=9.3763 rec=1.1665 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0035] loss=11.8091 cls=0.0134 smmd=0.0837 ct=9.3780 rec=1.1670 | train/val/test=1.000/0.696/0.720 | c=0.998437
[Epoch 0036] loss=11.8037 cls=0.0135 smmd=0.0767 ct=9.3763 rec=1.1686 | train/val/test=1.000/0.700/0.723 | c=0.998437
[Epoch 0037] loss=11.7836 cls=0.0136 smmd=0.0556 ct=9.3761 rec=1.1692 | train/val/test=1.000/0.694/0.722 | c=0.998437
[Epoch 0038] loss=11.7826 cls=0.0133 smmd=0.0579 ct=9.3695 rec=1.1710 | train/val/test=1.000/0.696/0.724 | c=0.998437
[Epoch 0039] loss=11.7950 cls=0.0146 smmd=0.0614 ct=9.3761 rec=1.1715 | train/val/test=1.000/0.696/0.722 | c=0.998437
[Epoch 0040] loss=11.8105 cls=0.0150 smmd=0.0679 ct=9.3757 rec=1.1759 | train/val/test=1.000/0.686/0.721 | c=0.998437
[Epoch 0041] loss=11.8378 cls=0.0190 smmd=0.0730 ct=9.3882 rec=1.1788 | train/val/test=1.000/0.698/0.725 | c=0.998437
[Epoch 0042] loss=11.8332 cls=0.0186 smmd=0.0846 ct=9.3799 rec=1.1750 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0043] loss=11.7638 cls=0.0126 smmd=0.0520 ct=9.3672 rec=1.1660 | train/val/test=1.000/0.708/0.724 | c=0.998437
[Epoch 0044] loss=11.7449 cls=0.0126 smmd=0.0364 ct=9.3667 rec=1.1646 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0045] loss=11.7792 cls=0.0145 smmd=0.0658 ct=9.3683 rec=1.1653 | train/val/test=1.000/0.702/0.727 | c=0.998437
[Epoch 0046] loss=11.7407 cls=0.0116 smmd=0.0439 ct=9.3597 rec=1.1628 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0047] loss=11.7270 cls=0.0125 smmd=0.0257 ct=9.3617 rec=1.1635 | train/val/test=1.000/0.698/0.725 | c=0.998437
[Epoch 0048] loss=11.7490 cls=0.0140 smmd=0.0422 ct=9.3608 rec=1.1660 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0049] loss=11.7271 cls=0.0135 smmd=0.0279 ct=9.3553 rec=1.1652 | train/val/test=1.000/0.694/0.724 | c=0.998437
[Epoch 0050] loss=11.7171 cls=0.0142 smmd=0.0167 ct=9.3543 rec=1.1659 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0051] loss=11.7402 cls=0.0152 smmd=0.0280 ct=9.3565 rec=1.1702 | train/val/test=1.000/0.696/0.727 | c=0.998437
[Epoch 0052] loss=11.7435 cls=0.0174 smmd=0.0260 ct=9.3602 rec=1.1699 | train/val/test=1.000/0.698/0.724 | c=0.998437
[Epoch 0053] loss=11.7158 cls=0.0150 smmd=0.0118 ct=9.3508 rec=1.1691 | train/val/test=1.000/0.702/0.725 | c=0.998437
[Epoch 0054] loss=11.6974 cls=0.0144 smmd=0.0037 ct=9.3448 rec=1.1672 | train/val/test=1.000/0.698/0.724 | c=0.998437
[Epoch 0055] loss=11.7087 cls=0.0151 smmd=0.0103 ct=9.3481 rec=1.1676 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0056] loss=11.7157 cls=0.0152 smmd=0.0124 ct=9.3509 rec=1.1686 | train/val/test=1.000/0.700/0.726 | c=0.998437
[Epoch 0057] loss=11.7023 cls=0.0139 smmd=0.0115 ct=9.3446 rec=1.1661 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0058] loss=11.6888 cls=0.0132 smmd=0.0080 ct=9.3385 rec=1.1646 | train/val/test=1.000/0.706/0.726 | c=0.998437
[Epoch 0059] loss=11.6945 cls=0.0137 smmd=0.0103 ct=9.3403 rec=1.1650 | train/val/test=1.000/0.704/0.725 | c=0.998437
[Epoch 0060] loss=11.6928 cls=0.0137 smmd=0.0067 ct=9.3407 rec=1.1659 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0061] loss=11.6838 cls=0.0139 smmd=-0.0004 ct=9.3387 rec=1.1658 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0062] loss=11.6790 cls=0.0139 smmd=-0.0011 ct=9.3339 rec=1.1661 | train/val/test=1.000/0.702/0.722 | c=0.998437
[Epoch 0063] loss=11.6847 cls=0.0148 smmd=0.0025 ct=9.3325 rec=1.1675 | train/val/test=1.000/0.700/0.727 | c=0.998437
[Epoch 0064] loss=11.6885 cls=0.0151 smmd=0.0003 ct=9.3342 rec=1.1694 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0065] loss=11.6835 cls=0.0159 smmd=-0.0016 ct=9.3313 rec=1.1689 | train/val/test=1.000/0.704/0.725 | c=0.998437
[Epoch 0066] loss=11.6737 cls=0.0148 smmd=-0.0095 ct=9.3302 rec=1.1691 | train/val/test=1.000/0.700/0.721 | c=0.998437
[Epoch 0067] loss=11.6765 cls=0.0149 smmd=-0.0011 ct=9.3271 rec=1.1678 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0068] loss=11.6678 cls=0.0143 smmd=-0.0059 ct=9.3248 rec=1.1673 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0069] loss=11.6615 cls=0.0140 smmd=-0.0099 ct=9.3236 rec=1.1669 | train/val/test=1.000/0.700/0.722 | c=0.998437
[Epoch 0070] loss=11.6637 cls=0.0141 smmd=-0.0056 ct=9.3215 rec=1.1668 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0071] loss=11.6633 cls=0.0140 smmd=-0.0057 ct=9.3210 rec=1.1670 | train/val/test=1.000/0.700/0.722 | c=0.998437
[Epoch 0072] loss=11.6602 cls=0.0141 smmd=-0.0085 ct=9.3206 rec=1.1670 | train/val/test=1.000/0.704/0.726 | c=0.998437
[Epoch 0073] loss=11.6560 cls=0.0142 smmd=-0.0121 ct=9.3195 rec=1.1672 | train/val/test=1.000/0.702/0.725 | c=0.998437
[Epoch 0074] loss=11.6564 cls=0.0145 smmd=-0.0102 ct=9.3171 rec=1.1675 | train/val/test=1.000/0.696/0.723 | c=0.998437
[Epoch 0075] loss=11.6551 cls=0.0151 smmd=-0.0131 ct=9.3169 rec=1.1681 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0076] loss=11.6561 cls=0.0156 smmd=-0.0125 ct=9.3151 rec=1.1690 | train/val/test=1.000/0.694/0.725 | c=0.998437
[Epoch 0077] loss=11.6606 cls=0.0162 smmd=-0.0116 ct=9.3171 rec=1.1695 | train/val/test=1.000/0.700/0.723 | c=0.998437
[Epoch 0078] loss=11.6644 cls=0.0163 smmd=-0.0096 ct=9.3164 rec=1.1707 | train/val/test=1.000/0.692/0.723 | c=0.998437
[Epoch 0079] loss=11.6713 cls=0.0171 smmd=-0.0072 ct=9.3198 rec=1.1708 | train/val/test=1.000/0.698/0.722 | c=0.998437
[Epoch 0080] loss=11.6772 cls=0.0164 smmd=-0.0025 ct=9.3204 rec=1.1715 | train/val/test=1.000/0.694/0.727 | c=0.998437
[Epoch 0081] loss=11.6809 cls=0.0163 smmd=0.0012 ct=9.3234 rec=1.1700 | train/val/test=1.000/0.694/0.722 | c=0.998437
[Epoch 0082] loss=11.6667 cls=0.0141 smmd=0.0023 ct=9.3155 rec=1.1674 | train/val/test=1.000/0.692/0.725 | c=0.998437
[Epoch 0083] loss=11.6440 cls=0.0124 smmd=-0.0097 ct=9.3129 rec=1.1643 | train/val/test=1.000/0.698/0.723 | c=0.998437
[Epoch 0084] loss=11.6365 cls=0.0118 smmd=-0.0111 ct=9.3091 rec=1.1634 | train/val/test=1.000/0.704/0.724 | c=0.998437
[Epoch 0085] loss=11.6468 cls=0.0124 smmd=-0.0077 ct=9.3119 rec=1.1651 | train/val/test=1.000/0.696/0.724 | c=0.998437
[Epoch 0086] loss=11.6507 cls=0.0134 smmd=-0.0092 ct=9.3131 rec=1.1667 | train/val/test=1.000/0.702/0.724 | c=0.998437
[Epoch 0087] loss=11.6454 cls=0.0136 smmd=-0.0120 ct=9.3090 rec=1.1674 | train/val/test=1.000/0.700/0.724 | c=0.998437
[Epoch 0088] loss=11.6394 cls=0.0140 smmd=-0.0132 ct=9.3037 rec=1.1675 | train/val/test=1.000/0.706/0.725 | c=0.998437
[Epoch 0089] loss=11.6344 cls=0.0144 smmd=-0.0209 ct=9.3047 rec=1.1681 | train/val/test=1.000/0.700/0.725 | c=0.998437
[Epoch 0090] loss=11.6355 cls=0.0151 smmd=-0.0203 ct=9.3036 rec=1.1685 | train/val/test=1.000/0.696/0.728 | c=0.998437
[Epoch 0091] loss=11.6377 cls=0.0158 smmd=-0.0178 ct=9.3015 rec=1.1691 | train/val/test=1.000/0.706/0.724 | c=0.998437
[Epoch 0092] loss=11.6413 cls=0.0162 smmd=-0.0174 ct=9.3037 rec=1.1694 | train/val/test=1.000/0.696/0.729 | c=0.998437
[Epoch 0093] loss=11.6525 cls=0.0171 smmd=-0.0120 ct=9.3077 rec=1.1699 | train/val/test=1.000/0.698/0.723 | c=0.998437
[Epoch 0094] loss=11.6646 cls=0.0175 smmd=-0.0032 ct=9.3082 rec=1.1711 | train/val/test=1.000/0.694/0.735 | c=0.998437
[Epoch 0095] loss=11.6785 cls=0.0184 smmd=-0.0017 ct=9.3185 rec=1.1716 | train/val/test=1.000/0.702/0.723 | c=0.998437
[Epoch 0096] loss=11.6792 cls=0.0171 smmd=0.0163 ct=9.3056 rec=1.1701 | train/val/test=1.000/0.696/0.729 | c=0.998437
[Epoch 0097] loss=11.6481 cls=0.0133 smmd=-0.0124 ct=9.3162 rec=1.1655 | train/val/test=1.000/0.690/0.721 | c=0.998437
[Epoch 0098] loss=11.6294 cls=0.0119 smmd=-0.0108 ct=9.3022 rec=1.1630 | train/val/test=1.000/0.704/0.728 | c=0.998437
[Epoch 0099] loss=11.6538 cls=0.0125 smmd=0.0000 ct=9.3085 rec=1.1664 | train/val/test=1.000/0.692/0.728 | c=0.998437
=== Best @ epoch 43: val=0.7080, test=0.7240 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-4 completed in 24.11 seconds.
==================================================
