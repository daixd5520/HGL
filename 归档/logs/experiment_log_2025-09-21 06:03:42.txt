Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 - 2025-09-21 06:03:42:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.2523 cls=1.0962 smmd=5.6574 ct=11.2760 rec=1.4137 | train/val/test=0.385/0.214/0.200 | c=0.998347
[Epoch 0001] loss=30.4024 cls=1.0782 smmd=3.7018 ct=11.2128 rec=1.4136 | train/val/test=0.577/0.474/0.432 | c=0.998347
[Epoch 0002] loss=35.8662 cls=1.0471 smmd=4.7927 ct=11.2377 rec=1.4131 | train/val/test=0.538/0.486/0.486 | c=0.998347
[Epoch 0003] loss=35.0645 cls=0.9709 smmd=4.6568 ct=11.1539 rec=1.4089 | train/val/test=0.577/0.518/0.510 | c=0.998347
[Epoch 0004] loss=25.5205 cls=0.8700 smmd=2.7934 ct=10.9791 rec=1.3933 | train/val/test=0.615/0.510/0.509 | c=0.998347
[Epoch 0005] loss=26.2606 cls=0.7960 smmd=2.9565 ct=10.9428 rec=1.3748 | train/val/test=0.615/0.514/0.512 | c=0.998347
[Epoch 0006] loss=28.7688 cls=0.7350 smmd=3.4626 ct=10.9518 rec=1.3636 | train/val/test=0.654/0.566/0.552 | c=0.998347
[Epoch 0007] loss=26.8410 cls=0.6538 smmd=3.0899 ct=10.9290 rec=1.3566 | train/val/test=0.769/0.594/0.589 | c=0.998347
[Epoch 0008] loss=22.7826 cls=0.5601 smmd=2.1519 ct=11.6075 rec=1.3543 | train/val/test=0.846/0.644/0.637 | c=0.998347
[Epoch 0009] loss=23.1492 cls=0.4567 smmd=2.2414 ct=11.5781 rec=1.3584 | train/val/test=0.962/0.742/0.719 | c=0.998347
[Epoch 0010] loss=25.2314 cls=0.3664 smmd=2.6712 ct=11.5564 rec=1.3588 | train/val/test=0.962/0.754/0.743 | c=0.998347
[Epoch 0011] loss=23.5493 cls=0.3198 smmd=2.3473 ct=11.5171 rec=1.3603 | train/val/test=1.000/0.790/0.778 | c=0.998347
[Epoch 0012] loss=20.1338 cls=0.2578 smmd=1.6768 ct=11.4855 rec=1.3553 | train/val/test=1.000/0.764/0.736 | c=0.998347
[Epoch 0013] loss=23.6814 cls=0.2330 smmd=2.3603 ct=11.6280 rec=1.3536 | train/val/test=1.000/0.760/0.728 | c=0.998347
[Epoch 0014] loss=22.2939 cls=0.1900 smmd=2.0980 ct=11.5748 rec=1.3424 | train/val/test=1.000/0.758/0.717 | c=0.998347
[Epoch 0015] loss=19.9404 cls=0.1524 smmd=1.6486 ct=11.4877 rec=1.3321 | train/val/test=1.000/0.766/0.740 | c=0.998347
[Epoch 0016] loss=19.8358 cls=0.1148 smmd=1.6324 ct=11.4843 rec=1.3218 | train/val/test=1.000/0.776/0.742 | c=0.998347
[Epoch 0017] loss=20.3709 cls=0.0922 smmd=1.7315 ct=11.5356 rec=1.3170 | train/val/test=1.000/0.796/0.763 | c=0.998347
[Epoch 0018] loss=19.0656 cls=0.0720 smmd=1.4774 ct=11.5119 rec=1.3088 | train/val/test=1.000/0.784/0.760 | c=0.998347
[Epoch 0019] loss=18.1244 cls=0.0677 smmd=1.2930 ct=11.4950 rec=1.3027 | train/val/test=1.000/0.788/0.772 | c=0.998347
[Epoch 0020] loss=18.4339 cls=0.0692 smmd=1.3468 ct=11.5349 rec=1.3050 | train/val/test=1.000/0.808/0.781 | c=0.998347
[Epoch 0021] loss=18.7289 cls=0.0796 smmd=1.4086 ct=11.5152 rec=1.3094 | train/val/test=1.000/0.804/0.783 | c=0.998347
[Epoch 0022] loss=17.2786 cls=0.0889 smmd=1.1189 ct=11.5094 rec=1.3010 | train/val/test=1.000/0.808/0.781 | c=0.998347
[Epoch 0023] loss=18.4613 cls=0.0956 smmd=1.3450 ct=11.5584 rec=1.2991 | train/val/test=1.000/0.794/0.779 | c=0.998347
[Epoch 0024] loss=17.1507 cls=0.0986 smmd=1.0905 ct=11.5190 rec=1.2964 | train/val/test=1.000/0.808/0.785 | c=0.998347
[Epoch 0025] loss=17.3163 cls=0.0780 smmd=1.1292 ct=11.5017 rec=1.2961 | train/val/test=1.000/0.798/0.779 | c=0.998347
[Epoch 0026] loss=16.4493 cls=0.0683 smmd=0.9585 ct=11.4930 rec=1.2950 | train/val/test=1.000/0.792/0.769 | c=0.998347
[Epoch 0027] loss=16.7332 cls=0.0648 smmd=1.0106 ct=11.5182 rec=1.2933 | train/val/test=1.000/0.808/0.762 | c=0.998347
[Epoch 0028] loss=15.9109 cls=0.0673 smmd=0.8510 ct=11.4922 rec=1.3018 | train/val/test=1.000/0.788/0.754 | c=0.998347
[Epoch 0029] loss=16.1636 cls=0.0671 smmd=0.9075 ct=11.4629 rec=1.2975 | train/val/test=1.000/0.776/0.762 | c=0.998347
[Epoch 0030] loss=15.7529 cls=0.0700 smmd=0.8131 ct=11.5227 rec=1.2978 | train/val/test=1.000/0.788/0.764 | c=0.998347
[Epoch 0031] loss=16.0534 cls=0.0770 smmd=0.8706 ct=11.5314 rec=1.3050 | train/val/test=1.000/0.772/0.760 | c=0.998347
[Epoch 0032] loss=16.1186 cls=0.0755 smmd=0.8948 ct=11.4766 rec=1.3020 | train/val/test=1.000/0.788/0.742 | c=0.998347
[Epoch 0033] loss=15.8445 cls=0.0818 smmd=0.8293 ct=11.5257 rec=1.3148 | train/val/test=1.000/0.782/0.761 | c=0.998347
[Epoch 0034] loss=15.8322 cls=0.0703 smmd=0.8260 ct=11.5372 rec=1.2993 | train/val/test=1.000/0.792/0.764 | c=0.998347
[Epoch 0035] loss=15.3451 cls=0.0655 smmd=0.7378 ct=11.4927 rec=1.3080 | train/val/test=1.000/0.784/0.764 | c=0.998347
[Epoch 0036] loss=14.9898 cls=0.0706 smmd=0.6669 ct=11.4905 rec=1.2945 | train/val/test=1.000/0.800/0.777 | c=0.998347
[Epoch 0037] loss=15.1397 cls=0.0546 smmd=0.6935 ct=11.5156 rec=1.2915 | train/val/test=1.000/0.812/0.778 | c=0.998347
[Epoch 0038] loss=14.8492 cls=0.0567 smmd=0.6415 ct=11.4838 rec=1.2954 | train/val/test=1.000/0.812/0.779 | c=0.998347
[Epoch 0039] loss=14.8643 cls=0.0606 smmd=0.6415 ct=11.4966 rec=1.2982 | train/val/test=1.000/0.796/0.778 | c=0.998347
[Epoch 0040] loss=15.0109 cls=0.0690 smmd=0.6658 ct=11.5171 rec=1.3025 | train/val/test=1.000/0.778/0.753 | c=0.998347
[Epoch 0041] loss=15.2686 cls=0.0836 smmd=0.7126 ct=11.5317 rec=1.3193 | train/val/test=1.000/0.744/0.730 | c=0.998347
[Epoch 0042] loss=15.5183 cls=0.1069 smmd=0.7592 ct=11.5362 rec=1.3248 | train/val/test=1.000/0.696/0.679 | c=0.998347
[Epoch 0043] loss=15.0063 cls=0.1184 smmd=0.6556 ct=11.5350 rec=1.3413 | train/val/test=1.000/0.734/0.719 | c=0.998347
[Epoch 0044] loss=14.8463 cls=0.1070 smmd=0.6304 ct=11.5079 rec=1.3260 | train/val/test=1.000/0.712/0.694 | c=0.998347
[Epoch 0045] loss=14.6648 cls=0.0944 smmd=0.5888 ct=11.5409 rec=1.3267 | train/val/test=1.000/0.798/0.774 | c=0.998347
[Epoch 0046] loss=14.4694 cls=0.0653 smmd=0.5643 ct=11.4851 rec=1.3034 | train/val/test=1.000/0.800/0.755 | c=0.998347
[Epoch 0047] loss=14.2202 cls=0.0614 smmd=0.5168 ct=11.4751 rec=1.3041 | train/val/test=1.000/0.804/0.773 | c=0.998347
[Epoch 0048] loss=14.2870 cls=0.0630 smmd=0.5217 ct=11.5169 rec=1.3035 | train/val/test=1.000/0.812/0.762 | c=0.998347
[Epoch 0049] loss=14.5010 cls=0.0726 smmd=0.5687 ct=11.4897 rec=1.3139 | train/val/test=1.000/0.794/0.761 | c=0.998347
[Epoch 0050] loss=14.8094 cls=0.0861 smmd=0.6273 ct=11.4980 rec=1.3178 | train/val/test=1.000/0.774/0.747 | c=0.998347
[Epoch 0051] loss=15.1287 cls=0.0961 smmd=0.6807 ct=11.5440 rec=1.3295 | train/val/test=1.000/0.784/0.747 | c=0.998347
[Epoch 0052] loss=14.8214 cls=0.1000 smmd=0.6313 ct=11.4821 rec=1.3267 | train/val/test=1.000/0.798/0.756 | c=0.998347
[Epoch 0053] loss=14.4460 cls=0.0904 smmd=0.5530 ct=11.5036 rec=1.3246 | train/val/test=1.000/0.800/0.759 | c=0.998347
[Epoch 0054] loss=14.2442 cls=0.0891 smmd=0.5201 ct=11.4670 rec=1.3218 | train/val/test=1.000/0.802/0.755 | c=0.998347
[Epoch 0055] loss=14.0075 cls=0.0923 smmd=0.4721 ct=11.4683 rec=1.3244 | train/val/test=1.000/0.782/0.749 | c=0.998347
[Epoch 0056] loss=13.9650 cls=0.0971 smmd=0.4608 ct=11.4799 rec=1.3274 | train/val/test=1.000/0.770/0.739 | c=0.998347
[Epoch 0057] loss=14.1052 cls=0.1125 smmd=0.4863 ct=11.4840 rec=1.3364 | train/val/test=1.000/0.722/0.695 | c=0.998347
[Epoch 0058] loss=14.3566 cls=0.1320 smmd=0.5318 ct=11.4972 rec=1.3444 | train/val/test=1.000/0.674/0.644 | c=0.998347
[Epoch 0059] loss=14.9367 cls=0.1793 smmd=0.6290 ct=11.5656 rec=1.3668 | train/val/test=0.962/0.640/0.621 | c=0.998347
[Epoch 0060] loss=15.0231 cls=0.2058 smmd=0.6448 ct=11.5588 rec=1.3774 | train/val/test=0.923/0.598/0.592 | c=0.998347
[Epoch 0061] loss=15.0753 cls=0.2226 smmd=0.6429 ct=11.6123 rec=1.3746 | train/val/test=1.000/0.708/0.690 | c=0.998347
[Epoch 0062] loss=14.4565 cls=0.1322 smmd=0.5516 ct=11.4975 rec=1.3517 | train/val/test=1.000/0.802/0.766 | c=0.998347
[Epoch 0063] loss=13.8167 cls=0.0600 smmd=0.4346 ct=11.4825 rec=1.3100 | train/val/test=1.000/0.818/0.777 | c=0.998347
[Epoch 0064] loss=14.0847 cls=0.0523 smmd=0.4919 ct=11.4684 rec=1.3054 | train/val/test=1.000/0.788/0.759 | c=0.998347
[Epoch 0065] loss=13.6006 cls=0.0591 smmd=0.3947 ct=11.4667 rec=1.3097 | train/val/test=1.000/0.812/0.779 | c=0.998347
[Epoch 0066] loss=13.7253 cls=0.0575 smmd=0.4147 ct=11.4916 rec=1.3127 | train/val/test=1.000/0.816/0.773 | c=0.998347
[Epoch 0067] loss=14.2961 cls=0.0684 smmd=0.5284 ct=11.4876 rec=1.3222 | train/val/test=1.000/0.794/0.762 | c=0.998347
[Epoch 0068] loss=14.5737 cls=0.0832 smmd=0.5738 ct=11.5298 rec=1.3331 | train/val/test=1.000/0.792/0.749 | c=0.998347
[Epoch 0069] loss=15.0494 cls=0.0952 smmd=0.6762 ct=11.4869 rec=1.3383 | train/val/test=1.000/0.790/0.744 | c=0.998347
[Epoch 0070] loss=14.9281 cls=0.0964 smmd=0.6493 ct=11.4995 rec=1.3383 | train/val/test=1.000/0.750/0.715 | c=0.998347
[Epoch 0071] loss=14.2122 cls=0.1068 smmd=0.5047 ct=11.5017 rec=1.3366 | train/val/test=1.000/0.778/0.731 | c=0.998347
[Epoch 0072] loss=13.8543 cls=0.0939 smmd=0.4458 ct=11.4449 rec=1.3328 | train/val/test=1.000/0.776/0.734 | c=0.998347
[Epoch 0073] loss=13.6677 cls=0.0997 smmd=0.4071 ct=11.4492 rec=1.3311 | train/val/test=1.000/0.798/0.743 | c=0.998347
[Epoch 0074] loss=13.6067 cls=0.0979 smmd=0.3918 ct=11.4659 rec=1.3307 | train/val/test=1.000/0.788/0.752 | c=0.998347
[Epoch 0075] loss=13.6065 cls=0.1080 smmd=0.3936 ct=11.4510 rec=1.3336 | train/val/test=1.000/0.778/0.742 | c=0.998347
[Epoch 0076] loss=14.0003 cls=0.1249 smmd=0.4606 ct=11.5010 rec=1.3402 | train/val/test=1.000/0.734/0.720 | c=0.998347
[Epoch 0077] loss=14.9367 cls=0.1517 smmd=0.6419 ct=11.5164 rec=1.3515 | train/val/test=1.000/0.632/0.612 | c=0.998347
[Epoch 0078] loss=15.5706 cls=0.2007 smmd=0.7477 ct=11.5946 rec=1.3700 | train/val/test=0.846/0.596/0.594 | c=0.998347
[Epoch 0079] loss=15.5519 cls=0.2880 smmd=0.7366 ct=11.5847 rec=1.4013 | train/val/test=0.846/0.568/0.569 | c=0.998347
[Epoch 0080] loss=14.7987 cls=0.2266 smmd=0.5832 ct=11.6313 rec=1.3798 | train/val/test=1.000/0.738/0.727 | c=0.998347
[Epoch 0081] loss=14.3309 cls=0.0855 smmd=0.5407 ct=11.4526 rec=1.3187 | train/val/test=1.000/0.796/0.760 | c=0.998347
[Epoch 0082] loss=13.5780 cls=0.0428 smmd=0.3965 ct=11.4444 rec=1.2951 | train/val/test=1.000/0.740/0.703 | c=0.998347
[Epoch 0083] loss=13.8802 cls=0.0575 smmd=0.4418 ct=11.5105 rec=1.3206 | train/val/test=1.000/0.772/0.750 | c=0.998347
[Epoch 0084] loss=13.7887 cls=0.0472 smmd=0.4370 ct=11.4490 rec=1.3084 | train/val/test=1.000/0.796/0.755 | c=0.998347
[Epoch 0085] loss=13.7010 cls=0.0471 smmd=0.4122 ct=11.4855 rec=1.3120 | train/val/test=1.000/0.768/0.725 | c=0.998347
[Epoch 0086] loss=14.2807 cls=0.0685 smmd=0.5206 ct=11.5103 rec=1.3292 | train/val/test=1.000/0.724/0.707 | c=0.998347
[Epoch 0087] loss=15.0413 cls=0.0941 smmd=0.6660 ct=11.5300 rec=1.3423 | train/val/test=1.000/0.760/0.718 | c=0.998347
[Epoch 0088] loss=15.4190 cls=0.0981 smmd=0.7410 ct=11.5307 rec=1.3427 | train/val/test=1.000/0.752/0.706 | c=0.998347
[Epoch 0089] loss=14.7537 cls=0.0983 smmd=0.6156 ct=11.4925 rec=1.3395 | train/val/test=1.000/0.756/0.720 | c=0.998347
[Epoch 0090] loss=13.8785 cls=0.0906 smmd=0.4430 ct=11.4848 rec=1.3349 | train/val/test=1.000/0.744/0.704 | c=0.998347
[Epoch 0091] loss=13.7591 cls=0.0965 smmd=0.4261 ct=11.4468 rec=1.3329 | train/val/test=1.000/0.784/0.757 | c=0.998347
[Epoch 0092] loss=13.4605 cls=0.0831 smmd=0.3692 ct=11.4404 rec=1.3253 | train/val/test=1.000/0.802/0.751 | c=0.998347
[Epoch 0093] loss=13.3708 cls=0.0947 smmd=0.3487 ct=11.4476 rec=1.3259 | train/val/test=1.000/0.802/0.765 | c=0.998347
[Epoch 0094] loss=13.4308 cls=0.1022 smmd=0.3594 ct=11.4499 rec=1.3268 | train/val/test=1.000/0.806/0.766 | c=0.998347
[Epoch 0095] loss=13.5587 cls=0.1249 smmd=0.3770 ct=11.4779 rec=1.3339 | train/val/test=1.000/0.812/0.777 | c=0.998347
[Epoch 0096] loss=14.2990 cls=0.1426 smmd=0.5163 ct=11.5123 rec=1.3412 | train/val/test=1.000/0.808/0.774 | c=0.998347
[Epoch 0097] loss=15.3445 cls=0.1489 smmd=0.7233 ct=11.5192 rec=1.3458 | train/val/test=1.000/0.778/0.765 | c=0.998347
[Epoch 0098] loss=15.3403 cls=0.1340 smmd=0.7270 ct=11.5041 rec=1.3428 | train/val/test=1.000/0.798/0.754 | c=0.998347
[Epoch 0099] loss=14.3408 cls=0.1087 smmd=0.5298 ct=11.5040 rec=1.3373 | train/val/test=1.000/0.746/0.740 | c=0.998347
=== Best @ epoch 63: val=0.8180, test=0.7770 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 - 2025-09-21 06:03:42:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.2523 cls=1.0962 smmd=5.6574 ct=11.2760 rec=1.4137 | train/val/test=0.385/0.214/0.200 | c=0.998347
[Epoch 0001] loss=30.4024 cls=1.0782 smmd=3.7018 ct=11.2128 rec=1.4136 | train/val/test=0.577/0.474/0.432 | c=0.998347
[Epoch 0002] loss=35.8662 cls=1.0471 smmd=4.7927 ct=11.2377 rec=1.4131 | train/val/test=0.538/0.486/0.486 | c=0.998347
[Epoch 0003] loss=35.0645 cls=0.9709 smmd=4.6568 ct=11.1539 rec=1.4089 | train/val/test=0.577/0.518/0.510 | c=0.998347
[Epoch 0004] loss=25.5205 cls=0.8700 smmd=2.7934 ct=10.9791 rec=1.3933 | train/val/test=0.615/0.510/0.509 | c=0.998347
[Epoch 0005] loss=26.2606 cls=0.7960 smmd=2.9565 ct=10.9428 rec=1.3748 | train/val/test=0.615/0.514/0.512 | c=0.998347
[Epoch 0006] loss=28.7688 cls=0.7350 smmd=3.4626 ct=10.9518 rec=1.3636 | train/val/test=0.654/0.566/0.552 | c=0.998347
[Epoch 0007] loss=26.8410 cls=0.6538 smmd=3.0899 ct=10.9290 rec=1.3566 | train/val/test=0.769/0.594/0.589 | c=0.998347
[Epoch 0008] loss=22.7826 cls=0.5601 smmd=2.1519 ct=11.6075 rec=1.3543 | train/val/test=0.846/0.644/0.637 | c=0.998347
[Epoch 0009] loss=23.1492 cls=0.4567 smmd=2.2414 ct=11.5781 rec=1.3584 | train/val/test=0.962/0.742/0.719 | c=0.998347
[Epoch 0010] loss=25.2314 cls=0.3664 smmd=2.6712 ct=11.5564 rec=1.3588 | train/val/test=0.962/0.754/0.743 | c=0.998347
[Epoch 0011] loss=23.5493 cls=0.3198 smmd=2.3473 ct=11.5171 rec=1.3603 | train/val/test=1.000/0.790/0.778 | c=0.998347
[Epoch 0012] loss=20.1338 cls=0.2578 smmd=1.6768 ct=11.4855 rec=1.3553 | train/val/test=1.000/0.764/0.736 | c=0.998347
[Epoch 0013] loss=23.6814 cls=0.2330 smmd=2.3603 ct=11.6280 rec=1.3536 | train/val/test=1.000/0.760/0.728 | c=0.998347
[Epoch 0014] loss=22.2939 cls=0.1900 smmd=2.0980 ct=11.5748 rec=1.3424 | train/val/test=1.000/0.758/0.717 | c=0.998347
[Epoch 0015] loss=19.9404 cls=0.1524 smmd=1.6486 ct=11.4877 rec=1.3321 | train/val/test=1.000/0.766/0.740 | c=0.998347
[Epoch 0016] loss=19.8358 cls=0.1148 smmd=1.6324 ct=11.4843 rec=1.3218 | train/val/test=1.000/0.776/0.742 | c=0.998347
[Epoch 0017] loss=20.3709 cls=0.0922 smmd=1.7315 ct=11.5356 rec=1.3170 | train/val/test=1.000/0.796/0.763 | c=0.998347
[Epoch 0018] loss=19.0656 cls=0.0720 smmd=1.4774 ct=11.5119 rec=1.3088 | train/val/test=1.000/0.784/0.760 | c=0.998347
[Epoch 0019] loss=18.1244 cls=0.0677 smmd=1.2930 ct=11.4950 rec=1.3027 | train/val/test=1.000/0.788/0.772 | c=0.998347
[Epoch 0020] loss=18.4339 cls=0.0692 smmd=1.3468 ct=11.5349 rec=1.3050 | train/val/test=1.000/0.808/0.781 | c=0.998347
[Epoch 0021] loss=18.7289 cls=0.0796 smmd=1.4086 ct=11.5152 rec=1.3094 | train/val/test=1.000/0.804/0.783 | c=0.998347
[Epoch 0022] loss=17.2786 cls=0.0889 smmd=1.1189 ct=11.5094 rec=1.3010 | train/val/test=1.000/0.808/0.781 | c=0.998347
[Epoch 0023] loss=18.4613 cls=0.0956 smmd=1.3450 ct=11.5584 rec=1.2991 | train/val/test=1.000/0.794/0.779 | c=0.998347
[Epoch 0024] loss=17.1507 cls=0.0986 smmd=1.0905 ct=11.5190 rec=1.2964 | train/val/test=1.000/0.808/0.785 | c=0.998347
[Epoch 0025] loss=17.3163 cls=0.0780 smmd=1.1292 ct=11.5017 rec=1.2961 | train/val/test=1.000/0.798/0.779 | c=0.998347
[Epoch 0026] loss=16.4493 cls=0.0683 smmd=0.9585 ct=11.4930 rec=1.2950 | train/val/test=1.000/0.792/0.769 | c=0.998347
[Epoch 0027] loss=16.7332 cls=0.0648 smmd=1.0106 ct=11.5182 rec=1.2933 | train/val/test=1.000/0.808/0.762 | c=0.998347
[Epoch 0028] loss=15.9109 cls=0.0673 smmd=0.8510 ct=11.4922 rec=1.3018 | train/val/test=1.000/0.788/0.754 | c=0.998347
[Epoch 0029] loss=16.1636 cls=0.0671 smmd=0.9075 ct=11.4629 rec=1.2975 | train/val/test=1.000/0.776/0.762 | c=0.998347
[Epoch 0030] loss=15.7529 cls=0.0700 smmd=0.8131 ct=11.5227 rec=1.2978 | train/val/test=1.000/0.788/0.764 | c=0.998347
[Epoch 0031] loss=16.0534 cls=0.0770 smmd=0.8706 ct=11.5314 rec=1.3050 | train/val/test=1.000/0.772/0.760 | c=0.998347
[Epoch 0032] loss=16.1186 cls=0.0755 smmd=0.8948 ct=11.4766 rec=1.3020 | train/val/test=1.000/0.788/0.742 | c=0.998347
[Epoch 0033] loss=15.8445 cls=0.0818 smmd=0.8293 ct=11.5257 rec=1.3148 | train/val/test=1.000/0.782/0.761 | c=0.998347
[Epoch 0034] loss=15.8322 cls=0.0703 smmd=0.8260 ct=11.5372 rec=1.2993 | train/val/test=1.000/0.792/0.764 | c=0.998347
[Epoch 0035] loss=15.3451 cls=0.0655 smmd=0.7378 ct=11.4927 rec=1.3080 | train/val/test=1.000/0.784/0.764 | c=0.998347
[Epoch 0036] loss=14.9898 cls=0.0706 smmd=0.6669 ct=11.4905 rec=1.2945 | train/val/test=1.000/0.800/0.777 | c=0.998347
[Epoch 0037] loss=15.1397 cls=0.0546 smmd=0.6935 ct=11.5156 rec=1.2915 | train/val/test=1.000/0.812/0.778 | c=0.998347
[Epoch 0038] loss=14.8492 cls=0.0567 smmd=0.6415 ct=11.4838 rec=1.2954 | train/val/test=1.000/0.812/0.779 | c=0.998347
[Epoch 0039] loss=14.8643 cls=0.0606 smmd=0.6415 ct=11.4966 rec=1.2982 | train/val/test=1.000/0.796/0.778 | c=0.998347
[Epoch 0040] loss=15.0109 cls=0.0690 smmd=0.6658 ct=11.5171 rec=1.3025 | train/val/test=1.000/0.778/0.753 | c=0.998347
[Epoch 0041] loss=15.2686 cls=0.0836 smmd=0.7126 ct=11.5317 rec=1.3193 | train/val/test=1.000/0.744/0.730 | c=0.998347
[Epoch 0042] loss=15.5183 cls=0.1069 smmd=0.7592 ct=11.5362 rec=1.3248 | train/val/test=1.000/0.696/0.679 | c=0.998347
[Epoch 0043] loss=15.0063 cls=0.1184 smmd=0.6556 ct=11.5350 rec=1.3413 | train/val/test=1.000/0.734/0.719 | c=0.998347
[Epoch 0044] loss=14.8463 cls=0.1070 smmd=0.6304 ct=11.5079 rec=1.3260 | train/val/test=1.000/0.712/0.694 | c=0.998347
[Epoch 0045] loss=14.6648 cls=0.0944 smmd=0.5888 ct=11.5409 rec=1.3267 | train/val/test=1.000/0.798/0.774 | c=0.998347
[Epoch 0046] loss=14.4694 cls=0.0653 smmd=0.5643 ct=11.4851 rec=1.3034 | train/val/test=1.000/0.800/0.755 | c=0.998347
[Epoch 0047] loss=14.2202 cls=0.0614 smmd=0.5168 ct=11.4751 rec=1.3041 | train/val/test=1.000/0.804/0.773 | c=0.998347
[Epoch 0048] loss=14.2870 cls=0.0630 smmd=0.5217 ct=11.5169 rec=1.3035 | train/val/test=1.000/0.812/0.762 | c=0.998347
[Epoch 0049] loss=14.5010 cls=0.0726 smmd=0.5687 ct=11.4897 rec=1.3139 | train/val/test=1.000/0.794/0.761 | c=0.998347
[Epoch 0050] loss=14.8094 cls=0.0861 smmd=0.6273 ct=11.4980 rec=1.3178 | train/val/test=1.000/0.774/0.747 | c=0.998347
[Epoch 0051] loss=15.1287 cls=0.0961 smmd=0.6807 ct=11.5440 rec=1.3295 | train/val/test=1.000/0.784/0.747 | c=0.998347
[Epoch 0052] loss=14.8214 cls=0.1000 smmd=0.6313 ct=11.4821 rec=1.3267 | train/val/test=1.000/0.798/0.756 | c=0.998347
[Epoch 0053] loss=14.4460 cls=0.0904 smmd=0.5530 ct=11.5036 rec=1.3246 | train/val/test=1.000/0.800/0.759 | c=0.998347
[Epoch 0054] loss=14.2442 cls=0.0891 smmd=0.5201 ct=11.4670 rec=1.3218 | train/val/test=1.000/0.802/0.755 | c=0.998347
[Epoch 0055] loss=14.0075 cls=0.0923 smmd=0.4721 ct=11.4683 rec=1.3244 | train/val/test=1.000/0.782/0.749 | c=0.998347
[Epoch 0056] loss=13.9650 cls=0.0971 smmd=0.4608 ct=11.4799 rec=1.3274 | train/val/test=1.000/0.770/0.739 | c=0.998347
[Epoch 0057] loss=14.1052 cls=0.1125 smmd=0.4863 ct=11.4840 rec=1.3364 | train/val/test=1.000/0.722/0.695 | c=0.998347
[Epoch 0058] loss=14.3566 cls=0.1320 smmd=0.5318 ct=11.4972 rec=1.3444 | train/val/test=1.000/0.674/0.644 | c=0.998347
[Epoch 0059] loss=14.9367 cls=0.1793 smmd=0.6290 ct=11.5656 rec=1.3668 | train/val/test=0.962/0.640/0.621 | c=0.998347
[Epoch 0060] loss=15.0231 cls=0.2058 smmd=0.6448 ct=11.5588 rec=1.3774 | train/val/test=0.923/0.598/0.592 | c=0.998347
[Epoch 0061] loss=15.0753 cls=0.2226 smmd=0.6429 ct=11.6123 rec=1.3746 | train/val/test=1.000/0.708/0.690 | c=0.998347
[Epoch 0062] loss=14.4565 cls=0.1322 smmd=0.5516 ct=11.4975 rec=1.3517 | train/val/test=1.000/0.802/0.766 | c=0.998347
[Epoch 0063] loss=13.8167 cls=0.0600 smmd=0.4346 ct=11.4825 rec=1.3100 | train/val/test=1.000/0.818/0.777 | c=0.998347
[Epoch 0064] loss=14.0847 cls=0.0523 smmd=0.4919 ct=11.4684 rec=1.3054 | train/val/test=1.000/0.788/0.759 | c=0.998347
[Epoch 0065] loss=13.6006 cls=0.0591 smmd=0.3947 ct=11.4667 rec=1.3097 | train/val/test=1.000/0.812/0.779 | c=0.998347
[Epoch 0066] loss=13.7253 cls=0.0575 smmd=0.4147 ct=11.4916 rec=1.3127 | train/val/test=1.000/0.816/0.773 | c=0.998347
[Epoch 0067] loss=14.2961 cls=0.0684 smmd=0.5284 ct=11.4876 rec=1.3222 | train/val/test=1.000/0.794/0.762 | c=0.998347
[Epoch 0068] loss=14.5737 cls=0.0832 smmd=0.5738 ct=11.5298 rec=1.3331 | train/val/test=1.000/0.792/0.749 | c=0.998347
[Epoch 0069] loss=15.0494 cls=0.0952 smmd=0.6762 ct=11.4869 rec=1.3383 | train/val/test=1.000/0.790/0.744 | c=0.998347
[Epoch 0070] loss=14.9281 cls=0.0964 smmd=0.6493 ct=11.4995 rec=1.3383 | train/val/test=1.000/0.750/0.715 | c=0.998347
[Epoch 0071] loss=14.2122 cls=0.1068 smmd=0.5047 ct=11.5017 rec=1.3366 | train/val/test=1.000/0.778/0.731 | c=0.998347
[Epoch 0072] loss=13.8543 cls=0.0939 smmd=0.4458 ct=11.4449 rec=1.3328 | train/val/test=1.000/0.776/0.734 | c=0.998347
[Epoch 0073] loss=13.6677 cls=0.0997 smmd=0.4071 ct=11.4492 rec=1.3311 | train/val/test=1.000/0.798/0.743 | c=0.998347
[Epoch 0074] loss=13.6067 cls=0.0979 smmd=0.3918 ct=11.4659 rec=1.3307 | train/val/test=1.000/0.788/0.752 | c=0.998347
[Epoch 0075] loss=13.6065 cls=0.1080 smmd=0.3936 ct=11.4510 rec=1.3336 | train/val/test=1.000/0.778/0.742 | c=0.998347
[Epoch 0076] loss=14.0003 cls=0.1249 smmd=0.4606 ct=11.5010 rec=1.3402 | train/val/test=1.000/0.734/0.720 | c=0.998347
[Epoch 0077] loss=14.9367 cls=0.1517 smmd=0.6419 ct=11.5164 rec=1.3515 | train/val/test=1.000/0.632/0.612 | c=0.998347
[Epoch 0078] loss=15.5706 cls=0.2007 smmd=0.7477 ct=11.5946 rec=1.3700 | train/val/test=0.846/0.596/0.594 | c=0.998347
[Epoch 0079] loss=15.5519 cls=0.2880 smmd=0.7366 ct=11.5847 rec=1.4013 | train/val/test=0.846/0.568/0.569 | c=0.998347
[Epoch 0080] loss=14.7987 cls=0.2266 smmd=0.5832 ct=11.6313 rec=1.3798 | train/val/test=1.000/0.738/0.727 | c=0.998347
[Epoch 0081] loss=14.3309 cls=0.0855 smmd=0.5407 ct=11.4526 rec=1.3187 | train/val/test=1.000/0.796/0.760 | c=0.998347
[Epoch 0082] loss=13.5780 cls=0.0428 smmd=0.3965 ct=11.4444 rec=1.2951 | train/val/test=1.000/0.740/0.703 | c=0.998347
[Epoch 0083] loss=13.8802 cls=0.0575 smmd=0.4418 ct=11.5105 rec=1.3206 | train/val/test=1.000/0.772/0.750 | c=0.998347
[Epoch 0084] loss=13.7887 cls=0.0472 smmd=0.4370 ct=11.4490 rec=1.3084 | train/val/test=1.000/0.796/0.755 | c=0.998347
[Epoch 0085] loss=13.7010 cls=0.0471 smmd=0.4122 ct=11.4855 rec=1.3120 | train/val/test=1.000/0.768/0.725 | c=0.998347
[Epoch 0086] loss=14.2807 cls=0.0685 smmd=0.5206 ct=11.5103 rec=1.3292 | train/val/test=1.000/0.724/0.707 | c=0.998347
[Epoch 0087] loss=15.0413 cls=0.0941 smmd=0.6660 ct=11.5300 rec=1.3423 | train/val/test=1.000/0.760/0.718 | c=0.998347
[Epoch 0088] loss=15.4190 cls=0.0981 smmd=0.7410 ct=11.5307 rec=1.3427 | train/val/test=1.000/0.752/0.706 | c=0.998347
[Epoch 0089] loss=14.7537 cls=0.0983 smmd=0.6156 ct=11.4925 rec=1.3395 | train/val/test=1.000/0.756/0.720 | c=0.998347
[Epoch 0090] loss=13.8785 cls=0.0906 smmd=0.4430 ct=11.4848 rec=1.3349 | train/val/test=1.000/0.744/0.704 | c=0.998347
[Epoch 0091] loss=13.7591 cls=0.0965 smmd=0.4261 ct=11.4468 rec=1.3329 | train/val/test=1.000/0.784/0.757 | c=0.998347
[Epoch 0092] loss=13.4605 cls=0.0831 smmd=0.3692 ct=11.4404 rec=1.3253 | train/val/test=1.000/0.802/0.751 | c=0.998347
[Epoch 0093] loss=13.3708 cls=0.0947 smmd=0.3487 ct=11.4476 rec=1.3259 | train/val/test=1.000/0.802/0.765 | c=0.998347
[Epoch 0094] loss=13.4308 cls=0.1022 smmd=0.3594 ct=11.4499 rec=1.3268 | train/val/test=1.000/0.806/0.766 | c=0.998347
[Epoch 0095] loss=13.5587 cls=0.1249 smmd=0.3770 ct=11.4779 rec=1.3339 | train/val/test=1.000/0.812/0.777 | c=0.998347
[Epoch 0096] loss=14.2990 cls=0.1426 smmd=0.5163 ct=11.5123 rec=1.3412 | train/val/test=1.000/0.808/0.774 | c=0.998347
[Epoch 0097] loss=15.3445 cls=0.1489 smmd=0.7233 ct=11.5192 rec=1.3458 | train/val/test=1.000/0.778/0.765 | c=0.998347
[Epoch 0098] loss=15.3403 cls=0.1340 smmd=0.7270 ct=11.5041 rec=1.3428 | train/val/test=1.000/0.798/0.754 | c=0.998347
[Epoch 0099] loss=14.3408 cls=0.1087 smmd=0.5298 ct=11.5040 rec=1.3373 | train/val/test=1.000/0.746/0.740 | c=0.998347
=== Best @ epoch 63: val=0.8180, test=0.7770 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 completed in 189.64 seconds.
==================================================
