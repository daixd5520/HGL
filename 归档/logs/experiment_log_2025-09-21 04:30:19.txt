Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4 - 2025-09-21 04:30:19:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2032 cls=1.9466 smmd=4.2201 ct=9.2587 rec=1.3889 | train/val/test=0.190/0.074/0.098 | c=0.998437
[Epoch 0001] loss=16.7179 cls=1.9168 smmd=2.8156 ct=9.2064 rec=1.3896 | train/val/test=0.655/0.318/0.326 | c=0.998437
[Epoch 0002] loss=15.1776 cls=1.8058 smmd=1.5014 ct=9.0929 rec=1.3887 | train/val/test=0.603/0.310/0.294 | c=0.998437
[Epoch 0003] loss=14.8883 cls=1.6108 smmd=1.4547 ct=9.0480 rec=1.3875 | train/val/test=0.517/0.298/0.304 | c=0.998437
[Epoch 0004] loss=14.8274 cls=1.3620 smmd=1.7127 ct=8.9903 rec=1.3812 | train/val/test=0.897/0.566/0.566 | c=0.998437
[Epoch 0005] loss=14.2798 cls=1.0013 smmd=1.6655 ct=8.8793 rec=1.3668 | train/val/test=0.897/0.508/0.525 | c=0.998437
[Epoch 0006] loss=13.6563 cls=0.7422 smmd=1.3758 ct=8.8504 rec=1.3439 | train/val/test=0.931/0.544/0.516 | c=0.998437
[Epoch 0007] loss=13.0410 cls=0.5304 smmd=1.0700 ct=8.8135 rec=1.3136 | train/val/test=0.948/0.614/0.603 | c=0.998437
[Epoch 0008] loss=12.7343 cls=0.3650 smmd=0.9845 ct=8.8102 rec=1.2873 | train/val/test=0.966/0.654/0.645 | c=0.998437
[Epoch 0009] loss=12.6408 cls=0.2334 smmd=1.0790 ct=8.8056 rec=1.2614 | train/val/test=0.983/0.698/0.679 | c=0.998437
[Epoch 0010] loss=12.5496 cls=0.1521 smmd=1.1243 ct=8.7965 rec=1.2384 | train/val/test=0.983/0.682/0.683 | c=0.998437
[Epoch 0011] loss=12.3971 cls=0.0996 smmd=1.0708 ct=8.7845 rec=1.2212 | train/val/test=0.983/0.688/0.675 | c=0.998437
[Epoch 0012] loss=12.1580 cls=0.0687 smmd=0.8954 ct=8.7765 rec=1.2088 | train/val/test=0.983/0.694/0.672 | c=0.998437
[Epoch 0013] loss=11.9729 cls=0.0473 smmd=0.7569 ct=8.7720 rec=1.1984 | train/val/test=0.983/0.688/0.667 | c=0.998437
[Epoch 0014] loss=11.8888 cls=0.0352 smmd=0.6964 ct=8.7734 rec=1.1919 | train/val/test=1.000/0.698/0.677 | c=0.998437
[Epoch 0015] loss=11.8595 cls=0.0252 smmd=0.6865 ct=8.7736 rec=1.1871 | train/val/test=1.000/0.702/0.691 | c=0.998437
[Epoch 0016] loss=11.8513 cls=0.0174 smmd=0.6927 ct=8.7731 rec=1.1841 | train/val/test=1.000/0.700/0.690 | c=0.998437
[Epoch 0017] loss=11.7282 cls=0.0136 smmd=0.5745 ct=8.7739 rec=1.1831 | train/val/test=1.000/0.706/0.679 | c=0.998437
[Epoch 0018] loss=11.6071 cls=0.0110 smmd=0.4543 ct=8.7756 rec=1.1831 | train/val/test=1.000/0.706/0.678 | c=0.998437
[Epoch 0019] loss=11.5610 cls=0.0113 smmd=0.3976 ct=8.7830 rec=1.1845 | train/val/test=1.000/0.706/0.685 | c=0.998437
[Epoch 0020] loss=11.5818 cls=0.0120 smmd=0.4097 ct=8.7909 rec=1.1846 | train/val/test=1.000/0.706/0.687 | c=0.998437
[Epoch 0021] loss=11.5237 cls=0.0137 smmd=0.3395 ct=8.7988 rec=1.1858 | train/val/test=1.000/0.718/0.686 | c=0.998437
[Epoch 0022] loss=11.4533 cls=0.0161 smmd=0.2578 ct=8.8046 rec=1.1874 | train/val/test=1.000/0.718/0.698 | c=0.998437
[Epoch 0023] loss=11.4125 cls=0.0191 smmd=0.2147 ct=8.8056 rec=1.1866 | train/val/test=1.000/0.720/0.695 | c=0.998437
[Epoch 0024] loss=11.4110 cls=0.0223 smmd=0.2126 ct=8.8062 rec=1.1849 | train/val/test=1.000/0.708/0.692 | c=0.998437
[Epoch 0025] loss=11.3967 cls=0.0253 smmd=0.1936 ct=8.8102 rec=1.1838 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0026] loss=11.3599 cls=0.0279 smmd=0.1544 ct=8.8146 rec=1.1815 | train/val/test=1.000/0.714/0.697 | c=0.998437
[Epoch 0027] loss=11.3268 cls=0.0298 smmd=0.1249 ct=8.8164 rec=1.1779 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0028] loss=11.3270 cls=0.0318 smmd=0.1311 ct=8.8132 rec=1.1755 | train/val/test=1.000/0.714/0.702 | c=0.998437
[Epoch 0029] loss=11.3074 cls=0.0304 smmd=0.1266 ct=8.8068 rec=1.1718 | train/val/test=1.000/0.708/0.700 | c=0.998437
[Epoch 0030] loss=11.2739 cls=0.0300 smmd=0.1030 ct=8.8033 rec=1.1688 | train/val/test=1.000/0.712/0.703 | c=0.998437
[Epoch 0031] loss=11.2571 cls=0.0280 smmd=0.0942 ct=8.8020 rec=1.1665 | train/val/test=1.000/0.712/0.706 | c=0.998437
[Epoch 0032] loss=11.9639 cls=0.0252 smmd=0.0932 ct=9.5154 rec=1.1650 | train/val/test=1.000/0.704/0.696 | c=0.998437
[Epoch 0033] loss=11.9436 cls=0.0279 smmd=0.1729 ct=9.4115 rec=1.1656 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0034] loss=11.9329 cls=0.0218 smmd=0.1745 ct=9.4102 rec=1.1632 | train/val/test=1.000/0.716/0.699 | c=0.998437
[Epoch 0035] loss=11.8964 cls=0.0204 smmd=0.0981 ct=9.4518 rec=1.1631 | train/val/test=1.000/0.716/0.700 | c=0.998437
[Epoch 0036] loss=11.8835 cls=0.0199 smmd=0.0791 ct=9.4574 rec=1.1636 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0037] loss=11.8797 cls=0.0193 smmd=0.0980 ct=9.4353 rec=1.1636 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0038] loss=11.8613 cls=0.0195 smmd=0.0950 ct=9.4157 rec=1.1655 | train/val/test=1.000/0.716/0.709 | c=0.998437
[Epoch 0039] loss=11.8371 cls=0.0182 smmd=0.0672 ct=9.4238 rec=1.1640 | train/val/test=1.000/0.720/0.702 | c=0.998437
[Epoch 0040] loss=11.8181 cls=0.0178 smmd=0.0584 ct=9.4142 rec=1.1639 | train/val/test=1.000/0.718/0.703 | c=0.998437
[Epoch 0041] loss=11.8183 cls=0.0182 smmd=0.0703 ct=9.4010 rec=1.1643 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0042] loss=11.7994 cls=0.0182 smmd=0.0510 ct=9.4024 rec=1.1639 | train/val/test=1.000/0.724/0.703 | c=0.998437
[Epoch 0043] loss=11.7933 cls=0.0201 smmd=0.0535 ct=9.3908 rec=1.1644 | train/val/test=1.000/0.718/0.705 | c=0.998437
[Epoch 0044] loss=11.7776 cls=0.0191 smmd=0.0420 ct=9.3903 rec=1.1631 | train/val/test=1.000/0.718/0.703 | c=0.998437
[Epoch 0045] loss=11.7614 cls=0.0199 smmd=0.0387 ct=9.3763 rec=1.1633 | train/val/test=1.000/0.720/0.704 | c=0.998437
[Epoch 0046] loss=11.7587 cls=0.0219 smmd=0.0404 ct=9.3688 rec=1.1638 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0047] loss=11.7520 cls=0.0212 smmd=0.0297 ct=9.3728 rec=1.1641 | train/val/test=1.000/0.720/0.705 | c=0.998437
[Epoch 0048] loss=11.7457 cls=0.0234 smmd=0.0324 ct=9.3598 rec=1.1650 | train/val/test=1.000/0.720/0.706 | c=0.998437
[Epoch 0049] loss=11.7464 cls=0.0229 smmd=0.0379 ct=9.3558 rec=1.1649 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0050] loss=11.7434 cls=0.0239 smmd=0.0395 ct=9.3470 rec=1.1665 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0051] loss=11.7357 cls=0.0238 smmd=0.0253 ct=9.3546 rec=1.1660 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0052] loss=11.7394 cls=0.0248 smmd=0.0348 ct=9.3441 rec=1.1678 | train/val/test=1.000/0.722/0.718 | c=0.998437
[Epoch 0053] loss=11.7411 cls=0.0235 smmd=0.0333 ct=9.3507 rec=1.1668 | train/val/test=1.000/0.722/0.704 | c=0.998437
[Epoch 0054] loss=11.7372 cls=0.0243 smmd=0.0434 ct=9.3352 rec=1.1672 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0055] loss=11.7224 cls=0.0205 smmd=0.0277 ct=9.3451 rec=1.1645 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0056] loss=11.6995 cls=0.0197 smmd=0.0220 ct=9.3306 rec=1.1636 | train/val/test=1.000/0.722/0.704 | c=0.998437
[Epoch 0057] loss=11.6991 cls=0.0197 smmd=0.0210 ct=9.3307 rec=1.1638 | train/val/test=1.000/0.720/0.707 | c=0.998437
[Epoch 0058] loss=11.7049 cls=0.0198 smmd=0.0203 ct=9.3343 rec=1.1653 | train/val/test=1.000/0.722/0.705 | c=0.998437
[Epoch 0059] loss=11.7100 cls=0.0221 smmd=0.0262 ct=9.3275 rec=1.1671 | train/val/test=1.000/0.724/0.707 | c=0.998437
[Epoch 0060] loss=11.7046 cls=0.0215 smmd=0.0140 ct=9.3348 rec=1.1671 | train/val/test=1.000/0.718/0.704 | c=0.998437
[Epoch 0061] loss=11.7013 cls=0.0223 smmd=0.0241 ct=9.3192 rec=1.1679 | train/val/test=1.000/0.722/0.705 | c=0.998437
[Epoch 0062] loss=11.6943 cls=0.0222 smmd=0.0149 ct=9.3231 rec=1.1671 | train/val/test=1.000/0.716/0.701 | c=0.998437
[Epoch 0063] loss=11.6933 cls=0.0222 smmd=0.0143 ct=9.3212 rec=1.1678 | train/val/test=1.000/0.714/0.704 | c=0.998437
[Epoch 0064] loss=11.6855 cls=0.0227 smmd=0.0063 ct=9.3230 rec=1.1668 | train/val/test=1.000/0.720/0.703 | c=0.998437
[Epoch 0065] loss=11.6885 cls=0.0225 smmd=0.0170 ct=9.3148 rec=1.1671 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0066] loss=11.6875 cls=0.0234 smmd=0.0137 ct=9.3167 rec=1.1668 | train/val/test=1.000/0.716/0.708 | c=0.998437
[Epoch 0067] loss=11.6880 cls=0.0229 smmd=0.0183 ct=9.3126 rec=1.1671 | train/val/test=1.000/0.714/0.708 | c=0.998437
[Epoch 0068] loss=11.6833 cls=0.0239 smmd=0.0085 ct=9.3175 rec=1.1668 | train/val/test=1.000/0.722/0.708 | c=0.998437
[Epoch 0069] loss=11.6860 cls=0.0224 smmd=0.0164 ct=9.3141 rec=1.1665 | train/val/test=1.000/0.716/0.703 | c=0.998437
[Epoch 0070] loss=11.6798 cls=0.0234 smmd=0.0165 ct=9.3073 rec=1.1663 | train/val/test=1.000/0.716/0.711 | c=0.998437
[Epoch 0071] loss=11.6778 cls=0.0220 smmd=0.0131 ct=9.3104 rec=1.1661 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0072] loss=11.6777 cls=0.0229 smmd=0.0168 ct=9.3046 rec=1.1667 | train/val/test=1.000/0.710/0.715 | c=0.998437
[Epoch 0073] loss=11.6789 cls=0.0232 smmd=0.0077 ct=9.3139 rec=1.1671 | train/val/test=1.000/0.712/0.697 | c=0.998437
[Epoch 0074] loss=11.6927 cls=0.0249 smmd=0.0284 ct=9.3006 rec=1.1694 | train/val/test=1.000/0.718/0.717 | c=0.998437
[Epoch 0075] loss=11.7107 cls=0.0278 smmd=0.0210 ct=9.3208 rec=1.1705 | train/val/test=1.000/0.706/0.692 | c=0.998437
[Epoch 0076] loss=11.7392 cls=0.0314 smmd=0.0502 ct=9.3068 rec=1.1754 | train/val/test=1.000/0.722/0.721 | c=0.998437
[Epoch 0077] loss=11.7457 cls=0.0314 smmd=0.0362 ct=9.3315 rec=1.1733 | train/val/test=1.000/0.712/0.696 | c=0.998437
[Epoch 0078] loss=11.7060 cls=0.0244 smmd=0.0421 ct=9.3011 rec=1.1692 | train/val/test=1.000/0.720/0.711 | c=0.998437
[Epoch 0079] loss=11.6668 cls=0.0174 smmd=0.0219 ct=9.3011 rec=1.1632 | train/val/test=1.000/0.712/0.709 | c=0.998437
[Epoch 0080] loss=11.6847 cls=0.0211 smmd=0.0207 ct=9.3144 rec=1.1642 | train/val/test=1.000/0.714/0.696 | c=0.998437
[Epoch 0081] loss=11.6887 cls=0.0205 smmd=0.0390 ct=9.2971 rec=1.1661 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0082] loss=11.6606 cls=0.0173 smmd=0.0158 ct=9.3010 rec=1.1633 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0083] loss=11.6645 cls=0.0199 smmd=0.0124 ct=9.3011 rec=1.1655 | train/val/test=1.000/0.716/0.701 | c=0.998437
[Epoch 0084] loss=11.6937 cls=0.0228 smmd=0.0362 ct=9.2931 rec=1.1707 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0085] loss=11.6789 cls=0.0246 smmd=0.0087 ct=9.3050 rec=1.1703 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0086] loss=11.6579 cls=0.0218 smmd=0.0108 ct=9.2876 rec=1.1688 | train/val/test=1.000/0.710/0.703 | c=0.998437
[Epoch 0087] loss=11.6521 cls=0.0219 smmd=0.0047 ct=9.2881 rec=1.1687 | train/val/test=1.000/0.714/0.706 | c=0.998437
[Epoch 0088] loss=11.6695 cls=0.0247 smmd=0.0096 ct=9.2967 rec=1.1692 | train/val/test=1.000/0.712/0.695 | c=0.998437
[Epoch 0089] loss=11.6745 cls=0.0262 smmd=0.0213 ct=9.2849 rec=1.1711 | train/val/test=1.000/0.712/0.714 | c=0.998437
[Epoch 0090] loss=11.6677 cls=0.0242 smmd=0.0091 ct=9.2985 rec=1.1679 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0091] loss=11.6511 cls=0.0230 smmd=0.0141 ct=9.2811 rec=1.1664 | train/val/test=1.000/0.720/0.709 | c=0.998437
[Epoch 0092] loss=11.6533 cls=0.0216 smmd=0.0130 ct=9.2854 rec=1.1666 | train/val/test=1.000/0.714/0.706 | c=0.998437
[Epoch 0093] loss=11.6581 cls=0.0244 smmd=0.0116 ct=9.2881 rec=1.1670 | train/val/test=1.000/0.720/0.706 | c=0.998437
[Epoch 0094] loss=11.6571 cls=0.0226 smmd=0.0167 ct=9.2813 rec=1.1682 | train/val/test=1.000/0.710/0.702 | c=0.998437
[Epoch 0095] loss=11.6537 cls=0.0233 smmd=0.0076 ct=9.2871 rec=1.1679 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0096] loss=11.6469 cls=0.0221 smmd=0.0112 ct=9.2790 rec=1.1673 | train/val/test=1.000/0.708/0.697 | c=0.998437
[Epoch 0097] loss=11.6478 cls=0.0230 smmd=0.0129 ct=9.2747 rec=1.1686 | train/val/test=1.000/0.718/0.716 | c=0.998437
[Epoch 0098] loss=11.6596 cls=0.0236 smmd=0.0095 ct=9.2901 rec=1.1682 | train/val/test=1.000/0.716/0.700 | c=0.998437
[Epoch 0099] loss=11.6593 cls=0.0244 smmd=0.0239 ct=9.2729 rec=1.1690 | train/val/test=1.000/0.714/0.714 | c=0.998437
=== Best @ epoch 42: val=0.7240, test=0.7030 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4 - 2025-09-21 04:30:19:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2032 cls=1.9466 smmd=4.2201 ct=9.2587 rec=1.3889 | train/val/test=0.190/0.074/0.098 | c=0.998437
[Epoch 0001] loss=16.7179 cls=1.9168 smmd=2.8156 ct=9.2064 rec=1.3896 | train/val/test=0.655/0.318/0.326 | c=0.998437
[Epoch 0002] loss=15.1776 cls=1.8058 smmd=1.5014 ct=9.0929 rec=1.3887 | train/val/test=0.603/0.310/0.294 | c=0.998437
[Epoch 0003] loss=14.8883 cls=1.6108 smmd=1.4547 ct=9.0480 rec=1.3875 | train/val/test=0.517/0.298/0.304 | c=0.998437
[Epoch 0004] loss=14.8274 cls=1.3620 smmd=1.7127 ct=8.9903 rec=1.3812 | train/val/test=0.897/0.566/0.566 | c=0.998437
[Epoch 0005] loss=14.2798 cls=1.0013 smmd=1.6655 ct=8.8793 rec=1.3668 | train/val/test=0.897/0.508/0.525 | c=0.998437
[Epoch 0006] loss=13.6563 cls=0.7422 smmd=1.3758 ct=8.8504 rec=1.3439 | train/val/test=0.931/0.544/0.516 | c=0.998437
[Epoch 0007] loss=13.0410 cls=0.5304 smmd=1.0700 ct=8.8135 rec=1.3136 | train/val/test=0.948/0.614/0.603 | c=0.998437
[Epoch 0008] loss=12.7343 cls=0.3650 smmd=0.9845 ct=8.8102 rec=1.2873 | train/val/test=0.966/0.654/0.645 | c=0.998437
[Epoch 0009] loss=12.6408 cls=0.2334 smmd=1.0790 ct=8.8056 rec=1.2614 | train/val/test=0.983/0.698/0.679 | c=0.998437
[Epoch 0010] loss=12.5496 cls=0.1521 smmd=1.1243 ct=8.7965 rec=1.2384 | train/val/test=0.983/0.682/0.683 | c=0.998437
[Epoch 0011] loss=12.3971 cls=0.0996 smmd=1.0708 ct=8.7845 rec=1.2212 | train/val/test=0.983/0.688/0.675 | c=0.998437
[Epoch 0012] loss=12.1580 cls=0.0687 smmd=0.8954 ct=8.7765 rec=1.2088 | train/val/test=0.983/0.694/0.672 | c=0.998437
[Epoch 0013] loss=11.9729 cls=0.0473 smmd=0.7569 ct=8.7720 rec=1.1984 | train/val/test=0.983/0.688/0.667 | c=0.998437
[Epoch 0014] loss=11.8888 cls=0.0352 smmd=0.6964 ct=8.7734 rec=1.1919 | train/val/test=1.000/0.698/0.677 | c=0.998437
[Epoch 0015] loss=11.8595 cls=0.0252 smmd=0.6865 ct=8.7736 rec=1.1871 | train/val/test=1.000/0.702/0.691 | c=0.998437
[Epoch 0016] loss=11.8513 cls=0.0174 smmd=0.6927 ct=8.7731 rec=1.1841 | train/val/test=1.000/0.700/0.690 | c=0.998437
[Epoch 0017] loss=11.7282 cls=0.0136 smmd=0.5745 ct=8.7739 rec=1.1831 | train/val/test=1.000/0.706/0.679 | c=0.998437
[Epoch 0018] loss=11.6071 cls=0.0110 smmd=0.4543 ct=8.7756 rec=1.1831 | train/val/test=1.000/0.706/0.678 | c=0.998437
[Epoch 0019] loss=11.5610 cls=0.0113 smmd=0.3976 ct=8.7830 rec=1.1845 | train/val/test=1.000/0.706/0.685 | c=0.998437
[Epoch 0020] loss=11.5818 cls=0.0120 smmd=0.4097 ct=8.7909 rec=1.1846 | train/val/test=1.000/0.706/0.687 | c=0.998437
[Epoch 0021] loss=11.5237 cls=0.0137 smmd=0.3395 ct=8.7988 rec=1.1858 | train/val/test=1.000/0.718/0.686 | c=0.998437
[Epoch 0022] loss=11.4533 cls=0.0161 smmd=0.2578 ct=8.8046 rec=1.1874 | train/val/test=1.000/0.718/0.698 | c=0.998437
[Epoch 0023] loss=11.4125 cls=0.0191 smmd=0.2147 ct=8.8056 rec=1.1866 | train/val/test=1.000/0.720/0.695 | c=0.998437
[Epoch 0024] loss=11.4110 cls=0.0223 smmd=0.2126 ct=8.8062 rec=1.1849 | train/val/test=1.000/0.708/0.692 | c=0.998437
[Epoch 0025] loss=11.3967 cls=0.0253 smmd=0.1936 ct=8.8102 rec=1.1838 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0026] loss=11.3599 cls=0.0279 smmd=0.1544 ct=8.8146 rec=1.1815 | train/val/test=1.000/0.714/0.697 | c=0.998437
[Epoch 0027] loss=11.3268 cls=0.0298 smmd=0.1249 ct=8.8164 rec=1.1779 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0028] loss=11.3270 cls=0.0318 smmd=0.1311 ct=8.8132 rec=1.1755 | train/val/test=1.000/0.714/0.702 | c=0.998437
[Epoch 0029] loss=11.3074 cls=0.0304 smmd=0.1266 ct=8.8068 rec=1.1718 | train/val/test=1.000/0.708/0.700 | c=0.998437
[Epoch 0030] loss=11.2739 cls=0.0300 smmd=0.1030 ct=8.8033 rec=1.1688 | train/val/test=1.000/0.712/0.703 | c=0.998437
[Epoch 0031] loss=11.2571 cls=0.0280 smmd=0.0942 ct=8.8020 rec=1.1665 | train/val/test=1.000/0.712/0.706 | c=0.998437
[Epoch 0032] loss=11.9639 cls=0.0252 smmd=0.0932 ct=9.5154 rec=1.1650 | train/val/test=1.000/0.704/0.696 | c=0.998437
[Epoch 0033] loss=11.9436 cls=0.0279 smmd=0.1729 ct=9.4115 rec=1.1656 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0034] loss=11.9329 cls=0.0218 smmd=0.1745 ct=9.4102 rec=1.1632 | train/val/test=1.000/0.716/0.699 | c=0.998437
[Epoch 0035] loss=11.8964 cls=0.0204 smmd=0.0981 ct=9.4518 rec=1.1631 | train/val/test=1.000/0.716/0.700 | c=0.998437
[Epoch 0036] loss=11.8835 cls=0.0199 smmd=0.0791 ct=9.4574 rec=1.1636 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0037] loss=11.8797 cls=0.0193 smmd=0.0980 ct=9.4353 rec=1.1636 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0038] loss=11.8613 cls=0.0195 smmd=0.0950 ct=9.4157 rec=1.1655 | train/val/test=1.000/0.716/0.709 | c=0.998437
[Epoch 0039] loss=11.8371 cls=0.0182 smmd=0.0672 ct=9.4238 rec=1.1640 | train/val/test=1.000/0.720/0.702 | c=0.998437
[Epoch 0040] loss=11.8181 cls=0.0178 smmd=0.0584 ct=9.4142 rec=1.1639 | train/val/test=1.000/0.718/0.703 | c=0.998437
[Epoch 0041] loss=11.8183 cls=0.0182 smmd=0.0703 ct=9.4010 rec=1.1643 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0042] loss=11.7994 cls=0.0182 smmd=0.0510 ct=9.4024 rec=1.1639 | train/val/test=1.000/0.724/0.703 | c=0.998437
[Epoch 0043] loss=11.7933 cls=0.0201 smmd=0.0535 ct=9.3908 rec=1.1644 | train/val/test=1.000/0.718/0.705 | c=0.998437
[Epoch 0044] loss=11.7776 cls=0.0191 smmd=0.0420 ct=9.3903 rec=1.1631 | train/val/test=1.000/0.718/0.703 | c=0.998437
[Epoch 0045] loss=11.7614 cls=0.0199 smmd=0.0387 ct=9.3763 rec=1.1633 | train/val/test=1.000/0.720/0.704 | c=0.998437
[Epoch 0046] loss=11.7587 cls=0.0219 smmd=0.0404 ct=9.3688 rec=1.1638 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0047] loss=11.7520 cls=0.0212 smmd=0.0297 ct=9.3728 rec=1.1641 | train/val/test=1.000/0.720/0.705 | c=0.998437
[Epoch 0048] loss=11.7457 cls=0.0234 smmd=0.0324 ct=9.3598 rec=1.1650 | train/val/test=1.000/0.720/0.706 | c=0.998437
[Epoch 0049] loss=11.7464 cls=0.0229 smmd=0.0379 ct=9.3558 rec=1.1649 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0050] loss=11.7434 cls=0.0239 smmd=0.0395 ct=9.3470 rec=1.1665 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0051] loss=11.7357 cls=0.0238 smmd=0.0253 ct=9.3546 rec=1.1660 | train/val/test=1.000/0.718/0.702 | c=0.998437
[Epoch 0052] loss=11.7394 cls=0.0248 smmd=0.0348 ct=9.3441 rec=1.1678 | train/val/test=1.000/0.722/0.718 | c=0.998437
[Epoch 0053] loss=11.7411 cls=0.0235 smmd=0.0333 ct=9.3507 rec=1.1668 | train/val/test=1.000/0.722/0.704 | c=0.998437
[Epoch 0054] loss=11.7372 cls=0.0243 smmd=0.0434 ct=9.3352 rec=1.1672 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0055] loss=11.7224 cls=0.0205 smmd=0.0277 ct=9.3451 rec=1.1645 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0056] loss=11.6995 cls=0.0197 smmd=0.0220 ct=9.3306 rec=1.1636 | train/val/test=1.000/0.722/0.704 | c=0.998437
[Epoch 0057] loss=11.6991 cls=0.0197 smmd=0.0210 ct=9.3307 rec=1.1638 | train/val/test=1.000/0.720/0.707 | c=0.998437
[Epoch 0058] loss=11.7049 cls=0.0198 smmd=0.0203 ct=9.3343 rec=1.1653 | train/val/test=1.000/0.722/0.705 | c=0.998437
[Epoch 0059] loss=11.7100 cls=0.0221 smmd=0.0262 ct=9.3275 rec=1.1671 | train/val/test=1.000/0.724/0.707 | c=0.998437
[Epoch 0060] loss=11.7046 cls=0.0215 smmd=0.0140 ct=9.3348 rec=1.1671 | train/val/test=1.000/0.718/0.704 | c=0.998437
[Epoch 0061] loss=11.7013 cls=0.0223 smmd=0.0241 ct=9.3192 rec=1.1679 | train/val/test=1.000/0.722/0.705 | c=0.998437
[Epoch 0062] loss=11.6943 cls=0.0222 smmd=0.0149 ct=9.3231 rec=1.1671 | train/val/test=1.000/0.716/0.701 | c=0.998437
[Epoch 0063] loss=11.6933 cls=0.0222 smmd=0.0143 ct=9.3212 rec=1.1678 | train/val/test=1.000/0.714/0.704 | c=0.998437
[Epoch 0064] loss=11.6855 cls=0.0227 smmd=0.0063 ct=9.3230 rec=1.1668 | train/val/test=1.000/0.720/0.703 | c=0.998437
[Epoch 0065] loss=11.6885 cls=0.0225 smmd=0.0170 ct=9.3148 rec=1.1671 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0066] loss=11.6875 cls=0.0234 smmd=0.0137 ct=9.3167 rec=1.1668 | train/val/test=1.000/0.716/0.708 | c=0.998437
[Epoch 0067] loss=11.6880 cls=0.0229 smmd=0.0183 ct=9.3126 rec=1.1671 | train/val/test=1.000/0.714/0.708 | c=0.998437
[Epoch 0068] loss=11.6833 cls=0.0239 smmd=0.0085 ct=9.3175 rec=1.1668 | train/val/test=1.000/0.722/0.708 | c=0.998437
[Epoch 0069] loss=11.6860 cls=0.0224 smmd=0.0164 ct=9.3141 rec=1.1665 | train/val/test=1.000/0.716/0.703 | c=0.998437
[Epoch 0070] loss=11.6798 cls=0.0234 smmd=0.0165 ct=9.3073 rec=1.1663 | train/val/test=1.000/0.716/0.711 | c=0.998437
[Epoch 0071] loss=11.6778 cls=0.0220 smmd=0.0131 ct=9.3104 rec=1.1661 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0072] loss=11.6777 cls=0.0229 smmd=0.0168 ct=9.3046 rec=1.1667 | train/val/test=1.000/0.710/0.715 | c=0.998437
[Epoch 0073] loss=11.6789 cls=0.0232 smmd=0.0077 ct=9.3139 rec=1.1671 | train/val/test=1.000/0.712/0.697 | c=0.998437
[Epoch 0074] loss=11.6927 cls=0.0249 smmd=0.0284 ct=9.3006 rec=1.1694 | train/val/test=1.000/0.718/0.717 | c=0.998437
[Epoch 0075] loss=11.7107 cls=0.0278 smmd=0.0210 ct=9.3208 rec=1.1705 | train/val/test=1.000/0.706/0.692 | c=0.998437
[Epoch 0076] loss=11.7392 cls=0.0314 smmd=0.0502 ct=9.3068 rec=1.1754 | train/val/test=1.000/0.722/0.721 | c=0.998437
[Epoch 0077] loss=11.7457 cls=0.0314 smmd=0.0362 ct=9.3315 rec=1.1733 | train/val/test=1.000/0.712/0.696 | c=0.998437
[Epoch 0078] loss=11.7060 cls=0.0244 smmd=0.0421 ct=9.3011 rec=1.1692 | train/val/test=1.000/0.720/0.711 | c=0.998437
[Epoch 0079] loss=11.6668 cls=0.0174 smmd=0.0219 ct=9.3011 rec=1.1632 | train/val/test=1.000/0.712/0.709 | c=0.998437
[Epoch 0080] loss=11.6847 cls=0.0211 smmd=0.0207 ct=9.3144 rec=1.1642 | train/val/test=1.000/0.714/0.696 | c=0.998437
[Epoch 0081] loss=11.6887 cls=0.0205 smmd=0.0390 ct=9.2971 rec=1.1661 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0082] loss=11.6606 cls=0.0173 smmd=0.0158 ct=9.3010 rec=1.1633 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0083] loss=11.6645 cls=0.0199 smmd=0.0124 ct=9.3011 rec=1.1655 | train/val/test=1.000/0.716/0.701 | c=0.998437
[Epoch 0084] loss=11.6937 cls=0.0228 smmd=0.0362 ct=9.2931 rec=1.1707 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0085] loss=11.6789 cls=0.0246 smmd=0.0087 ct=9.3050 rec=1.1703 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0086] loss=11.6579 cls=0.0218 smmd=0.0108 ct=9.2876 rec=1.1688 | train/val/test=1.000/0.710/0.703 | c=0.998437
[Epoch 0087] loss=11.6521 cls=0.0219 smmd=0.0047 ct=9.2881 rec=1.1687 | train/val/test=1.000/0.714/0.706 | c=0.998437
[Epoch 0088] loss=11.6695 cls=0.0247 smmd=0.0096 ct=9.2967 rec=1.1692 | train/val/test=1.000/0.712/0.695 | c=0.998437
[Epoch 0089] loss=11.6745 cls=0.0262 smmd=0.0213 ct=9.2849 rec=1.1711 | train/val/test=1.000/0.712/0.714 | c=0.998437
[Epoch 0090] loss=11.6677 cls=0.0242 smmd=0.0091 ct=9.2985 rec=1.1679 | train/val/test=1.000/0.722/0.703 | c=0.998437
[Epoch 0091] loss=11.6511 cls=0.0230 smmd=0.0141 ct=9.2811 rec=1.1664 | train/val/test=1.000/0.720/0.709 | c=0.998437
[Epoch 0092] loss=11.6533 cls=0.0216 smmd=0.0130 ct=9.2854 rec=1.1666 | train/val/test=1.000/0.714/0.706 | c=0.998437
[Epoch 0093] loss=11.6581 cls=0.0244 smmd=0.0116 ct=9.2881 rec=1.1670 | train/val/test=1.000/0.720/0.706 | c=0.998437
[Epoch 0094] loss=11.6571 cls=0.0226 smmd=0.0167 ct=9.2813 rec=1.1682 | train/val/test=1.000/0.710/0.702 | c=0.998437
[Epoch 0095] loss=11.6537 cls=0.0233 smmd=0.0076 ct=9.2871 rec=1.1679 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0096] loss=11.6469 cls=0.0221 smmd=0.0112 ct=9.2790 rec=1.1673 | train/val/test=1.000/0.708/0.697 | c=0.998437
[Epoch 0097] loss=11.6478 cls=0.0230 smmd=0.0129 ct=9.2747 rec=1.1686 | train/val/test=1.000/0.718/0.716 | c=0.998437
[Epoch 0098] loss=11.6596 cls=0.0236 smmd=0.0095 ct=9.2901 rec=1.1682 | train/val/test=1.000/0.716/0.700 | c=0.998437
[Epoch 0099] loss=11.6593 cls=0.0244 smmd=0.0239 ct=9.2729 rec=1.1690 | train/val/test=1.000/0.714/0.714 | c=0.998437
=== Best @ epoch 42: val=0.7240, test=0.7030 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-4 completed in 29.42 seconds.
==================================================
