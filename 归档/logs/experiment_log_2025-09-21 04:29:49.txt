Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3 - 2025-09-21 04:29:49:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2746 cls=1.9513 smmd=4.2773 ct=9.2682 rec=1.3889 | train/val/test=0.397/0.186/0.219 | c=0.998437
[Epoch 0001] loss=16.8242 cls=1.9232 smmd=2.9071 ct=9.2147 rec=1.3896 | train/val/test=0.362/0.222/0.203 | c=0.998437
[Epoch 0002] loss=15.3035 cls=1.8244 smmd=1.5421 ct=9.1593 rec=1.3888 | train/val/test=0.569/0.218/0.263 | c=0.998437
[Epoch 0003] loss=15.0620 cls=1.6538 smmd=1.5101 ct=9.1211 rec=1.3885 | train/val/test=0.879/0.550/0.563 | c=0.998437
[Epoch 0004] loss=14.9212 cls=1.4049 smmd=1.7471 ct=8.9995 rec=1.3849 | train/val/test=0.845/0.562/0.577 | c=0.998437
[Epoch 0005] loss=14.3664 cls=1.0703 smmd=1.6629 ct=8.8855 rec=1.3739 | train/val/test=0.845/0.552/0.548 | c=0.998437
[Epoch 0006] loss=13.7027 cls=0.7883 smmd=1.3675 ct=8.8397 rec=1.3536 | train/val/test=0.914/0.644/0.632 | c=0.998437
[Epoch 0007] loss=13.0942 cls=0.5276 smmd=1.1024 ct=8.8110 rec=1.3266 | train/val/test=0.983/0.698/0.687 | c=0.998437
[Epoch 0008] loss=12.7869 cls=0.3359 smmd=1.0483 ct=8.8093 rec=1.2967 | train/val/test=0.966/0.688/0.685 | c=0.998437
[Epoch 0009] loss=12.6545 cls=0.2061 smmd=1.1196 ct=8.7993 rec=1.2647 | train/val/test=0.983/0.684/0.676 | c=0.998437
[Epoch 0010] loss=12.5151 cls=0.1274 smmd=1.1166 ct=8.7923 rec=1.2394 | train/val/test=0.983/0.722/0.699 | c=0.998437
[Epoch 0011] loss=12.3486 cls=0.0733 smmd=1.0530 ct=8.7835 rec=1.2194 | train/val/test=1.000/0.740/0.727 | c=0.998437
[Epoch 0012] loss=12.0950 cls=0.0442 smmd=0.8634 ct=8.7778 rec=1.2048 | train/val/test=1.000/0.740/0.723 | c=0.998437
[Epoch 0013] loss=11.9917 cls=0.0279 smmd=0.8042 ct=8.7714 rec=1.1941 | train/val/test=1.000/0.736/0.713 | c=0.998437
[Epoch 0014] loss=11.9072 cls=0.0174 smmd=0.7503 ct=8.7666 rec=1.1864 | train/val/test=1.000/0.732/0.714 | c=0.998437
[Epoch 0015] loss=11.8933 cls=0.0121 smmd=0.7525 ct=8.7659 rec=1.1814 | train/val/test=1.000/0.744/0.717 | c=0.998437
[Epoch 0016] loss=11.8209 cls=0.0098 smmd=0.6873 ct=8.7668 rec=1.1785 | train/val/test=1.000/0.744/0.730 | c=0.998437
[Epoch 0017] loss=11.7252 cls=0.0088 smmd=0.5923 ct=8.7695 rec=1.1773 | train/val/test=1.000/0.738/0.734 | c=0.998437
[Epoch 0018] loss=11.6097 cls=0.0080 smmd=0.4762 ct=8.7723 rec=1.1766 | train/val/test=1.000/0.742/0.724 | c=0.998437
[Epoch 0019] loss=11.5794 cls=0.0084 smmd=0.4387 ct=8.7782 rec=1.1771 | train/val/test=1.000/0.746/0.726 | c=0.998437
[Epoch 0020] loss=11.5432 cls=0.0096 smmd=0.3900 ct=8.7862 rec=1.1787 | train/val/test=1.000/0.746/0.748 | c=0.998437
[Epoch 0021] loss=11.5228 cls=0.0122 smmd=0.3515 ct=8.7959 rec=1.1816 | train/val/test=1.000/0.746/0.743 | c=0.998437
[Epoch 0022] loss=11.4580 cls=0.0156 smmd=0.2748 ct=8.8021 rec=1.1827 | train/val/test=1.000/0.750/0.744 | c=0.998437
[Epoch 0023] loss=11.4052 cls=0.0204 smmd=0.2099 ct=8.8073 rec=1.1838 | train/val/test=1.000/0.754/0.754 | c=0.998437
[Epoch 0024] loss=11.4130 cls=0.0261 smmd=0.2049 ct=8.8132 rec=1.1844 | train/val/test=1.000/0.756/0.744 | c=0.998437
[Epoch 0025] loss=11.3916 cls=0.0317 smmd=0.1789 ct=8.8171 rec=1.1820 | train/val/test=1.000/0.760/0.756 | c=0.998437
[Epoch 0026] loss=11.3658 cls=0.0342 smmd=0.1527 ct=8.8185 rec=1.1802 | train/val/test=1.000/0.762/0.744 | c=0.998437
[Epoch 0027] loss=11.3503 cls=0.0361 smmd=0.1457 ct=8.8166 rec=1.1759 | train/val/test=1.000/0.768/0.754 | c=0.998437
[Epoch 0028] loss=11.3273 cls=0.0342 smmd=0.1311 ct=8.8151 rec=1.1735 | train/val/test=1.000/0.758/0.737 | c=0.998437
[Epoch 0029] loss=11.2964 cls=0.0328 smmd=0.1160 ct=8.8098 rec=1.1689 | train/val/test=1.000/0.770/0.753 | c=0.998437
[Epoch 0030] loss=11.2791 cls=0.0280 smmd=0.1119 ct=8.8066 rec=1.1663 | train/val/test=1.000/0.762/0.742 | c=0.998437
[Epoch 0031] loss=11.2437 cls=0.0233 smmd=0.0926 ct=8.8016 rec=1.1631 | train/val/test=1.000/0.764/0.742 | c=0.998437
[Epoch 0032] loss=11.2337 cls=0.0197 smmd=0.0923 ct=8.7995 rec=1.1611 | train/val/test=1.000/0.760/0.747 | c=0.998437
[Epoch 0033] loss=11.2197 cls=0.0168 smmd=0.0844 ct=8.7983 rec=1.1601 | train/val/test=1.000/0.762/0.736 | c=0.998437
[Epoch 0034] loss=11.2020 cls=0.0154 smmd=0.0705 ct=8.7975 rec=1.1593 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0035] loss=11.1949 cls=0.0142 smmd=0.0631 ct=8.7988 rec=1.1593 | train/val/test=1.000/0.758/0.734 | c=0.998437
[Epoch 0036] loss=11.1849 cls=0.0139 smmd=0.0521 ct=8.7997 rec=1.1596 | train/val/test=1.000/0.760/0.734 | c=0.998437
[Epoch 0037] loss=11.1762 cls=0.0144 smmd=0.0411 ct=8.8007 rec=1.1600 | train/val/test=1.000/0.758/0.743 | c=0.998437
[Epoch 0038] loss=11.1729 cls=0.0148 smmd=0.0328 ct=8.8030 rec=1.1611 | train/val/test=1.000/0.762/0.735 | c=0.998437
[Epoch 0039] loss=11.1712 cls=0.0158 smmd=0.0315 ct=8.8021 rec=1.1609 | train/val/test=1.000/0.760/0.739 | c=0.998437
[Epoch 0040] loss=11.1641 cls=0.0161 smmd=0.0215 ct=8.8041 rec=1.1612 | train/val/test=1.000/0.762/0.738 | c=0.998437
[Epoch 0041] loss=11.1607 cls=0.0168 smmd=0.0169 ct=8.8052 rec=1.1608 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0042] loss=11.1584 cls=0.0175 smmd=0.0160 ct=8.8040 rec=1.1604 | train/val/test=1.000/0.764/0.739 | c=0.998437
[Epoch 0043] loss=11.1545 cls=0.0180 smmd=0.0135 ct=8.8028 rec=1.1601 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0044] loss=11.1494 cls=0.0185 smmd=0.0103 ct=8.8014 rec=1.1596 | train/val/test=1.000/0.762/0.745 | c=0.998437
[Epoch 0045] loss=11.1464 cls=0.0190 smmd=0.0058 ct=8.8022 rec=1.1597 | train/val/test=1.000/0.756/0.736 | c=0.998437
[Epoch 0046] loss=11.1503 cls=0.0197 smmd=0.0131 ct=8.7992 rec=1.1592 | train/val/test=1.000/0.756/0.748 | c=0.998437
[Epoch 0047] loss=11.1526 cls=0.0197 smmd=0.0081 ct=8.8036 rec=1.1606 | train/val/test=1.000/0.744/0.732 | c=0.998437
[Epoch 0048] loss=11.1554 cls=0.0207 smmd=0.0154 ct=8.7996 rec=1.1598 | train/val/test=1.000/0.756/0.749 | c=0.998437
[Epoch 0049] loss=11.1542 cls=0.0191 smmd=0.0104 ct=8.8033 rec=1.1608 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0050] loss=11.1431 cls=0.0176 smmd=0.0132 ct=8.7957 rec=1.1583 | train/val/test=1.000/0.754/0.741 | c=0.998437
[Epoch 0051] loss=11.1245 cls=0.0157 smmd=-0.0042 ct=8.7972 rec=1.1579 | train/val/test=1.000/0.754/0.740 | c=0.998437
[Epoch 0052] loss=11.1261 cls=0.0154 smmd=-0.0022 ct=8.7969 rec=1.1580 | train/val/test=1.000/0.754/0.734 | c=0.998437
[Epoch 0053] loss=11.1235 cls=0.0161 smmd=-0.0046 ct=8.7955 rec=1.1582 | train/val/test=1.000/0.754/0.741 | c=0.998437
[Epoch 0054] loss=11.1287 cls=0.0163 smmd=-0.0064 ct=8.7993 rec=1.1598 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0055] loss=11.1278 cls=0.0169 smmd=-0.0051 ct=8.7971 rec=1.1594 | train/val/test=1.000/0.754/0.740 | c=0.998437
[Epoch 0056] loss=11.1220 cls=0.0169 smmd=-0.0116 ct=8.7974 rec=1.1597 | train/val/test=1.000/0.756/0.734 | c=0.998437
[Epoch 0057] loss=11.1219 cls=0.0169 smmd=-0.0109 ct=8.7967 rec=1.1596 | train/val/test=1.000/0.756/0.739 | c=0.998437
[Epoch 0058] loss=11.1181 cls=0.0178 smmd=-0.0158 ct=8.7971 rec=1.1596 | train/val/test=1.000/0.752/0.738 | c=0.998437
[Epoch 0059] loss=11.1247 cls=0.0179 smmd=-0.0103 ct=8.7972 rec=1.1600 | train/val/test=1.000/0.750/0.732 | c=0.998437
[Epoch 0060] loss=11.1257 cls=0.0189 smmd=-0.0083 ct=8.7948 rec=1.1601 | train/val/test=1.000/0.750/0.742 | c=0.998437
[Epoch 0061] loss=11.1266 cls=0.0192 smmd=-0.0125 ct=8.7983 rec=1.1608 | train/val/test=1.000/0.748/0.728 | c=0.998437
[Epoch 0062] loss=11.1333 cls=0.0196 smmd=-0.0018 ct=8.7947 rec=1.1605 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0063] loss=11.1270 cls=0.0190 smmd=-0.0114 ct=8.7979 rec=1.1608 | train/val/test=1.000/0.750/0.730 | c=0.998437
[Epoch 0064] loss=11.1194 cls=0.0185 smmd=-0.0109 ct=8.7926 rec=1.1596 | train/val/test=1.000/0.748/0.740 | c=0.998437
[Epoch 0065] loss=11.1143 cls=0.0172 smmd=-0.0148 ct=8.7931 rec=1.1594 | train/val/test=1.000/0.754/0.743 | c=0.998437
[Epoch 0066] loss=11.1116 cls=0.0169 smmd=-0.0145 ct=8.7916 rec=1.1588 | train/val/test=1.000/0.752/0.736 | c=0.998437
[Epoch 0067] loss=11.1137 cls=0.0169 smmd=-0.0137 ct=8.7921 rec=1.1592 | train/val/test=1.000/0.748/0.743 | c=0.998437
[Epoch 0068] loss=11.1139 cls=0.0175 smmd=-0.0182 ct=8.7937 rec=1.1605 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0069] loss=11.1185 cls=0.0184 smmd=-0.0129 ct=8.7917 rec=1.1607 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0070] loss=11.1222 cls=0.0185 smmd=-0.0168 ct=8.7965 rec=1.1620 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0071] loss=11.1238 cls=0.0196 smmd=-0.0128 ct=8.7937 rec=1.1617 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0072] loss=11.1284 cls=0.0192 smmd=-0.0123 ct=8.7965 rec=1.1625 | train/val/test=1.000/0.748/0.730 | c=0.998437
[Epoch 0073] loss=11.1263 cls=0.0195 smmd=-0.0089 ct=8.7929 rec=1.1614 | train/val/test=1.000/0.748/0.740 | c=0.998437
[Epoch 0074] loss=11.1217 cls=0.0185 smmd=-0.0134 ct=8.7944 rec=1.1611 | train/val/test=1.000/0.748/0.732 | c=0.998437
[Epoch 0075] loss=11.1195 cls=0.0184 smmd=-0.0107 ct=8.7910 rec=1.1604 | train/val/test=1.000/0.750/0.741 | c=0.998437
[Epoch 0076] loss=11.1162 cls=0.0176 smmd=-0.0136 ct=8.7920 rec=1.1601 | train/val/test=1.000/0.750/0.736 | c=0.998437
[Epoch 0077] loss=11.1119 cls=0.0175 smmd=-0.0169 ct=8.7914 rec=1.1600 | train/val/test=1.000/0.754/0.737 | c=0.998437
[Epoch 0078] loss=11.1113 cls=0.0177 smmd=-0.0173 ct=8.7904 rec=1.1602 | train/val/test=1.000/0.748/0.741 | c=0.998437
[Epoch 0079] loss=11.1143 cls=0.0179 smmd=-0.0179 ct=8.7926 rec=1.1609 | train/val/test=1.000/0.748/0.733 | c=0.998437
[Epoch 0080] loss=11.1182 cls=0.0185 smmd=-0.0156 ct=8.7926 rec=1.1614 | train/val/test=1.000/0.750/0.741 | c=0.998437
[Epoch 0081] loss=11.1221 cls=0.0190 smmd=-0.0158 ct=8.7944 rec=1.1623 | train/val/test=1.000/0.748/0.729 | c=0.998437
[Epoch 0082] loss=11.1190 cls=0.0199 smmd=-0.0189 ct=8.7930 rec=1.1625 | train/val/test=1.000/0.752/0.749 | c=0.998437
[Epoch 0083] loss=11.1262 cls=0.0204 smmd=-0.0194 ct=8.7978 rec=1.1637 | train/val/test=1.000/0.742/0.728 | c=0.998437
[Epoch 0084] loss=11.1415 cls=0.0221 smmd=-0.0035 ct=8.7949 rec=1.1639 | train/val/test=1.000/0.758/0.753 | c=0.998437
[Epoch 0085] loss=11.1569 cls=0.0227 smmd=-0.0012 ct=8.8033 rec=1.1661 | train/val/test=1.000/0.736/0.724 | c=0.998437
[Epoch 0086] loss=11.1666 cls=0.0239 smmd=0.0180 ct=8.7957 rec=1.1646 | train/val/test=1.000/0.762/0.750 | c=0.998437
[Epoch 0087] loss=11.1477 cls=0.0195 smmd=0.0026 ct=8.7993 rec=1.1631 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0088] loss=11.1087 cls=0.0150 smmd=-0.0088 ct=8.7869 rec=1.1578 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0089] loss=11.1052 cls=0.0144 smmd=-0.0108 ct=8.7864 rec=1.1576 | train/val/test=1.000/0.758/0.748 | c=0.998437
[Epoch 0090] loss=11.1332 cls=0.0166 smmd=-0.0006 ct=8.7949 rec=1.1611 | train/val/test=1.000/0.742/0.726 | c=0.998437
[Epoch 0091] loss=11.1324 cls=0.0165 smmd=0.0047 ct=8.7905 rec=1.1604 | train/val/test=1.000/0.752/0.743 | c=0.998437
[Epoch 0092] loss=11.1075 cls=0.0150 smmd=-0.0169 ct=8.7906 rec=1.1594 | train/val/test=1.000/0.746/0.742 | c=0.998437
[Epoch 0093] loss=11.1092 cls=0.0156 smmd=-0.0180 ct=8.7909 rec=1.1603 | train/val/test=1.000/0.748/0.726 | c=0.998437
[Epoch 0094] loss=11.1328 cls=0.0193 smmd=-0.0038 ct=8.7920 rec=1.1627 | train/val/test=1.000/0.764/0.749 | c=0.998437
[Epoch 0095] loss=11.1510 cls=0.0206 smmd=-0.0034 ct=8.8024 rec=1.1657 | train/val/test=1.000/0.742/0.722 | c=0.998437
[Epoch 0096] loss=11.1553 cls=0.0233 smmd=0.0056 ct=8.7958 rec=1.1653 | train/val/test=1.000/0.760/0.748 | c=0.998437
[Epoch 0097] loss=11.1485 cls=0.0206 smmd=-0.0006 ct=8.8004 rec=1.1640 | train/val/test=1.000/0.746/0.726 | c=0.998437
[Epoch 0098] loss=11.1268 cls=0.0181 smmd=-0.0070 ct=8.7927 rec=1.1615 | train/val/test=1.000/0.754/0.738 | c=0.998437
[Epoch 0099] loss=11.1154 cls=0.0178 smmd=-0.0116 ct=8.7894 rec=1.1599 | train/val/test=1.000/0.750/0.739 | c=0.998437
=== Best @ epoch 29: val=0.7700, test=0.7530 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3 - 2025-09-21 04:29:49:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2746 cls=1.9513 smmd=4.2773 ct=9.2682 rec=1.3889 | train/val/test=0.397/0.186/0.219 | c=0.998437
[Epoch 0001] loss=16.8242 cls=1.9232 smmd=2.9071 ct=9.2147 rec=1.3896 | train/val/test=0.362/0.222/0.203 | c=0.998437
[Epoch 0002] loss=15.3035 cls=1.8244 smmd=1.5421 ct=9.1593 rec=1.3888 | train/val/test=0.569/0.218/0.263 | c=0.998437
[Epoch 0003] loss=15.0620 cls=1.6538 smmd=1.5101 ct=9.1211 rec=1.3885 | train/val/test=0.879/0.550/0.563 | c=0.998437
[Epoch 0004] loss=14.9212 cls=1.4049 smmd=1.7471 ct=8.9995 rec=1.3849 | train/val/test=0.845/0.562/0.577 | c=0.998437
[Epoch 0005] loss=14.3664 cls=1.0703 smmd=1.6629 ct=8.8855 rec=1.3739 | train/val/test=0.845/0.552/0.548 | c=0.998437
[Epoch 0006] loss=13.7027 cls=0.7883 smmd=1.3675 ct=8.8397 rec=1.3536 | train/val/test=0.914/0.644/0.632 | c=0.998437
[Epoch 0007] loss=13.0942 cls=0.5276 smmd=1.1024 ct=8.8110 rec=1.3266 | train/val/test=0.983/0.698/0.687 | c=0.998437
[Epoch 0008] loss=12.7869 cls=0.3359 smmd=1.0483 ct=8.8093 rec=1.2967 | train/val/test=0.966/0.688/0.685 | c=0.998437
[Epoch 0009] loss=12.6545 cls=0.2061 smmd=1.1196 ct=8.7993 rec=1.2647 | train/val/test=0.983/0.684/0.676 | c=0.998437
[Epoch 0010] loss=12.5151 cls=0.1274 smmd=1.1166 ct=8.7923 rec=1.2394 | train/val/test=0.983/0.722/0.699 | c=0.998437
[Epoch 0011] loss=12.3486 cls=0.0733 smmd=1.0530 ct=8.7835 rec=1.2194 | train/val/test=1.000/0.740/0.727 | c=0.998437
[Epoch 0012] loss=12.0950 cls=0.0442 smmd=0.8634 ct=8.7778 rec=1.2048 | train/val/test=1.000/0.740/0.723 | c=0.998437
[Epoch 0013] loss=11.9917 cls=0.0279 smmd=0.8042 ct=8.7714 rec=1.1941 | train/val/test=1.000/0.736/0.713 | c=0.998437
[Epoch 0014] loss=11.9072 cls=0.0174 smmd=0.7503 ct=8.7666 rec=1.1864 | train/val/test=1.000/0.732/0.714 | c=0.998437
[Epoch 0015] loss=11.8933 cls=0.0121 smmd=0.7525 ct=8.7659 rec=1.1814 | train/val/test=1.000/0.744/0.717 | c=0.998437
[Epoch 0016] loss=11.8209 cls=0.0098 smmd=0.6873 ct=8.7668 rec=1.1785 | train/val/test=1.000/0.744/0.730 | c=0.998437
[Epoch 0017] loss=11.7252 cls=0.0088 smmd=0.5923 ct=8.7695 rec=1.1773 | train/val/test=1.000/0.738/0.734 | c=0.998437
[Epoch 0018] loss=11.6097 cls=0.0080 smmd=0.4762 ct=8.7723 rec=1.1766 | train/val/test=1.000/0.742/0.724 | c=0.998437
[Epoch 0019] loss=11.5794 cls=0.0084 smmd=0.4387 ct=8.7782 rec=1.1771 | train/val/test=1.000/0.746/0.726 | c=0.998437
[Epoch 0020] loss=11.5432 cls=0.0096 smmd=0.3900 ct=8.7862 rec=1.1787 | train/val/test=1.000/0.746/0.748 | c=0.998437
[Epoch 0021] loss=11.5228 cls=0.0122 smmd=0.3515 ct=8.7959 rec=1.1816 | train/val/test=1.000/0.746/0.743 | c=0.998437
[Epoch 0022] loss=11.4580 cls=0.0156 smmd=0.2748 ct=8.8021 rec=1.1827 | train/val/test=1.000/0.750/0.744 | c=0.998437
[Epoch 0023] loss=11.4052 cls=0.0204 smmd=0.2099 ct=8.8073 rec=1.1838 | train/val/test=1.000/0.754/0.754 | c=0.998437
[Epoch 0024] loss=11.4130 cls=0.0261 smmd=0.2049 ct=8.8132 rec=1.1844 | train/val/test=1.000/0.756/0.744 | c=0.998437
[Epoch 0025] loss=11.3916 cls=0.0317 smmd=0.1789 ct=8.8171 rec=1.1820 | train/val/test=1.000/0.760/0.756 | c=0.998437
[Epoch 0026] loss=11.3658 cls=0.0342 smmd=0.1527 ct=8.8185 rec=1.1802 | train/val/test=1.000/0.762/0.744 | c=0.998437
[Epoch 0027] loss=11.3503 cls=0.0361 smmd=0.1457 ct=8.8166 rec=1.1759 | train/val/test=1.000/0.768/0.754 | c=0.998437
[Epoch 0028] loss=11.3273 cls=0.0342 smmd=0.1311 ct=8.8151 rec=1.1735 | train/val/test=1.000/0.758/0.737 | c=0.998437
[Epoch 0029] loss=11.2964 cls=0.0328 smmd=0.1160 ct=8.8098 rec=1.1689 | train/val/test=1.000/0.770/0.753 | c=0.998437
[Epoch 0030] loss=11.2791 cls=0.0280 smmd=0.1119 ct=8.8066 rec=1.1663 | train/val/test=1.000/0.762/0.742 | c=0.998437
[Epoch 0031] loss=11.2437 cls=0.0233 smmd=0.0926 ct=8.8016 rec=1.1631 | train/val/test=1.000/0.764/0.742 | c=0.998437
[Epoch 0032] loss=11.2337 cls=0.0197 smmd=0.0923 ct=8.7995 rec=1.1611 | train/val/test=1.000/0.760/0.747 | c=0.998437
[Epoch 0033] loss=11.2197 cls=0.0168 smmd=0.0844 ct=8.7983 rec=1.1601 | train/val/test=1.000/0.762/0.736 | c=0.998437
[Epoch 0034] loss=11.2020 cls=0.0154 smmd=0.0705 ct=8.7975 rec=1.1593 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0035] loss=11.1949 cls=0.0142 smmd=0.0631 ct=8.7988 rec=1.1593 | train/val/test=1.000/0.758/0.734 | c=0.998437
[Epoch 0036] loss=11.1849 cls=0.0139 smmd=0.0521 ct=8.7997 rec=1.1596 | train/val/test=1.000/0.760/0.734 | c=0.998437
[Epoch 0037] loss=11.1762 cls=0.0144 smmd=0.0411 ct=8.8007 rec=1.1600 | train/val/test=1.000/0.758/0.743 | c=0.998437
[Epoch 0038] loss=11.1729 cls=0.0148 smmd=0.0328 ct=8.8030 rec=1.1611 | train/val/test=1.000/0.762/0.735 | c=0.998437
[Epoch 0039] loss=11.1712 cls=0.0158 smmd=0.0315 ct=8.8021 rec=1.1609 | train/val/test=1.000/0.760/0.739 | c=0.998437
[Epoch 0040] loss=11.1641 cls=0.0161 smmd=0.0215 ct=8.8041 rec=1.1612 | train/val/test=1.000/0.762/0.738 | c=0.998437
[Epoch 0041] loss=11.1607 cls=0.0168 smmd=0.0169 ct=8.8052 rec=1.1608 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0042] loss=11.1584 cls=0.0175 smmd=0.0160 ct=8.8040 rec=1.1604 | train/val/test=1.000/0.764/0.739 | c=0.998437
[Epoch 0043] loss=11.1545 cls=0.0180 smmd=0.0135 ct=8.8028 rec=1.1601 | train/val/test=1.000/0.762/0.737 | c=0.998437
[Epoch 0044] loss=11.1494 cls=0.0185 smmd=0.0103 ct=8.8014 rec=1.1596 | train/val/test=1.000/0.762/0.745 | c=0.998437
[Epoch 0045] loss=11.1464 cls=0.0190 smmd=0.0058 ct=8.8022 rec=1.1597 | train/val/test=1.000/0.756/0.736 | c=0.998437
[Epoch 0046] loss=11.1503 cls=0.0197 smmd=0.0131 ct=8.7992 rec=1.1592 | train/val/test=1.000/0.756/0.748 | c=0.998437
[Epoch 0047] loss=11.1526 cls=0.0197 smmd=0.0081 ct=8.8036 rec=1.1606 | train/val/test=1.000/0.744/0.732 | c=0.998437
[Epoch 0048] loss=11.1554 cls=0.0207 smmd=0.0154 ct=8.7996 rec=1.1598 | train/val/test=1.000/0.756/0.749 | c=0.998437
[Epoch 0049] loss=11.1542 cls=0.0191 smmd=0.0104 ct=8.8033 rec=1.1608 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0050] loss=11.1431 cls=0.0176 smmd=0.0132 ct=8.7957 rec=1.1583 | train/val/test=1.000/0.754/0.741 | c=0.998437
[Epoch 0051] loss=11.1245 cls=0.0157 smmd=-0.0042 ct=8.7972 rec=1.1579 | train/val/test=1.000/0.754/0.740 | c=0.998437
[Epoch 0052] loss=11.1261 cls=0.0154 smmd=-0.0022 ct=8.7969 rec=1.1580 | train/val/test=1.000/0.754/0.734 | c=0.998437
[Epoch 0053] loss=11.1235 cls=0.0161 smmd=-0.0046 ct=8.7955 rec=1.1582 | train/val/test=1.000/0.754/0.741 | c=0.998437
[Epoch 0054] loss=11.1287 cls=0.0163 smmd=-0.0064 ct=8.7993 rec=1.1598 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0055] loss=11.1278 cls=0.0169 smmd=-0.0051 ct=8.7971 rec=1.1594 | train/val/test=1.000/0.754/0.740 | c=0.998437
[Epoch 0056] loss=11.1220 cls=0.0169 smmd=-0.0116 ct=8.7974 rec=1.1597 | train/val/test=1.000/0.756/0.734 | c=0.998437
[Epoch 0057] loss=11.1219 cls=0.0169 smmd=-0.0109 ct=8.7967 rec=1.1596 | train/val/test=1.000/0.756/0.739 | c=0.998437
[Epoch 0058] loss=11.1181 cls=0.0178 smmd=-0.0158 ct=8.7971 rec=1.1596 | train/val/test=1.000/0.752/0.738 | c=0.998437
[Epoch 0059] loss=11.1247 cls=0.0179 smmd=-0.0103 ct=8.7972 rec=1.1600 | train/val/test=1.000/0.750/0.732 | c=0.998437
[Epoch 0060] loss=11.1257 cls=0.0189 smmd=-0.0083 ct=8.7948 rec=1.1601 | train/val/test=1.000/0.750/0.742 | c=0.998437
[Epoch 0061] loss=11.1266 cls=0.0192 smmd=-0.0125 ct=8.7983 rec=1.1608 | train/val/test=1.000/0.748/0.728 | c=0.998437
[Epoch 0062] loss=11.1333 cls=0.0196 smmd=-0.0018 ct=8.7947 rec=1.1605 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0063] loss=11.1270 cls=0.0190 smmd=-0.0114 ct=8.7979 rec=1.1608 | train/val/test=1.000/0.750/0.730 | c=0.998437
[Epoch 0064] loss=11.1194 cls=0.0185 smmd=-0.0109 ct=8.7926 rec=1.1596 | train/val/test=1.000/0.748/0.740 | c=0.998437
[Epoch 0065] loss=11.1143 cls=0.0172 smmd=-0.0148 ct=8.7931 rec=1.1594 | train/val/test=1.000/0.754/0.743 | c=0.998437
[Epoch 0066] loss=11.1116 cls=0.0169 smmd=-0.0145 ct=8.7916 rec=1.1588 | train/val/test=1.000/0.752/0.736 | c=0.998437
[Epoch 0067] loss=11.1137 cls=0.0169 smmd=-0.0137 ct=8.7921 rec=1.1592 | train/val/test=1.000/0.748/0.743 | c=0.998437
[Epoch 0068] loss=11.1139 cls=0.0175 smmd=-0.0182 ct=8.7937 rec=1.1605 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0069] loss=11.1185 cls=0.0184 smmd=-0.0129 ct=8.7917 rec=1.1607 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0070] loss=11.1222 cls=0.0185 smmd=-0.0168 ct=8.7965 rec=1.1620 | train/val/test=1.000/0.748/0.731 | c=0.998437
[Epoch 0071] loss=11.1238 cls=0.0196 smmd=-0.0128 ct=8.7937 rec=1.1617 | train/val/test=1.000/0.748/0.744 | c=0.998437
[Epoch 0072] loss=11.1284 cls=0.0192 smmd=-0.0123 ct=8.7965 rec=1.1625 | train/val/test=1.000/0.748/0.730 | c=0.998437
[Epoch 0073] loss=11.1263 cls=0.0195 smmd=-0.0089 ct=8.7929 rec=1.1614 | train/val/test=1.000/0.748/0.740 | c=0.998437
[Epoch 0074] loss=11.1217 cls=0.0185 smmd=-0.0134 ct=8.7944 rec=1.1611 | train/val/test=1.000/0.748/0.732 | c=0.998437
[Epoch 0075] loss=11.1195 cls=0.0184 smmd=-0.0107 ct=8.7910 rec=1.1604 | train/val/test=1.000/0.750/0.741 | c=0.998437
[Epoch 0076] loss=11.1162 cls=0.0176 smmd=-0.0136 ct=8.7920 rec=1.1601 | train/val/test=1.000/0.750/0.736 | c=0.998437
[Epoch 0077] loss=11.1119 cls=0.0175 smmd=-0.0169 ct=8.7914 rec=1.1600 | train/val/test=1.000/0.754/0.737 | c=0.998437
[Epoch 0078] loss=11.1113 cls=0.0177 smmd=-0.0173 ct=8.7904 rec=1.1602 | train/val/test=1.000/0.748/0.741 | c=0.998437
[Epoch 0079] loss=11.1143 cls=0.0179 smmd=-0.0179 ct=8.7926 rec=1.1609 | train/val/test=1.000/0.748/0.733 | c=0.998437
[Epoch 0080] loss=11.1182 cls=0.0185 smmd=-0.0156 ct=8.7926 rec=1.1614 | train/val/test=1.000/0.750/0.741 | c=0.998437
[Epoch 0081] loss=11.1221 cls=0.0190 smmd=-0.0158 ct=8.7944 rec=1.1623 | train/val/test=1.000/0.748/0.729 | c=0.998437
[Epoch 0082] loss=11.1190 cls=0.0199 smmd=-0.0189 ct=8.7930 rec=1.1625 | train/val/test=1.000/0.752/0.749 | c=0.998437
[Epoch 0083] loss=11.1262 cls=0.0204 smmd=-0.0194 ct=8.7978 rec=1.1637 | train/val/test=1.000/0.742/0.728 | c=0.998437
[Epoch 0084] loss=11.1415 cls=0.0221 smmd=-0.0035 ct=8.7949 rec=1.1639 | train/val/test=1.000/0.758/0.753 | c=0.998437
[Epoch 0085] loss=11.1569 cls=0.0227 smmd=-0.0012 ct=8.8033 rec=1.1661 | train/val/test=1.000/0.736/0.724 | c=0.998437
[Epoch 0086] loss=11.1666 cls=0.0239 smmd=0.0180 ct=8.7957 rec=1.1646 | train/val/test=1.000/0.762/0.750 | c=0.998437
[Epoch 0087] loss=11.1477 cls=0.0195 smmd=0.0026 ct=8.7993 rec=1.1631 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0088] loss=11.1087 cls=0.0150 smmd=-0.0088 ct=8.7869 rec=1.1578 | train/val/test=1.000/0.746/0.730 | c=0.998437
[Epoch 0089] loss=11.1052 cls=0.0144 smmd=-0.0108 ct=8.7864 rec=1.1576 | train/val/test=1.000/0.758/0.748 | c=0.998437
[Epoch 0090] loss=11.1332 cls=0.0166 smmd=-0.0006 ct=8.7949 rec=1.1611 | train/val/test=1.000/0.742/0.726 | c=0.998437
[Epoch 0091] loss=11.1324 cls=0.0165 smmd=0.0047 ct=8.7905 rec=1.1604 | train/val/test=1.000/0.752/0.743 | c=0.998437
[Epoch 0092] loss=11.1075 cls=0.0150 smmd=-0.0169 ct=8.7906 rec=1.1594 | train/val/test=1.000/0.746/0.742 | c=0.998437
[Epoch 0093] loss=11.1092 cls=0.0156 smmd=-0.0180 ct=8.7909 rec=1.1603 | train/val/test=1.000/0.748/0.726 | c=0.998437
[Epoch 0094] loss=11.1328 cls=0.0193 smmd=-0.0038 ct=8.7920 rec=1.1627 | train/val/test=1.000/0.764/0.749 | c=0.998437
[Epoch 0095] loss=11.1510 cls=0.0206 smmd=-0.0034 ct=8.8024 rec=1.1657 | train/val/test=1.000/0.742/0.722 | c=0.998437
[Epoch 0096] loss=11.1553 cls=0.0233 smmd=0.0056 ct=8.7958 rec=1.1653 | train/val/test=1.000/0.760/0.748 | c=0.998437
[Epoch 0097] loss=11.1485 cls=0.0206 smmd=-0.0006 ct=8.8004 rec=1.1640 | train/val/test=1.000/0.746/0.726 | c=0.998437
[Epoch 0098] loss=11.1268 cls=0.0181 smmd=-0.0070 ct=8.7927 rec=1.1615 | train/val/test=1.000/0.754/0.738 | c=0.998437
[Epoch 0099] loss=11.1154 cls=0.0178 smmd=-0.0116 ct=8.7894 rec=1.1599 | train/val/test=1.000/0.750/0.739 | c=0.998437
=== Best @ epoch 29: val=0.7700, test=0.7530 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-3 completed in 24.11 seconds.
==================================================
