Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 - 2025-09-21 06:06:53:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.3163 cls=1.1010 smmd=5.6693 ct=11.2780 rec=1.4138 | train/val/test=0.385/0.388/0.414 | c=0.998347
[Epoch 0001] loss=30.2369 cls=1.0829 smmd=3.6623 ct=11.2423 rec=1.4136 | train/val/test=0.577/0.500/0.487 | c=0.998347
[Epoch 0002] loss=35.9964 cls=1.0601 smmd=4.8158 ct=11.2459 rec=1.4136 | train/val/test=0.577/0.548/0.537 | c=0.998347
[Epoch 0003] loss=35.1124 cls=0.9961 smmd=4.6564 ct=11.1911 rec=1.4135 | train/val/test=0.577/0.574/0.572 | c=0.998347
[Epoch 0004] loss=25.9901 cls=0.8987 smmd=2.8805 ct=10.9970 rec=1.4111 | train/val/test=0.731/0.640/0.641 | c=0.998347
[Epoch 0005] loss=25.9397 cls=0.7733 smmd=2.8861 ct=10.9829 rec=1.3988 | train/val/test=0.692/0.670/0.662 | c=0.998347
[Epoch 0006] loss=28.2237 cls=0.6517 smmd=3.3592 ct=10.9639 rec=1.3812 | train/val/test=0.692/0.682/0.669 | c=0.998347
[Epoch 0007] loss=26.4751 cls=0.5523 smmd=3.0318 ct=10.9040 rec=1.3594 | train/val/test=0.769/0.674/0.677 | c=0.998347
[Epoch 0008] loss=22.1687 cls=0.4759 smmd=2.1859 ct=10.8670 rec=1.3403 | train/val/test=0.808/0.670/0.676 | c=0.998347
[Epoch 0009] loss=22.2169 cls=0.4118 smmd=2.2012 ct=10.8717 rec=1.3335 | train/val/test=0.885/0.702/0.703 | c=0.998347
[Epoch 0010] loss=24.8581 cls=0.3249 smmd=2.5911 ct=11.6071 rec=1.3316 | train/val/test=0.962/0.746/0.741 | c=0.998347
[Epoch 0011] loss=23.4408 cls=0.2508 smmd=2.3370 ct=11.4969 rec=1.3349 | train/val/test=1.000/0.748/0.744 | c=0.998347
[Epoch 0012] loss=20.3869 cls=0.2035 smmd=1.7396 ct=11.4538 rec=1.3356 | train/val/test=1.000/0.734/0.735 | c=0.998347
[Epoch 0013] loss=23.5121 cls=0.1950 smmd=2.3398 ct=11.5814 rec=1.3417 | train/val/test=1.000/0.778/0.775 | c=0.998347
[Epoch 0014] loss=22.3905 cls=0.1388 smmd=2.1418 ct=11.4793 rec=1.3303 | train/val/test=1.000/0.774/0.774 | c=0.998347
[Epoch 0015] loss=19.6806 cls=0.1035 smmd=1.5990 ct=11.5021 rec=1.3174 | train/val/test=1.000/0.778/0.782 | c=0.998347
[Epoch 0016] loss=19.9780 cls=0.0834 smmd=1.6639 ct=11.4860 rec=1.3100 | train/val/test=1.000/0.778/0.783 | c=0.998347
[Epoch 0017] loss=20.5210 cls=0.0753 smmd=1.7754 ct=11.4758 rec=1.3066 | train/val/test=1.000/0.778/0.778 | c=0.998347
[Epoch 0018] loss=18.8649 cls=0.0654 smmd=1.4421 ct=11.4916 rec=1.3018 | train/val/test=1.000/0.778/0.777 | c=0.998347
[Epoch 0019] loss=18.1461 cls=0.0616 smmd=1.2943 ct=11.5138 rec=1.3002 | train/val/test=1.000/0.788/0.787 | c=0.998347
[Epoch 0020] loss=18.7799 cls=0.0709 smmd=1.4314 ct=11.4571 rec=1.3039 | train/val/test=1.000/0.790/0.793 | c=0.998347
[Epoch 0021] loss=18.2987 cls=0.0778 smmd=1.3308 ct=11.4753 rec=1.3052 | train/val/test=1.000/0.790/0.787 | c=0.998347
[Epoch 0022] loss=17.6177 cls=0.0899 smmd=1.1878 ct=11.5034 rec=1.3020 | train/val/test=1.000/0.796/0.787 | c=0.998347
[Epoch 0023] loss=18.3501 cls=0.0927 smmd=1.3319 ct=11.5141 rec=1.3001 | train/val/test=1.000/0.790/0.789 | c=0.998347
[Epoch 0024] loss=17.4147 cls=0.0847 smmd=1.1440 ct=11.5225 rec=1.2999 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0025] loss=17.2814 cls=0.0736 smmd=1.1257 ct=11.4863 rec=1.2971 | train/val/test=1.000/0.804/0.783 | c=0.998347
[Epoch 0026] loss=16.6202 cls=0.0649 smmd=1.0009 ct=11.4540 rec=1.2939 | train/val/test=1.000/0.800/0.790 | c=0.998347
[Epoch 0027] loss=16.7496 cls=0.0591 smmd=1.0227 ct=11.4772 rec=1.2943 | train/val/test=1.000/0.784/0.776 | c=0.998347
[Epoch 0028] loss=16.0463 cls=0.0582 smmd=0.8813 ct=11.4815 rec=1.2910 | train/val/test=1.000/0.796/0.781 | c=0.998347
[Epoch 0029] loss=16.1426 cls=0.0556 smmd=0.9044 ct=11.4633 rec=1.2932 | train/val/test=1.000/0.796/0.784 | c=0.998347
[Epoch 0030] loss=15.8414 cls=0.0595 smmd=0.8401 ct=11.4812 rec=1.2970 | train/val/test=1.000/0.798/0.778 | c=0.998347
[Epoch 0031] loss=16.0828 cls=0.0672 smmd=0.8829 ct=11.5043 rec=1.3024 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0032] loss=16.0824 cls=0.0675 smmd=0.8877 ct=11.4801 rec=1.3007 | train/val/test=1.000/0.814/0.781 | c=0.998347
[Epoch 0033] loss=15.8678 cls=0.0678 smmd=0.8417 ct=11.4950 rec=1.3030 | train/val/test=1.000/0.788/0.788 | c=0.998347
[Epoch 0034] loss=15.6713 cls=0.0660 smmd=0.8074 ct=11.4713 rec=1.2992 | train/val/test=1.000/0.804/0.786 | c=0.998347
[Epoch 0035] loss=15.3032 cls=0.0592 smmd=0.7345 ct=11.4716 rec=1.2953 | train/val/test=1.000/0.812/0.788 | c=0.998347
[Epoch 0036] loss=15.3637 cls=0.0610 smmd=0.7427 ct=11.4900 rec=1.2973 | train/val/test=1.000/0.798/0.781 | c=0.998347
[Epoch 0037] loss=14.8953 cls=0.0612 smmd=0.6503 ct=11.4837 rec=1.2927 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0038] loss=14.7791 cls=0.0625 smmd=0.6334 ct=11.4511 rec=1.2956 | train/val/test=1.000/0.812/0.791 | c=0.998347
[Epoch 0039] loss=14.9763 cls=0.0692 smmd=0.6685 ct=11.4688 rec=1.3030 | train/val/test=1.000/0.794/0.776 | c=0.998347
[Epoch 0040] loss=14.9091 cls=0.0805 smmd=0.6449 ct=11.5141 rec=1.3030 | train/val/test=1.000/0.816/0.780 | c=0.998347
[Epoch 0041] loss=15.1035 cls=0.0853 smmd=0.6914 ct=11.4724 rec=1.3148 | train/val/test=1.000/0.790/0.769 | c=0.998347
[Epoch 0042] loss=15.1329 cls=0.0871 smmd=0.6872 ct=11.5225 rec=1.3086 | train/val/test=1.000/0.808/0.776 | c=0.998347
[Epoch 0043] loss=14.7693 cls=0.0801 smmd=0.6224 ct=11.4863 rec=1.3112 | train/val/test=1.000/0.796/0.779 | c=0.998347
[Epoch 0044] loss=14.7539 cls=0.0765 smmd=0.6255 ct=11.4573 rec=1.3070 | train/val/test=1.000/0.786/0.755 | c=0.998347
[Epoch 0045] loss=14.6127 cls=0.0824 smmd=0.5852 ct=11.5141 rec=1.3132 | train/val/test=1.000/0.784/0.769 | c=0.998347
[Epoch 0046] loss=14.3604 cls=0.0774 smmd=0.5459 ct=11.4618 rec=1.3069 | train/val/test=1.000/0.804/0.776 | c=0.998347
[Epoch 0047] loss=14.3539 cls=0.0755 smmd=0.5394 ct=11.4879 rec=1.3118 | train/val/test=1.000/0.782/0.771 | c=0.998347
[Epoch 0048] loss=14.2919 cls=0.0798 smmd=0.5272 ct=11.4854 rec=1.3055 | train/val/test=1.000/0.812/0.776 | c=0.998347
[Epoch 0049] loss=14.4045 cls=0.0862 smmd=0.5507 ct=11.4757 rec=1.3202 | train/val/test=1.000/0.758/0.749 | c=0.998347
[Epoch 0050] loss=14.5895 cls=0.0999 smmd=0.5841 ct=11.4873 rec=1.3165 | train/val/test=1.000/0.768/0.751 | c=0.998347
[Epoch 0051] loss=14.6797 cls=0.1028 smmd=0.5919 ct=11.5356 rec=1.3295 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0052] loss=14.8183 cls=0.1259 smmd=0.6299 ct=11.4729 rec=1.3319 | train/val/test=1.000/0.734/0.732 | c=0.998347
[Epoch 0053] loss=14.4661 cls=0.1082 smmd=0.5469 ct=11.5444 rec=1.3300 | train/val/test=1.000/0.754/0.742 | c=0.998347
[Epoch 0054] loss=14.3027 cls=0.1003 smmd=0.5332 ct=11.4547 rec=1.3185 | train/val/test=1.000/0.786/0.763 | c=0.998347
[Epoch 0055] loss=14.2443 cls=0.0819 smmd=0.5167 ct=11.4877 rec=1.3195 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0056] loss=14.0860 cls=0.0811 smmd=0.4889 ct=11.4699 rec=1.3088 | train/val/test=1.000/0.810/0.775 | c=0.998347
[Epoch 0057] loss=14.0266 cls=0.0773 smmd=0.4782 ct=11.4653 rec=1.3161 | train/val/test=1.000/0.774/0.774 | c=0.998347
[Epoch 0058] loss=14.1684 cls=0.0827 smmd=0.5059 ct=11.4661 rec=1.3157 | train/val/test=1.000/0.788/0.768 | c=0.998347
[Epoch 0059] loss=14.4194 cls=0.0927 smmd=0.5460 ct=11.5108 rec=1.3241 | train/val/test=1.000/0.770/0.764 | c=0.998347
[Epoch 0060] loss=14.7557 cls=0.0972 smmd=0.6243 ct=11.4528 rec=1.3285 | train/val/test=1.000/0.760/0.745 | c=0.998347
[Epoch 0061] loss=14.5049 cls=0.1067 smmd=0.5616 ct=11.5101 rec=1.3323 | train/val/test=1.000/0.760/0.736 | c=0.998347
[Epoch 0062] loss=14.3175 cls=0.1031 smmd=0.5328 ct=11.4685 rec=1.3345 | train/val/test=1.000/0.722/0.715 | c=0.998347
[Epoch 0063] loss=14.1608 cls=0.1104 smmd=0.4978 ct=11.4830 rec=1.3353 | train/val/test=1.000/0.768/0.735 | c=0.998347
[Epoch 0064] loss=14.0900 cls=0.0965 smmd=0.4898 ct=11.4596 rec=1.3300 | train/val/test=1.000/0.742/0.742 | c=0.998347
[Epoch 0065] loss=13.8230 cls=0.0950 smmd=0.4312 ct=11.4865 rec=1.3285 | train/val/test=1.000/0.770/0.742 | c=0.998347
[Epoch 0066] loss=13.9780 cls=0.0974 smmd=0.4731 ct=11.4311 rec=1.3279 | train/val/test=1.000/0.746/0.732 | c=0.998347
[Epoch 0067] loss=14.2129 cls=0.1023 smmd=0.4994 ct=11.5313 rec=1.3359 | train/val/test=1.000/0.742/0.718 | c=0.998347
[Epoch 0068] loss=14.3150 cls=0.1211 smmd=0.5312 ct=11.4643 rec=1.3409 | train/val/test=1.000/0.700/0.682 | c=0.998347
[Epoch 0069] loss=14.4807 cls=0.1210 smmd=0.5508 ct=11.5313 rec=1.3469 | train/val/test=1.000/0.708/0.696 | c=0.998347
[Epoch 0070] loss=14.7584 cls=0.1410 smmd=0.6126 ct=11.4895 rec=1.3556 | train/val/test=1.000/0.718/0.695 | c=0.998347
[Epoch 0071] loss=14.1489 cls=0.1051 smmd=0.4871 ct=11.5274 rec=1.3368 | train/val/test=1.000/0.774/0.756 | c=0.998347
[Epoch 0072] loss=13.8572 cls=0.0783 smmd=0.4498 ct=11.4373 rec=1.3202 | train/val/test=1.000/0.810/0.778 | c=0.998347
[Epoch 0073] loss=13.7082 cls=0.0596 smmd=0.4169 ct=11.4629 rec=1.3104 | train/val/test=1.000/0.810/0.767 | c=0.998347
[Epoch 0074] loss=13.6602 cls=0.0615 smmd=0.4129 ct=11.4337 rec=1.3108 | train/val/test=1.000/0.804/0.778 | c=0.998347
[Epoch 0075] loss=13.6724 cls=0.0682 smmd=0.4116 ct=11.4485 rec=1.3178 | train/val/test=1.000/0.788/0.756 | c=0.998347
[Epoch 0076] loss=13.7284 cls=0.0821 smmd=0.4154 ct=11.4776 rec=1.3260 | train/val/test=1.000/0.792/0.757 | c=0.998347
[Epoch 0077] loss=14.3719 cls=0.0965 smmd=0.5452 ct=11.4639 rec=1.3363 | train/val/test=1.000/0.730/0.709 | c=0.998347
[Epoch 0078] loss=14.8774 cls=0.1144 smmd=0.6382 ct=11.4950 rec=1.3432 | train/val/test=1.000/0.666/0.649 | c=0.998347
[Epoch 0079] loss=14.4184 cls=0.1450 smmd=0.5352 ct=11.5346 rec=1.3532 | train/val/test=1.000/0.656/0.641 | c=0.998347
[Epoch 0080] loss=14.3502 cls=0.1613 smmd=0.5297 ct=11.4846 rec=1.3646 | train/val/test=0.962/0.570/0.584 | c=0.998347
[Epoch 0081] loss=14.3364 cls=0.1946 smmd=0.5044 ct=11.5807 rec=1.3633 | train/val/test=1.000/0.692/0.694 | c=0.998347
[Epoch 0082] loss=14.0371 cls=0.1312 smmd=0.4747 ct=11.4626 rec=1.3527 | train/val/test=1.000/0.750/0.731 | c=0.998347
[Epoch 0083] loss=13.5376 cls=0.0786 smmd=0.3827 ct=11.4523 rec=1.3258 | train/val/test=1.000/0.802/0.784 | c=0.998347
[Epoch 0084] loss=13.5044 cls=0.0633 smmd=0.3794 ct=11.4446 rec=1.3109 | train/val/test=1.000/0.772/0.764 | c=0.998347
[Epoch 0085] loss=13.6723 cls=0.0784 smmd=0.4100 ct=11.4513 rec=1.3199 | train/val/test=1.000/0.754/0.734 | c=0.998347
[Epoch 0086] loss=14.1085 cls=0.0941 smmd=0.4844 ct=11.5058 rec=1.3378 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0087] loss=14.9645 cls=0.1255 smmd=0.6550 ct=11.4917 rec=1.3515 | train/val/test=1.000/0.716/0.684 | c=0.998347
[Epoch 0088] loss=14.7207 cls=0.1103 smmd=0.5943 ct=11.5590 rec=1.3520 | train/val/test=1.000/0.724/0.711 | c=0.998347
[Epoch 0089] loss=14.1474 cls=0.0982 smmd=0.5029 ct=11.4502 rec=1.3351 | train/val/test=1.000/0.796/0.758 | c=0.998347
[Epoch 0090] loss=13.8131 cls=0.0663 smmd=0.4323 ct=11.4863 rec=1.3227 | train/val/test=1.000/0.792/0.753 | c=0.998347
[Epoch 0091] loss=13.4562 cls=0.0598 smmd=0.3728 ct=11.4311 rec=1.3122 | train/val/test=1.000/0.802/0.754 | c=0.998347
[Epoch 0092] loss=13.2356 cls=0.0609 smmd=0.3335 ct=11.4062 rec=1.3139 | train/val/test=1.000/0.796/0.774 | c=0.998347
[Epoch 0093] loss=13.3602 cls=0.0681 smmd=0.3493 ct=11.4473 rec=1.3206 | train/val/test=1.000/0.774/0.735 | c=0.998347
[Epoch 0094] loss=13.5910 cls=0.0947 smmd=0.3886 ct=11.4673 rec=1.3327 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0095] loss=14.0428 cls=0.1095 smmd=0.4803 ct=11.4526 rec=1.3407 | train/val/test=1.000/0.746/0.731 | c=0.998347
[Epoch 0096] loss=14.6492 cls=0.1338 smmd=0.5840 ct=11.5276 rec=1.3467 | train/val/test=1.000/0.754/0.728 | c=0.998347
[Epoch 0097] loss=15.0730 cls=0.1306 smmd=0.6802 ct=11.4717 rec=1.3481 | train/val/test=1.000/0.702/0.675 | c=0.998347
[Epoch 0098] loss=14.4771 cls=0.1358 smmd=0.5578 ct=11.4858 rec=1.3457 | train/val/test=1.000/0.708/0.693 | c=0.998347
[Epoch 0099] loss=13.9200 cls=0.1425 smmd=0.4476 ct=11.4756 rec=1.3529 | train/val/test=1.000/0.596/0.599 | c=0.998347
=== Best @ epoch 40: val=0.8160, test=0.7800 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 - 2025-09-21 06:06:53:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.3163 cls=1.1010 smmd=5.6693 ct=11.2780 rec=1.4138 | train/val/test=0.385/0.388/0.414 | c=0.998347
[Epoch 0001] loss=30.2369 cls=1.0829 smmd=3.6623 ct=11.2423 rec=1.4136 | train/val/test=0.577/0.500/0.487 | c=0.998347
[Epoch 0002] loss=35.9964 cls=1.0601 smmd=4.8158 ct=11.2459 rec=1.4136 | train/val/test=0.577/0.548/0.537 | c=0.998347
[Epoch 0003] loss=35.1124 cls=0.9961 smmd=4.6564 ct=11.1911 rec=1.4135 | train/val/test=0.577/0.574/0.572 | c=0.998347
[Epoch 0004] loss=25.9901 cls=0.8987 smmd=2.8805 ct=10.9970 rec=1.4111 | train/val/test=0.731/0.640/0.641 | c=0.998347
[Epoch 0005] loss=25.9397 cls=0.7733 smmd=2.8861 ct=10.9829 rec=1.3988 | train/val/test=0.692/0.670/0.662 | c=0.998347
[Epoch 0006] loss=28.2237 cls=0.6517 smmd=3.3592 ct=10.9639 rec=1.3812 | train/val/test=0.692/0.682/0.669 | c=0.998347
[Epoch 0007] loss=26.4751 cls=0.5523 smmd=3.0318 ct=10.9040 rec=1.3594 | train/val/test=0.769/0.674/0.677 | c=0.998347
[Epoch 0008] loss=22.1687 cls=0.4759 smmd=2.1859 ct=10.8670 rec=1.3403 | train/val/test=0.808/0.670/0.676 | c=0.998347
[Epoch 0009] loss=22.2169 cls=0.4118 smmd=2.2012 ct=10.8717 rec=1.3335 | train/val/test=0.885/0.702/0.703 | c=0.998347
[Epoch 0010] loss=24.8581 cls=0.3249 smmd=2.5911 ct=11.6071 rec=1.3316 | train/val/test=0.962/0.746/0.741 | c=0.998347
[Epoch 0011] loss=23.4408 cls=0.2508 smmd=2.3370 ct=11.4969 rec=1.3349 | train/val/test=1.000/0.748/0.744 | c=0.998347
[Epoch 0012] loss=20.3869 cls=0.2035 smmd=1.7396 ct=11.4538 rec=1.3356 | train/val/test=1.000/0.734/0.735 | c=0.998347
[Epoch 0013] loss=23.5121 cls=0.1950 smmd=2.3398 ct=11.5814 rec=1.3417 | train/val/test=1.000/0.778/0.775 | c=0.998347
[Epoch 0014] loss=22.3905 cls=0.1388 smmd=2.1418 ct=11.4793 rec=1.3303 | train/val/test=1.000/0.774/0.774 | c=0.998347
[Epoch 0015] loss=19.6806 cls=0.1035 smmd=1.5990 ct=11.5021 rec=1.3174 | train/val/test=1.000/0.778/0.782 | c=0.998347
[Epoch 0016] loss=19.9780 cls=0.0834 smmd=1.6639 ct=11.4860 rec=1.3100 | train/val/test=1.000/0.778/0.783 | c=0.998347
[Epoch 0017] loss=20.5210 cls=0.0753 smmd=1.7754 ct=11.4758 rec=1.3066 | train/val/test=1.000/0.778/0.778 | c=0.998347
[Epoch 0018] loss=18.8649 cls=0.0654 smmd=1.4421 ct=11.4916 rec=1.3018 | train/val/test=1.000/0.778/0.777 | c=0.998347
[Epoch 0019] loss=18.1461 cls=0.0616 smmd=1.2943 ct=11.5138 rec=1.3002 | train/val/test=1.000/0.788/0.787 | c=0.998347
[Epoch 0020] loss=18.7799 cls=0.0709 smmd=1.4314 ct=11.4571 rec=1.3039 | train/val/test=1.000/0.790/0.793 | c=0.998347
[Epoch 0021] loss=18.2987 cls=0.0778 smmd=1.3308 ct=11.4753 rec=1.3052 | train/val/test=1.000/0.790/0.787 | c=0.998347
[Epoch 0022] loss=17.6177 cls=0.0899 smmd=1.1878 ct=11.5034 rec=1.3020 | train/val/test=1.000/0.796/0.787 | c=0.998347
[Epoch 0023] loss=18.3501 cls=0.0927 smmd=1.3319 ct=11.5141 rec=1.3001 | train/val/test=1.000/0.790/0.789 | c=0.998347
[Epoch 0024] loss=17.4147 cls=0.0847 smmd=1.1440 ct=11.5225 rec=1.2999 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0025] loss=17.2814 cls=0.0736 smmd=1.1257 ct=11.4863 rec=1.2971 | train/val/test=1.000/0.804/0.783 | c=0.998347
[Epoch 0026] loss=16.6202 cls=0.0649 smmd=1.0009 ct=11.4540 rec=1.2939 | train/val/test=1.000/0.800/0.790 | c=0.998347
[Epoch 0027] loss=16.7496 cls=0.0591 smmd=1.0227 ct=11.4772 rec=1.2943 | train/val/test=1.000/0.784/0.776 | c=0.998347
[Epoch 0028] loss=16.0463 cls=0.0582 smmd=0.8813 ct=11.4815 rec=1.2910 | train/val/test=1.000/0.796/0.781 | c=0.998347
[Epoch 0029] loss=16.1426 cls=0.0556 smmd=0.9044 ct=11.4633 rec=1.2932 | train/val/test=1.000/0.796/0.784 | c=0.998347
[Epoch 0030] loss=15.8414 cls=0.0595 smmd=0.8401 ct=11.4812 rec=1.2970 | train/val/test=1.000/0.798/0.778 | c=0.998347
[Epoch 0031] loss=16.0828 cls=0.0672 smmd=0.8829 ct=11.5043 rec=1.3024 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0032] loss=16.0824 cls=0.0675 smmd=0.8877 ct=11.4801 rec=1.3007 | train/val/test=1.000/0.814/0.781 | c=0.998347
[Epoch 0033] loss=15.8678 cls=0.0678 smmd=0.8417 ct=11.4950 rec=1.3030 | train/val/test=1.000/0.788/0.788 | c=0.998347
[Epoch 0034] loss=15.6713 cls=0.0660 smmd=0.8074 ct=11.4713 rec=1.2992 | train/val/test=1.000/0.804/0.786 | c=0.998347
[Epoch 0035] loss=15.3032 cls=0.0592 smmd=0.7345 ct=11.4716 rec=1.2953 | train/val/test=1.000/0.812/0.788 | c=0.998347
[Epoch 0036] loss=15.3637 cls=0.0610 smmd=0.7427 ct=11.4900 rec=1.2973 | train/val/test=1.000/0.798/0.781 | c=0.998347
[Epoch 0037] loss=14.8953 cls=0.0612 smmd=0.6503 ct=11.4837 rec=1.2927 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0038] loss=14.7791 cls=0.0625 smmd=0.6334 ct=11.4511 rec=1.2956 | train/val/test=1.000/0.812/0.791 | c=0.998347
[Epoch 0039] loss=14.9763 cls=0.0692 smmd=0.6685 ct=11.4688 rec=1.3030 | train/val/test=1.000/0.794/0.776 | c=0.998347
[Epoch 0040] loss=14.9091 cls=0.0805 smmd=0.6449 ct=11.5141 rec=1.3030 | train/val/test=1.000/0.816/0.780 | c=0.998347
[Epoch 0041] loss=15.1035 cls=0.0853 smmd=0.6914 ct=11.4724 rec=1.3148 | train/val/test=1.000/0.790/0.769 | c=0.998347
[Epoch 0042] loss=15.1329 cls=0.0871 smmd=0.6872 ct=11.5225 rec=1.3086 | train/val/test=1.000/0.808/0.776 | c=0.998347
[Epoch 0043] loss=14.7693 cls=0.0801 smmd=0.6224 ct=11.4863 rec=1.3112 | train/val/test=1.000/0.796/0.779 | c=0.998347
[Epoch 0044] loss=14.7539 cls=0.0765 smmd=0.6255 ct=11.4573 rec=1.3070 | train/val/test=1.000/0.786/0.755 | c=0.998347
[Epoch 0045] loss=14.6127 cls=0.0824 smmd=0.5852 ct=11.5141 rec=1.3132 | train/val/test=1.000/0.784/0.769 | c=0.998347
[Epoch 0046] loss=14.3604 cls=0.0774 smmd=0.5459 ct=11.4618 rec=1.3069 | train/val/test=1.000/0.804/0.776 | c=0.998347
[Epoch 0047] loss=14.3539 cls=0.0755 smmd=0.5394 ct=11.4879 rec=1.3118 | train/val/test=1.000/0.782/0.771 | c=0.998347
[Epoch 0048] loss=14.2919 cls=0.0798 smmd=0.5272 ct=11.4854 rec=1.3055 | train/val/test=1.000/0.812/0.776 | c=0.998347
[Epoch 0049] loss=14.4045 cls=0.0862 smmd=0.5507 ct=11.4757 rec=1.3202 | train/val/test=1.000/0.758/0.749 | c=0.998347
[Epoch 0050] loss=14.5895 cls=0.0999 smmd=0.5841 ct=11.4873 rec=1.3165 | train/val/test=1.000/0.768/0.751 | c=0.998347
[Epoch 0051] loss=14.6797 cls=0.1028 smmd=0.5919 ct=11.5356 rec=1.3295 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0052] loss=14.8183 cls=0.1259 smmd=0.6299 ct=11.4729 rec=1.3319 | train/val/test=1.000/0.734/0.732 | c=0.998347
[Epoch 0053] loss=14.4661 cls=0.1082 smmd=0.5469 ct=11.5444 rec=1.3300 | train/val/test=1.000/0.754/0.742 | c=0.998347
[Epoch 0054] loss=14.3027 cls=0.1003 smmd=0.5332 ct=11.4547 rec=1.3185 | train/val/test=1.000/0.786/0.763 | c=0.998347
[Epoch 0055] loss=14.2443 cls=0.0819 smmd=0.5167 ct=11.4877 rec=1.3195 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0056] loss=14.0860 cls=0.0811 smmd=0.4889 ct=11.4699 rec=1.3088 | train/val/test=1.000/0.810/0.775 | c=0.998347
[Epoch 0057] loss=14.0266 cls=0.0773 smmd=0.4782 ct=11.4653 rec=1.3161 | train/val/test=1.000/0.774/0.774 | c=0.998347
[Epoch 0058] loss=14.1684 cls=0.0827 smmd=0.5059 ct=11.4661 rec=1.3157 | train/val/test=1.000/0.788/0.768 | c=0.998347
[Epoch 0059] loss=14.4194 cls=0.0927 smmd=0.5460 ct=11.5108 rec=1.3241 | train/val/test=1.000/0.770/0.764 | c=0.998347
[Epoch 0060] loss=14.7557 cls=0.0972 smmd=0.6243 ct=11.4528 rec=1.3285 | train/val/test=1.000/0.760/0.745 | c=0.998347
[Epoch 0061] loss=14.5049 cls=0.1067 smmd=0.5616 ct=11.5101 rec=1.3323 | train/val/test=1.000/0.760/0.736 | c=0.998347
[Epoch 0062] loss=14.3175 cls=0.1031 smmd=0.5328 ct=11.4685 rec=1.3345 | train/val/test=1.000/0.722/0.715 | c=0.998347
[Epoch 0063] loss=14.1608 cls=0.1104 smmd=0.4978 ct=11.4830 rec=1.3353 | train/val/test=1.000/0.768/0.735 | c=0.998347
[Epoch 0064] loss=14.0900 cls=0.0965 smmd=0.4898 ct=11.4596 rec=1.3300 | train/val/test=1.000/0.742/0.742 | c=0.998347
[Epoch 0065] loss=13.8230 cls=0.0950 smmd=0.4312 ct=11.4865 rec=1.3285 | train/val/test=1.000/0.770/0.742 | c=0.998347
[Epoch 0066] loss=13.9780 cls=0.0974 smmd=0.4731 ct=11.4311 rec=1.3279 | train/val/test=1.000/0.746/0.732 | c=0.998347
[Epoch 0067] loss=14.2129 cls=0.1023 smmd=0.4994 ct=11.5313 rec=1.3359 | train/val/test=1.000/0.742/0.718 | c=0.998347
[Epoch 0068] loss=14.3150 cls=0.1211 smmd=0.5312 ct=11.4643 rec=1.3409 | train/val/test=1.000/0.700/0.682 | c=0.998347
[Epoch 0069] loss=14.4807 cls=0.1210 smmd=0.5508 ct=11.5313 rec=1.3469 | train/val/test=1.000/0.708/0.696 | c=0.998347
[Epoch 0070] loss=14.7584 cls=0.1410 smmd=0.6126 ct=11.4895 rec=1.3556 | train/val/test=1.000/0.718/0.695 | c=0.998347
[Epoch 0071] loss=14.1489 cls=0.1051 smmd=0.4871 ct=11.5274 rec=1.3368 | train/val/test=1.000/0.774/0.756 | c=0.998347
[Epoch 0072] loss=13.8572 cls=0.0783 smmd=0.4498 ct=11.4373 rec=1.3202 | train/val/test=1.000/0.810/0.778 | c=0.998347
[Epoch 0073] loss=13.7082 cls=0.0596 smmd=0.4169 ct=11.4629 rec=1.3104 | train/val/test=1.000/0.810/0.767 | c=0.998347
[Epoch 0074] loss=13.6602 cls=0.0615 smmd=0.4129 ct=11.4337 rec=1.3108 | train/val/test=1.000/0.804/0.778 | c=0.998347
[Epoch 0075] loss=13.6724 cls=0.0682 smmd=0.4116 ct=11.4485 rec=1.3178 | train/val/test=1.000/0.788/0.756 | c=0.998347
[Epoch 0076] loss=13.7284 cls=0.0821 smmd=0.4154 ct=11.4776 rec=1.3260 | train/val/test=1.000/0.792/0.757 | c=0.998347
[Epoch 0077] loss=14.3719 cls=0.0965 smmd=0.5452 ct=11.4639 rec=1.3363 | train/val/test=1.000/0.730/0.709 | c=0.998347
[Epoch 0078] loss=14.8774 cls=0.1144 smmd=0.6382 ct=11.4950 rec=1.3432 | train/val/test=1.000/0.666/0.649 | c=0.998347
[Epoch 0079] loss=14.4184 cls=0.1450 smmd=0.5352 ct=11.5346 rec=1.3532 | train/val/test=1.000/0.656/0.641 | c=0.998347
[Epoch 0080] loss=14.3502 cls=0.1613 smmd=0.5297 ct=11.4846 rec=1.3646 | train/val/test=0.962/0.570/0.584 | c=0.998347
[Epoch 0081] loss=14.3364 cls=0.1946 smmd=0.5044 ct=11.5807 rec=1.3633 | train/val/test=1.000/0.692/0.694 | c=0.998347
[Epoch 0082] loss=14.0371 cls=0.1312 smmd=0.4747 ct=11.4626 rec=1.3527 | train/val/test=1.000/0.750/0.731 | c=0.998347
[Epoch 0083] loss=13.5376 cls=0.0786 smmd=0.3827 ct=11.4523 rec=1.3258 | train/val/test=1.000/0.802/0.784 | c=0.998347
[Epoch 0084] loss=13.5044 cls=0.0633 smmd=0.3794 ct=11.4446 rec=1.3109 | train/val/test=1.000/0.772/0.764 | c=0.998347
[Epoch 0085] loss=13.6723 cls=0.0784 smmd=0.4100 ct=11.4513 rec=1.3199 | train/val/test=1.000/0.754/0.734 | c=0.998347
[Epoch 0086] loss=14.1085 cls=0.0941 smmd=0.4844 ct=11.5058 rec=1.3378 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0087] loss=14.9645 cls=0.1255 smmd=0.6550 ct=11.4917 rec=1.3515 | train/val/test=1.000/0.716/0.684 | c=0.998347
[Epoch 0088] loss=14.7207 cls=0.1103 smmd=0.5943 ct=11.5590 rec=1.3520 | train/val/test=1.000/0.724/0.711 | c=0.998347
[Epoch 0089] loss=14.1474 cls=0.0982 smmd=0.5029 ct=11.4502 rec=1.3351 | train/val/test=1.000/0.796/0.758 | c=0.998347
[Epoch 0090] loss=13.8131 cls=0.0663 smmd=0.4323 ct=11.4863 rec=1.3227 | train/val/test=1.000/0.792/0.753 | c=0.998347
[Epoch 0091] loss=13.4562 cls=0.0598 smmd=0.3728 ct=11.4311 rec=1.3122 | train/val/test=1.000/0.802/0.754 | c=0.998347
[Epoch 0092] loss=13.2356 cls=0.0609 smmd=0.3335 ct=11.4062 rec=1.3139 | train/val/test=1.000/0.796/0.774 | c=0.998347
[Epoch 0093] loss=13.3602 cls=0.0681 smmd=0.3493 ct=11.4473 rec=1.3206 | train/val/test=1.000/0.774/0.735 | c=0.998347
[Epoch 0094] loss=13.5910 cls=0.0947 smmd=0.3886 ct=11.4673 rec=1.3327 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0095] loss=14.0428 cls=0.1095 smmd=0.4803 ct=11.4526 rec=1.3407 | train/val/test=1.000/0.746/0.731 | c=0.998347
[Epoch 0096] loss=14.6492 cls=0.1338 smmd=0.5840 ct=11.5276 rec=1.3467 | train/val/test=1.000/0.754/0.728 | c=0.998347
[Epoch 0097] loss=15.0730 cls=0.1306 smmd=0.6802 ct=11.4717 rec=1.3481 | train/val/test=1.000/0.702/0.675 | c=0.998347
[Epoch 0098] loss=14.4771 cls=0.1358 smmd=0.5578 ct=11.4858 rec=1.3457 | train/val/test=1.000/0.708/0.693 | c=0.998347
[Epoch 0099] loss=13.9200 cls=0.1425 smmd=0.4476 ct=11.4756 rec=1.3529 | train/val/test=1.000/0.596/0.599 | c=0.998347
=== Best @ epoch 40: val=0.8160, test=0.7800 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-4 completed in 190.64 seconds.
==================================================
