Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 - 2025-09-21 05:42:05:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.7006 cls=1.0987 smmd=5.6647 ct=11.2825 rec=1.4138 | train/val/test=0.462/0.394/0.415 | c=0.998347
[Epoch 0001] loss=22.4929 cls=1.0877 smmd=4.0026 ct=11.2357 rec=1.4136 | train/val/test=0.538/0.416/0.469 | c=0.998347
[Epoch 0002] loss=24.7728 cls=1.0795 smmd=4.9095 ct=11.2525 rec=1.4136 | train/val/test=0.615/0.550/0.589 | c=0.998347
[Epoch 0003] loss=23.6454 cls=1.0573 smmd=4.5053 ct=11.1466 rec=1.4136 | train/val/test=0.692/0.576/0.609 | c=0.998347
[Epoch 0004] loss=18.2651 cls=1.0178 smmd=2.4345 ct=10.9635 rec=1.4131 | train/val/test=0.692/0.644/0.655 | c=0.998347
[Epoch 0005] loss=19.7344 cls=0.9776 smmd=3.0434 ct=10.9310 rec=1.4123 | train/val/test=0.769/0.660/0.663 | c=0.998347
[Epoch 0006] loss=20.4305 cls=0.9354 smmd=3.3422 ct=10.9019 rec=1.4109 | train/val/test=0.846/0.674/0.663 | c=0.998347
[Epoch 0007] loss=19.2226 cls=0.8929 smmd=2.5919 ct=11.5921 rec=1.4087 | train/val/test=0.846/0.660/0.655 | c=0.998347
[Epoch 0008] loss=17.1403 cls=0.8627 smmd=1.8294 ct=11.4324 rec=1.4060 | train/val/test=0.846/0.646/0.657 | c=0.998347
[Epoch 0009] loss=19.0466 cls=0.8461 smmd=2.5800 ct=11.4710 rec=1.4052 | train/val/test=0.923/0.664/0.663 | c=0.998347
[Epoch 0010] loss=19.2052 cls=0.8179 smmd=2.6609 ct=11.4414 rec=1.4053 | train/val/test=0.923/0.654/0.669 | c=0.998347
[Epoch 0011] loss=16.6205 cls=0.7891 smmd=1.6372 ct=11.4303 rec=1.4053 | train/val/test=0.923/0.674/0.675 | c=0.998347
[Epoch 0012] loss=18.6394 cls=0.7689 smmd=2.3578 ct=11.6585 rec=1.4039 | train/val/test=0.923/0.664/0.670 | c=0.998347
[Epoch 0013] loss=18.3145 cls=0.7167 smmd=2.3041 ct=11.4944 rec=1.4031 | train/val/test=0.923/0.682/0.687 | c=0.998347
[Epoch 0014] loss=16.6175 cls=0.6475 smmd=1.6731 ct=11.4129 rec=1.3962 | train/val/test=1.000/0.674/0.682 | c=0.998347
[Epoch 0015] loss=16.5126 cls=0.5888 smmd=1.6277 ct=11.4545 rec=1.3891 | train/val/test=0.923/0.686/0.685 | c=0.998347
[Epoch 0016] loss=16.5933 cls=0.5359 smmd=1.6755 ct=11.4447 rec=1.3839 | train/val/test=0.923/0.690/0.694 | c=0.998347
[Epoch 0017] loss=16.0113 cls=0.4989 smmd=1.4516 ct=11.4426 rec=1.3805 | train/val/test=1.000/0.692/0.697 | c=0.998347
[Epoch 0018] loss=15.6118 cls=0.4786 smmd=1.2991 ct=11.4350 rec=1.3795 | train/val/test=1.000/0.698/0.699 | c=0.998347
[Epoch 0019] loss=15.6928 cls=0.4730 smmd=1.3295 ct=11.4419 rec=1.3812 | train/val/test=1.000/0.708/0.716 | c=0.998347
[Epoch 0020] loss=15.6941 cls=0.4717 smmd=1.3157 ct=11.4765 rec=1.3847 | train/val/test=1.000/0.698/0.701 | c=0.998347
[Epoch 0021] loss=15.5258 cls=0.4701 smmd=1.2488 ct=11.4737 rec=1.3901 | train/val/test=1.000/0.728/0.723 | c=0.998347
[Epoch 0022] loss=15.6858 cls=0.4588 smmd=1.3219 ct=11.4575 rec=1.3880 | train/val/test=1.000/0.714/0.727 | c=0.998347
[Epoch 0023] loss=15.3377 cls=0.4203 smmd=1.1956 ct=11.4461 rec=1.3849 | train/val/test=1.000/0.728/0.722 | c=0.998347
[Epoch 0024] loss=15.0934 cls=0.3738 smmd=1.0908 ct=11.4909 rec=1.3771 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0025] loss=14.8926 cls=0.3313 smmd=1.0416 ct=11.4375 rec=1.3706 | train/val/test=1.000/0.716/0.720 | c=0.998347
[Epoch 0026] loss=14.5476 cls=0.3044 smmd=0.9088 ct=11.4419 rec=1.3631 | train/val/test=1.000/0.722/0.724 | c=0.998347
[Epoch 0027] loss=14.5664 cls=0.2802 smmd=0.9156 ct=11.4576 rec=1.3594 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0028] loss=14.2668 cls=0.2714 smmd=0.8032 ct=11.4430 rec=1.3601 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0029] loss=14.4399 cls=0.2779 smmd=0.8630 ct=11.4630 rec=1.3609 | train/val/test=1.000/0.732/0.724 | c=0.998347
[Epoch 0030] loss=14.5600 cls=0.2819 smmd=0.9099 ct=11.4620 rec=1.3645 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0031] loss=14.5298 cls=0.2836 smmd=0.8947 ct=11.4688 rec=1.3650 | train/val/test=1.000/0.716/0.725 | c=0.998347
[Epoch 0032] loss=14.7796 cls=0.2722 smmd=0.9838 ct=11.5014 rec=1.3650 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0033] loss=14.2539 cls=0.2474 smmd=0.8028 ct=11.4445 rec=1.3575 | train/val/test=1.000/0.726/0.729 | c=0.998347
[Epoch 0034] loss=14.2928 cls=0.2243 smmd=0.8105 ct=11.4789 rec=1.3508 | train/val/test=1.000/0.716/0.727 | c=0.998347
[Epoch 0035] loss=13.9671 cls=0.2053 smmd=0.7021 ct=11.4363 rec=1.3459 | train/val/test=1.000/0.730/0.723 | c=0.998347
[Epoch 0036] loss=13.8247 cls=0.1964 smmd=0.6368 ct=11.4633 rec=1.3423 | train/val/test=1.000/0.722/0.725 | c=0.998347
[Epoch 0037] loss=13.8691 cls=0.1875 smmd=0.6657 ct=11.4396 rec=1.3429 | train/val/test=1.000/0.736/0.733 | c=0.998347
[Epoch 0038] loss=13.8411 cls=0.1958 smmd=0.6466 ct=11.4544 rec=1.3447 | train/val/test=1.000/0.728/0.728 | c=0.998347
[Epoch 0039] loss=13.8645 cls=0.2029 smmd=0.6446 ct=11.4766 rec=1.3500 | train/val/test=1.000/0.726/0.731 | c=0.998347
[Epoch 0040] loss=14.2985 cls=0.2100 smmd=0.8256 ct=11.4534 rec=1.3523 | train/val/test=1.000/0.736/0.731 | c=0.998347
[Epoch 0041] loss=14.0726 cls=0.2022 smmd=0.7347 ct=11.4599 rec=1.3500 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0042] loss=14.0331 cls=0.1849 smmd=0.7077 ct=11.4987 rec=1.3452 | train/val/test=1.000/0.738/0.733 | c=0.998347
[Epoch 0043] loss=13.9038 cls=0.1656 smmd=0.6947 ct=11.4156 rec=1.3375 | train/val/test=1.000/0.734/0.726 | c=0.998347
[Epoch 0044] loss=13.5281 cls=0.1504 smmd=0.5302 ct=11.4619 rec=1.3313 | train/val/test=1.000/0.722/0.732 | c=0.998347
[Epoch 0045] loss=13.6636 cls=0.1427 smmd=0.5922 ct=11.4467 rec=1.3301 | train/val/test=1.000/0.728/0.727 | c=0.998347
[Epoch 0046] loss=13.4521 cls=0.1486 smmd=0.5098 ct=11.4377 rec=1.3312 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0047] loss=13.5828 cls=0.1547 smmd=0.5586 ct=11.4402 rec=1.3379 | train/val/test=1.000/0.718/0.722 | c=0.998347
[Epoch 0048] loss=13.8796 cls=0.1743 smmd=0.6348 ct=11.5340 rec=1.3427 | train/val/test=1.000/0.688/0.707 | c=0.998347
[Epoch 0049] loss=14.2486 cls=0.2008 smmd=0.8099 ct=11.4431 rec=1.3607 | train/val/test=1.000/0.620/0.616 | c=0.998347
[Epoch 0050] loss=14.2046 cls=0.2294 smmd=0.7239 ct=11.6010 rec=1.3585 | train/val/test=0.923/0.650/0.670 | c=0.998347
[Epoch 0051] loss=14.2840 cls=0.2372 smmd=0.7912 ct=11.4964 rec=1.3820 | train/val/test=1.000/0.704/0.695 | c=0.998347
[Epoch 0052] loss=13.5348 cls=0.1256 smmd=0.5284 ct=11.4893 rec=1.3236 | train/val/test=1.000/0.720/0.711 | c=0.998347
[Epoch 0053] loss=13.7365 cls=0.0853 smmd=0.6396 ct=11.4408 rec=1.3081 | train/val/test=1.000/0.712/0.726 | c=0.998347
[Epoch 0054] loss=13.2513 cls=0.0765 smmd=0.4503 ct=11.4325 rec=1.3096 | train/val/test=1.000/0.738/0.731 | c=0.998347
[Epoch 0055] loss=13.5510 cls=0.0841 smmd=0.5550 ct=11.4665 rec=1.3101 | train/val/test=1.000/0.734/0.733 | c=0.998347
[Epoch 0056] loss=13.5586 cls=0.1026 smmd=0.5632 ct=11.4391 rec=1.3206 | train/val/test=1.000/0.734/0.734 | c=0.998347
[Epoch 0057] loss=13.7477 cls=0.1286 smmd=0.6053 ct=11.5039 rec=1.3327 | train/val/test=1.000/0.700/0.716 | c=0.998347
[Epoch 0058] loss=14.3492 cls=0.1772 smmd=0.8388 ct=11.4854 rec=1.3566 | train/val/test=1.000/0.566/0.533 | c=0.998347
[Epoch 0059] loss=14.4665 cls=0.2378 smmd=0.8110 ct=11.6395 rec=1.3613 | train/val/test=0.846/0.622/0.645 | c=0.998347
[Epoch 0060] loss=14.4828 cls=0.3031 smmd=0.8366 ct=11.5402 rec=1.3990 | train/val/test=1.000/0.612/0.621 | c=0.998347
[Epoch 0061] loss=13.8052 cls=0.1725 smmd=0.6096 ct=11.5272 rec=1.3356 | train/val/test=1.000/0.736/0.736 | c=0.998347
[Epoch 0062] loss=13.4560 cls=0.0648 smmd=0.5398 ct=11.4225 rec=1.3031 | train/val/test=1.000/0.720/0.728 | c=0.998347
[Epoch 0063] loss=13.3857 cls=0.0670 smmd=0.5092 ct=11.4249 rec=1.3088 | train/val/test=1.000/0.706/0.709 | c=0.998347
[Epoch 0064] loss=13.4731 cls=0.0900 smmd=0.5121 ct=11.4899 rec=1.3157 | train/val/test=1.000/0.732/0.735 | c=0.998347
[Epoch 0065] loss=13.4226 cls=0.0723 smmd=0.5227 ct=11.4245 rec=1.3107 | train/val/test=1.000/0.732/0.736 | c=0.998347
[Epoch 0066] loss=13.5597 cls=0.0905 smmd=0.5511 ct=11.4773 rec=1.3189 | train/val/test=1.000/0.742/0.742 | c=0.998347
[Epoch 0067] loss=14.1239 cls=0.1145 smmd=0.7559 ct=11.5125 rec=1.3290 | train/val/test=1.000/0.726/0.730 | c=0.998347
[Epoch 0068] loss=14.3317 cls=0.1248 smmd=0.8581 ct=11.4570 rec=1.3341 | train/val/test=1.000/0.746/0.735 | c=0.998347
[Epoch 0069] loss=13.6992 cls=0.1098 smmd=0.5928 ct=11.5003 rec=1.3239 | train/val/test=1.000/0.738/0.744 | c=0.998347
[Epoch 0070] loss=13.4771 cls=0.0964 smmd=0.5314 ct=11.4430 rec=1.3150 | train/val/test=1.000/0.736/0.737 | c=0.998347
[Epoch 0071] loss=13.3397 cls=0.0895 smmd=0.4879 ct=11.4205 rec=1.3094 | train/val/test=1.000/0.732/0.744 | c=0.998347
[Epoch 0072] loss=13.1446 cls=0.0865 smmd=0.4038 ct=11.4374 rec=1.3091 | train/val/test=1.000/0.750/0.747 | c=0.998347
[Epoch 0073] loss=13.2266 cls=0.1085 smmd=0.4288 ct=11.4427 rec=1.3152 | train/val/test=1.000/0.742/0.748 | c=0.998347
[Epoch 0074] loss=13.2226 cls=0.1188 smmd=0.4310 ct=11.4238 rec=1.3236 | train/val/test=1.000/0.746/0.738 | c=0.998347
[Epoch 0075] loss=13.4418 cls=0.1447 smmd=0.4803 ct=11.5034 rec=1.3307 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0076] loss=14.0759 cls=0.1726 smmd=0.7314 ct=11.4863 rec=1.3498 | train/val/test=1.000/0.602/0.581 | c=0.998347
[Epoch 0077] loss=14.3364 cls=0.2295 smmd=0.7733 ct=11.6121 rec=1.3523 | train/val/test=0.923/0.604/0.621 | c=0.998347
[Epoch 0078] loss=14.6310 cls=0.2906 smmd=0.9023 ct=11.5244 rec=1.4112 | train/val/test=1.000/0.672/0.663 | c=0.998347
[Epoch 0079] loss=13.6120 cls=0.1391 smmd=0.5361 ct=11.5385 rec=1.3273 | train/val/test=1.000/0.748/0.727 | c=0.998347
[Epoch 0080] loss=13.6333 cls=0.0343 smmd=0.6162 ct=11.4329 rec=1.2853 | train/val/test=1.000/0.732/0.724 | c=0.998347
[Epoch 0081] loss=13.2449 cls=0.0300 smmd=0.4689 ct=11.4125 rec=1.2903 | train/val/test=1.000/0.752/0.734 | c=0.998347
[Epoch 0082] loss=13.5630 cls=0.0316 smmd=0.5757 ct=11.4642 rec=1.2873 | train/val/test=1.000/0.756/0.739 | c=0.998347
[Epoch 0083] loss=13.1789 cls=0.0390 smmd=0.4198 ct=11.4627 rec=1.2945 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0084] loss=13.5211 cls=0.0564 smmd=0.5558 ct=11.4476 rec=1.3114 | train/val/test=1.000/0.754/0.747 | c=0.998347
[Epoch 0085] loss=14.0062 cls=0.0852 smmd=0.7072 ct=11.5335 rec=1.3240 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0086] loss=14.3481 cls=0.1150 smmd=0.8518 ct=11.4907 rec=1.3408 | train/val/test=1.000/0.686/0.688 | c=0.998347
[Epoch 0087] loss=14.2364 cls=0.1178 smmd=0.8083 ct=11.4914 rec=1.3308 | train/val/test=1.000/0.700/0.713 | c=0.998347
[Epoch 0088] loss=13.4675 cls=0.1314 smmd=0.5016 ct=11.4795 rec=1.3367 | train/val/test=1.000/0.618/0.616 | c=0.998347
[Epoch 0089] loss=13.4902 cls=0.1277 smmd=0.5069 ct=11.4974 rec=1.3234 | train/val/test=1.000/0.696/0.710 | c=0.998347
[Epoch 0090] loss=13.4422 cls=0.1193 smmd=0.5189 ct=11.4229 rec=1.3248 | train/val/test=1.000/0.688/0.688 | c=0.998347
[Epoch 0091] loss=13.2113 cls=0.0914 smmd=0.4236 ct=11.4518 rec=1.3098 | train/val/test=1.000/0.726/0.730 | c=0.998347
[Epoch 0092] loss=13.1596 cls=0.0702 smmd=0.4115 ct=11.4431 rec=1.3050 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0093] loss=13.1166 cls=0.0761 smmd=0.3895 ct=11.4505 rec=1.3087 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0094] loss=13.6353 cls=0.1269 smmd=0.5642 ct=11.4982 rec=1.3266 | train/val/test=1.000/0.630/0.637 | c=0.998347
[Epoch 0095] loss=14.6638 cls=0.2180 smmd=0.9185 ct=11.5602 rec=1.3965 | train/val/test=0.846/0.534/0.490 | c=0.998347
[Epoch 0096] loss=14.7645 cls=0.3486 smmd=0.8679 ct=11.7344 rec=1.3722 | train/val/test=1.000/0.686/0.704 | c=0.998347
[Epoch 0097] loss=14.0611 cls=0.1008 smmd=0.7560 ct=11.4566 rec=1.3283 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0098] loss=13.3178 cls=0.0247 smmd=0.4768 ct=11.4703 rec=1.2863 | train/val/test=1.000/0.682/0.660 | c=0.998347
[Epoch 0099] loss=13.6125 cls=0.0468 smmd=0.5435 ct=11.5799 rec=1.3011 | train/val/test=1.000/0.738/0.725 | c=0.998347
=== Best @ epoch 82: val=0.7560, test=0.7390 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 - 2025-09-21 05:42:05:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.7006 cls=1.0987 smmd=5.6647 ct=11.2825 rec=1.4138 | train/val/test=0.462/0.394/0.415 | c=0.998347
[Epoch 0001] loss=22.4929 cls=1.0877 smmd=4.0026 ct=11.2357 rec=1.4136 | train/val/test=0.538/0.416/0.469 | c=0.998347
[Epoch 0002] loss=24.7728 cls=1.0795 smmd=4.9095 ct=11.2525 rec=1.4136 | train/val/test=0.615/0.550/0.589 | c=0.998347
[Epoch 0003] loss=23.6454 cls=1.0573 smmd=4.5053 ct=11.1466 rec=1.4136 | train/val/test=0.692/0.576/0.609 | c=0.998347
[Epoch 0004] loss=18.2651 cls=1.0178 smmd=2.4345 ct=10.9635 rec=1.4131 | train/val/test=0.692/0.644/0.655 | c=0.998347
[Epoch 0005] loss=19.7344 cls=0.9776 smmd=3.0434 ct=10.9310 rec=1.4123 | train/val/test=0.769/0.660/0.663 | c=0.998347
[Epoch 0006] loss=20.4305 cls=0.9354 smmd=3.3422 ct=10.9019 rec=1.4109 | train/val/test=0.846/0.674/0.663 | c=0.998347
[Epoch 0007] loss=19.2226 cls=0.8929 smmd=2.5919 ct=11.5921 rec=1.4087 | train/val/test=0.846/0.660/0.655 | c=0.998347
[Epoch 0008] loss=17.1403 cls=0.8627 smmd=1.8294 ct=11.4324 rec=1.4060 | train/val/test=0.846/0.646/0.657 | c=0.998347
[Epoch 0009] loss=19.0466 cls=0.8461 smmd=2.5800 ct=11.4710 rec=1.4052 | train/val/test=0.923/0.664/0.663 | c=0.998347
[Epoch 0010] loss=19.2052 cls=0.8179 smmd=2.6609 ct=11.4414 rec=1.4053 | train/val/test=0.923/0.654/0.669 | c=0.998347
[Epoch 0011] loss=16.6205 cls=0.7891 smmd=1.6372 ct=11.4303 rec=1.4053 | train/val/test=0.923/0.674/0.675 | c=0.998347
[Epoch 0012] loss=18.6394 cls=0.7689 smmd=2.3578 ct=11.6585 rec=1.4039 | train/val/test=0.923/0.664/0.670 | c=0.998347
[Epoch 0013] loss=18.3145 cls=0.7167 smmd=2.3041 ct=11.4944 rec=1.4031 | train/val/test=0.923/0.682/0.687 | c=0.998347
[Epoch 0014] loss=16.6175 cls=0.6475 smmd=1.6731 ct=11.4129 rec=1.3962 | train/val/test=1.000/0.674/0.682 | c=0.998347
[Epoch 0015] loss=16.5126 cls=0.5888 smmd=1.6277 ct=11.4545 rec=1.3891 | train/val/test=0.923/0.686/0.685 | c=0.998347
[Epoch 0016] loss=16.5933 cls=0.5359 smmd=1.6755 ct=11.4447 rec=1.3839 | train/val/test=0.923/0.690/0.694 | c=0.998347
[Epoch 0017] loss=16.0113 cls=0.4989 smmd=1.4516 ct=11.4426 rec=1.3805 | train/val/test=1.000/0.692/0.697 | c=0.998347
[Epoch 0018] loss=15.6118 cls=0.4786 smmd=1.2991 ct=11.4350 rec=1.3795 | train/val/test=1.000/0.698/0.699 | c=0.998347
[Epoch 0019] loss=15.6928 cls=0.4730 smmd=1.3295 ct=11.4419 rec=1.3812 | train/val/test=1.000/0.708/0.716 | c=0.998347
[Epoch 0020] loss=15.6941 cls=0.4717 smmd=1.3157 ct=11.4765 rec=1.3847 | train/val/test=1.000/0.698/0.701 | c=0.998347
[Epoch 0021] loss=15.5258 cls=0.4701 smmd=1.2488 ct=11.4737 rec=1.3901 | train/val/test=1.000/0.728/0.723 | c=0.998347
[Epoch 0022] loss=15.6858 cls=0.4588 smmd=1.3219 ct=11.4575 rec=1.3880 | train/val/test=1.000/0.714/0.727 | c=0.998347
[Epoch 0023] loss=15.3377 cls=0.4203 smmd=1.1956 ct=11.4461 rec=1.3849 | train/val/test=1.000/0.728/0.722 | c=0.998347
[Epoch 0024] loss=15.0934 cls=0.3738 smmd=1.0908 ct=11.4909 rec=1.3771 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0025] loss=14.8926 cls=0.3313 smmd=1.0416 ct=11.4375 rec=1.3706 | train/val/test=1.000/0.716/0.720 | c=0.998347
[Epoch 0026] loss=14.5476 cls=0.3044 smmd=0.9088 ct=11.4419 rec=1.3631 | train/val/test=1.000/0.722/0.724 | c=0.998347
[Epoch 0027] loss=14.5664 cls=0.2802 smmd=0.9156 ct=11.4576 rec=1.3594 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0028] loss=14.2668 cls=0.2714 smmd=0.8032 ct=11.4430 rec=1.3601 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0029] loss=14.4399 cls=0.2779 smmd=0.8630 ct=11.4630 rec=1.3609 | train/val/test=1.000/0.732/0.724 | c=0.998347
[Epoch 0030] loss=14.5600 cls=0.2819 smmd=0.9099 ct=11.4620 rec=1.3645 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0031] loss=14.5298 cls=0.2836 smmd=0.8947 ct=11.4688 rec=1.3650 | train/val/test=1.000/0.716/0.725 | c=0.998347
[Epoch 0032] loss=14.7796 cls=0.2722 smmd=0.9838 ct=11.5014 rec=1.3650 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0033] loss=14.2539 cls=0.2474 smmd=0.8028 ct=11.4445 rec=1.3575 | train/val/test=1.000/0.726/0.729 | c=0.998347
[Epoch 0034] loss=14.2928 cls=0.2243 smmd=0.8105 ct=11.4789 rec=1.3508 | train/val/test=1.000/0.716/0.727 | c=0.998347
[Epoch 0035] loss=13.9671 cls=0.2053 smmd=0.7021 ct=11.4363 rec=1.3459 | train/val/test=1.000/0.730/0.723 | c=0.998347
[Epoch 0036] loss=13.8247 cls=0.1964 smmd=0.6368 ct=11.4633 rec=1.3423 | train/val/test=1.000/0.722/0.725 | c=0.998347
[Epoch 0037] loss=13.8691 cls=0.1875 smmd=0.6657 ct=11.4396 rec=1.3429 | train/val/test=1.000/0.736/0.733 | c=0.998347
[Epoch 0038] loss=13.8411 cls=0.1958 smmd=0.6466 ct=11.4544 rec=1.3447 | train/val/test=1.000/0.728/0.728 | c=0.998347
[Epoch 0039] loss=13.8645 cls=0.2029 smmd=0.6446 ct=11.4766 rec=1.3500 | train/val/test=1.000/0.726/0.731 | c=0.998347
[Epoch 0040] loss=14.2985 cls=0.2100 smmd=0.8256 ct=11.4534 rec=1.3523 | train/val/test=1.000/0.736/0.731 | c=0.998347
[Epoch 0041] loss=14.0726 cls=0.2022 smmd=0.7347 ct=11.4599 rec=1.3500 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0042] loss=14.0331 cls=0.1849 smmd=0.7077 ct=11.4987 rec=1.3452 | train/val/test=1.000/0.738/0.733 | c=0.998347
[Epoch 0043] loss=13.9038 cls=0.1656 smmd=0.6947 ct=11.4156 rec=1.3375 | train/val/test=1.000/0.734/0.726 | c=0.998347
[Epoch 0044] loss=13.5281 cls=0.1504 smmd=0.5302 ct=11.4619 rec=1.3313 | train/val/test=1.000/0.722/0.732 | c=0.998347
[Epoch 0045] loss=13.6636 cls=0.1427 smmd=0.5922 ct=11.4467 rec=1.3301 | train/val/test=1.000/0.728/0.727 | c=0.998347
[Epoch 0046] loss=13.4521 cls=0.1486 smmd=0.5098 ct=11.4377 rec=1.3312 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0047] loss=13.5828 cls=0.1547 smmd=0.5586 ct=11.4402 rec=1.3379 | train/val/test=1.000/0.718/0.722 | c=0.998347
[Epoch 0048] loss=13.8796 cls=0.1743 smmd=0.6348 ct=11.5340 rec=1.3427 | train/val/test=1.000/0.688/0.707 | c=0.998347
[Epoch 0049] loss=14.2486 cls=0.2008 smmd=0.8099 ct=11.4431 rec=1.3607 | train/val/test=1.000/0.620/0.616 | c=0.998347
[Epoch 0050] loss=14.2046 cls=0.2294 smmd=0.7239 ct=11.6010 rec=1.3585 | train/val/test=0.923/0.650/0.670 | c=0.998347
[Epoch 0051] loss=14.2840 cls=0.2372 smmd=0.7912 ct=11.4964 rec=1.3820 | train/val/test=1.000/0.704/0.695 | c=0.998347
[Epoch 0052] loss=13.5348 cls=0.1256 smmd=0.5284 ct=11.4893 rec=1.3236 | train/val/test=1.000/0.720/0.711 | c=0.998347
[Epoch 0053] loss=13.7365 cls=0.0853 smmd=0.6396 ct=11.4408 rec=1.3081 | train/val/test=1.000/0.712/0.726 | c=0.998347
[Epoch 0054] loss=13.2513 cls=0.0765 smmd=0.4503 ct=11.4325 rec=1.3096 | train/val/test=1.000/0.738/0.731 | c=0.998347
[Epoch 0055] loss=13.5510 cls=0.0841 smmd=0.5550 ct=11.4665 rec=1.3101 | train/val/test=1.000/0.734/0.733 | c=0.998347
[Epoch 0056] loss=13.5586 cls=0.1026 smmd=0.5632 ct=11.4391 rec=1.3206 | train/val/test=1.000/0.734/0.734 | c=0.998347
[Epoch 0057] loss=13.7477 cls=0.1286 smmd=0.6053 ct=11.5039 rec=1.3327 | train/val/test=1.000/0.700/0.716 | c=0.998347
[Epoch 0058] loss=14.3492 cls=0.1772 smmd=0.8388 ct=11.4854 rec=1.3566 | train/val/test=1.000/0.566/0.533 | c=0.998347
[Epoch 0059] loss=14.4665 cls=0.2378 smmd=0.8110 ct=11.6395 rec=1.3613 | train/val/test=0.846/0.622/0.645 | c=0.998347
[Epoch 0060] loss=14.4828 cls=0.3031 smmd=0.8366 ct=11.5402 rec=1.3990 | train/val/test=1.000/0.612/0.621 | c=0.998347
[Epoch 0061] loss=13.8052 cls=0.1725 smmd=0.6096 ct=11.5272 rec=1.3356 | train/val/test=1.000/0.736/0.736 | c=0.998347
[Epoch 0062] loss=13.4560 cls=0.0648 smmd=0.5398 ct=11.4225 rec=1.3031 | train/val/test=1.000/0.720/0.728 | c=0.998347
[Epoch 0063] loss=13.3857 cls=0.0670 smmd=0.5092 ct=11.4249 rec=1.3088 | train/val/test=1.000/0.706/0.709 | c=0.998347
[Epoch 0064] loss=13.4731 cls=0.0900 smmd=0.5121 ct=11.4899 rec=1.3157 | train/val/test=1.000/0.732/0.735 | c=0.998347
[Epoch 0065] loss=13.4226 cls=0.0723 smmd=0.5227 ct=11.4245 rec=1.3107 | train/val/test=1.000/0.732/0.736 | c=0.998347
[Epoch 0066] loss=13.5597 cls=0.0905 smmd=0.5511 ct=11.4773 rec=1.3189 | train/val/test=1.000/0.742/0.742 | c=0.998347
[Epoch 0067] loss=14.1239 cls=0.1145 smmd=0.7559 ct=11.5125 rec=1.3290 | train/val/test=1.000/0.726/0.730 | c=0.998347
[Epoch 0068] loss=14.3317 cls=0.1248 smmd=0.8581 ct=11.4570 rec=1.3341 | train/val/test=1.000/0.746/0.735 | c=0.998347
[Epoch 0069] loss=13.6992 cls=0.1098 smmd=0.5928 ct=11.5003 rec=1.3239 | train/val/test=1.000/0.738/0.744 | c=0.998347
[Epoch 0070] loss=13.4771 cls=0.0964 smmd=0.5314 ct=11.4430 rec=1.3150 | train/val/test=1.000/0.736/0.737 | c=0.998347
[Epoch 0071] loss=13.3397 cls=0.0895 smmd=0.4879 ct=11.4205 rec=1.3094 | train/val/test=1.000/0.732/0.744 | c=0.998347
[Epoch 0072] loss=13.1446 cls=0.0865 smmd=0.4038 ct=11.4374 rec=1.3091 | train/val/test=1.000/0.750/0.747 | c=0.998347
[Epoch 0073] loss=13.2266 cls=0.1085 smmd=0.4288 ct=11.4427 rec=1.3152 | train/val/test=1.000/0.742/0.748 | c=0.998347
[Epoch 0074] loss=13.2226 cls=0.1188 smmd=0.4310 ct=11.4238 rec=1.3236 | train/val/test=1.000/0.746/0.738 | c=0.998347
[Epoch 0075] loss=13.4418 cls=0.1447 smmd=0.4803 ct=11.5034 rec=1.3307 | train/val/test=1.000/0.724/0.727 | c=0.998347
[Epoch 0076] loss=14.0759 cls=0.1726 smmd=0.7314 ct=11.4863 rec=1.3498 | train/val/test=1.000/0.602/0.581 | c=0.998347
[Epoch 0077] loss=14.3364 cls=0.2295 smmd=0.7733 ct=11.6121 rec=1.3523 | train/val/test=0.923/0.604/0.621 | c=0.998347
[Epoch 0078] loss=14.6310 cls=0.2906 smmd=0.9023 ct=11.5244 rec=1.4112 | train/val/test=1.000/0.672/0.663 | c=0.998347
[Epoch 0079] loss=13.6120 cls=0.1391 smmd=0.5361 ct=11.5385 rec=1.3273 | train/val/test=1.000/0.748/0.727 | c=0.998347
[Epoch 0080] loss=13.6333 cls=0.0343 smmd=0.6162 ct=11.4329 rec=1.2853 | train/val/test=1.000/0.732/0.724 | c=0.998347
[Epoch 0081] loss=13.2449 cls=0.0300 smmd=0.4689 ct=11.4125 rec=1.2903 | train/val/test=1.000/0.752/0.734 | c=0.998347
[Epoch 0082] loss=13.5630 cls=0.0316 smmd=0.5757 ct=11.4642 rec=1.2873 | train/val/test=1.000/0.756/0.739 | c=0.998347
[Epoch 0083] loss=13.1789 cls=0.0390 smmd=0.4198 ct=11.4627 rec=1.2945 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0084] loss=13.5211 cls=0.0564 smmd=0.5558 ct=11.4476 rec=1.3114 | train/val/test=1.000/0.754/0.747 | c=0.998347
[Epoch 0085] loss=14.0062 cls=0.0852 smmd=0.7072 ct=11.5335 rec=1.3240 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0086] loss=14.3481 cls=0.1150 smmd=0.8518 ct=11.4907 rec=1.3408 | train/val/test=1.000/0.686/0.688 | c=0.998347
[Epoch 0087] loss=14.2364 cls=0.1178 smmd=0.8083 ct=11.4914 rec=1.3308 | train/val/test=1.000/0.700/0.713 | c=0.998347
[Epoch 0088] loss=13.4675 cls=0.1314 smmd=0.5016 ct=11.4795 rec=1.3367 | train/val/test=1.000/0.618/0.616 | c=0.998347
[Epoch 0089] loss=13.4902 cls=0.1277 smmd=0.5069 ct=11.4974 rec=1.3234 | train/val/test=1.000/0.696/0.710 | c=0.998347
[Epoch 0090] loss=13.4422 cls=0.1193 smmd=0.5189 ct=11.4229 rec=1.3248 | train/val/test=1.000/0.688/0.688 | c=0.998347
[Epoch 0091] loss=13.2113 cls=0.0914 smmd=0.4236 ct=11.4518 rec=1.3098 | train/val/test=1.000/0.726/0.730 | c=0.998347
[Epoch 0092] loss=13.1596 cls=0.0702 smmd=0.4115 ct=11.4431 rec=1.3050 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0093] loss=13.1166 cls=0.0761 smmd=0.3895 ct=11.4505 rec=1.3087 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0094] loss=13.6353 cls=0.1269 smmd=0.5642 ct=11.4982 rec=1.3266 | train/val/test=1.000/0.630/0.637 | c=0.998347
[Epoch 0095] loss=14.6638 cls=0.2180 smmd=0.9185 ct=11.5602 rec=1.3965 | train/val/test=0.846/0.534/0.490 | c=0.998347
[Epoch 0096] loss=14.7645 cls=0.3486 smmd=0.8679 ct=11.7344 rec=1.3722 | train/val/test=1.000/0.686/0.704 | c=0.998347
[Epoch 0097] loss=14.0611 cls=0.1008 smmd=0.7560 ct=11.4566 rec=1.3283 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0098] loss=13.3178 cls=0.0247 smmd=0.4768 ct=11.4703 rec=1.2863 | train/val/test=1.000/0.682/0.660 | c=0.998347
[Epoch 0099] loss=13.6125 cls=0.0468 smmd=0.5435 ct=11.5799 rec=1.3011 | train/val/test=1.000/0.738/0.725 | c=0.998347
=== Best @ epoch 82: val=0.7560, test=0.7390 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 completed in 195.83 seconds.
==================================================
