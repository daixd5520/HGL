Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1 - 2025-09-21 04:27:02:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.2393 cls=1.9550 smmd=4.2367 ct=9.2698 rec=1.3889 | train/val/test=0.172/0.074/0.093 | c=0.998437
[Epoch 0001] loss=16.7294 cls=1.9055 smmd=2.8513 ct=9.1938 rec=1.3894 | train/val/test=0.621/0.206/0.241 | c=0.998437
[Epoch 0002] loss=15.2220 cls=1.7877 smmd=1.5543 ct=9.1023 rec=1.3888 | train/val/test=0.517/0.376/0.411 | c=0.998437
[Epoch 0003] loss=15.0165 cls=1.5737 smmd=1.5231 ct=9.1422 rec=1.3887 | train/val/test=0.862/0.552/0.540 | c=0.998437
[Epoch 0004] loss=14.7335 cls=1.2768 smmd=1.7184 ct=8.9709 rec=1.3837 | train/val/test=0.862/0.600/0.568 | c=0.998437
[Epoch 0005] loss=14.2956 cls=0.9479 smmd=1.6721 ct=8.9299 rec=1.3729 | train/val/test=0.862/0.580/0.571 | c=0.998437
[Epoch 0006] loss=13.6527 cls=0.6780 smmd=1.4019 ct=8.8637 rec=1.3545 | train/val/test=0.862/0.552/0.534 | c=0.998437
[Epoch 0007] loss=13.0825 cls=0.4798 smmd=1.1030 ct=8.8351 rec=1.3323 | train/val/test=0.931/0.548/0.535 | c=0.998437
[Epoch 0008] loss=12.7768 cls=0.3380 smmd=0.9970 ct=8.8242 rec=1.3088 | train/val/test=0.966/0.538/0.554 | c=0.998437
[Epoch 0009] loss=12.7134 cls=0.2449 smmd=1.0802 ct=8.8150 rec=1.2867 | train/val/test=0.966/0.546/0.565 | c=0.998437
[Epoch 0010] loss=12.6641 cls=0.1725 smmd=1.1372 ct=8.8134 rec=1.2705 | train/val/test=1.000/0.536/0.555 | c=0.998437
[Epoch 0011] loss=12.4779 cls=0.1147 smmd=1.0427 ct=8.8051 rec=1.2577 | train/val/test=1.000/0.526/0.548 | c=0.998437
[Epoch 0012] loss=12.2249 cls=0.0764 smmd=0.8517 ct=8.8022 rec=1.2473 | train/val/test=1.000/0.538/0.554 | c=0.998437
[Epoch 0013] loss=12.0776 cls=0.0502 smmd=0.7546 ct=8.7978 rec=1.2375 | train/val/test=1.000/0.550/0.560 | c=0.998437
[Epoch 0014] loss=11.9620 cls=0.0333 smmd=0.6761 ct=8.7960 rec=1.2282 | train/val/test=1.000/0.556/0.566 | c=0.998437
[Epoch 0015] loss=12.6303 cls=0.0230 smmd=0.6945 ct=9.4705 rec=1.2211 | train/val/test=1.000/0.562/0.568 | c=0.998437
[Epoch 0016] loss=12.5406 cls=0.0177 smmd=0.6929 ct=9.3957 rec=1.2172 | train/val/test=1.000/0.568/0.570 | c=0.998437
[Epoch 0017] loss=12.4325 cls=0.0137 smmd=0.6575 ct=9.3324 rec=1.2145 | train/val/test=1.000/0.566/0.577 | c=0.998437
[Epoch 0018] loss=12.3347 cls=0.0102 smmd=0.5734 ct=9.3276 rec=1.2117 | train/val/test=1.000/0.570/0.586 | c=0.998437
[Epoch 0019] loss=12.3054 cls=0.0097 smmd=0.4944 ct=9.3811 rec=1.2101 | train/val/test=1.000/0.566/0.579 | c=0.998437
[Epoch 0020] loss=12.2643 cls=0.0127 smmd=0.4002 ct=9.4330 rec=1.2092 | train/val/test=1.000/0.572/0.574 | c=0.998437
[Epoch 0021] loss=12.2131 cls=0.0161 smmd=0.3376 ct=9.4417 rec=1.2088 | train/val/test=1.000/0.578/0.582 | c=0.998437
[Epoch 0022] loss=12.1507 cls=0.0168 smmd=0.3028 ct=9.4149 rec=1.2081 | train/val/test=1.000/0.564/0.567 | c=0.998437
[Epoch 0023] loss=12.1161 cls=0.0208 smmd=0.2959 ct=9.3826 rec=1.2084 | train/val/test=1.000/0.570/0.577 | c=0.998437
[Epoch 0024] loss=12.0749 cls=0.0220 smmd=0.2602 ct=9.3805 rec=1.2061 | train/val/test=1.000/0.574/0.583 | c=0.998437
[Epoch 0025] loss=12.0581 cls=0.0226 smmd=0.2334 ct=9.3936 rec=1.2043 | train/val/test=1.000/0.572/0.562 | c=0.998437
[Epoch 0026] loss=12.0347 cls=0.0243 smmd=0.2107 ct=9.3939 rec=1.2029 | train/val/test=1.000/0.592/0.592 | c=0.998437
[Epoch 0027] loss=11.9890 cls=0.0253 smmd=0.1780 ct=9.3869 rec=1.1994 | train/val/test=1.000/0.580/0.565 | c=0.998437
[Epoch 0028] loss=11.9635 cls=0.0287 smmd=0.1653 ct=9.3776 rec=1.1959 | train/val/test=1.000/0.592/0.587 | c=0.998437
[Epoch 0029] loss=11.9580 cls=0.0273 smmd=0.1681 ct=9.3772 rec=1.1927 | train/val/test=1.000/0.574/0.557 | c=0.998437
[Epoch 0030] loss=11.9020 cls=0.0280 smmd=0.1290 ct=9.3682 rec=1.1884 | train/val/test=1.000/0.596/0.603 | c=0.998437
[Epoch 0031] loss=11.9007 cls=0.0242 smmd=0.1286 ct=9.3743 rec=1.1868 | train/val/test=1.000/0.574/0.559 | c=0.998437
[Epoch 0032] loss=11.8756 cls=0.0216 smmd=0.1272 ct=9.3617 rec=1.1826 | train/val/test=1.000/0.592/0.597 | c=0.998437
[Epoch 0033] loss=11.8392 cls=0.0156 smmd=0.1141 ct=9.3493 rec=1.1802 | train/val/test=1.000/0.586/0.584 | c=0.998437
[Epoch 0034] loss=11.7959 cls=0.0130 smmd=0.0966 ct=9.3337 rec=1.1763 | train/val/test=1.000/0.586/0.583 | c=0.998437
[Epoch 0035] loss=11.7982 cls=0.0133 smmd=0.0957 ct=9.3375 rec=1.1759 | train/val/test=1.000/0.598/0.597 | c=0.998437
[Epoch 0036] loss=11.7884 cls=0.0116 smmd=0.0730 ct=9.3508 rec=1.1765 | train/val/test=1.000/0.590/0.584 | c=0.998437
[Epoch 0037] loss=11.7712 cls=0.0134 smmd=0.0656 ct=9.3395 rec=1.1763 | train/val/test=1.000/0.596/0.598 | c=0.998437
[Epoch 0038] loss=11.7553 cls=0.0117 smmd=0.0611 ct=9.3299 rec=1.1763 | train/val/test=1.000/0.604/0.602 | c=0.998437
[Epoch 0039] loss=11.7521 cls=0.0119 smmd=0.0617 ct=9.3242 rec=1.1772 | train/val/test=1.000/0.594/0.591 | c=0.998437
[Epoch 0040] loss=11.7436 cls=0.0131 smmd=0.0489 ct=9.3235 rec=1.1791 | train/val/test=1.000/0.600/0.606 | c=0.998437
[Epoch 0041] loss=11.7473 cls=0.0137 smmd=0.0385 ct=9.3351 rec=1.1800 | train/val/test=1.000/0.590/0.587 | c=0.998437
[Epoch 0042] loss=11.7495 cls=0.0165 smmd=0.0439 ct=9.3265 rec=1.1813 | train/val/test=1.000/0.608/0.611 | c=0.998437
[Epoch 0043] loss=11.7525 cls=0.0162 smmd=0.0405 ct=9.3321 rec=1.1818 | train/val/test=1.000/0.584/0.585 | c=0.998437
[Epoch 0044] loss=11.7747 cls=0.0224 smmd=0.0633 ct=9.3235 rec=1.1828 | train/val/test=1.000/0.618/0.630 | c=0.998437
[Epoch 0045] loss=11.7934 cls=0.0193 smmd=0.0526 ct=9.3496 rec=1.1859 | train/val/test=1.000/0.588/0.576 | c=0.998437
[Epoch 0046] loss=11.7852 cls=0.0259 smmd=0.0706 ct=9.3274 rec=1.1807 | train/val/test=1.000/0.614/0.618 | c=0.998437
[Epoch 0047] loss=11.7232 cls=0.0129 smmd=0.0359 ct=9.3274 rec=1.1735 | train/val/test=1.000/0.612/0.611 | c=0.998437
[Epoch 0048] loss=11.6811 cls=0.0104 smmd=0.0223 ct=9.3129 rec=1.1678 | train/val/test=1.000/0.596/0.593 | c=0.998437
[Epoch 0049] loss=11.7208 cls=0.0134 smmd=0.0548 ct=9.3122 rec=1.1702 | train/val/test=1.000/0.610/0.613 | c=0.998437
[Epoch 0050] loss=11.7039 cls=0.0108 smmd=0.0343 ct=9.3176 rec=1.1706 | train/val/test=1.000/0.612/0.610 | c=0.998437
[Epoch 0051] loss=11.6669 cls=0.0094 smmd=0.0172 ct=9.3052 rec=1.1676 | train/val/test=1.000/0.600/0.598 | c=0.998437
[Epoch 0052] loss=11.6939 cls=0.0118 smmd=0.0319 ct=9.3071 rec=1.1716 | train/val/test=1.000/0.606/0.614 | c=0.998437
[Epoch 0053] loss=11.6929 cls=0.0115 smmd=0.0176 ct=9.3150 rec=1.1744 | train/val/test=1.000/0.610/0.609 | c=0.998437
[Epoch 0054] loss=11.6662 cls=0.0113 smmd=0.0122 ct=9.2988 rec=1.1719 | train/val/test=1.000/0.602/0.605 | c=0.998437
[Epoch 0055] loss=11.6690 cls=0.0128 smmd=0.0120 ct=9.2961 rec=1.1740 | train/val/test=1.000/0.608/0.617 | c=0.998437
[Epoch 0056] loss=11.6953 cls=0.0142 smmd=0.0150 ct=9.3093 rec=1.1784 | train/val/test=1.000/0.590/0.589 | c=0.998437
[Epoch 0057] loss=11.6988 cls=0.0193 smmd=0.0201 ct=9.3011 rec=1.1791 | train/val/test=1.000/0.612/0.615 | c=0.998437
[Epoch 0058] loss=11.6775 cls=0.0145 smmd=0.0040 ct=9.3055 rec=1.1767 | train/val/test=1.000/0.604/0.606 | c=0.998437
[Epoch 0059] loss=11.6503 cls=0.0146 smmd=0.0006 ct=9.2902 rec=1.1725 | train/val/test=1.000/0.608/0.608 | c=0.998437
[Epoch 0060] loss=11.6514 cls=0.0144 smmd=0.0068 ct=9.2870 rec=1.1716 | train/val/test=1.000/0.620/0.615 | c=0.998437
[Epoch 0061] loss=11.6610 cls=0.0140 smmd=0.0022 ct=9.2975 rec=1.1736 | train/val/test=1.000/0.604/0.597 | c=0.998437
[Epoch 0062] loss=11.6608 cls=0.0165 smmd=0.0107 ct=9.2881 rec=1.1728 | train/val/test=1.000/0.618/0.615 | c=0.998437
[Epoch 0063] loss=11.6426 cls=0.0137 smmd=-0.0032 ct=9.2876 rec=1.1723 | train/val/test=1.000/0.612/0.608 | c=0.998437
[Epoch 0064] loss=11.6363 cls=0.0134 smmd=-0.0005 ct=9.2810 rec=1.1712 | train/val/test=1.000/0.604/0.609 | c=0.998437
[Epoch 0065] loss=11.6392 cls=0.0144 smmd=0.0001 ct=9.2800 rec=1.1723 | train/val/test=1.000/0.616/0.612 | c=0.998437
[Epoch 0066] loss=11.6442 cls=0.0139 smmd=-0.0040 ct=9.2849 rec=1.1746 | train/val/test=1.000/0.600/0.597 | c=0.998437
[Epoch 0067] loss=11.6436 cls=0.0156 smmd=0.0018 ct=9.2763 rec=1.1750 | train/val/test=1.000/0.618/0.612 | c=0.998437
[Epoch 0068] loss=11.6334 cls=0.0138 smmd=-0.0081 ct=9.2780 rec=1.1748 | train/val/test=1.000/0.610/0.608 | c=0.998437
[Epoch 0069] loss=11.6226 cls=0.0142 smmd=-0.0113 ct=9.2722 rec=1.1737 | train/val/test=1.000/0.612/0.606 | c=0.998437
[Epoch 0070] loss=11.6259 cls=0.0135 smmd=-0.0081 ct=9.2739 rec=1.1733 | train/val/test=1.000/0.618/0.609 | c=0.998437
[Epoch 0071] loss=11.6242 cls=0.0134 smmd=-0.0101 ct=9.2739 rec=1.1735 | train/val/test=1.000/0.608/0.604 | c=0.998437
[Epoch 0072] loss=11.6217 cls=0.0149 smmd=-0.0095 ct=9.2682 rec=1.1740 | train/val/test=1.000/0.620/0.616 | c=0.998437
[Epoch 0073] loss=11.6225 cls=0.0138 smmd=-0.0138 ct=9.2727 rec=1.1749 | train/val/test=1.000/0.606/0.601 | c=0.998437
[Epoch 0074] loss=11.6277 cls=0.0159 smmd=-0.0047 ct=9.2673 rec=1.1746 | train/val/test=1.000/0.620/0.616 | c=0.998437
[Epoch 0075] loss=11.6259 cls=0.0139 smmd=-0.0151 ct=9.2769 rec=1.1751 | train/val/test=1.000/0.606/0.602 | c=0.998437
[Epoch 0076] loss=11.6202 cls=0.0153 smmd=-0.0079 ct=9.2647 rec=1.1740 | train/val/test=1.000/0.618/0.616 | c=0.998437
[Epoch 0077] loss=11.6144 cls=0.0138 smmd=-0.0141 ct=9.2664 rec=1.1742 | train/val/test=1.000/0.614/0.606 | c=0.998437
[Epoch 0078] loss=11.6091 cls=0.0144 smmd=-0.0141 ct=9.2621 rec=1.1734 | train/val/test=1.000/0.620/0.609 | c=0.998437
[Epoch 0079] loss=11.6071 cls=0.0138 smmd=-0.0166 ct=9.2631 rec=1.1734 | train/val/test=1.000/0.618/0.606 | c=0.998437
[Epoch 0080] loss=11.6017 cls=0.0140 smmd=-0.0225 ct=9.2630 rec=1.1736 | train/val/test=1.000/0.618/0.608 | c=0.998437
[Epoch 0081] loss=11.6048 cls=0.0144 smmd=-0.0169 ct=9.2596 rec=1.1738 | train/val/test=1.000/0.622/0.612 | c=0.998437
[Epoch 0082] loss=11.6030 cls=0.0142 smmd=-0.0210 ct=9.2608 rec=1.1745 | train/val/test=1.000/0.610/0.603 | c=0.998437
[Epoch 0083] loss=11.6061 cls=0.0159 smmd=-0.0151 ct=9.2559 rec=1.1747 | train/val/test=1.000/0.622/0.618 | c=0.998437
[Epoch 0084] loss=11.6158 cls=0.0147 smmd=-0.0175 ct=9.2661 rec=1.1763 | train/val/test=1.000/0.604/0.589 | c=0.998437
[Epoch 0085] loss=11.6285 cls=0.0192 smmd=-0.0064 ct=9.2606 rec=1.1775 | train/val/test=1.000/0.628/0.630 | c=0.998437
[Epoch 0086] loss=11.6676 cls=0.0179 smmd=-0.0010 ct=9.2847 rec=1.1830 | train/val/test=1.000/0.592/0.581 | c=0.998437
[Epoch 0087] loss=11.7201 cls=0.0285 smmd=0.0407 ct=9.2776 rec=1.1867 | train/val/test=1.000/0.642/0.641 | c=0.998437
[Epoch 0088] loss=11.7421 cls=0.0230 smmd=0.0308 ct=9.3091 rec=1.1896 | train/val/test=1.000/0.602/0.590 | c=0.998437
[Epoch 0089] loss=11.6494 cls=0.0127 smmd=0.0242 ct=9.2668 rec=1.1728 | train/val/test=1.000/0.614/0.608 | c=0.998437
[Epoch 0090] loss=11.5945 cls=0.0081 smmd=-0.0050 ct=9.2594 rec=1.1660 | train/val/test=1.000/0.638/0.638 | c=0.998437
[Epoch 0091] loss=11.6659 cls=0.0104 smmd=0.0138 ct=9.2923 rec=1.1747 | train/val/test=1.000/0.612/0.599 | c=0.998437
[Epoch 0092] loss=11.6109 cls=0.0084 smmd=0.0066 ct=9.2621 rec=1.1669 | train/val/test=1.000/0.614/0.599 | c=0.998437
[Epoch 0093] loss=11.6097 cls=0.0092 smmd=0.0045 ct=9.2593 rec=1.1684 | train/val/test=1.000/0.630/0.629 | c=0.998437
[Epoch 0094] loss=11.6535 cls=0.0106 smmd=0.0075 ct=9.2837 rec=1.1758 | train/val/test=1.000/0.614/0.604 | c=0.998437
[Epoch 0095] loss=11.6014 cls=0.0112 smmd=-0.0096 ct=9.2574 rec=1.1712 | train/val/test=1.000/0.614/0.602 | c=0.998437
[Epoch 0096] loss=11.6071 cls=0.0136 smmd=-0.0117 ct=9.2568 rec=1.1742 | train/val/test=1.000/0.632/0.627 | c=0.998437
[Epoch 0097] loss=11.6647 cls=0.0151 smmd=-0.0003 ct=9.2827 rec=1.1836 | train/val/test=1.000/0.598/0.588 | c=0.998437
[Epoch 0098] loss=11.6659 cls=0.0247 smmd=0.0099 ct=9.2640 rec=1.1837 | train/val/test=1.000/0.634/0.628 | c=0.998437
[Epoch 0099] loss=11.6332 cls=0.0149 smmd=-0.0108 ct=9.2713 rec=1.1789 | train/val/test=1.000/0.620/0.607 | c=0.998437
=== Best @ epoch 87: val=0.6420, test=0.6410 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1 - 2025-09-21 04:27:02:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.2393 cls=1.9550 smmd=4.2367 ct=9.2698 rec=1.3889 | train/val/test=0.172/0.074/0.093 | c=0.998437
[Epoch 0001] loss=16.7294 cls=1.9055 smmd=2.8513 ct=9.1938 rec=1.3894 | train/val/test=0.621/0.206/0.241 | c=0.998437
[Epoch 0002] loss=15.2220 cls=1.7877 smmd=1.5543 ct=9.1023 rec=1.3888 | train/val/test=0.517/0.376/0.411 | c=0.998437
[Epoch 0003] loss=15.0165 cls=1.5737 smmd=1.5231 ct=9.1422 rec=1.3887 | train/val/test=0.862/0.552/0.540 | c=0.998437
[Epoch 0004] loss=14.7335 cls=1.2768 smmd=1.7184 ct=8.9709 rec=1.3837 | train/val/test=0.862/0.600/0.568 | c=0.998437
[Epoch 0005] loss=14.2956 cls=0.9479 smmd=1.6721 ct=8.9299 rec=1.3729 | train/val/test=0.862/0.580/0.571 | c=0.998437
[Epoch 0006] loss=13.6527 cls=0.6780 smmd=1.4019 ct=8.8637 rec=1.3545 | train/val/test=0.862/0.552/0.534 | c=0.998437
[Epoch 0007] loss=13.0825 cls=0.4798 smmd=1.1030 ct=8.8351 rec=1.3323 | train/val/test=0.931/0.548/0.535 | c=0.998437
[Epoch 0008] loss=12.7768 cls=0.3380 smmd=0.9970 ct=8.8242 rec=1.3088 | train/val/test=0.966/0.538/0.554 | c=0.998437
[Epoch 0009] loss=12.7134 cls=0.2449 smmd=1.0802 ct=8.8150 rec=1.2867 | train/val/test=0.966/0.546/0.565 | c=0.998437
[Epoch 0010] loss=12.6641 cls=0.1725 smmd=1.1372 ct=8.8134 rec=1.2705 | train/val/test=1.000/0.536/0.555 | c=0.998437
[Epoch 0011] loss=12.4779 cls=0.1147 smmd=1.0427 ct=8.8051 rec=1.2577 | train/val/test=1.000/0.526/0.548 | c=0.998437
[Epoch 0012] loss=12.2249 cls=0.0764 smmd=0.8517 ct=8.8022 rec=1.2473 | train/val/test=1.000/0.538/0.554 | c=0.998437
[Epoch 0013] loss=12.0776 cls=0.0502 smmd=0.7546 ct=8.7978 rec=1.2375 | train/val/test=1.000/0.550/0.560 | c=0.998437
[Epoch 0014] loss=11.9620 cls=0.0333 smmd=0.6761 ct=8.7960 rec=1.2282 | train/val/test=1.000/0.556/0.566 | c=0.998437
[Epoch 0015] loss=12.6303 cls=0.0230 smmd=0.6945 ct=9.4705 rec=1.2211 | train/val/test=1.000/0.562/0.568 | c=0.998437
[Epoch 0016] loss=12.5406 cls=0.0177 smmd=0.6929 ct=9.3957 rec=1.2172 | train/val/test=1.000/0.568/0.570 | c=0.998437
[Epoch 0017] loss=12.4325 cls=0.0137 smmd=0.6575 ct=9.3324 rec=1.2145 | train/val/test=1.000/0.566/0.577 | c=0.998437
[Epoch 0018] loss=12.3347 cls=0.0102 smmd=0.5734 ct=9.3276 rec=1.2117 | train/val/test=1.000/0.570/0.586 | c=0.998437
[Epoch 0019] loss=12.3054 cls=0.0097 smmd=0.4944 ct=9.3811 rec=1.2101 | train/val/test=1.000/0.566/0.579 | c=0.998437
[Epoch 0020] loss=12.2643 cls=0.0127 smmd=0.4002 ct=9.4330 rec=1.2092 | train/val/test=1.000/0.572/0.574 | c=0.998437
[Epoch 0021] loss=12.2131 cls=0.0161 smmd=0.3376 ct=9.4417 rec=1.2088 | train/val/test=1.000/0.578/0.582 | c=0.998437
[Epoch 0022] loss=12.1507 cls=0.0168 smmd=0.3028 ct=9.4149 rec=1.2081 | train/val/test=1.000/0.564/0.567 | c=0.998437
[Epoch 0023] loss=12.1161 cls=0.0208 smmd=0.2959 ct=9.3826 rec=1.2084 | train/val/test=1.000/0.570/0.577 | c=0.998437
[Epoch 0024] loss=12.0749 cls=0.0220 smmd=0.2602 ct=9.3805 rec=1.2061 | train/val/test=1.000/0.574/0.583 | c=0.998437
[Epoch 0025] loss=12.0581 cls=0.0226 smmd=0.2334 ct=9.3936 rec=1.2043 | train/val/test=1.000/0.572/0.562 | c=0.998437
[Epoch 0026] loss=12.0347 cls=0.0243 smmd=0.2107 ct=9.3939 rec=1.2029 | train/val/test=1.000/0.592/0.592 | c=0.998437
[Epoch 0027] loss=11.9890 cls=0.0253 smmd=0.1780 ct=9.3869 rec=1.1994 | train/val/test=1.000/0.580/0.565 | c=0.998437
[Epoch 0028] loss=11.9635 cls=0.0287 smmd=0.1653 ct=9.3776 rec=1.1959 | train/val/test=1.000/0.592/0.587 | c=0.998437
[Epoch 0029] loss=11.9580 cls=0.0273 smmd=0.1681 ct=9.3772 rec=1.1927 | train/val/test=1.000/0.574/0.557 | c=0.998437
[Epoch 0030] loss=11.9020 cls=0.0280 smmd=0.1290 ct=9.3682 rec=1.1884 | train/val/test=1.000/0.596/0.603 | c=0.998437
[Epoch 0031] loss=11.9007 cls=0.0242 smmd=0.1286 ct=9.3743 rec=1.1868 | train/val/test=1.000/0.574/0.559 | c=0.998437
[Epoch 0032] loss=11.8756 cls=0.0216 smmd=0.1272 ct=9.3617 rec=1.1826 | train/val/test=1.000/0.592/0.597 | c=0.998437
[Epoch 0033] loss=11.8392 cls=0.0156 smmd=0.1141 ct=9.3493 rec=1.1802 | train/val/test=1.000/0.586/0.584 | c=0.998437
[Epoch 0034] loss=11.7959 cls=0.0130 smmd=0.0966 ct=9.3337 rec=1.1763 | train/val/test=1.000/0.586/0.583 | c=0.998437
[Epoch 0035] loss=11.7982 cls=0.0133 smmd=0.0957 ct=9.3375 rec=1.1759 | train/val/test=1.000/0.598/0.597 | c=0.998437
[Epoch 0036] loss=11.7884 cls=0.0116 smmd=0.0730 ct=9.3508 rec=1.1765 | train/val/test=1.000/0.590/0.584 | c=0.998437
[Epoch 0037] loss=11.7712 cls=0.0134 smmd=0.0656 ct=9.3395 rec=1.1763 | train/val/test=1.000/0.596/0.598 | c=0.998437
[Epoch 0038] loss=11.7553 cls=0.0117 smmd=0.0611 ct=9.3299 rec=1.1763 | train/val/test=1.000/0.604/0.602 | c=0.998437
[Epoch 0039] loss=11.7521 cls=0.0119 smmd=0.0617 ct=9.3242 rec=1.1772 | train/val/test=1.000/0.594/0.591 | c=0.998437
[Epoch 0040] loss=11.7436 cls=0.0131 smmd=0.0489 ct=9.3235 rec=1.1791 | train/val/test=1.000/0.600/0.606 | c=0.998437
[Epoch 0041] loss=11.7473 cls=0.0137 smmd=0.0385 ct=9.3351 rec=1.1800 | train/val/test=1.000/0.590/0.587 | c=0.998437
[Epoch 0042] loss=11.7495 cls=0.0165 smmd=0.0439 ct=9.3265 rec=1.1813 | train/val/test=1.000/0.608/0.611 | c=0.998437
[Epoch 0043] loss=11.7525 cls=0.0162 smmd=0.0405 ct=9.3321 rec=1.1818 | train/val/test=1.000/0.584/0.585 | c=0.998437
[Epoch 0044] loss=11.7747 cls=0.0224 smmd=0.0633 ct=9.3235 rec=1.1828 | train/val/test=1.000/0.618/0.630 | c=0.998437
[Epoch 0045] loss=11.7934 cls=0.0193 smmd=0.0526 ct=9.3496 rec=1.1859 | train/val/test=1.000/0.588/0.576 | c=0.998437
[Epoch 0046] loss=11.7852 cls=0.0259 smmd=0.0706 ct=9.3274 rec=1.1807 | train/val/test=1.000/0.614/0.618 | c=0.998437
[Epoch 0047] loss=11.7232 cls=0.0129 smmd=0.0359 ct=9.3274 rec=1.1735 | train/val/test=1.000/0.612/0.611 | c=0.998437
[Epoch 0048] loss=11.6811 cls=0.0104 smmd=0.0223 ct=9.3129 rec=1.1678 | train/val/test=1.000/0.596/0.593 | c=0.998437
[Epoch 0049] loss=11.7208 cls=0.0134 smmd=0.0548 ct=9.3122 rec=1.1702 | train/val/test=1.000/0.610/0.613 | c=0.998437
[Epoch 0050] loss=11.7039 cls=0.0108 smmd=0.0343 ct=9.3176 rec=1.1706 | train/val/test=1.000/0.612/0.610 | c=0.998437
[Epoch 0051] loss=11.6669 cls=0.0094 smmd=0.0172 ct=9.3052 rec=1.1676 | train/val/test=1.000/0.600/0.598 | c=0.998437
[Epoch 0052] loss=11.6939 cls=0.0118 smmd=0.0319 ct=9.3071 rec=1.1716 | train/val/test=1.000/0.606/0.614 | c=0.998437
[Epoch 0053] loss=11.6929 cls=0.0115 smmd=0.0176 ct=9.3150 rec=1.1744 | train/val/test=1.000/0.610/0.609 | c=0.998437
[Epoch 0054] loss=11.6662 cls=0.0113 smmd=0.0122 ct=9.2988 rec=1.1719 | train/val/test=1.000/0.602/0.605 | c=0.998437
[Epoch 0055] loss=11.6690 cls=0.0128 smmd=0.0120 ct=9.2961 rec=1.1740 | train/val/test=1.000/0.608/0.617 | c=0.998437
[Epoch 0056] loss=11.6953 cls=0.0142 smmd=0.0150 ct=9.3093 rec=1.1784 | train/val/test=1.000/0.590/0.589 | c=0.998437
[Epoch 0057] loss=11.6988 cls=0.0193 smmd=0.0201 ct=9.3011 rec=1.1791 | train/val/test=1.000/0.612/0.615 | c=0.998437
[Epoch 0058] loss=11.6775 cls=0.0145 smmd=0.0040 ct=9.3055 rec=1.1767 | train/val/test=1.000/0.604/0.606 | c=0.998437
[Epoch 0059] loss=11.6503 cls=0.0146 smmd=0.0006 ct=9.2902 rec=1.1725 | train/val/test=1.000/0.608/0.608 | c=0.998437
[Epoch 0060] loss=11.6514 cls=0.0144 smmd=0.0068 ct=9.2870 rec=1.1716 | train/val/test=1.000/0.620/0.615 | c=0.998437
[Epoch 0061] loss=11.6610 cls=0.0140 smmd=0.0022 ct=9.2975 rec=1.1736 | train/val/test=1.000/0.604/0.597 | c=0.998437
[Epoch 0062] loss=11.6608 cls=0.0165 smmd=0.0107 ct=9.2881 rec=1.1728 | train/val/test=1.000/0.618/0.615 | c=0.998437
[Epoch 0063] loss=11.6426 cls=0.0137 smmd=-0.0032 ct=9.2876 rec=1.1723 | train/val/test=1.000/0.612/0.608 | c=0.998437
[Epoch 0064] loss=11.6363 cls=0.0134 smmd=-0.0005 ct=9.2810 rec=1.1712 | train/val/test=1.000/0.604/0.609 | c=0.998437
[Epoch 0065] loss=11.6392 cls=0.0144 smmd=0.0001 ct=9.2800 rec=1.1723 | train/val/test=1.000/0.616/0.612 | c=0.998437
[Epoch 0066] loss=11.6442 cls=0.0139 smmd=-0.0040 ct=9.2849 rec=1.1746 | train/val/test=1.000/0.600/0.597 | c=0.998437
[Epoch 0067] loss=11.6436 cls=0.0156 smmd=0.0018 ct=9.2763 rec=1.1750 | train/val/test=1.000/0.618/0.612 | c=0.998437
[Epoch 0068] loss=11.6334 cls=0.0138 smmd=-0.0081 ct=9.2780 rec=1.1748 | train/val/test=1.000/0.610/0.608 | c=0.998437
[Epoch 0069] loss=11.6226 cls=0.0142 smmd=-0.0113 ct=9.2722 rec=1.1737 | train/val/test=1.000/0.612/0.606 | c=0.998437
[Epoch 0070] loss=11.6259 cls=0.0135 smmd=-0.0081 ct=9.2739 rec=1.1733 | train/val/test=1.000/0.618/0.609 | c=0.998437
[Epoch 0071] loss=11.6242 cls=0.0134 smmd=-0.0101 ct=9.2739 rec=1.1735 | train/val/test=1.000/0.608/0.604 | c=0.998437
[Epoch 0072] loss=11.6217 cls=0.0149 smmd=-0.0095 ct=9.2682 rec=1.1740 | train/val/test=1.000/0.620/0.616 | c=0.998437
[Epoch 0073] loss=11.6225 cls=0.0138 smmd=-0.0138 ct=9.2727 rec=1.1749 | train/val/test=1.000/0.606/0.601 | c=0.998437
[Epoch 0074] loss=11.6277 cls=0.0159 smmd=-0.0047 ct=9.2673 rec=1.1746 | train/val/test=1.000/0.620/0.616 | c=0.998437
[Epoch 0075] loss=11.6259 cls=0.0139 smmd=-0.0151 ct=9.2769 rec=1.1751 | train/val/test=1.000/0.606/0.602 | c=0.998437
[Epoch 0076] loss=11.6202 cls=0.0153 smmd=-0.0079 ct=9.2647 rec=1.1740 | train/val/test=1.000/0.618/0.616 | c=0.998437
[Epoch 0077] loss=11.6144 cls=0.0138 smmd=-0.0141 ct=9.2664 rec=1.1742 | train/val/test=1.000/0.614/0.606 | c=0.998437
[Epoch 0078] loss=11.6091 cls=0.0144 smmd=-0.0141 ct=9.2621 rec=1.1734 | train/val/test=1.000/0.620/0.609 | c=0.998437
[Epoch 0079] loss=11.6071 cls=0.0138 smmd=-0.0166 ct=9.2631 rec=1.1734 | train/val/test=1.000/0.618/0.606 | c=0.998437
[Epoch 0080] loss=11.6017 cls=0.0140 smmd=-0.0225 ct=9.2630 rec=1.1736 | train/val/test=1.000/0.618/0.608 | c=0.998437
[Epoch 0081] loss=11.6048 cls=0.0144 smmd=-0.0169 ct=9.2596 rec=1.1738 | train/val/test=1.000/0.622/0.612 | c=0.998437
[Epoch 0082] loss=11.6030 cls=0.0142 smmd=-0.0210 ct=9.2608 rec=1.1745 | train/val/test=1.000/0.610/0.603 | c=0.998437
[Epoch 0083] loss=11.6061 cls=0.0159 smmd=-0.0151 ct=9.2559 rec=1.1747 | train/val/test=1.000/0.622/0.618 | c=0.998437
[Epoch 0084] loss=11.6158 cls=0.0147 smmd=-0.0175 ct=9.2661 rec=1.1763 | train/val/test=1.000/0.604/0.589 | c=0.998437
[Epoch 0085] loss=11.6285 cls=0.0192 smmd=-0.0064 ct=9.2606 rec=1.1775 | train/val/test=1.000/0.628/0.630 | c=0.998437
[Epoch 0086] loss=11.6676 cls=0.0179 smmd=-0.0010 ct=9.2847 rec=1.1830 | train/val/test=1.000/0.592/0.581 | c=0.998437
[Epoch 0087] loss=11.7201 cls=0.0285 smmd=0.0407 ct=9.2776 rec=1.1867 | train/val/test=1.000/0.642/0.641 | c=0.998437
[Epoch 0088] loss=11.7421 cls=0.0230 smmd=0.0308 ct=9.3091 rec=1.1896 | train/val/test=1.000/0.602/0.590 | c=0.998437
[Epoch 0089] loss=11.6494 cls=0.0127 smmd=0.0242 ct=9.2668 rec=1.1728 | train/val/test=1.000/0.614/0.608 | c=0.998437
[Epoch 0090] loss=11.5945 cls=0.0081 smmd=-0.0050 ct=9.2594 rec=1.1660 | train/val/test=1.000/0.638/0.638 | c=0.998437
[Epoch 0091] loss=11.6659 cls=0.0104 smmd=0.0138 ct=9.2923 rec=1.1747 | train/val/test=1.000/0.612/0.599 | c=0.998437
[Epoch 0092] loss=11.6109 cls=0.0084 smmd=0.0066 ct=9.2621 rec=1.1669 | train/val/test=1.000/0.614/0.599 | c=0.998437
[Epoch 0093] loss=11.6097 cls=0.0092 smmd=0.0045 ct=9.2593 rec=1.1684 | train/val/test=1.000/0.630/0.629 | c=0.998437
[Epoch 0094] loss=11.6535 cls=0.0106 smmd=0.0075 ct=9.2837 rec=1.1758 | train/val/test=1.000/0.614/0.604 | c=0.998437
[Epoch 0095] loss=11.6014 cls=0.0112 smmd=-0.0096 ct=9.2574 rec=1.1712 | train/val/test=1.000/0.614/0.602 | c=0.998437
[Epoch 0096] loss=11.6071 cls=0.0136 smmd=-0.0117 ct=9.2568 rec=1.1742 | train/val/test=1.000/0.632/0.627 | c=0.998437
[Epoch 0097] loss=11.6647 cls=0.0151 smmd=-0.0003 ct=9.2827 rec=1.1836 | train/val/test=1.000/0.598/0.588 | c=0.998437
[Epoch 0098] loss=11.6659 cls=0.0247 smmd=0.0099 ct=9.2640 rec=1.1837 | train/val/test=1.000/0.634/0.628 | c=0.998437
[Epoch 0099] loss=11.6332 cls=0.0149 smmd=-0.0108 ct=9.2713 rec=1.1789 | train/val/test=1.000/0.620/0.607 | c=0.998437
=== Best @ epoch 87: val=0.6420, test=0.6410 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-5-1 completed in 20.95 seconds.
==================================================
