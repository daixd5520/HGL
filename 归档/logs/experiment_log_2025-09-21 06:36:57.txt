Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 - 2025-09-21 06:36:57:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1203 cls=1.9445 smmd=4.1471 ct=9.2509 rec=1.3889 | train/val/test=0.397/0.244/0.248 | c=0.998347
[Epoch 0001] loss=16.5623 cls=1.8880 smmd=2.7248 ct=9.1710 rec=1.3892 | train/val/test=0.776/0.390/0.382 | c=0.998347
[Epoch 0002] loss=14.9757 cls=1.7187 smmd=1.4470 ct=9.0338 rec=1.3880 | train/val/test=0.655/0.340/0.344 | c=0.998347
[Epoch 0003] loss=14.6767 cls=1.4345 smmd=1.4790 ct=8.9962 rec=1.3835 | train/val/test=0.793/0.420/0.431 | c=0.998347
[Epoch 0004] loss=14.5301 cls=1.1482 smmd=1.7437 ct=8.8947 rec=1.3718 | train/val/test=0.862/0.542/0.506 | c=0.998347
[Epoch 0005] loss=14.0911 cls=0.8144 smmd=1.7072 ct=8.8762 rec=1.3466 | train/val/test=0.862/0.564/0.537 | c=0.998347
[Epoch 0006] loss=14.0034 cls=0.5524 smmd=1.3456 ct=9.4820 rec=1.3117 | train/val/test=0.897/0.580/0.568 | c=0.998347
[Epoch 0007] loss=13.4388 cls=0.4022 smmd=1.1085 ct=9.3585 rec=1.2848 | train/val/test=0.966/0.690/0.682 | c=0.998347
[Epoch 0008] loss=13.1945 cls=0.2494 smmd=1.1354 ct=9.2899 rec=1.2599 | train/val/test=0.966/0.702/0.687 | c=0.998347
[Epoch 0009] loss=13.1796 cls=0.1623 smmd=1.2530 ct=9.2834 rec=1.2405 | train/val/test=0.966/0.708/0.678 | c=0.998347
[Epoch 0010] loss=13.1305 cls=0.1126 smmd=1.2641 ct=9.3059 rec=1.2239 | train/val/test=1.000/0.712/0.692 | c=0.998347
[Epoch 0011] loss=12.9632 cls=0.0711 smmd=1.1376 ct=9.3338 rec=1.2103 | train/val/test=1.000/0.708/0.710 | c=0.998347
[Epoch 0012] loss=12.7421 cls=0.0427 smmd=0.9362 ct=9.3588 rec=1.2022 | train/val/test=1.000/0.716/0.730 | c=0.998347
[Epoch 0013] loss=12.5932 cls=0.0285 smmd=0.8014 ct=9.3704 rec=1.1964 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0014] loss=12.4973 cls=0.0195 smmd=0.7289 ct=9.3690 rec=1.1900 | train/val/test=1.000/0.704/0.692 | c=0.998347
[Epoch 0015] loss=12.4931 cls=0.0139 smmd=0.7393 ct=9.3663 rec=1.1868 | train/val/test=1.000/0.706/0.696 | c=0.998347
[Epoch 0016] loss=12.4360 cls=0.0105 smmd=0.7039 ct=9.3519 rec=1.1848 | train/val/test=1.000/0.700/0.709 | c=0.998347
[Epoch 0017] loss=12.3225 cls=0.0101 smmd=0.6125 ct=9.3320 rec=1.1839 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0018] loss=12.2305 cls=0.0118 smmd=0.5210 ct=9.3264 rec=1.1856 | train/val/test=1.000/0.694/0.700 | c=0.998347
[Epoch 0019] loss=12.1809 cls=0.0124 smmd=0.4611 ct=9.3340 rec=1.1866 | train/val/test=1.000/0.694/0.699 | c=0.998347
[Epoch 0020] loss=12.1613 cls=0.0149 smmd=0.4233 ct=9.3441 rec=1.1895 | train/val/test=1.000/0.684/0.711 | c=0.998347
[Epoch 0021] loss=12.1275 cls=0.0192 smmd=0.3842 ct=9.3408 rec=1.1917 | train/val/test=1.000/0.684/0.704 | c=0.998347
[Epoch 0022] loss=12.0600 cls=0.0219 smmd=0.3144 ct=9.3401 rec=1.1918 | train/val/test=1.000/0.696/0.696 | c=0.998347
[Epoch 0023] loss=12.0334 cls=0.0246 smmd=0.2842 ct=9.3412 rec=1.1917 | train/val/test=1.000/0.696/0.712 | c=0.998347
[Epoch 0024] loss=11.9923 cls=0.0282 smmd=0.2507 ct=9.3334 rec=1.1900 | train/val/test=1.000/0.700/0.704 | c=0.998347
[Epoch 0025] loss=11.9668 cls=0.0296 smmd=0.2272 ct=9.3363 rec=1.1868 | train/val/test=1.000/0.706/0.698 | c=0.998347
[Epoch 0026] loss=11.9292 cls=0.0306 smmd=0.1922 ct=9.3393 rec=1.1836 | train/val/test=1.000/0.700/0.708 | c=0.998347
[Epoch 0027] loss=11.9017 cls=0.0301 smmd=0.1819 ct=9.3315 rec=1.1791 | train/val/test=1.000/0.718/0.709 | c=0.998347
[Epoch 0028] loss=11.8699 cls=0.0300 smmd=0.1553 ct=9.3325 rec=1.1760 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0029] loss=11.8508 cls=0.0289 smmd=0.1483 ct=9.3301 rec=1.1718 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0030] loss=11.8302 cls=0.0278 smmd=0.1445 ct=9.3181 rec=1.1699 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0031] loss=11.8011 cls=0.0249 smmd=0.1260 ct=9.3165 rec=1.1669 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0032] loss=11.7914 cls=0.0233 smmd=0.1201 ct=9.3168 rec=1.1656 | train/val/test=1.000/0.710/0.718 | c=0.998347
[Epoch 0033] loss=11.7623 cls=0.0217 smmd=0.0974 ct=9.3129 rec=1.1651 | train/val/test=1.000/0.708/0.709 | c=0.998347
[Epoch 0034] loss=11.7485 cls=0.0207 smmd=0.0917 ct=9.3059 rec=1.1651 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0035] loss=11.7328 cls=0.0201 smmd=0.0820 ct=9.2993 rec=1.1657 | train/val/test=1.000/0.696/0.707 | c=0.998347
[Epoch 0036] loss=11.7159 cls=0.0199 smmd=0.0632 ct=9.2998 rec=1.1666 | train/val/test=1.000/0.700/0.704 | c=0.998347
[Epoch 0037] loss=11.7187 cls=0.0193 smmd=0.0659 ct=9.2993 rec=1.1671 | train/val/test=1.000/0.700/0.703 | c=0.998347
[Epoch 0038] loss=11.7089 cls=0.0196 smmd=0.0633 ct=9.2906 rec=1.1677 | train/val/test=1.000/0.702/0.711 | c=0.998347
[Epoch 0039] loss=11.7058 cls=0.0194 smmd=0.0626 ct=9.2871 rec=1.1683 | train/val/test=1.000/0.704/0.698 | c=0.998347
[Epoch 0040] loss=11.6998 cls=0.0201 smmd=0.0535 ct=9.2889 rec=1.1686 | train/val/test=1.000/0.708/0.718 | c=0.998347
[Epoch 0041] loss=11.7195 cls=0.0232 smmd=0.0568 ct=9.2967 rec=1.1714 | train/val/test=1.000/0.692/0.691 | c=0.998347
[Epoch 0042] loss=11.7497 cls=0.0261 smmd=0.0747 ct=9.3009 rec=1.1740 | train/val/test=1.000/0.708/0.720 | c=0.998347
[Epoch 0043] loss=11.7596 cls=0.0322 smmd=0.0807 ct=9.2981 rec=1.1743 | train/val/test=1.000/0.696/0.697 | c=0.998347
[Epoch 0044] loss=11.6894 cls=0.0173 smmd=0.0618 ct=9.2828 rec=1.1637 | train/val/test=1.000/0.706/0.704 | c=0.998347
[Epoch 0045] loss=11.6528 cls=0.0140 smmd=0.0421 ct=9.2775 rec=1.1596 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0046] loss=11.6934 cls=0.0195 smmd=0.0625 ct=9.2836 rec=1.1639 | train/val/test=1.000/0.700/0.700 | c=0.998347
[Epoch 0047] loss=11.6601 cls=0.0157 smmd=0.0467 ct=9.2761 rec=1.1608 | train/val/test=1.000/0.700/0.702 | c=0.998347
[Epoch 0048] loss=11.6351 cls=0.0147 smmd=0.0293 ct=9.2697 rec=1.1607 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0049] loss=11.6563 cls=0.0175 smmd=0.0336 ct=9.2771 rec=1.1640 | train/val/test=1.000/0.698/0.696 | c=0.998347
[Epoch 0050] loss=11.6488 cls=0.0192 smmd=0.0267 ct=9.2721 rec=1.1654 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0051] loss=11.6329 cls=0.0182 smmd=0.0239 ct=9.2601 rec=1.1653 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0052] loss=11.6384 cls=0.0197 smmd=0.0186 ct=9.2668 rec=1.1667 | train/val/test=1.000/0.694/0.693 | c=0.998347
[Epoch 0053] loss=11.6479 cls=0.0236 smmd=0.0204 ct=9.2643 rec=1.1698 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0054] loss=11.6482 cls=0.0238 smmd=0.0190 ct=9.2671 rec=1.1691 | train/val/test=1.000/0.698/0.698 | c=0.998347
[Epoch 0055] loss=11.6276 cls=0.0210 smmd=0.0139 ct=9.2587 rec=1.1670 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0056] loss=11.6162 cls=0.0201 smmd=0.0128 ct=9.2529 rec=1.1652 | train/val/test=1.000/0.710/0.713 | c=0.998347
[Epoch 0057] loss=11.6127 cls=0.0199 smmd=0.0060 ct=9.2583 rec=1.1643 | train/val/test=1.000/0.698/0.705 | c=0.998347
[Epoch 0058] loss=11.6120 cls=0.0195 smmd=0.0122 ct=9.2516 rec=1.1644 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0059] loss=11.6042 cls=0.0205 smmd=0.0045 ct=9.2514 rec=1.1639 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0060] loss=11.5951 cls=0.0189 smmd=0.0029 ct=9.2462 rec=1.1635 | train/val/test=1.000/0.704/0.708 | c=0.998347
[Epoch 0061] loss=11.5949 cls=0.0195 smmd=0.0036 ct=9.2438 rec=1.1640 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0062] loss=11.5972 cls=0.0210 smmd=-0.0054 ct=9.2517 rec=1.1650 | train/val/test=1.000/0.694/0.706 | c=0.998347
[Epoch 0063] loss=11.5964 cls=0.0211 smmd=0.0040 ct=9.2384 rec=1.1664 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0064] loss=11.5942 cls=0.0210 smmd=-0.0007 ct=9.2426 rec=1.1657 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0065] loss=11.5824 cls=0.0209 smmd=-0.0067 ct=9.2367 rec=1.1658 | train/val/test=1.000/0.704/0.704 | c=0.998347
[Epoch 0066] loss=11.5826 cls=0.0206 smmd=-0.0090 ct=9.2393 rec=1.1658 | train/val/test=1.000/0.708/0.713 | c=0.998347
[Epoch 0067] loss=11.5819 cls=0.0214 smmd=-0.0075 ct=9.2363 rec=1.1658 | train/val/test=1.000/0.708/0.704 | c=0.998347
[Epoch 0068] loss=11.5826 cls=0.0208 smmd=-0.0014 ct=9.2318 rec=1.1657 | train/val/test=1.000/0.712/0.710 | c=0.998347
[Epoch 0069] loss=11.5798 cls=0.0210 smmd=-0.0071 ct=9.2345 rec=1.1657 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0070] loss=11.5750 cls=0.0207 smmd=-0.0089 ct=9.2324 rec=1.1654 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0071] loss=11.5725 cls=0.0207 smmd=-0.0087 ct=9.2297 rec=1.1654 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0072] loss=11.5696 cls=0.0207 smmd=-0.0108 ct=9.2282 rec=1.1658 | train/val/test=1.000/0.714/0.712 | c=0.998347
[Epoch 0073] loss=11.5721 cls=0.0214 smmd=-0.0053 ct=9.2235 rec=1.1663 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0074] loss=11.5691 cls=0.0212 smmd=-0.0134 ct=9.2284 rec=1.1664 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0075] loss=11.5680 cls=0.0218 smmd=-0.0110 ct=9.2230 rec=1.1671 | train/val/test=1.000/0.710/0.702 | c=0.998347
[Epoch 0076] loss=11.5727 cls=0.0221 smmd=-0.0106 ct=9.2271 rec=1.1671 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0077] loss=11.5774 cls=0.0232 smmd=-0.0046 ct=9.2216 rec=1.1686 | train/val/test=1.000/0.704/0.696 | c=0.998347
[Epoch 0078] loss=11.5917 cls=0.0241 smmd=-0.0034 ct=9.2330 rec=1.1690 | train/val/test=1.000/0.714/0.726 | c=0.998347
[Epoch 0079] loss=11.6127 cls=0.0275 smmd=0.0132 ct=9.2267 rec=1.1727 | train/val/test=1.000/0.694/0.691 | c=0.998347
[Epoch 0080] loss=11.6358 cls=0.0275 smmd=0.0178 ct=9.2461 rec=1.1722 | train/val/test=1.000/0.712/0.721 | c=0.998347
[Epoch 0081] loss=11.6232 cls=0.0260 smmd=0.0247 ct=9.2318 rec=1.1703 | train/val/test=1.000/0.696/0.695 | c=0.998347
[Epoch 0082] loss=11.5731 cls=0.0177 smmd=0.0028 ct=9.2261 rec=1.1633 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0083] loss=11.5638 cls=0.0160 smmd=-0.0066 ct=9.2317 rec=1.1613 | train/val/test=1.000/0.706/0.717 | c=0.998347
[Epoch 0084] loss=11.5802 cls=0.0174 smmd=0.0142 ct=9.2203 rec=1.1641 | train/val/test=1.000/0.706/0.693 | c=0.998347
[Epoch 0085] loss=11.5665 cls=0.0165 smmd=-0.0029 ct=9.2255 rec=1.1637 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0086] loss=11.5579 cls=0.0169 smmd=-0.0123 ct=9.2261 rec=1.1636 | train/val/test=1.000/0.700/0.708 | c=0.998347
[Epoch 0087] loss=11.5677 cls=0.0193 smmd=-0.0006 ct=9.2156 rec=1.1667 | train/val/test=1.000/0.704/0.697 | c=0.998347
[Epoch 0088] loss=11.5732 cls=0.0209 smmd=-0.0144 ct=9.2284 rec=1.1692 | train/val/test=1.000/0.702/0.715 | c=0.998347
[Epoch 0089] loss=11.5722 cls=0.0223 smmd=-0.0060 ct=9.2162 rec=1.1698 | train/val/test=1.000/0.706/0.693 | c=0.998347
[Epoch 0090] loss=11.5710 cls=0.0223 smmd=-0.0089 ct=9.2189 rec=1.1693 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0091] loss=11.5694 cls=0.0226 smmd=-0.0141 ct=9.2253 rec=1.1678 | train/val/test=1.000/0.696/0.699 | c=0.998347
[Epoch 0092] loss=11.5814 cls=0.0226 smmd=0.0052 ct=9.2159 rec=1.1688 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0093] loss=11.5779 cls=0.0241 smmd=-0.0093 ct=9.2299 rec=1.1666 | train/val/test=1.000/0.704/0.704 | c=0.998347
[Epoch 0094] loss=11.5552 cls=0.0196 smmd=-0.0076 ct=9.2136 rec=1.1648 | train/val/test=1.000/0.718/0.710 | c=0.998347
[Epoch 0095] loss=11.5421 cls=0.0181 smmd=-0.0146 ct=9.2140 rec=1.1623 | train/val/test=1.000/0.714/0.708 | c=0.998347
[Epoch 0096] loss=11.5492 cls=0.0187 smmd=-0.0159 ct=9.2202 rec=1.1631 | train/val/test=1.000/0.702/0.704 | c=0.998347
[Epoch 0097] loss=11.5555 cls=0.0194 smmd=-0.0051 ct=9.2111 rec=1.1650 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0098] loss=11.5537 cls=0.0210 smmd=-0.0169 ct=9.2197 rec=1.1650 | train/val/test=1.000/0.708/0.704 | c=0.998347
[Epoch 0099] loss=11.5508 cls=0.0200 smmd=-0.0150 ct=9.2137 rec=1.1660 | train/val/test=1.000/0.716/0.713 | c=0.998347
=== Best @ epoch 27: val=0.7180, test=0.7090 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 - 2025-09-21 06:36:57:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1203 cls=1.9445 smmd=4.1471 ct=9.2509 rec=1.3889 | train/val/test=0.397/0.244/0.248 | c=0.998347
[Epoch 0001] loss=16.5623 cls=1.8880 smmd=2.7248 ct=9.1710 rec=1.3892 | train/val/test=0.776/0.390/0.382 | c=0.998347
[Epoch 0002] loss=14.9757 cls=1.7187 smmd=1.4470 ct=9.0338 rec=1.3880 | train/val/test=0.655/0.340/0.344 | c=0.998347
[Epoch 0003] loss=14.6767 cls=1.4345 smmd=1.4790 ct=8.9962 rec=1.3835 | train/val/test=0.793/0.420/0.431 | c=0.998347
[Epoch 0004] loss=14.5301 cls=1.1482 smmd=1.7437 ct=8.8947 rec=1.3718 | train/val/test=0.862/0.542/0.506 | c=0.998347
[Epoch 0005] loss=14.0911 cls=0.8144 smmd=1.7072 ct=8.8762 rec=1.3466 | train/val/test=0.862/0.564/0.537 | c=0.998347
[Epoch 0006] loss=14.0034 cls=0.5524 smmd=1.3456 ct=9.4820 rec=1.3117 | train/val/test=0.897/0.580/0.568 | c=0.998347
[Epoch 0007] loss=13.4388 cls=0.4022 smmd=1.1085 ct=9.3585 rec=1.2848 | train/val/test=0.966/0.690/0.682 | c=0.998347
[Epoch 0008] loss=13.1945 cls=0.2494 smmd=1.1354 ct=9.2899 rec=1.2599 | train/val/test=0.966/0.702/0.687 | c=0.998347
[Epoch 0009] loss=13.1796 cls=0.1623 smmd=1.2530 ct=9.2834 rec=1.2405 | train/val/test=0.966/0.708/0.678 | c=0.998347
[Epoch 0010] loss=13.1305 cls=0.1126 smmd=1.2641 ct=9.3059 rec=1.2239 | train/val/test=1.000/0.712/0.692 | c=0.998347
[Epoch 0011] loss=12.9632 cls=0.0711 smmd=1.1376 ct=9.3338 rec=1.2103 | train/val/test=1.000/0.708/0.710 | c=0.998347
[Epoch 0012] loss=12.7421 cls=0.0427 smmd=0.9362 ct=9.3588 rec=1.2022 | train/val/test=1.000/0.716/0.730 | c=0.998347
[Epoch 0013] loss=12.5932 cls=0.0285 smmd=0.8014 ct=9.3704 rec=1.1964 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0014] loss=12.4973 cls=0.0195 smmd=0.7289 ct=9.3690 rec=1.1900 | train/val/test=1.000/0.704/0.692 | c=0.998347
[Epoch 0015] loss=12.4931 cls=0.0139 smmd=0.7393 ct=9.3663 rec=1.1868 | train/val/test=1.000/0.706/0.696 | c=0.998347
[Epoch 0016] loss=12.4360 cls=0.0105 smmd=0.7039 ct=9.3519 rec=1.1848 | train/val/test=1.000/0.700/0.709 | c=0.998347
[Epoch 0017] loss=12.3225 cls=0.0101 smmd=0.6125 ct=9.3320 rec=1.1839 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0018] loss=12.2305 cls=0.0118 smmd=0.5210 ct=9.3264 rec=1.1856 | train/val/test=1.000/0.694/0.700 | c=0.998347
[Epoch 0019] loss=12.1809 cls=0.0124 smmd=0.4611 ct=9.3340 rec=1.1866 | train/val/test=1.000/0.694/0.699 | c=0.998347
[Epoch 0020] loss=12.1613 cls=0.0149 smmd=0.4233 ct=9.3441 rec=1.1895 | train/val/test=1.000/0.684/0.711 | c=0.998347
[Epoch 0021] loss=12.1275 cls=0.0192 smmd=0.3842 ct=9.3408 rec=1.1917 | train/val/test=1.000/0.684/0.704 | c=0.998347
[Epoch 0022] loss=12.0600 cls=0.0219 smmd=0.3144 ct=9.3401 rec=1.1918 | train/val/test=1.000/0.696/0.696 | c=0.998347
[Epoch 0023] loss=12.0334 cls=0.0246 smmd=0.2842 ct=9.3412 rec=1.1917 | train/val/test=1.000/0.696/0.712 | c=0.998347
[Epoch 0024] loss=11.9923 cls=0.0282 smmd=0.2507 ct=9.3334 rec=1.1900 | train/val/test=1.000/0.700/0.704 | c=0.998347
[Epoch 0025] loss=11.9668 cls=0.0296 smmd=0.2272 ct=9.3363 rec=1.1868 | train/val/test=1.000/0.706/0.698 | c=0.998347
[Epoch 0026] loss=11.9292 cls=0.0306 smmd=0.1922 ct=9.3393 rec=1.1836 | train/val/test=1.000/0.700/0.708 | c=0.998347
[Epoch 0027] loss=11.9017 cls=0.0301 smmd=0.1819 ct=9.3315 rec=1.1791 | train/val/test=1.000/0.718/0.709 | c=0.998347
[Epoch 0028] loss=11.8699 cls=0.0300 smmd=0.1553 ct=9.3325 rec=1.1760 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0029] loss=11.8508 cls=0.0289 smmd=0.1483 ct=9.3301 rec=1.1718 | train/val/test=1.000/0.716/0.718 | c=0.998347
[Epoch 0030] loss=11.8302 cls=0.0278 smmd=0.1445 ct=9.3181 rec=1.1699 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0031] loss=11.8011 cls=0.0249 smmd=0.1260 ct=9.3165 rec=1.1669 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0032] loss=11.7914 cls=0.0233 smmd=0.1201 ct=9.3168 rec=1.1656 | train/val/test=1.000/0.710/0.718 | c=0.998347
[Epoch 0033] loss=11.7623 cls=0.0217 smmd=0.0974 ct=9.3129 rec=1.1651 | train/val/test=1.000/0.708/0.709 | c=0.998347
[Epoch 0034] loss=11.7485 cls=0.0207 smmd=0.0917 ct=9.3059 rec=1.1651 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0035] loss=11.7328 cls=0.0201 smmd=0.0820 ct=9.2993 rec=1.1657 | train/val/test=1.000/0.696/0.707 | c=0.998347
[Epoch 0036] loss=11.7159 cls=0.0199 smmd=0.0632 ct=9.2998 rec=1.1666 | train/val/test=1.000/0.700/0.704 | c=0.998347
[Epoch 0037] loss=11.7187 cls=0.0193 smmd=0.0659 ct=9.2993 rec=1.1671 | train/val/test=1.000/0.700/0.703 | c=0.998347
[Epoch 0038] loss=11.7089 cls=0.0196 smmd=0.0633 ct=9.2906 rec=1.1677 | train/val/test=1.000/0.702/0.711 | c=0.998347
[Epoch 0039] loss=11.7058 cls=0.0194 smmd=0.0626 ct=9.2871 rec=1.1683 | train/val/test=1.000/0.704/0.698 | c=0.998347
[Epoch 0040] loss=11.6998 cls=0.0201 smmd=0.0535 ct=9.2889 rec=1.1686 | train/val/test=1.000/0.708/0.718 | c=0.998347
[Epoch 0041] loss=11.7195 cls=0.0232 smmd=0.0568 ct=9.2967 rec=1.1714 | train/val/test=1.000/0.692/0.691 | c=0.998347
[Epoch 0042] loss=11.7497 cls=0.0261 smmd=0.0747 ct=9.3009 rec=1.1740 | train/val/test=1.000/0.708/0.720 | c=0.998347
[Epoch 0043] loss=11.7596 cls=0.0322 smmd=0.0807 ct=9.2981 rec=1.1743 | train/val/test=1.000/0.696/0.697 | c=0.998347
[Epoch 0044] loss=11.6894 cls=0.0173 smmd=0.0618 ct=9.2828 rec=1.1637 | train/val/test=1.000/0.706/0.704 | c=0.998347
[Epoch 0045] loss=11.6528 cls=0.0140 smmd=0.0421 ct=9.2775 rec=1.1596 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0046] loss=11.6934 cls=0.0195 smmd=0.0625 ct=9.2836 rec=1.1639 | train/val/test=1.000/0.700/0.700 | c=0.998347
[Epoch 0047] loss=11.6601 cls=0.0157 smmd=0.0467 ct=9.2761 rec=1.1608 | train/val/test=1.000/0.700/0.702 | c=0.998347
[Epoch 0048] loss=11.6351 cls=0.0147 smmd=0.0293 ct=9.2697 rec=1.1607 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0049] loss=11.6563 cls=0.0175 smmd=0.0336 ct=9.2771 rec=1.1640 | train/val/test=1.000/0.698/0.696 | c=0.998347
[Epoch 0050] loss=11.6488 cls=0.0192 smmd=0.0267 ct=9.2721 rec=1.1654 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0051] loss=11.6329 cls=0.0182 smmd=0.0239 ct=9.2601 rec=1.1653 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0052] loss=11.6384 cls=0.0197 smmd=0.0186 ct=9.2668 rec=1.1667 | train/val/test=1.000/0.694/0.693 | c=0.998347
[Epoch 0053] loss=11.6479 cls=0.0236 smmd=0.0204 ct=9.2643 rec=1.1698 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0054] loss=11.6482 cls=0.0238 smmd=0.0190 ct=9.2671 rec=1.1691 | train/val/test=1.000/0.698/0.698 | c=0.998347
[Epoch 0055] loss=11.6276 cls=0.0210 smmd=0.0139 ct=9.2587 rec=1.1670 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0056] loss=11.6162 cls=0.0201 smmd=0.0128 ct=9.2529 rec=1.1652 | train/val/test=1.000/0.710/0.713 | c=0.998347
[Epoch 0057] loss=11.6127 cls=0.0199 smmd=0.0060 ct=9.2583 rec=1.1643 | train/val/test=1.000/0.698/0.705 | c=0.998347
[Epoch 0058] loss=11.6120 cls=0.0195 smmd=0.0122 ct=9.2516 rec=1.1644 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0059] loss=11.6042 cls=0.0205 smmd=0.0045 ct=9.2514 rec=1.1639 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0060] loss=11.5951 cls=0.0189 smmd=0.0029 ct=9.2462 rec=1.1635 | train/val/test=1.000/0.704/0.708 | c=0.998347
[Epoch 0061] loss=11.5949 cls=0.0195 smmd=0.0036 ct=9.2438 rec=1.1640 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0062] loss=11.5972 cls=0.0210 smmd=-0.0054 ct=9.2517 rec=1.1650 | train/val/test=1.000/0.694/0.706 | c=0.998347
[Epoch 0063] loss=11.5964 cls=0.0211 smmd=0.0040 ct=9.2384 rec=1.1664 | train/val/test=1.000/0.702/0.712 | c=0.998347
[Epoch 0064] loss=11.5942 cls=0.0210 smmd=-0.0007 ct=9.2426 rec=1.1657 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0065] loss=11.5824 cls=0.0209 smmd=-0.0067 ct=9.2367 rec=1.1658 | train/val/test=1.000/0.704/0.704 | c=0.998347
[Epoch 0066] loss=11.5826 cls=0.0206 smmd=-0.0090 ct=9.2393 rec=1.1658 | train/val/test=1.000/0.708/0.713 | c=0.998347
[Epoch 0067] loss=11.5819 cls=0.0214 smmd=-0.0075 ct=9.2363 rec=1.1658 | train/val/test=1.000/0.708/0.704 | c=0.998347
[Epoch 0068] loss=11.5826 cls=0.0208 smmd=-0.0014 ct=9.2318 rec=1.1657 | train/val/test=1.000/0.712/0.710 | c=0.998347
[Epoch 0069] loss=11.5798 cls=0.0210 smmd=-0.0071 ct=9.2345 rec=1.1657 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0070] loss=11.5750 cls=0.0207 smmd=-0.0089 ct=9.2324 rec=1.1654 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0071] loss=11.5725 cls=0.0207 smmd=-0.0087 ct=9.2297 rec=1.1654 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0072] loss=11.5696 cls=0.0207 smmd=-0.0108 ct=9.2282 rec=1.1658 | train/val/test=1.000/0.714/0.712 | c=0.998347
[Epoch 0073] loss=11.5721 cls=0.0214 smmd=-0.0053 ct=9.2235 rec=1.1663 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0074] loss=11.5691 cls=0.0212 smmd=-0.0134 ct=9.2284 rec=1.1664 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0075] loss=11.5680 cls=0.0218 smmd=-0.0110 ct=9.2230 rec=1.1671 | train/val/test=1.000/0.710/0.702 | c=0.998347
[Epoch 0076] loss=11.5727 cls=0.0221 smmd=-0.0106 ct=9.2271 rec=1.1671 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0077] loss=11.5774 cls=0.0232 smmd=-0.0046 ct=9.2216 rec=1.1686 | train/val/test=1.000/0.704/0.696 | c=0.998347
[Epoch 0078] loss=11.5917 cls=0.0241 smmd=-0.0034 ct=9.2330 rec=1.1690 | train/val/test=1.000/0.714/0.726 | c=0.998347
[Epoch 0079] loss=11.6127 cls=0.0275 smmd=0.0132 ct=9.2267 rec=1.1727 | train/val/test=1.000/0.694/0.691 | c=0.998347
[Epoch 0080] loss=11.6358 cls=0.0275 smmd=0.0178 ct=9.2461 rec=1.1722 | train/val/test=1.000/0.712/0.721 | c=0.998347
[Epoch 0081] loss=11.6232 cls=0.0260 smmd=0.0247 ct=9.2318 rec=1.1703 | train/val/test=1.000/0.696/0.695 | c=0.998347
[Epoch 0082] loss=11.5731 cls=0.0177 smmd=0.0028 ct=9.2261 rec=1.1633 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0083] loss=11.5638 cls=0.0160 smmd=-0.0066 ct=9.2317 rec=1.1613 | train/val/test=1.000/0.706/0.717 | c=0.998347
[Epoch 0084] loss=11.5802 cls=0.0174 smmd=0.0142 ct=9.2203 rec=1.1641 | train/val/test=1.000/0.706/0.693 | c=0.998347
[Epoch 0085] loss=11.5665 cls=0.0165 smmd=-0.0029 ct=9.2255 rec=1.1637 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0086] loss=11.5579 cls=0.0169 smmd=-0.0123 ct=9.2261 rec=1.1636 | train/val/test=1.000/0.700/0.708 | c=0.998347
[Epoch 0087] loss=11.5677 cls=0.0193 smmd=-0.0006 ct=9.2156 rec=1.1667 | train/val/test=1.000/0.704/0.697 | c=0.998347
[Epoch 0088] loss=11.5732 cls=0.0209 smmd=-0.0144 ct=9.2284 rec=1.1692 | train/val/test=1.000/0.702/0.715 | c=0.998347
[Epoch 0089] loss=11.5722 cls=0.0223 smmd=-0.0060 ct=9.2162 rec=1.1698 | train/val/test=1.000/0.706/0.693 | c=0.998347
[Epoch 0090] loss=11.5710 cls=0.0223 smmd=-0.0089 ct=9.2189 rec=1.1693 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0091] loss=11.5694 cls=0.0226 smmd=-0.0141 ct=9.2253 rec=1.1678 | train/val/test=1.000/0.696/0.699 | c=0.998347
[Epoch 0092] loss=11.5814 cls=0.0226 smmd=0.0052 ct=9.2159 rec=1.1688 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0093] loss=11.5779 cls=0.0241 smmd=-0.0093 ct=9.2299 rec=1.1666 | train/val/test=1.000/0.704/0.704 | c=0.998347
[Epoch 0094] loss=11.5552 cls=0.0196 smmd=-0.0076 ct=9.2136 rec=1.1648 | train/val/test=1.000/0.718/0.710 | c=0.998347
[Epoch 0095] loss=11.5421 cls=0.0181 smmd=-0.0146 ct=9.2140 rec=1.1623 | train/val/test=1.000/0.714/0.708 | c=0.998347
[Epoch 0096] loss=11.5492 cls=0.0187 smmd=-0.0159 ct=9.2202 rec=1.1631 | train/val/test=1.000/0.702/0.704 | c=0.998347
[Epoch 0097] loss=11.5555 cls=0.0194 smmd=-0.0051 ct=9.2111 rec=1.1650 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0098] loss=11.5537 cls=0.0210 smmd=-0.0169 ct=9.2197 rec=1.1650 | train/val/test=1.000/0.708/0.704 | c=0.998347
[Epoch 0099] loss=11.5508 cls=0.0200 smmd=-0.0150 ct=9.2137 rec=1.1660 | train/val/test=1.000/0.716/0.713 | c=0.998347
=== Best @ epoch 27: val=0.7180, test=0.7090 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 completed in 21.24 seconds.
==================================================
