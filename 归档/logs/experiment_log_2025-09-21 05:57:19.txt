Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 - 2025-09-21 05:57:19:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.1637 cls=1.0970 smmd=5.6386 ct=11.2807 rec=1.4139 | train/val/test=0.577/0.440/0.417 | c=0.998347
[Epoch 0001] loss=30.4191 cls=1.0788 smmd=3.7010 ct=11.2334 rec=1.4136 | train/val/test=0.615/0.490/0.505 | c=0.998347
[Epoch 0002] loss=35.5861 cls=1.0362 smmd=4.7430 ct=11.2116 rec=1.4138 | train/val/test=0.769/0.666/0.658 | c=0.998347
[Epoch 0003] loss=34.3949 cls=0.9440 smmd=4.5337 ct=11.1134 rec=1.4111 | train/val/test=0.731/0.644/0.638 | c=0.998347
[Epoch 0004] loss=24.8347 cls=0.8010 smmd=2.6699 ct=10.9445 rec=1.4014 | train/val/test=0.692/0.676/0.672 | c=0.998347
[Epoch 0005] loss=26.5574 cls=0.6643 smmd=3.0255 ct=10.9596 rec=1.3824 | train/val/test=0.769/0.686/0.683 | c=0.998347
[Epoch 0006] loss=28.9078 cls=0.5474 smmd=3.5071 ct=10.9628 rec=1.3582 | train/val/test=0.769/0.686/0.687 | c=0.998347
[Epoch 0007] loss=27.2831 cls=0.4567 smmd=3.0658 ct=11.5922 rec=1.3357 | train/val/test=0.846/0.684/0.682 | c=0.998347
[Epoch 0008] loss=22.6259 cls=0.3833 smmd=2.1673 ct=11.4662 rec=1.3169 | train/val/test=0.923/0.704/0.690 | c=0.998347
[Epoch 0009] loss=23.0871 cls=0.3153 smmd=2.2599 ct=11.4987 rec=1.3125 | train/val/test=0.962/0.744/0.705 | c=0.998347
[Epoch 0010] loss=25.1372 cls=0.2372 smmd=2.6801 ct=11.4868 rec=1.3130 | train/val/test=1.000/0.760/0.742 | c=0.998347
[Epoch 0011] loss=23.4694 cls=0.1919 smmd=2.3521 ct=11.4809 rec=1.3204 | train/val/test=1.000/0.770/0.753 | c=0.998347
[Epoch 0012] loss=20.8438 cls=0.1656 smmd=1.8201 ct=11.5284 rec=1.3235 | train/val/test=1.000/0.760/0.718 | c=0.998347
[Epoch 0013] loss=23.4498 cls=0.1521 smmd=2.3456 ct=11.5131 rec=1.3285 | train/val/test=1.000/0.756/0.762 | c=0.998347
[Epoch 0014] loss=22.6716 cls=0.1247 smmd=2.1794 ct=11.5802 rec=1.3219 | train/val/test=1.000/0.776/0.769 | c=0.998347
[Epoch 0015] loss=19.6158 cls=0.0976 smmd=1.5673 ct=11.5993 rec=1.3128 | train/val/test=1.000/0.768/0.761 | c=0.998347
[Epoch 0016] loss=20.2993 cls=0.0819 smmd=1.7381 ct=11.4375 rec=1.3067 | train/val/test=1.000/0.764/0.757 | c=0.998347
[Epoch 0017] loss=20.3438 cls=0.0703 smmd=1.7527 ct=11.4150 rec=1.3009 | train/val/test=1.000/0.770/0.756 | c=0.998347
[Epoch 0018] loss=18.7518 cls=0.0626 smmd=1.4171 ct=11.5054 rec=1.2938 | train/val/test=1.000/0.782/0.760 | c=0.998347
[Epoch 0019] loss=18.3296 cls=0.0643 smmd=1.3238 ct=11.5488 rec=1.2944 | train/val/test=1.000/0.790/0.768 | c=0.998347
[Epoch 0020] loss=18.6004 cls=0.0702 smmd=1.3845 ct=11.5127 rec=1.2995 | train/val/test=1.000/0.770/0.758 | c=0.998347
[Epoch 0021] loss=18.1879 cls=0.0874 smmd=1.3087 ct=11.4707 rec=1.3022 | train/val/test=1.000/0.794/0.777 | c=0.998347
[Epoch 0022] loss=17.7765 cls=0.0937 smmd=1.2088 ct=11.5544 rec=1.3101 | train/val/test=1.000/0.778/0.764 | c=0.998347
[Epoch 0023] loss=17.8509 cls=0.1015 smmd=1.2302 ct=11.5184 rec=1.3071 | train/val/test=1.000/0.786/0.768 | c=0.998347
[Epoch 0024] loss=17.9374 cls=0.1030 smmd=1.2495 ct=11.5077 rec=1.3052 | train/val/test=1.000/0.784/0.771 | c=0.998347
[Epoch 0025] loss=16.9877 cls=0.0877 smmd=1.0505 ct=11.5607 rec=1.3054 | train/val/test=1.000/0.780/0.769 | c=0.998347
[Epoch 0026] loss=17.2470 cls=0.0762 smmd=1.1179 ct=11.4891 rec=1.3010 | train/val/test=1.000/0.776/0.782 | c=0.998347
[Epoch 0027] loss=16.4513 cls=0.0706 smmd=0.9572 ct=11.5000 rec=1.2984 | train/val/test=1.000/0.782/0.773 | c=0.998347
[Epoch 0028] loss=16.0999 cls=0.0579 smmd=0.8855 ct=11.5137 rec=1.2953 | train/val/test=1.000/0.788/0.783 | c=0.998347
[Epoch 0029] loss=16.1285 cls=0.0603 smmd=0.8959 ct=11.4889 rec=1.2970 | train/val/test=1.000/0.772/0.770 | c=0.998347
[Epoch 0030] loss=15.7070 cls=0.0551 smmd=0.8092 ct=11.5042 rec=1.2943 | train/val/test=1.000/0.774/0.763 | c=0.998347
[Epoch 0031] loss=15.7567 cls=0.0613 smmd=0.8246 ct=11.4728 rec=1.3003 | train/val/test=1.000/0.788/0.782 | c=0.998347
[Epoch 0032] loss=16.0714 cls=0.0651 smmd=0.8776 ct=11.5205 rec=1.3054 | train/val/test=1.000/0.776/0.770 | c=0.998347
[Epoch 0033] loss=15.8721 cls=0.0676 smmd=0.8354 ct=11.5308 rec=1.3051 | train/val/test=1.000/0.800/0.783 | c=0.998347
[Epoch 0034] loss=15.8294 cls=0.0707 smmd=0.8242 ct=11.5420 rec=1.3097 | train/val/test=1.000/0.770/0.759 | c=0.998347
[Epoch 0035] loss=15.6767 cls=0.0674 smmd=0.8111 ct=11.4574 rec=1.2998 | train/val/test=1.000/0.788/0.792 | c=0.998347
[Epoch 0036] loss=15.1890 cls=0.0587 smmd=0.7077 ct=11.4915 rec=1.2986 | train/val/test=1.000/0.790/0.791 | c=0.998347
[Epoch 0037] loss=14.9510 cls=0.0575 smmd=0.6524 ct=11.5306 rec=1.2965 | train/val/test=1.000/0.794/0.794 | c=0.998347
[Epoch 0038] loss=14.9710 cls=0.0592 smmd=0.6670 ct=11.4767 rec=1.2973 | train/val/test=1.000/0.782/0.783 | c=0.998347
[Epoch 0039] loss=14.8145 cls=0.0666 smmd=0.6352 ct=11.4755 rec=1.2980 | train/val/test=1.000/0.802/0.788 | c=0.998347
[Epoch 0040] loss=14.8230 cls=0.0752 smmd=0.6236 ct=11.5369 rec=1.3071 | train/val/test=1.000/0.786/0.778 | c=0.998347
[Epoch 0041] loss=14.9932 cls=0.0844 smmd=0.6695 ct=11.4728 rec=1.3048 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0042] loss=15.2739 cls=0.0913 smmd=0.7074 ct=11.5597 rec=1.3146 | train/val/test=1.000/0.780/0.767 | c=0.998347
[Epoch 0043] loss=15.5544 cls=0.0948 smmd=0.7764 ct=11.4940 rec=1.3098 | train/val/test=1.000/0.802/0.784 | c=0.998347
[Epoch 0044] loss=14.7631 cls=0.0951 smmd=0.6104 ct=11.5316 rec=1.3184 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0045] loss=14.7361 cls=0.0818 smmd=0.6124 ct=11.5032 rec=1.3021 | train/val/test=1.000/0.798/0.785 | c=0.998347
[Epoch 0046] loss=14.4492 cls=0.0754 smmd=0.5590 ct=11.4861 rec=1.3042 | train/val/test=1.000/0.802/0.789 | c=0.998347
[Epoch 0047] loss=14.3967 cls=0.0734 smmd=0.5471 ct=11.4944 rec=1.3000 | train/val/test=1.000/0.796/0.791 | c=0.998347
[Epoch 0048] loss=14.2387 cls=0.0764 smmd=0.5101 ct=11.5193 rec=1.3055 | train/val/test=1.000/0.786/0.781 | c=0.998347
[Epoch 0049] loss=14.3530 cls=0.0855 smmd=0.5405 ct=11.4772 rec=1.3064 | train/val/test=1.000/0.808/0.777 | c=0.998347
[Epoch 0050] loss=14.6400 cls=0.0968 smmd=0.5830 ct=11.5442 rec=1.3219 | train/val/test=1.000/0.752/0.749 | c=0.998347
[Epoch 0051] loss=14.9937 cls=0.1180 smmd=0.6568 ct=11.5187 rec=1.3222 | train/val/test=1.000/0.780/0.757 | c=0.998347
[Epoch 0052] loss=15.0045 cls=0.1099 smmd=0.6534 ct=11.5490 rec=1.3337 | train/val/test=1.000/0.762/0.748 | c=0.998347
[Epoch 0053] loss=14.5710 cls=0.1077 smmd=0.5759 ct=11.5060 rec=1.3153 | train/val/test=1.000/0.800/0.771 | c=0.998347
[Epoch 0054] loss=14.2825 cls=0.0816 smmd=0.5191 ct=11.5147 rec=1.3144 | train/val/test=1.000/0.778/0.778 | c=0.998347
[Epoch 0055] loss=14.0115 cls=0.0747 smmd=0.4723 ct=11.4827 rec=1.3008 | train/val/test=1.000/0.802/0.785 | c=0.998347
[Epoch 0056] loss=13.9356 cls=0.0722 smmd=0.4525 ct=11.5065 rec=1.3042 | train/val/test=1.000/0.786/0.773 | c=0.998347
[Epoch 0057] loss=13.8821 cls=0.0778 smmd=0.4465 ct=11.4801 rec=1.3037 | train/val/test=1.000/0.804/0.781 | c=0.998347
[Epoch 0058] loss=14.2104 cls=0.0860 smmd=0.4994 ct=11.5387 rec=1.3155 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0059] loss=14.5593 cls=0.0997 smmd=0.5783 ct=11.4865 rec=1.3160 | train/val/test=1.000/0.780/0.753 | c=0.998347
[Epoch 0060] loss=14.8074 cls=0.1092 smmd=0.6088 ct=11.5758 rec=1.3307 | train/val/test=1.000/0.752/0.745 | c=0.998347
[Epoch 0061] loss=14.5998 cls=0.1148 smmd=0.5854 ct=11.4832 rec=1.3235 | train/val/test=1.000/0.746/0.733 | c=0.998347
[Epoch 0062] loss=14.3943 cls=0.1077 smmd=0.5283 ct=11.5656 rec=1.3330 | train/val/test=1.000/0.760/0.745 | c=0.998347
[Epoch 0063] loss=14.1829 cls=0.0951 smmd=0.5022 ct=11.4930 rec=1.3117 | train/val/test=1.000/0.794/0.759 | c=0.998347
[Epoch 0064] loss=13.8511 cls=0.0777 smmd=0.4346 ct=11.5078 rec=1.3146 | train/val/test=1.000/0.784/0.763 | c=0.998347
[Epoch 0065] loss=13.7844 cls=0.0723 smmd=0.4261 ct=11.4872 rec=1.3027 | train/val/test=1.000/0.804/0.779 | c=0.998347
[Epoch 0066] loss=13.9142 cls=0.0734 smmd=0.4477 ct=11.5081 rec=1.3100 | train/val/test=1.000/0.776/0.769 | c=0.998347
[Epoch 0067] loss=14.1819 cls=0.0820 smmd=0.5003 ct=11.5085 rec=1.3110 | train/val/test=1.000/0.798/0.762 | c=0.998347
[Epoch 0068] loss=14.3640 cls=0.0883 smmd=0.5335 ct=11.5202 rec=1.3198 | train/val/test=1.000/0.756/0.749 | c=0.998347
[Epoch 0069] loss=14.7047 cls=0.1057 smmd=0.6006 ct=11.5162 rec=1.3245 | train/val/test=1.000/0.734/0.709 | c=0.998347
[Epoch 0070] loss=14.6703 cls=0.1131 smmd=0.5822 ct=11.5695 rec=1.3338 | train/val/test=1.000/0.732/0.725 | c=0.998347
[Epoch 0071] loss=14.3403 cls=0.1187 smmd=0.5309 ct=11.4936 rec=1.3288 | train/val/test=1.000/0.758/0.734 | c=0.998347
[Epoch 0072] loss=13.8905 cls=0.0853 smmd=0.4362 ct=11.5349 rec=1.3218 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0073] loss=13.6856 cls=0.0716 smmd=0.4083 ct=11.4784 rec=1.3002 | train/val/test=1.000/0.798/0.773 | c=0.998347
[Epoch 0074] loss=13.6372 cls=0.0661 smmd=0.3976 ct=11.4859 rec=1.3040 | train/val/test=1.000/0.796/0.769 | c=0.998347
[Epoch 0075] loss=13.6243 cls=0.0711 smmd=0.3913 ct=11.5018 rec=1.3053 | train/val/test=1.000/0.788/0.759 | c=0.998347
[Epoch 0076] loss=13.8139 cls=0.0810 smmd=0.4266 ct=11.5093 rec=1.3119 | train/val/test=1.000/0.776/0.770 | c=0.998347
[Epoch 0077] loss=14.6146 cls=0.0956 smmd=0.5857 ct=11.5062 rec=1.3178 | train/val/test=1.000/0.756/0.735 | c=0.998347
[Epoch 0078] loss=14.9764 cls=0.1085 smmd=0.6417 ct=11.5809 rec=1.3288 | train/val/test=1.000/0.750/0.736 | c=0.998347
[Epoch 0079] loss=14.7490 cls=0.1145 smmd=0.6181 ct=11.4688 rec=1.3255 | train/val/test=1.000/0.706/0.691 | c=0.998347
[Epoch 0080] loss=14.2105 cls=0.1116 smmd=0.4854 ct=11.5939 rec=1.3366 | train/val/test=1.000/0.764/0.740 | c=0.998347
[Epoch 0081] loss=13.7713 cls=0.0906 smmd=0.4264 ct=11.4632 rec=1.3077 | train/val/test=1.000/0.776/0.738 | c=0.998347
[Epoch 0082] loss=13.6018 cls=0.0707 smmd=0.3878 ct=11.4961 rec=1.3135 | train/val/test=1.000/0.792/0.769 | c=0.998347
[Epoch 0083] loss=13.4291 cls=0.0635 smmd=0.3553 ct=11.4910 rec=1.3003 | train/val/test=1.000/0.790/0.762 | c=0.998347
[Epoch 0084] loss=13.4752 cls=0.0699 smmd=0.3656 ct=11.4820 rec=1.3045 | train/val/test=1.000/0.802/0.762 | c=0.998347
[Epoch 0085] loss=13.8514 cls=0.0815 smmd=0.4341 ct=11.5087 rec=1.3160 | train/val/test=1.000/0.782/0.761 | c=0.998347
[Epoch 0086] loss=14.4039 cls=0.0947 smmd=0.5377 ct=11.5361 rec=1.3180 | train/val/test=1.000/0.794/0.764 | c=0.998347
[Epoch 0087] loss=15.0670 cls=0.0977 smmd=0.6745 ct=11.5132 rec=1.3220 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0088] loss=15.0120 cls=0.0922 smmd=0.6603 ct=11.5327 rec=1.3177 | train/val/test=1.000/0.786/0.754 | c=0.998347
[Epoch 0089] loss=14.1233 cls=0.0835 smmd=0.4910 ct=11.4954 rec=1.3093 | train/val/test=1.000/0.796/0.764 | c=0.998347
[Epoch 0090] loss=13.6138 cls=0.0748 smmd=0.3896 ct=11.4972 rec=1.3134 | train/val/test=1.000/0.780/0.751 | c=0.998347
[Epoch 0091] loss=13.5042 cls=0.0754 smmd=0.3717 ct=11.4779 rec=1.3010 | train/val/test=1.000/0.794/0.764 | c=0.998347
[Epoch 0092] loss=13.3798 cls=0.0716 smmd=0.3444 ct=11.4913 rec=1.3066 | train/val/test=1.000/0.796/0.773 | c=0.998347
[Epoch 0093] loss=13.4460 cls=0.0787 smmd=0.3581 ct=11.4851 rec=1.3094 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0094] loss=13.6978 cls=0.0947 smmd=0.4021 ct=11.5085 rec=1.3152 | train/val/test=1.000/0.810/0.766 | c=0.998347
[Epoch 0095] loss=14.2071 cls=0.1062 smmd=0.4937 ct=11.5529 rec=1.3248 | train/val/test=1.000/0.778/0.767 | c=0.998347
[Epoch 0096] loss=15.2108 cls=0.1099 smmd=0.7049 ct=11.4988 rec=1.3252 | train/val/test=1.000/0.774/0.766 | c=0.998347
[Epoch 0097] loss=15.2159 cls=0.0999 smmd=0.6983 ct=11.5423 rec=1.3202 | train/val/test=1.000/0.802/0.775 | c=0.998347
[Epoch 0098] loss=14.1701 cls=0.0865 smmd=0.4938 ct=11.5259 rec=1.3180 | train/val/test=1.000/0.774/0.741 | c=0.998347
[Epoch 0099] loss=13.8696 cls=0.0811 smmd=0.4494 ct=11.4516 rec=1.3048 | train/val/test=1.000/0.806/0.767 | c=0.998347
=== Best @ epoch 94: val=0.8100, test=0.7660 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 - 2025-09-21 05:57:19:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.1637 cls=1.0970 smmd=5.6386 ct=11.2807 rec=1.4139 | train/val/test=0.577/0.440/0.417 | c=0.998347
[Epoch 0001] loss=30.4191 cls=1.0788 smmd=3.7010 ct=11.2334 rec=1.4136 | train/val/test=0.615/0.490/0.505 | c=0.998347
[Epoch 0002] loss=35.5861 cls=1.0362 smmd=4.7430 ct=11.2116 rec=1.4138 | train/val/test=0.769/0.666/0.658 | c=0.998347
[Epoch 0003] loss=34.3949 cls=0.9440 smmd=4.5337 ct=11.1134 rec=1.4111 | train/val/test=0.731/0.644/0.638 | c=0.998347
[Epoch 0004] loss=24.8347 cls=0.8010 smmd=2.6699 ct=10.9445 rec=1.4014 | train/val/test=0.692/0.676/0.672 | c=0.998347
[Epoch 0005] loss=26.5574 cls=0.6643 smmd=3.0255 ct=10.9596 rec=1.3824 | train/val/test=0.769/0.686/0.683 | c=0.998347
[Epoch 0006] loss=28.9078 cls=0.5474 smmd=3.5071 ct=10.9628 rec=1.3582 | train/val/test=0.769/0.686/0.687 | c=0.998347
[Epoch 0007] loss=27.2831 cls=0.4567 smmd=3.0658 ct=11.5922 rec=1.3357 | train/val/test=0.846/0.684/0.682 | c=0.998347
[Epoch 0008] loss=22.6259 cls=0.3833 smmd=2.1673 ct=11.4662 rec=1.3169 | train/val/test=0.923/0.704/0.690 | c=0.998347
[Epoch 0009] loss=23.0871 cls=0.3153 smmd=2.2599 ct=11.4987 rec=1.3125 | train/val/test=0.962/0.744/0.705 | c=0.998347
[Epoch 0010] loss=25.1372 cls=0.2372 smmd=2.6801 ct=11.4868 rec=1.3130 | train/val/test=1.000/0.760/0.742 | c=0.998347
[Epoch 0011] loss=23.4694 cls=0.1919 smmd=2.3521 ct=11.4809 rec=1.3204 | train/val/test=1.000/0.770/0.753 | c=0.998347
[Epoch 0012] loss=20.8438 cls=0.1656 smmd=1.8201 ct=11.5284 rec=1.3235 | train/val/test=1.000/0.760/0.718 | c=0.998347
[Epoch 0013] loss=23.4498 cls=0.1521 smmd=2.3456 ct=11.5131 rec=1.3285 | train/val/test=1.000/0.756/0.762 | c=0.998347
[Epoch 0014] loss=22.6716 cls=0.1247 smmd=2.1794 ct=11.5802 rec=1.3219 | train/val/test=1.000/0.776/0.769 | c=0.998347
[Epoch 0015] loss=19.6158 cls=0.0976 smmd=1.5673 ct=11.5993 rec=1.3128 | train/val/test=1.000/0.768/0.761 | c=0.998347
[Epoch 0016] loss=20.2993 cls=0.0819 smmd=1.7381 ct=11.4375 rec=1.3067 | train/val/test=1.000/0.764/0.757 | c=0.998347
[Epoch 0017] loss=20.3438 cls=0.0703 smmd=1.7527 ct=11.4150 rec=1.3009 | train/val/test=1.000/0.770/0.756 | c=0.998347
[Epoch 0018] loss=18.7518 cls=0.0626 smmd=1.4171 ct=11.5054 rec=1.2938 | train/val/test=1.000/0.782/0.760 | c=0.998347
[Epoch 0019] loss=18.3296 cls=0.0643 smmd=1.3238 ct=11.5488 rec=1.2944 | train/val/test=1.000/0.790/0.768 | c=0.998347
[Epoch 0020] loss=18.6004 cls=0.0702 smmd=1.3845 ct=11.5127 rec=1.2995 | train/val/test=1.000/0.770/0.758 | c=0.998347
[Epoch 0021] loss=18.1879 cls=0.0874 smmd=1.3087 ct=11.4707 rec=1.3022 | train/val/test=1.000/0.794/0.777 | c=0.998347
[Epoch 0022] loss=17.7765 cls=0.0937 smmd=1.2088 ct=11.5544 rec=1.3101 | train/val/test=1.000/0.778/0.764 | c=0.998347
[Epoch 0023] loss=17.8509 cls=0.1015 smmd=1.2302 ct=11.5184 rec=1.3071 | train/val/test=1.000/0.786/0.768 | c=0.998347
[Epoch 0024] loss=17.9374 cls=0.1030 smmd=1.2495 ct=11.5077 rec=1.3052 | train/val/test=1.000/0.784/0.771 | c=0.998347
[Epoch 0025] loss=16.9877 cls=0.0877 smmd=1.0505 ct=11.5607 rec=1.3054 | train/val/test=1.000/0.780/0.769 | c=0.998347
[Epoch 0026] loss=17.2470 cls=0.0762 smmd=1.1179 ct=11.4891 rec=1.3010 | train/val/test=1.000/0.776/0.782 | c=0.998347
[Epoch 0027] loss=16.4513 cls=0.0706 smmd=0.9572 ct=11.5000 rec=1.2984 | train/val/test=1.000/0.782/0.773 | c=0.998347
[Epoch 0028] loss=16.0999 cls=0.0579 smmd=0.8855 ct=11.5137 rec=1.2953 | train/val/test=1.000/0.788/0.783 | c=0.998347
[Epoch 0029] loss=16.1285 cls=0.0603 smmd=0.8959 ct=11.4889 rec=1.2970 | train/val/test=1.000/0.772/0.770 | c=0.998347
[Epoch 0030] loss=15.7070 cls=0.0551 smmd=0.8092 ct=11.5042 rec=1.2943 | train/val/test=1.000/0.774/0.763 | c=0.998347
[Epoch 0031] loss=15.7567 cls=0.0613 smmd=0.8246 ct=11.4728 rec=1.3003 | train/val/test=1.000/0.788/0.782 | c=0.998347
[Epoch 0032] loss=16.0714 cls=0.0651 smmd=0.8776 ct=11.5205 rec=1.3054 | train/val/test=1.000/0.776/0.770 | c=0.998347
[Epoch 0033] loss=15.8721 cls=0.0676 smmd=0.8354 ct=11.5308 rec=1.3051 | train/val/test=1.000/0.800/0.783 | c=0.998347
[Epoch 0034] loss=15.8294 cls=0.0707 smmd=0.8242 ct=11.5420 rec=1.3097 | train/val/test=1.000/0.770/0.759 | c=0.998347
[Epoch 0035] loss=15.6767 cls=0.0674 smmd=0.8111 ct=11.4574 rec=1.2998 | train/val/test=1.000/0.788/0.792 | c=0.998347
[Epoch 0036] loss=15.1890 cls=0.0587 smmd=0.7077 ct=11.4915 rec=1.2986 | train/val/test=1.000/0.790/0.791 | c=0.998347
[Epoch 0037] loss=14.9510 cls=0.0575 smmd=0.6524 ct=11.5306 rec=1.2965 | train/val/test=1.000/0.794/0.794 | c=0.998347
[Epoch 0038] loss=14.9710 cls=0.0592 smmd=0.6670 ct=11.4767 rec=1.2973 | train/val/test=1.000/0.782/0.783 | c=0.998347
[Epoch 0039] loss=14.8145 cls=0.0666 smmd=0.6352 ct=11.4755 rec=1.2980 | train/val/test=1.000/0.802/0.788 | c=0.998347
[Epoch 0040] loss=14.8230 cls=0.0752 smmd=0.6236 ct=11.5369 rec=1.3071 | train/val/test=1.000/0.786/0.778 | c=0.998347
[Epoch 0041] loss=14.9932 cls=0.0844 smmd=0.6695 ct=11.4728 rec=1.3048 | train/val/test=1.000/0.806/0.790 | c=0.998347
[Epoch 0042] loss=15.2739 cls=0.0913 smmd=0.7074 ct=11.5597 rec=1.3146 | train/val/test=1.000/0.780/0.767 | c=0.998347
[Epoch 0043] loss=15.5544 cls=0.0948 smmd=0.7764 ct=11.4940 rec=1.3098 | train/val/test=1.000/0.802/0.784 | c=0.998347
[Epoch 0044] loss=14.7631 cls=0.0951 smmd=0.6104 ct=11.5316 rec=1.3184 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0045] loss=14.7361 cls=0.0818 smmd=0.6124 ct=11.5032 rec=1.3021 | train/val/test=1.000/0.798/0.785 | c=0.998347
[Epoch 0046] loss=14.4492 cls=0.0754 smmd=0.5590 ct=11.4861 rec=1.3042 | train/val/test=1.000/0.802/0.789 | c=0.998347
[Epoch 0047] loss=14.3967 cls=0.0734 smmd=0.5471 ct=11.4944 rec=1.3000 | train/val/test=1.000/0.796/0.791 | c=0.998347
[Epoch 0048] loss=14.2387 cls=0.0764 smmd=0.5101 ct=11.5193 rec=1.3055 | train/val/test=1.000/0.786/0.781 | c=0.998347
[Epoch 0049] loss=14.3530 cls=0.0855 smmd=0.5405 ct=11.4772 rec=1.3064 | train/val/test=1.000/0.808/0.777 | c=0.998347
[Epoch 0050] loss=14.6400 cls=0.0968 smmd=0.5830 ct=11.5442 rec=1.3219 | train/val/test=1.000/0.752/0.749 | c=0.998347
[Epoch 0051] loss=14.9937 cls=0.1180 smmd=0.6568 ct=11.5187 rec=1.3222 | train/val/test=1.000/0.780/0.757 | c=0.998347
[Epoch 0052] loss=15.0045 cls=0.1099 smmd=0.6534 ct=11.5490 rec=1.3337 | train/val/test=1.000/0.762/0.748 | c=0.998347
[Epoch 0053] loss=14.5710 cls=0.1077 smmd=0.5759 ct=11.5060 rec=1.3153 | train/val/test=1.000/0.800/0.771 | c=0.998347
[Epoch 0054] loss=14.2825 cls=0.0816 smmd=0.5191 ct=11.5147 rec=1.3144 | train/val/test=1.000/0.778/0.778 | c=0.998347
[Epoch 0055] loss=14.0115 cls=0.0747 smmd=0.4723 ct=11.4827 rec=1.3008 | train/val/test=1.000/0.802/0.785 | c=0.998347
[Epoch 0056] loss=13.9356 cls=0.0722 smmd=0.4525 ct=11.5065 rec=1.3042 | train/val/test=1.000/0.786/0.773 | c=0.998347
[Epoch 0057] loss=13.8821 cls=0.0778 smmd=0.4465 ct=11.4801 rec=1.3037 | train/val/test=1.000/0.804/0.781 | c=0.998347
[Epoch 0058] loss=14.2104 cls=0.0860 smmd=0.4994 ct=11.5387 rec=1.3155 | train/val/test=1.000/0.774/0.751 | c=0.998347
[Epoch 0059] loss=14.5593 cls=0.0997 smmd=0.5783 ct=11.4865 rec=1.3160 | train/val/test=1.000/0.780/0.753 | c=0.998347
[Epoch 0060] loss=14.8074 cls=0.1092 smmd=0.6088 ct=11.5758 rec=1.3307 | train/val/test=1.000/0.752/0.745 | c=0.998347
[Epoch 0061] loss=14.5998 cls=0.1148 smmd=0.5854 ct=11.4832 rec=1.3235 | train/val/test=1.000/0.746/0.733 | c=0.998347
[Epoch 0062] loss=14.3943 cls=0.1077 smmd=0.5283 ct=11.5656 rec=1.3330 | train/val/test=1.000/0.760/0.745 | c=0.998347
[Epoch 0063] loss=14.1829 cls=0.0951 smmd=0.5022 ct=11.4930 rec=1.3117 | train/val/test=1.000/0.794/0.759 | c=0.998347
[Epoch 0064] loss=13.8511 cls=0.0777 smmd=0.4346 ct=11.5078 rec=1.3146 | train/val/test=1.000/0.784/0.763 | c=0.998347
[Epoch 0065] loss=13.7844 cls=0.0723 smmd=0.4261 ct=11.4872 rec=1.3027 | train/val/test=1.000/0.804/0.779 | c=0.998347
[Epoch 0066] loss=13.9142 cls=0.0734 smmd=0.4477 ct=11.5081 rec=1.3100 | train/val/test=1.000/0.776/0.769 | c=0.998347
[Epoch 0067] loss=14.1819 cls=0.0820 smmd=0.5003 ct=11.5085 rec=1.3110 | train/val/test=1.000/0.798/0.762 | c=0.998347
[Epoch 0068] loss=14.3640 cls=0.0883 smmd=0.5335 ct=11.5202 rec=1.3198 | train/val/test=1.000/0.756/0.749 | c=0.998347
[Epoch 0069] loss=14.7047 cls=0.1057 smmd=0.6006 ct=11.5162 rec=1.3245 | train/val/test=1.000/0.734/0.709 | c=0.998347
[Epoch 0070] loss=14.6703 cls=0.1131 smmd=0.5822 ct=11.5695 rec=1.3338 | train/val/test=1.000/0.732/0.725 | c=0.998347
[Epoch 0071] loss=14.3403 cls=0.1187 smmd=0.5309 ct=11.4936 rec=1.3288 | train/val/test=1.000/0.758/0.734 | c=0.998347
[Epoch 0072] loss=13.8905 cls=0.0853 smmd=0.4362 ct=11.5349 rec=1.3218 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0073] loss=13.6856 cls=0.0716 smmd=0.4083 ct=11.4784 rec=1.3002 | train/val/test=1.000/0.798/0.773 | c=0.998347
[Epoch 0074] loss=13.6372 cls=0.0661 smmd=0.3976 ct=11.4859 rec=1.3040 | train/val/test=1.000/0.796/0.769 | c=0.998347
[Epoch 0075] loss=13.6243 cls=0.0711 smmd=0.3913 ct=11.5018 rec=1.3053 | train/val/test=1.000/0.788/0.759 | c=0.998347
[Epoch 0076] loss=13.8139 cls=0.0810 smmd=0.4266 ct=11.5093 rec=1.3119 | train/val/test=1.000/0.776/0.770 | c=0.998347
[Epoch 0077] loss=14.6146 cls=0.0956 smmd=0.5857 ct=11.5062 rec=1.3178 | train/val/test=1.000/0.756/0.735 | c=0.998347
[Epoch 0078] loss=14.9764 cls=0.1085 smmd=0.6417 ct=11.5809 rec=1.3288 | train/val/test=1.000/0.750/0.736 | c=0.998347
[Epoch 0079] loss=14.7490 cls=0.1145 smmd=0.6181 ct=11.4688 rec=1.3255 | train/val/test=1.000/0.706/0.691 | c=0.998347
[Epoch 0080] loss=14.2105 cls=0.1116 smmd=0.4854 ct=11.5939 rec=1.3366 | train/val/test=1.000/0.764/0.740 | c=0.998347
[Epoch 0081] loss=13.7713 cls=0.0906 smmd=0.4264 ct=11.4632 rec=1.3077 | train/val/test=1.000/0.776/0.738 | c=0.998347
[Epoch 0082] loss=13.6018 cls=0.0707 smmd=0.3878 ct=11.4961 rec=1.3135 | train/val/test=1.000/0.792/0.769 | c=0.998347
[Epoch 0083] loss=13.4291 cls=0.0635 smmd=0.3553 ct=11.4910 rec=1.3003 | train/val/test=1.000/0.790/0.762 | c=0.998347
[Epoch 0084] loss=13.4752 cls=0.0699 smmd=0.3656 ct=11.4820 rec=1.3045 | train/val/test=1.000/0.802/0.762 | c=0.998347
[Epoch 0085] loss=13.8514 cls=0.0815 smmd=0.4341 ct=11.5087 rec=1.3160 | train/val/test=1.000/0.782/0.761 | c=0.998347
[Epoch 0086] loss=14.4039 cls=0.0947 smmd=0.5377 ct=11.5361 rec=1.3180 | train/val/test=1.000/0.794/0.764 | c=0.998347
[Epoch 0087] loss=15.0670 cls=0.0977 smmd=0.6745 ct=11.5132 rec=1.3220 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0088] loss=15.0120 cls=0.0922 smmd=0.6603 ct=11.5327 rec=1.3177 | train/val/test=1.000/0.786/0.754 | c=0.998347
[Epoch 0089] loss=14.1233 cls=0.0835 smmd=0.4910 ct=11.4954 rec=1.3093 | train/val/test=1.000/0.796/0.764 | c=0.998347
[Epoch 0090] loss=13.6138 cls=0.0748 smmd=0.3896 ct=11.4972 rec=1.3134 | train/val/test=1.000/0.780/0.751 | c=0.998347
[Epoch 0091] loss=13.5042 cls=0.0754 smmd=0.3717 ct=11.4779 rec=1.3010 | train/val/test=1.000/0.794/0.764 | c=0.998347
[Epoch 0092] loss=13.3798 cls=0.0716 smmd=0.3444 ct=11.4913 rec=1.3066 | train/val/test=1.000/0.796/0.773 | c=0.998347
[Epoch 0093] loss=13.4460 cls=0.0787 smmd=0.3581 ct=11.4851 rec=1.3094 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0094] loss=13.6978 cls=0.0947 smmd=0.4021 ct=11.5085 rec=1.3152 | train/val/test=1.000/0.810/0.766 | c=0.998347
[Epoch 0095] loss=14.2071 cls=0.1062 smmd=0.4937 ct=11.5529 rec=1.3248 | train/val/test=1.000/0.778/0.767 | c=0.998347
[Epoch 0096] loss=15.2108 cls=0.1099 smmd=0.7049 ct=11.4988 rec=1.3252 | train/val/test=1.000/0.774/0.766 | c=0.998347
[Epoch 0097] loss=15.2159 cls=0.0999 smmd=0.6983 ct=11.5423 rec=1.3202 | train/val/test=1.000/0.802/0.775 | c=0.998347
[Epoch 0098] loss=14.1701 cls=0.0865 smmd=0.4938 ct=11.5259 rec=1.3180 | train/val/test=1.000/0.774/0.741 | c=0.998347
[Epoch 0099] loss=13.8696 cls=0.0811 smmd=0.4494 ct=11.4516 rec=1.3048 | train/val/test=1.000/0.806/0.767 | c=0.998347
=== Best @ epoch 94: val=0.8100, test=0.7660 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 completed in 192.79 seconds.
==================================================
