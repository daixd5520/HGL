Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 - 2025-09-21 05:50:54:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5987 cls=1.0969 smmd=5.6249 ct=11.2811 rec=1.4136 | train/val/test=0.385/0.388/0.413 | c=0.998347
[Epoch 0001] loss=22.7000 cls=1.0874 smmd=4.0802 ct=11.2490 rec=1.4137 | train/val/test=0.846/0.542/0.542 | c=0.998347
[Epoch 0002] loss=24.5207 cls=1.0787 smmd=4.8138 ct=11.2401 rec=1.4136 | train/val/test=0.538/0.546/0.578 | c=0.998347
[Epoch 0003] loss=23.1579 cls=1.0492 smmd=4.3234 ct=11.1182 rec=1.4134 | train/val/test=0.538/0.570/0.597 | c=0.998347
[Epoch 0004] loss=18.1387 cls=1.0005 smmd=2.3938 ct=10.9477 rec=1.4127 | train/val/test=0.692/0.564/0.587 | c=0.998347
[Epoch 0005] loss=19.6386 cls=0.9541 smmd=3.0117 ct=10.9266 rec=1.4112 | train/val/test=0.692/0.558/0.583 | c=0.998347
[Epoch 0006] loss=19.8454 cls=0.9092 smmd=3.1158 ct=10.8969 rec=1.4087 | train/val/test=0.692/0.558/0.578 | c=0.998347
[Epoch 0007] loss=17.7187 cls=0.8692 smmd=2.2866 ct=10.8650 rec=1.4054 | train/val/test=0.692/0.562/0.578 | c=0.998347
[Epoch 0008] loss=16.6543 cls=0.8424 smmd=1.8650 ct=10.8691 rec=1.4031 | train/val/test=0.692/0.556/0.576 | c=0.998347
[Epoch 0009] loss=18.2002 cls=0.8260 smmd=2.4814 ct=10.8822 rec=1.4032 | train/val/test=0.692/0.558/0.584 | c=0.998347
[Epoch 0010] loss=17.3620 cls=0.8133 smmd=2.1503 ct=10.8778 rec=1.4038 | train/val/test=0.692/0.560/0.591 | c=0.998347
[Epoch 0011] loss=16.1555 cls=0.8020 smmd=1.6702 ct=10.8772 rec=1.4035 | train/val/test=0.769/0.578/0.603 | c=0.998347
[Epoch 0012] loss=17.3396 cls=0.7863 smmd=2.1415 ct=10.8915 rec=1.4026 | train/val/test=0.769/0.576/0.612 | c=0.998347
[Epoch 0013] loss=16.7508 cls=0.7444 smmd=1.6521 ct=11.5488 rec=1.3993 | train/val/test=0.923/0.638/0.659 | c=0.998347
[Epoch 0014] loss=16.6116 cls=0.7072 smmd=1.6389 ct=11.4643 rec=1.3926 | train/val/test=0.923/0.674/0.687 | c=0.998347
[Epoch 0015] loss=16.3948 cls=0.6515 smmd=1.5523 ct=11.4941 rec=1.3884 | train/val/test=0.923/0.678/0.700 | c=0.998347
[Epoch 0016] loss=15.9015 cls=0.6163 smmd=1.3588 ct=11.5045 rec=1.3837 | train/val/test=0.923/0.676/0.683 | c=0.998347
[Epoch 0017] loss=15.5856 cls=0.6077 smmd=1.2464 ct=11.4752 rec=1.3809 | train/val/test=0.923/0.692/0.685 | c=0.998347
[Epoch 0018] loss=15.6426 cls=0.5913 smmd=1.2745 ct=11.4705 rec=1.3806 | train/val/test=0.923/0.704/0.707 | c=0.998347
[Epoch 0019] loss=15.3618 cls=0.5746 smmd=1.1525 ct=11.5020 rec=1.3826 | train/val/test=0.923/0.694/0.706 | c=0.998347
[Epoch 0020] loss=15.4945 cls=0.5725 smmd=1.1888 ct=11.5439 rec=1.3847 | train/val/test=0.923/0.692/0.700 | c=0.998347
[Epoch 0021] loss=15.5186 cls=0.5659 smmd=1.2134 ct=11.5090 rec=1.3861 | train/val/test=0.923/0.718/0.717 | c=0.998347
[Epoch 0022] loss=15.4027 cls=0.5495 smmd=1.1826 ct=11.4796 rec=1.3838 | train/val/test=0.923/0.722/0.722 | c=0.998347
[Epoch 0023] loss=15.0355 cls=0.5034 smmd=1.0233 ct=11.5348 rec=1.3815 | train/val/test=0.923/0.724/0.723 | c=0.998347
[Epoch 0024] loss=14.9285 cls=0.4688 smmd=1.0023 ct=11.5016 rec=1.3738 | train/val/test=1.000/0.720/0.725 | c=0.998347
[Epoch 0025] loss=14.5325 cls=0.4357 smmd=0.8679 ct=11.4605 rec=1.3687 | train/val/test=1.000/0.736/0.726 | c=0.998347
[Epoch 0026] loss=14.5070 cls=0.4066 smmd=0.8513 ct=11.4927 rec=1.3655 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0027] loss=14.3215 cls=0.3910 smmd=0.7872 ct=11.4759 rec=1.3641 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0028] loss=14.3238 cls=0.3897 smmd=0.7901 ct=11.4709 rec=1.3653 | train/val/test=1.000/0.734/0.741 | c=0.998347
[Epoch 0029] loss=14.5065 cls=0.3875 smmd=0.8490 ct=11.5062 rec=1.3679 | train/val/test=1.000/0.724/0.731 | c=0.998347
[Epoch 0030] loss=14.5428 cls=0.3825 smmd=0.8627 ct=11.5100 rec=1.3693 | train/val/test=1.000/0.718/0.727 | c=0.998347
[Epoch 0031] loss=14.6229 cls=0.3680 smmd=0.9000 ct=11.5043 rec=1.3690 | train/val/test=1.000/0.730/0.737 | c=0.998347
[Epoch 0032] loss=14.3958 cls=0.3483 smmd=0.8215 ct=11.4865 rec=1.3630 | train/val/test=1.000/0.732/0.723 | c=0.998347
[Epoch 0033] loss=14.1635 cls=0.3154 smmd=0.7317 ct=11.4976 rec=1.3582 | train/val/test=1.000/0.728/0.733 | c=0.998347
[Epoch 0034] loss=13.9215 cls=0.2922 smmd=0.6483 ct=11.4784 rec=1.3526 | train/val/test=1.000/0.726/0.736 | c=0.998347
[Epoch 0035] loss=13.9155 cls=0.2782 smmd=0.6555 ct=11.4628 rec=1.3496 | train/val/test=1.000/0.742/0.733 | c=0.998347
[Epoch 0036] loss=13.7523 cls=0.2698 smmd=0.5824 ct=11.4868 rec=1.3494 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0037] loss=13.7918 cls=0.2680 smmd=0.5995 ct=11.4832 rec=1.3519 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0038] loss=13.9963 cls=0.2739 smmd=0.6809 ct=11.4799 rec=1.3545 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0039] loss=14.1012 cls=0.2720 smmd=0.7061 ct=11.5215 rec=1.3568 | train/val/test=1.000/0.728/0.727 | c=0.998347
[Epoch 0040] loss=14.0637 cls=0.2609 smmd=0.7098 ct=11.4820 rec=1.3537 | train/val/test=1.000/0.724/0.721 | c=0.998347
[Epoch 0041] loss=14.0698 cls=0.2414 smmd=0.7092 ct=11.5010 rec=1.3500 | train/val/test=1.000/0.726/0.731 | c=0.998347
[Epoch 0042] loss=13.6568 cls=0.2217 smmd=0.5553 ct=11.4866 rec=1.3423 | train/val/test=1.000/0.734/0.724 | c=0.998347
[Epoch 0043] loss=13.6130 cls=0.2027 smmd=0.5484 ct=11.4719 rec=1.3374 | train/val/test=1.000/0.732/0.729 | c=0.998347
[Epoch 0044] loss=13.5898 cls=0.1973 smmd=0.5382 ct=11.4781 rec=1.3352 | train/val/test=1.000/0.724/0.725 | c=0.998347
[Epoch 0045] loss=13.5099 cls=0.1943 smmd=0.5030 ct=11.4866 rec=1.3372 | train/val/test=1.000/0.736/0.729 | c=0.998347
[Epoch 0046] loss=13.5149 cls=0.2034 smmd=0.5068 ct=11.4763 rec=1.3397 | train/val/test=1.000/0.706/0.718 | c=0.998347
[Epoch 0047] loss=13.9490 cls=0.2088 smmd=0.6615 ct=11.5171 rec=1.3477 | train/val/test=1.000/0.742/0.733 | c=0.998347
[Epoch 0048] loss=13.9605 cls=0.2094 smmd=0.6759 ct=11.4943 rec=1.3436 | train/val/test=1.000/0.708/0.721 | c=0.998347
[Epoch 0049] loss=13.8522 cls=0.1928 smmd=0.6308 ct=11.5069 rec=1.3441 | train/val/test=1.000/0.738/0.729 | c=0.998347
[Epoch 0050] loss=13.7466 cls=0.1808 smmd=0.6003 ct=11.4883 rec=1.3343 | train/val/test=1.000/0.720/0.722 | c=0.998347
[Epoch 0051] loss=13.4661 cls=0.1596 smmd=0.4932 ct=11.4875 rec=1.3317 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0052] loss=13.3939 cls=0.1593 smmd=0.4726 ct=11.4696 rec=1.3264 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0053] loss=13.4857 cls=0.1507 smmd=0.5051 ct=11.4834 rec=1.3284 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0054] loss=13.3932 cls=0.1613 smmd=0.4734 ct=11.4647 rec=1.3289 | train/val/test=1.000/0.706/0.709 | c=0.998347
[Epoch 0055] loss=13.6409 cls=0.1665 smmd=0.5466 ct=11.5210 rec=1.3403 | train/val/test=1.000/0.732/0.732 | c=0.998347
[Epoch 0056] loss=13.8153 cls=0.1824 smmd=0.6233 ct=11.4978 rec=1.3361 | train/val/test=1.000/0.690/0.684 | c=0.998347
[Epoch 0057] loss=13.9968 cls=0.1803 smmd=0.6668 ct=11.5629 rec=1.3532 | train/val/test=1.000/0.726/0.736 | c=0.998347
[Epoch 0058] loss=13.7916 cls=0.1714 smmd=0.6125 ct=11.5090 rec=1.3311 | train/val/test=1.000/0.710/0.711 | c=0.998347
[Epoch 0059] loss=13.4676 cls=0.1253 smmd=0.4979 ct=11.4987 rec=1.3228 | train/val/test=1.000/0.730/0.718 | c=0.998347
[Epoch 0060] loss=13.2924 cls=0.1144 smmd=0.4396 ct=11.4806 rec=1.3112 | train/val/test=1.000/0.734/0.722 | c=0.998347
[Epoch 0061] loss=13.3011 cls=0.1142 smmd=0.4486 ct=11.4665 rec=1.3119 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0062] loss=13.2780 cls=0.1179 smmd=0.4379 ct=11.4662 rec=1.3166 | train/val/test=1.000/0.738/0.730 | c=0.998347
[Epoch 0063] loss=13.4153 cls=0.1344 smmd=0.4657 ct=11.5212 rec=1.3255 | train/val/test=1.000/0.736/0.734 | c=0.998347
[Epoch 0064] loss=13.7488 cls=0.1503 smmd=0.6092 ct=11.4848 rec=1.3314 | train/val/test=1.000/0.726/0.723 | c=0.998347
[Epoch 0065] loss=14.0431 cls=0.1564 smmd=0.7139 ct=11.5115 rec=1.3377 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0066] loss=13.7849 cls=0.1403 smmd=0.6226 ct=11.4950 rec=1.3263 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0067] loss=13.2972 cls=0.1220 smmd=0.4389 ct=11.4783 rec=1.3211 | train/val/test=1.000/0.730/0.718 | c=0.998347
[Epoch 0068] loss=13.2886 cls=0.1155 smmd=0.4465 ct=11.4580 rec=1.3133 | train/val/test=1.000/0.728/0.726 | c=0.998347
[Epoch 0069] loss=13.2354 cls=0.1056 smmd=0.4209 ct=11.4739 rec=1.3127 | train/val/test=1.000/0.730/0.719 | c=0.998347
[Epoch 0070] loss=13.1075 cls=0.1126 smmd=0.3740 ct=11.4591 rec=1.3140 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0071] loss=13.3565 cls=0.1217 smmd=0.4500 ct=11.5085 rec=1.3245 | train/val/test=1.000/0.736/0.740 | c=0.998347
[Epoch 0072] loss=13.6938 cls=0.1423 smmd=0.5897 ct=11.4844 rec=1.3282 | train/val/test=1.000/0.702/0.687 | c=0.998347
[Epoch 0073] loss=14.1502 cls=0.1599 smmd=0.7250 ct=11.5818 rec=1.3518 | train/val/test=1.000/0.726/0.732 | c=0.998347
[Epoch 0074] loss=14.0941 cls=0.1641 smmd=0.7287 ct=11.5200 rec=1.3404 | train/val/test=1.000/0.712/0.694 | c=0.998347
[Epoch 0075] loss=13.6466 cls=0.1029 smmd=0.5579 ct=11.5384 rec=1.3238 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0076] loss=13.3075 cls=0.0770 smmd=0.4559 ct=11.4803 rec=1.2978 | train/val/test=1.000/0.724/0.724 | c=0.998347
[Epoch 0077] loss=13.2901 cls=0.0742 smmd=0.4615 ct=11.4501 rec=1.2981 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0078] loss=13.1848 cls=0.0680 smmd=0.4131 ct=11.4685 rec=1.2989 | train/val/test=1.000/0.736/0.719 | c=0.998347
[Epoch 0079] loss=13.2904 cls=0.0837 smmd=0.4347 ct=11.5092 rec=1.3054 | train/val/test=1.000/0.744/0.731 | c=0.998347
[Epoch 0080] loss=13.5160 cls=0.1042 smmd=0.5262 ct=11.4883 rec=1.3204 | train/val/test=1.000/0.746/0.741 | c=0.998347
[Epoch 0081] loss=13.9759 cls=0.1339 smmd=0.6967 ct=11.5019 rec=1.3305 | train/val/test=1.000/0.652/0.636 | c=0.998347
[Epoch 0082] loss=14.3428 cls=0.2262 smmd=0.7650 ct=11.6289 rec=1.3766 | train/val/test=0.923/0.598/0.599 | c=0.998347
[Epoch 0083] loss=14.4972 cls=0.2777 smmd=0.8532 ct=11.5320 rec=1.3867 | train/val/test=1.000/0.678/0.659 | c=0.998347
[Epoch 0084] loss=13.8541 cls=0.1517 smmd=0.6046 ct=11.5939 rec=1.3458 | train/val/test=1.000/0.724/0.715 | c=0.998347
[Epoch 0085] loss=13.3002 cls=0.0532 smmd=0.4618 ct=11.4748 rec=1.2888 | train/val/test=1.000/0.698/0.705 | c=0.998347
[Epoch 0086] loss=13.6611 cls=0.1107 smmd=0.5897 ct=11.4652 rec=1.3325 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0087] loss=13.3116 cls=0.0457 smmd=0.4664 ct=11.4789 rec=1.2878 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0088] loss=13.4246 cls=0.0508 smmd=0.4920 ct=11.5225 rec=1.2933 | train/val/test=1.000/0.738/0.751 | c=0.998347
[Epoch 0089] loss=13.3679 cls=0.0782 smmd=0.4687 ct=11.5018 rec=1.3108 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0090] loss=13.9619 cls=0.0798 smmd=0.6955 ct=11.5233 rec=1.3198 | train/val/test=1.000/0.750/0.739 | c=0.998347
[Epoch 0091] loss=14.2770 cls=0.0927 smmd=0.8035 ct=11.5633 rec=1.3173 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0092] loss=14.0272 cls=0.1020 smmd=0.7026 ct=11.5531 rec=1.3334 | train/val/test=1.000/0.722/0.718 | c=0.998347
[Epoch 0093] loss=13.9601 cls=0.1311 smmd=0.6928 ct=11.5018 rec=1.3215 | train/val/test=1.000/0.718/0.701 | c=0.998347
[Epoch 0094] loss=13.4974 cls=0.1014 smmd=0.4981 ct=11.5392 rec=1.3244 | train/val/test=1.000/0.730/0.714 | c=0.998347
[Epoch 0095] loss=13.2194 cls=0.0770 smmd=0.4236 ct=11.4729 rec=1.2980 | train/val/test=1.000/0.736/0.728 | c=0.998347
[Epoch 0096] loss=13.3383 cls=0.0814 smmd=0.4872 ct=11.4297 rec=1.2996 | train/val/test=1.000/0.732/0.723 | c=0.998347
[Epoch 0097] loss=13.1565 cls=0.0809 smmd=0.3985 ct=11.4662 rec=1.3072 | train/val/test=1.000/0.740/0.724 | c=0.998347
[Epoch 0098] loss=13.3446 cls=0.0869 smmd=0.4528 ct=11.5148 rec=1.3083 | train/val/test=1.000/0.718/0.728 | c=0.998347
[Epoch 0099] loss=13.5493 cls=0.1031 smmd=0.5446 ct=11.4760 rec=1.3207 | train/val/test=1.000/0.704/0.698 | c=0.998347
=== Best @ epoch 90: val=0.7500, test=0.7390 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 - 2025-09-21 05:50:54:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5987 cls=1.0969 smmd=5.6249 ct=11.2811 rec=1.4136 | train/val/test=0.385/0.388/0.413 | c=0.998347
[Epoch 0001] loss=22.7000 cls=1.0874 smmd=4.0802 ct=11.2490 rec=1.4137 | train/val/test=0.846/0.542/0.542 | c=0.998347
[Epoch 0002] loss=24.5207 cls=1.0787 smmd=4.8138 ct=11.2401 rec=1.4136 | train/val/test=0.538/0.546/0.578 | c=0.998347
[Epoch 0003] loss=23.1579 cls=1.0492 smmd=4.3234 ct=11.1182 rec=1.4134 | train/val/test=0.538/0.570/0.597 | c=0.998347
[Epoch 0004] loss=18.1387 cls=1.0005 smmd=2.3938 ct=10.9477 rec=1.4127 | train/val/test=0.692/0.564/0.587 | c=0.998347
[Epoch 0005] loss=19.6386 cls=0.9541 smmd=3.0117 ct=10.9266 rec=1.4112 | train/val/test=0.692/0.558/0.583 | c=0.998347
[Epoch 0006] loss=19.8454 cls=0.9092 smmd=3.1158 ct=10.8969 rec=1.4087 | train/val/test=0.692/0.558/0.578 | c=0.998347
[Epoch 0007] loss=17.7187 cls=0.8692 smmd=2.2866 ct=10.8650 rec=1.4054 | train/val/test=0.692/0.562/0.578 | c=0.998347
[Epoch 0008] loss=16.6543 cls=0.8424 smmd=1.8650 ct=10.8691 rec=1.4031 | train/val/test=0.692/0.556/0.576 | c=0.998347
[Epoch 0009] loss=18.2002 cls=0.8260 smmd=2.4814 ct=10.8822 rec=1.4032 | train/val/test=0.692/0.558/0.584 | c=0.998347
[Epoch 0010] loss=17.3620 cls=0.8133 smmd=2.1503 ct=10.8778 rec=1.4038 | train/val/test=0.692/0.560/0.591 | c=0.998347
[Epoch 0011] loss=16.1555 cls=0.8020 smmd=1.6702 ct=10.8772 rec=1.4035 | train/val/test=0.769/0.578/0.603 | c=0.998347
[Epoch 0012] loss=17.3396 cls=0.7863 smmd=2.1415 ct=10.8915 rec=1.4026 | train/val/test=0.769/0.576/0.612 | c=0.998347
[Epoch 0013] loss=16.7508 cls=0.7444 smmd=1.6521 ct=11.5488 rec=1.3993 | train/val/test=0.923/0.638/0.659 | c=0.998347
[Epoch 0014] loss=16.6116 cls=0.7072 smmd=1.6389 ct=11.4643 rec=1.3926 | train/val/test=0.923/0.674/0.687 | c=0.998347
[Epoch 0015] loss=16.3948 cls=0.6515 smmd=1.5523 ct=11.4941 rec=1.3884 | train/val/test=0.923/0.678/0.700 | c=0.998347
[Epoch 0016] loss=15.9015 cls=0.6163 smmd=1.3588 ct=11.5045 rec=1.3837 | train/val/test=0.923/0.676/0.683 | c=0.998347
[Epoch 0017] loss=15.5856 cls=0.6077 smmd=1.2464 ct=11.4752 rec=1.3809 | train/val/test=0.923/0.692/0.685 | c=0.998347
[Epoch 0018] loss=15.6426 cls=0.5913 smmd=1.2745 ct=11.4705 rec=1.3806 | train/val/test=0.923/0.704/0.707 | c=0.998347
[Epoch 0019] loss=15.3618 cls=0.5746 smmd=1.1525 ct=11.5020 rec=1.3826 | train/val/test=0.923/0.694/0.706 | c=0.998347
[Epoch 0020] loss=15.4945 cls=0.5725 smmd=1.1888 ct=11.5439 rec=1.3847 | train/val/test=0.923/0.692/0.700 | c=0.998347
[Epoch 0021] loss=15.5186 cls=0.5659 smmd=1.2134 ct=11.5090 rec=1.3861 | train/val/test=0.923/0.718/0.717 | c=0.998347
[Epoch 0022] loss=15.4027 cls=0.5495 smmd=1.1826 ct=11.4796 rec=1.3838 | train/val/test=0.923/0.722/0.722 | c=0.998347
[Epoch 0023] loss=15.0355 cls=0.5034 smmd=1.0233 ct=11.5348 rec=1.3815 | train/val/test=0.923/0.724/0.723 | c=0.998347
[Epoch 0024] loss=14.9285 cls=0.4688 smmd=1.0023 ct=11.5016 rec=1.3738 | train/val/test=1.000/0.720/0.725 | c=0.998347
[Epoch 0025] loss=14.5325 cls=0.4357 smmd=0.8679 ct=11.4605 rec=1.3687 | train/val/test=1.000/0.736/0.726 | c=0.998347
[Epoch 0026] loss=14.5070 cls=0.4066 smmd=0.8513 ct=11.4927 rec=1.3655 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0027] loss=14.3215 cls=0.3910 smmd=0.7872 ct=11.4759 rec=1.3641 | train/val/test=1.000/0.730/0.734 | c=0.998347
[Epoch 0028] loss=14.3238 cls=0.3897 smmd=0.7901 ct=11.4709 rec=1.3653 | train/val/test=1.000/0.734/0.741 | c=0.998347
[Epoch 0029] loss=14.5065 cls=0.3875 smmd=0.8490 ct=11.5062 rec=1.3679 | train/val/test=1.000/0.724/0.731 | c=0.998347
[Epoch 0030] loss=14.5428 cls=0.3825 smmd=0.8627 ct=11.5100 rec=1.3693 | train/val/test=1.000/0.718/0.727 | c=0.998347
[Epoch 0031] loss=14.6229 cls=0.3680 smmd=0.9000 ct=11.5043 rec=1.3690 | train/val/test=1.000/0.730/0.737 | c=0.998347
[Epoch 0032] loss=14.3958 cls=0.3483 smmd=0.8215 ct=11.4865 rec=1.3630 | train/val/test=1.000/0.732/0.723 | c=0.998347
[Epoch 0033] loss=14.1635 cls=0.3154 smmd=0.7317 ct=11.4976 rec=1.3582 | train/val/test=1.000/0.728/0.733 | c=0.998347
[Epoch 0034] loss=13.9215 cls=0.2922 smmd=0.6483 ct=11.4784 rec=1.3526 | train/val/test=1.000/0.726/0.736 | c=0.998347
[Epoch 0035] loss=13.9155 cls=0.2782 smmd=0.6555 ct=11.4628 rec=1.3496 | train/val/test=1.000/0.742/0.733 | c=0.998347
[Epoch 0036] loss=13.7523 cls=0.2698 smmd=0.5824 ct=11.4868 rec=1.3494 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0037] loss=13.7918 cls=0.2680 smmd=0.5995 ct=11.4832 rec=1.3519 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0038] loss=13.9963 cls=0.2739 smmd=0.6809 ct=11.4799 rec=1.3545 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0039] loss=14.1012 cls=0.2720 smmd=0.7061 ct=11.5215 rec=1.3568 | train/val/test=1.000/0.728/0.727 | c=0.998347
[Epoch 0040] loss=14.0637 cls=0.2609 smmd=0.7098 ct=11.4820 rec=1.3537 | train/val/test=1.000/0.724/0.721 | c=0.998347
[Epoch 0041] loss=14.0698 cls=0.2414 smmd=0.7092 ct=11.5010 rec=1.3500 | train/val/test=1.000/0.726/0.731 | c=0.998347
[Epoch 0042] loss=13.6568 cls=0.2217 smmd=0.5553 ct=11.4866 rec=1.3423 | train/val/test=1.000/0.734/0.724 | c=0.998347
[Epoch 0043] loss=13.6130 cls=0.2027 smmd=0.5484 ct=11.4719 rec=1.3374 | train/val/test=1.000/0.732/0.729 | c=0.998347
[Epoch 0044] loss=13.5898 cls=0.1973 smmd=0.5382 ct=11.4781 rec=1.3352 | train/val/test=1.000/0.724/0.725 | c=0.998347
[Epoch 0045] loss=13.5099 cls=0.1943 smmd=0.5030 ct=11.4866 rec=1.3372 | train/val/test=1.000/0.736/0.729 | c=0.998347
[Epoch 0046] loss=13.5149 cls=0.2034 smmd=0.5068 ct=11.4763 rec=1.3397 | train/val/test=1.000/0.706/0.718 | c=0.998347
[Epoch 0047] loss=13.9490 cls=0.2088 smmd=0.6615 ct=11.5171 rec=1.3477 | train/val/test=1.000/0.742/0.733 | c=0.998347
[Epoch 0048] loss=13.9605 cls=0.2094 smmd=0.6759 ct=11.4943 rec=1.3436 | train/val/test=1.000/0.708/0.721 | c=0.998347
[Epoch 0049] loss=13.8522 cls=0.1928 smmd=0.6308 ct=11.5069 rec=1.3441 | train/val/test=1.000/0.738/0.729 | c=0.998347
[Epoch 0050] loss=13.7466 cls=0.1808 smmd=0.6003 ct=11.4883 rec=1.3343 | train/val/test=1.000/0.720/0.722 | c=0.998347
[Epoch 0051] loss=13.4661 cls=0.1596 smmd=0.4932 ct=11.4875 rec=1.3317 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0052] loss=13.3939 cls=0.1593 smmd=0.4726 ct=11.4696 rec=1.3264 | train/val/test=1.000/0.722/0.726 | c=0.998347
[Epoch 0053] loss=13.4857 cls=0.1507 smmd=0.5051 ct=11.4834 rec=1.3284 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0054] loss=13.3932 cls=0.1613 smmd=0.4734 ct=11.4647 rec=1.3289 | train/val/test=1.000/0.706/0.709 | c=0.998347
[Epoch 0055] loss=13.6409 cls=0.1665 smmd=0.5466 ct=11.5210 rec=1.3403 | train/val/test=1.000/0.732/0.732 | c=0.998347
[Epoch 0056] loss=13.8153 cls=0.1824 smmd=0.6233 ct=11.4978 rec=1.3361 | train/val/test=1.000/0.690/0.684 | c=0.998347
[Epoch 0057] loss=13.9968 cls=0.1803 smmd=0.6668 ct=11.5629 rec=1.3532 | train/val/test=1.000/0.726/0.736 | c=0.998347
[Epoch 0058] loss=13.7916 cls=0.1714 smmd=0.6125 ct=11.5090 rec=1.3311 | train/val/test=1.000/0.710/0.711 | c=0.998347
[Epoch 0059] loss=13.4676 cls=0.1253 smmd=0.4979 ct=11.4987 rec=1.3228 | train/val/test=1.000/0.730/0.718 | c=0.998347
[Epoch 0060] loss=13.2924 cls=0.1144 smmd=0.4396 ct=11.4806 rec=1.3112 | train/val/test=1.000/0.734/0.722 | c=0.998347
[Epoch 0061] loss=13.3011 cls=0.1142 smmd=0.4486 ct=11.4665 rec=1.3119 | train/val/test=1.000/0.734/0.728 | c=0.998347
[Epoch 0062] loss=13.2780 cls=0.1179 smmd=0.4379 ct=11.4662 rec=1.3166 | train/val/test=1.000/0.738/0.730 | c=0.998347
[Epoch 0063] loss=13.4153 cls=0.1344 smmd=0.4657 ct=11.5212 rec=1.3255 | train/val/test=1.000/0.736/0.734 | c=0.998347
[Epoch 0064] loss=13.7488 cls=0.1503 smmd=0.6092 ct=11.4848 rec=1.3314 | train/val/test=1.000/0.726/0.723 | c=0.998347
[Epoch 0065] loss=14.0431 cls=0.1564 smmd=0.7139 ct=11.5115 rec=1.3377 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0066] loss=13.7849 cls=0.1403 smmd=0.6226 ct=11.4950 rec=1.3263 | train/val/test=1.000/0.726/0.728 | c=0.998347
[Epoch 0067] loss=13.2972 cls=0.1220 smmd=0.4389 ct=11.4783 rec=1.3211 | train/val/test=1.000/0.730/0.718 | c=0.998347
[Epoch 0068] loss=13.2886 cls=0.1155 smmd=0.4465 ct=11.4580 rec=1.3133 | train/val/test=1.000/0.728/0.726 | c=0.998347
[Epoch 0069] loss=13.2354 cls=0.1056 smmd=0.4209 ct=11.4739 rec=1.3127 | train/val/test=1.000/0.730/0.719 | c=0.998347
[Epoch 0070] loss=13.1075 cls=0.1126 smmd=0.3740 ct=11.4591 rec=1.3140 | train/val/test=1.000/0.718/0.721 | c=0.998347
[Epoch 0071] loss=13.3565 cls=0.1217 smmd=0.4500 ct=11.5085 rec=1.3245 | train/val/test=1.000/0.736/0.740 | c=0.998347
[Epoch 0072] loss=13.6938 cls=0.1423 smmd=0.5897 ct=11.4844 rec=1.3282 | train/val/test=1.000/0.702/0.687 | c=0.998347
[Epoch 0073] loss=14.1502 cls=0.1599 smmd=0.7250 ct=11.5818 rec=1.3518 | train/val/test=1.000/0.726/0.732 | c=0.998347
[Epoch 0074] loss=14.0941 cls=0.1641 smmd=0.7287 ct=11.5200 rec=1.3404 | train/val/test=1.000/0.712/0.694 | c=0.998347
[Epoch 0075] loss=13.6466 cls=0.1029 smmd=0.5579 ct=11.5384 rec=1.3238 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0076] loss=13.3075 cls=0.0770 smmd=0.4559 ct=11.4803 rec=1.2978 | train/val/test=1.000/0.724/0.724 | c=0.998347
[Epoch 0077] loss=13.2901 cls=0.0742 smmd=0.4615 ct=11.4501 rec=1.2981 | train/val/test=1.000/0.734/0.725 | c=0.998347
[Epoch 0078] loss=13.1848 cls=0.0680 smmd=0.4131 ct=11.4685 rec=1.2989 | train/val/test=1.000/0.736/0.719 | c=0.998347
[Epoch 0079] loss=13.2904 cls=0.0837 smmd=0.4347 ct=11.5092 rec=1.3054 | train/val/test=1.000/0.744/0.731 | c=0.998347
[Epoch 0080] loss=13.5160 cls=0.1042 smmd=0.5262 ct=11.4883 rec=1.3204 | train/val/test=1.000/0.746/0.741 | c=0.998347
[Epoch 0081] loss=13.9759 cls=0.1339 smmd=0.6967 ct=11.5019 rec=1.3305 | train/val/test=1.000/0.652/0.636 | c=0.998347
[Epoch 0082] loss=14.3428 cls=0.2262 smmd=0.7650 ct=11.6289 rec=1.3766 | train/val/test=0.923/0.598/0.599 | c=0.998347
[Epoch 0083] loss=14.4972 cls=0.2777 smmd=0.8532 ct=11.5320 rec=1.3867 | train/val/test=1.000/0.678/0.659 | c=0.998347
[Epoch 0084] loss=13.8541 cls=0.1517 smmd=0.6046 ct=11.5939 rec=1.3458 | train/val/test=1.000/0.724/0.715 | c=0.998347
[Epoch 0085] loss=13.3002 cls=0.0532 smmd=0.4618 ct=11.4748 rec=1.2888 | train/val/test=1.000/0.698/0.705 | c=0.998347
[Epoch 0086] loss=13.6611 cls=0.1107 smmd=0.5897 ct=11.4652 rec=1.3325 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0087] loss=13.3116 cls=0.0457 smmd=0.4664 ct=11.4789 rec=1.2878 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0088] loss=13.4246 cls=0.0508 smmd=0.4920 ct=11.5225 rec=1.2933 | train/val/test=1.000/0.738/0.751 | c=0.998347
[Epoch 0089] loss=13.3679 cls=0.0782 smmd=0.4687 ct=11.5018 rec=1.3108 | train/val/test=1.000/0.718/0.719 | c=0.998347
[Epoch 0090] loss=13.9619 cls=0.0798 smmd=0.6955 ct=11.5233 rec=1.3198 | train/val/test=1.000/0.750/0.739 | c=0.998347
[Epoch 0091] loss=14.2770 cls=0.0927 smmd=0.8035 ct=11.5633 rec=1.3173 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0092] loss=14.0272 cls=0.1020 smmd=0.7026 ct=11.5531 rec=1.3334 | train/val/test=1.000/0.722/0.718 | c=0.998347
[Epoch 0093] loss=13.9601 cls=0.1311 smmd=0.6928 ct=11.5018 rec=1.3215 | train/val/test=1.000/0.718/0.701 | c=0.998347
[Epoch 0094] loss=13.4974 cls=0.1014 smmd=0.4981 ct=11.5392 rec=1.3244 | train/val/test=1.000/0.730/0.714 | c=0.998347
[Epoch 0095] loss=13.2194 cls=0.0770 smmd=0.4236 ct=11.4729 rec=1.2980 | train/val/test=1.000/0.736/0.728 | c=0.998347
[Epoch 0096] loss=13.3383 cls=0.0814 smmd=0.4872 ct=11.4297 rec=1.2996 | train/val/test=1.000/0.732/0.723 | c=0.998347
[Epoch 0097] loss=13.1565 cls=0.0809 smmd=0.3985 ct=11.4662 rec=1.3072 | train/val/test=1.000/0.740/0.724 | c=0.998347
[Epoch 0098] loss=13.3446 cls=0.0869 smmd=0.4528 ct=11.5148 rec=1.3083 | train/val/test=1.000/0.718/0.728 | c=0.998347
[Epoch 0099] loss=13.5493 cls=0.1031 smmd=0.5446 ct=11.4760 rec=1.3207 | train/val/test=1.000/0.704/0.698 | c=0.998347
=== Best @ epoch 90: val=0.7500, test=0.7390 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 completed in 190.14 seconds.
==================================================
