Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 - 2025-09-21 06:33:57:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.2499 cls=1.9495 smmd=4.2606 ct=9.2621 rec=1.3889 | train/val/test=0.172/0.072/0.091 | c=0.998347
[Epoch 0001] loss=16.7053 cls=1.8890 smmd=2.8442 ct=9.1935 rec=1.3893 | train/val/test=0.655/0.374/0.435 | c=0.998347
[Epoch 0002] loss=15.0977 cls=1.7546 smmd=1.4854 ct=9.0800 rec=1.3888 | train/val/test=0.931/0.578/0.558 | c=0.998347
[Epoch 0003] loss=14.7417 cls=1.4837 smmd=1.4798 ct=9.0040 rec=1.3871 | train/val/test=1.000/0.520/0.523 | c=0.998347
[Epoch 0004] loss=15.2723 cls=1.1034 smmd=1.7801 ct=9.6278 rec=1.3805 | train/val/test=0.931/0.598/0.585 | c=0.998347
[Epoch 0005] loss=14.5585 cls=0.7475 smmd=1.7265 ct=9.3593 rec=1.3626 | train/val/test=0.966/0.624/0.617 | c=0.998347
[Epoch 0006] loss=13.9562 cls=0.4723 smmd=1.5379 ct=9.2706 rec=1.3377 | train/val/test=1.000/0.640/0.619 | c=0.998347
[Epoch 0007] loss=13.4214 cls=0.2661 smmd=1.2822 ct=9.2519 rec=1.3107 | train/val/test=1.000/0.582/0.578 | c=0.998347
[Epoch 0008] loss=13.2136 cls=0.1414 smmd=1.2012 ct=9.3050 rec=1.2830 | train/val/test=1.000/0.604/0.587 | c=0.998347
[Epoch 0009] loss=13.1187 cls=0.0735 smmd=1.1676 ct=9.3568 rec=1.2604 | train/val/test=1.000/0.642/0.617 | c=0.998347
[Epoch 0010] loss=13.0815 cls=0.0349 smmd=1.1646 ct=9.4006 rec=1.2407 | train/val/test=1.000/0.652/0.621 | c=0.998347
[Epoch 0011] loss=12.9256 cls=0.0172 smmd=1.0457 ct=9.4116 rec=1.2256 | train/val/test=1.000/0.642/0.619 | c=0.998347
[Epoch 0012] loss=12.7003 cls=0.0092 smmd=0.8711 ct=9.3888 rec=1.2156 | train/val/test=1.000/0.624/0.611 | c=0.998347
[Epoch 0013] loss=12.5615 cls=0.0060 smmd=0.7734 ct=9.3635 rec=1.2093 | train/val/test=1.000/0.628/0.619 | c=0.998347
[Epoch 0014] loss=12.4949 cls=0.0049 smmd=0.7337 ct=9.3471 rec=1.2046 | train/val/test=1.000/0.626/0.626 | c=0.998347
[Epoch 0015] loss=12.5062 cls=0.0045 smmd=0.7557 ct=9.3450 rec=1.2005 | train/val/test=1.000/0.630/0.629 | c=0.998347
[Epoch 0016] loss=12.4392 cls=0.0043 smmd=0.6879 ct=9.3524 rec=1.1973 | train/val/test=1.000/0.630/0.620 | c=0.998347
[Epoch 0017] loss=12.3345 cls=0.0042 smmd=0.5777 ct=9.3597 rec=1.1965 | train/val/test=1.000/0.628/0.619 | c=0.998347
[Epoch 0018] loss=12.2126 cls=0.0046 smmd=0.4532 ct=9.3614 rec=1.1967 | train/val/test=1.000/0.634/0.628 | c=0.998347
[Epoch 0019] loss=12.1909 cls=0.0057 smmd=0.4312 ct=9.3613 rec=1.1964 | train/val/test=1.000/0.646/0.642 | c=0.998347
[Epoch 0020] loss=12.1709 cls=0.0073 smmd=0.4086 ct=9.3622 rec=1.1964 | train/val/test=1.000/0.646/0.646 | c=0.998347
[Epoch 0021] loss=12.1272 cls=0.0099 smmd=0.3638 ct=9.3589 rec=1.1972 | train/val/test=1.000/0.648/0.640 | c=0.998347
[Epoch 0022] loss=12.0661 cls=0.0140 smmd=0.2963 ct=9.3599 rec=1.1980 | train/val/test=1.000/0.658/0.649 | c=0.998347
[Epoch 0023] loss=12.0177 cls=0.0182 smmd=0.2385 ct=9.3677 rec=1.1966 | train/val/test=1.000/0.668/0.661 | c=0.998347
[Epoch 0024] loss=12.0200 cls=0.0229 smmd=0.2388 ct=9.3696 rec=1.1943 | train/val/test=1.000/0.668/0.666 | c=0.998347
[Epoch 0025] loss=11.9854 cls=0.0280 smmd=0.2085 ct=9.3668 rec=1.1911 | train/val/test=1.000/0.674/0.668 | c=0.998347
[Epoch 0026] loss=11.9469 cls=0.0317 smmd=0.1812 ct=9.3618 rec=1.1861 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0027] loss=11.9534 cls=0.0374 smmd=0.1713 ct=9.3767 rec=1.1840 | train/val/test=1.000/0.684/0.654 | c=0.998347
[Epoch 0028] loss=11.9807 cls=0.0474 smmd=0.1995 ct=9.3676 rec=1.1831 | train/val/test=1.000/0.684/0.694 | c=0.998347
[Epoch 0029] loss=11.9850 cls=0.0463 smmd=0.1805 ct=9.3941 rec=1.1820 | train/val/test=1.000/0.686/0.667 | c=0.998347
[Epoch 0030] loss=11.8540 cls=0.0197 smmd=0.1463 ct=9.3536 rec=1.1672 | train/val/test=1.000/0.680/0.670 | c=0.998347
[Epoch 0031] loss=11.8406 cls=0.0180 smmd=0.1367 ct=9.3542 rec=1.1659 | train/val/test=1.000/0.684/0.693 | c=0.998347
[Epoch 0032] loss=11.8482 cls=0.0124 smmd=0.1308 ct=9.3741 rec=1.1655 | train/val/test=1.000/0.672/0.685 | c=0.998347
[Epoch 0033] loss=11.7948 cls=0.0079 smmd=0.1093 ct=9.3560 rec=1.1608 | train/val/test=1.000/0.684/0.668 | c=0.998347
[Epoch 0034] loss=11.8087 cls=0.0102 smmd=0.1263 ct=9.3446 rec=1.1638 | train/val/test=1.000/0.674/0.684 | c=0.998347
[Epoch 0035] loss=11.7587 cls=0.0064 smmd=0.0903 ct=9.3409 rec=1.1606 | train/val/test=1.000/0.668/0.688 | c=0.998347
[Epoch 0036] loss=11.7735 cls=0.0072 smmd=0.0837 ct=9.3553 rec=1.1637 | train/val/test=1.000/0.676/0.669 | c=0.998347
[Epoch 0037] loss=11.7522 cls=0.0071 smmd=0.0733 ct=9.3442 rec=1.1638 | train/val/test=1.000/0.680/0.673 | c=0.998347
[Epoch 0038] loss=11.7338 cls=0.0080 smmd=0.0525 ct=9.3417 rec=1.1658 | train/val/test=1.000/0.666/0.689 | c=0.998347
[Epoch 0039] loss=11.7482 cls=0.0096 smmd=0.0515 ct=9.3492 rec=1.1689 | train/val/test=1.000/0.676/0.672 | c=0.998347
[Epoch 0040] loss=11.7280 cls=0.0098 smmd=0.0469 ct=9.3352 rec=1.1680 | train/val/test=1.000/0.678/0.677 | c=0.998347
[Epoch 0041] loss=11.7361 cls=0.0111 smmd=0.0524 ct=9.3343 rec=1.1691 | train/val/test=1.000/0.672/0.687 | c=0.998347
[Epoch 0042] loss=11.7410 cls=0.0149 smmd=0.0375 ct=9.3464 rec=1.1711 | train/val/test=1.000/0.678/0.672 | c=0.998347
[Epoch 0043] loss=11.7205 cls=0.0137 smmd=0.0333 ct=9.3383 rec=1.1676 | train/val/test=1.000/0.674/0.680 | c=0.998347
[Epoch 0044] loss=11.7045 cls=0.0138 smmd=0.0306 ct=9.3306 rec=1.1648 | train/val/test=1.000/0.678/0.686 | c=0.998347
[Epoch 0045] loss=11.7032 cls=0.0154 smmd=0.0258 ct=9.3331 rec=1.1645 | train/val/test=1.000/0.680/0.676 | c=0.998347
[Epoch 0046] loss=11.7008 cls=0.0156 smmd=0.0317 ct=9.3277 rec=1.1629 | train/val/test=1.000/0.674/0.685 | c=0.998347
[Epoch 0047] loss=11.6787 cls=0.0147 smmd=0.0146 ct=9.3267 rec=1.1614 | train/val/test=1.000/0.678/0.681 | c=0.998347
[Epoch 0048] loss=11.6774 cls=0.0148 smmd=0.0148 ct=9.3252 rec=1.1613 | train/val/test=1.000/0.676/0.681 | c=0.998347
[Epoch 0049] loss=11.6706 cls=0.0148 smmd=0.0138 ct=9.3187 rec=1.1616 | train/val/test=1.000/0.676/0.686 | c=0.998347
[Epoch 0050] loss=11.6642 cls=0.0135 smmd=0.0081 ct=9.3197 rec=1.1614 | train/val/test=1.000/0.676/0.688 | c=0.998347
[Epoch 0051] loss=11.6598 cls=0.0133 smmd=0.0027 ct=9.3194 rec=1.1622 | train/val/test=1.000/0.676/0.680 | c=0.998347
[Epoch 0052] loss=11.6629 cls=0.0133 smmd=0.0090 ct=9.3134 rec=1.1636 | train/val/test=1.000/0.670/0.686 | c=0.998347
[Epoch 0053] loss=11.6553 cls=0.0132 smmd=0.0015 ct=9.3124 rec=1.1641 | train/val/test=1.000/0.674/0.684 | c=0.998347
[Epoch 0054] loss=11.6551 cls=0.0128 smmd=0.0036 ct=9.3094 rec=1.1646 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0055] loss=11.6513 cls=0.0131 smmd=-0.0010 ct=9.3083 rec=1.1654 | train/val/test=1.000/0.666/0.682 | c=0.998347
[Epoch 0056] loss=11.6466 cls=0.0134 smmd=-0.0093 ct=9.3110 rec=1.1657 | train/val/test=1.000/0.672/0.678 | c=0.998347
[Epoch 0057] loss=11.6437 cls=0.0133 smmd=-0.0059 ct=9.3050 rec=1.1657 | train/val/test=1.000/0.670/0.680 | c=0.998347
[Epoch 0058] loss=11.6382 cls=0.0132 smmd=-0.0087 ct=9.3034 rec=1.1652 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0059] loss=11.6368 cls=0.0133 smmd=-0.0129 ct=9.3062 rec=1.1652 | train/val/test=1.000/0.672/0.677 | c=0.998347
[Epoch 0060] loss=11.6359 cls=0.0136 smmd=-0.0072 ct=9.2994 rec=1.1650 | train/val/test=1.000/0.670/0.685 | c=0.998347
[Epoch 0061] loss=11.6328 cls=0.0134 smmd=-0.0129 ct=9.3022 rec=1.1650 | train/val/test=1.000/0.672/0.681 | c=0.998347
[Epoch 0062] loss=11.6293 cls=0.0139 smmd=-0.0131 ct=9.2989 rec=1.1648 | train/val/test=1.000/0.676/0.685 | c=0.998347
[Epoch 0063] loss=11.6291 cls=0.0136 smmd=-0.0148 ct=9.3004 rec=1.1649 | train/val/test=1.000/0.670/0.679 | c=0.998347
[Epoch 0064] loss=11.6278 cls=0.0140 smmd=-0.0112 ct=9.2958 rec=1.1646 | train/val/test=1.000/0.670/0.683 | c=0.998347
[Epoch 0065] loss=11.6245 cls=0.0136 smmd=-0.0122 ct=9.2933 rec=1.1649 | train/val/test=1.000/0.670/0.681 | c=0.998347
[Epoch 0066] loss=11.6179 cls=0.0139 smmd=-0.0176 ct=9.2921 rec=1.1648 | train/val/test=1.000/0.668/0.679 | c=0.998347
[Epoch 0067] loss=11.6173 cls=0.0137 smmd=-0.0170 ct=9.2904 rec=1.1651 | train/val/test=1.000/0.672/0.682 | c=0.998347
[Epoch 0068] loss=11.6132 cls=0.0137 smmd=-0.0232 ct=9.2917 rec=1.1655 | train/val/test=1.000/0.666/0.679 | c=0.998347
[Epoch 0069] loss=11.6132 cls=0.0141 smmd=-0.0201 ct=9.2878 rec=1.1657 | train/val/test=1.000/0.668/0.680 | c=0.998347
[Epoch 0070] loss=11.6169 cls=0.0138 smmd=-0.0158 ct=9.2860 rec=1.1665 | train/val/test=1.000/0.668/0.676 | c=0.998347
[Epoch 0071] loss=11.6171 cls=0.0146 smmd=-0.0131 ct=9.2828 rec=1.1664 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0072] loss=11.6142 cls=0.0139 smmd=-0.0211 ct=9.2876 rec=1.1669 | train/val/test=1.000/0.666/0.676 | c=0.998347
[Epoch 0073] loss=11.6165 cls=0.0148 smmd=-0.0167 ct=9.2852 rec=1.1666 | train/val/test=1.000/0.676/0.683 | c=0.998347
[Epoch 0074] loss=11.6124 cls=0.0142 smmd=-0.0193 ct=9.2840 rec=1.1668 | train/val/test=1.000/0.664/0.677 | c=0.998347
[Epoch 0075] loss=11.6205 cls=0.0149 smmd=-0.0094 ct=9.2816 rec=1.1667 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0076] loss=11.6167 cls=0.0142 smmd=-0.0137 ct=9.2827 rec=1.1667 | train/val/test=1.000/0.664/0.677 | c=0.998347
[Epoch 0077] loss=11.6131 cls=0.0143 smmd=-0.0149 ct=9.2818 rec=1.1660 | train/val/test=1.000/0.676/0.684 | c=0.998347
[Epoch 0078] loss=11.6050 cls=0.0131 smmd=-0.0175 ct=9.2790 rec=1.1652 | train/val/test=1.000/0.668/0.678 | c=0.998347
[Epoch 0079] loss=11.5953 cls=0.0131 smmd=-0.0212 ct=9.2746 rec=1.1644 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0080] loss=11.5918 cls=0.0129 smmd=-0.0254 ct=9.2753 rec=1.1645 | train/val/test=1.000/0.668/0.680 | c=0.998347
[Epoch 0081] loss=11.5940 cls=0.0132 smmd=-0.0230 ct=9.2736 rec=1.1651 | train/val/test=1.000/0.664/0.679 | c=0.998347
[Epoch 0082] loss=11.5960 cls=0.0139 smmd=-0.0211 ct=9.2718 rec=1.1657 | train/val/test=1.000/0.676/0.677 | c=0.998347
[Epoch 0083] loss=11.5958 cls=0.0140 smmd=-0.0268 ct=9.2745 rec=1.1671 | train/val/test=1.000/0.670/0.682 | c=0.998347
[Epoch 0084] loss=11.6012 cls=0.0152 smmd=-0.0212 ct=9.2722 rec=1.1675 | train/val/test=1.000/0.670/0.677 | c=0.998347
[Epoch 0085] loss=11.6057 cls=0.0150 smmd=-0.0178 ct=9.2724 rec=1.1681 | train/val/test=1.000/0.664/0.685 | c=0.998347
[Epoch 0086] loss=11.6152 cls=0.0163 smmd=-0.0165 ct=9.2771 rec=1.1691 | train/val/test=1.000/0.670/0.659 | c=0.998347
[Epoch 0087] loss=11.6471 cls=0.0200 smmd=0.0044 ct=9.2773 rec=1.1727 | train/val/test=1.000/0.674/0.702 | c=0.998347
[Epoch 0088] loss=11.7301 cls=0.0282 smmd=0.0208 ct=9.3134 rec=1.1839 | train/val/test=1.000/0.652/0.632 | c=0.998347
[Epoch 0089] loss=11.8282 cls=0.0402 smmd=0.0970 ct=9.3102 rec=1.1904 | train/val/test=1.000/0.676/0.695 | c=0.998347
[Epoch 0090] loss=11.7400 cls=0.0186 smmd=0.0438 ct=9.3257 rec=1.1759 | train/val/test=1.000/0.672/0.691 | c=0.998347
[Epoch 0091] loss=11.5998 cls=0.0062 smmd=0.0017 ct=9.2806 rec=1.1557 | train/val/test=1.000/0.672/0.662 | c=0.998347
[Epoch 0092] loss=11.7039 cls=0.0081 smmd=0.0745 ct=9.2936 rec=1.1638 | train/val/test=1.000/0.666/0.685 | c=0.998347
[Epoch 0093] loss=11.6048 cls=0.0047 smmd=0.0007 ct=9.2907 rec=1.1544 | train/val/test=1.000/0.680/0.703 | c=0.998347
[Epoch 0094] loss=11.6692 cls=0.0090 smmd=0.0307 ct=9.3057 rec=1.1619 | train/val/test=1.000/0.668/0.675 | c=0.998347
[Epoch 0095] loss=11.6211 cls=0.0058 smmd=0.0211 ct=9.2764 rec=1.1590 | train/val/test=1.000/0.660/0.659 | c=0.998347
[Epoch 0096] loss=11.6296 cls=0.0078 smmd=0.0079 ct=9.2842 rec=1.1648 | train/val/test=1.000/0.672/0.697 | c=0.998347
[Epoch 0097] loss=11.6600 cls=0.0130 smmd=0.0090 ct=9.2968 rec=1.1706 | train/val/test=1.000/0.662/0.676 | c=0.998347
[Epoch 0098] loss=11.6003 cls=0.0115 smmd=-0.0172 ct=9.2669 rec=1.1696 | train/val/test=1.000/0.654/0.657 | c=0.998347
[Epoch 0099] loss=11.6427 cls=0.0177 smmd=-0.0059 ct=9.2752 rec=1.1779 | train/val/test=1.000/0.664/0.691 | c=0.998347
=== Best @ epoch 29: val=0.6860, test=0.6670 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 - 2025-09-21 06:33:57:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.2499 cls=1.9495 smmd=4.2606 ct=9.2621 rec=1.3889 | train/val/test=0.172/0.072/0.091 | c=0.998347
[Epoch 0001] loss=16.7053 cls=1.8890 smmd=2.8442 ct=9.1935 rec=1.3893 | train/val/test=0.655/0.374/0.435 | c=0.998347
[Epoch 0002] loss=15.0977 cls=1.7546 smmd=1.4854 ct=9.0800 rec=1.3888 | train/val/test=0.931/0.578/0.558 | c=0.998347
[Epoch 0003] loss=14.7417 cls=1.4837 smmd=1.4798 ct=9.0040 rec=1.3871 | train/val/test=1.000/0.520/0.523 | c=0.998347
[Epoch 0004] loss=15.2723 cls=1.1034 smmd=1.7801 ct=9.6278 rec=1.3805 | train/val/test=0.931/0.598/0.585 | c=0.998347
[Epoch 0005] loss=14.5585 cls=0.7475 smmd=1.7265 ct=9.3593 rec=1.3626 | train/val/test=0.966/0.624/0.617 | c=0.998347
[Epoch 0006] loss=13.9562 cls=0.4723 smmd=1.5379 ct=9.2706 rec=1.3377 | train/val/test=1.000/0.640/0.619 | c=0.998347
[Epoch 0007] loss=13.4214 cls=0.2661 smmd=1.2822 ct=9.2519 rec=1.3107 | train/val/test=1.000/0.582/0.578 | c=0.998347
[Epoch 0008] loss=13.2136 cls=0.1414 smmd=1.2012 ct=9.3050 rec=1.2830 | train/val/test=1.000/0.604/0.587 | c=0.998347
[Epoch 0009] loss=13.1187 cls=0.0735 smmd=1.1676 ct=9.3568 rec=1.2604 | train/val/test=1.000/0.642/0.617 | c=0.998347
[Epoch 0010] loss=13.0815 cls=0.0349 smmd=1.1646 ct=9.4006 rec=1.2407 | train/val/test=1.000/0.652/0.621 | c=0.998347
[Epoch 0011] loss=12.9256 cls=0.0172 smmd=1.0457 ct=9.4116 rec=1.2256 | train/val/test=1.000/0.642/0.619 | c=0.998347
[Epoch 0012] loss=12.7003 cls=0.0092 smmd=0.8711 ct=9.3888 rec=1.2156 | train/val/test=1.000/0.624/0.611 | c=0.998347
[Epoch 0013] loss=12.5615 cls=0.0060 smmd=0.7734 ct=9.3635 rec=1.2093 | train/val/test=1.000/0.628/0.619 | c=0.998347
[Epoch 0014] loss=12.4949 cls=0.0049 smmd=0.7337 ct=9.3471 rec=1.2046 | train/val/test=1.000/0.626/0.626 | c=0.998347
[Epoch 0015] loss=12.5062 cls=0.0045 smmd=0.7557 ct=9.3450 rec=1.2005 | train/val/test=1.000/0.630/0.629 | c=0.998347
[Epoch 0016] loss=12.4392 cls=0.0043 smmd=0.6879 ct=9.3524 rec=1.1973 | train/val/test=1.000/0.630/0.620 | c=0.998347
[Epoch 0017] loss=12.3345 cls=0.0042 smmd=0.5777 ct=9.3597 rec=1.1965 | train/val/test=1.000/0.628/0.619 | c=0.998347
[Epoch 0018] loss=12.2126 cls=0.0046 smmd=0.4532 ct=9.3614 rec=1.1967 | train/val/test=1.000/0.634/0.628 | c=0.998347
[Epoch 0019] loss=12.1909 cls=0.0057 smmd=0.4312 ct=9.3613 rec=1.1964 | train/val/test=1.000/0.646/0.642 | c=0.998347
[Epoch 0020] loss=12.1709 cls=0.0073 smmd=0.4086 ct=9.3622 rec=1.1964 | train/val/test=1.000/0.646/0.646 | c=0.998347
[Epoch 0021] loss=12.1272 cls=0.0099 smmd=0.3638 ct=9.3589 rec=1.1972 | train/val/test=1.000/0.648/0.640 | c=0.998347
[Epoch 0022] loss=12.0661 cls=0.0140 smmd=0.2963 ct=9.3599 rec=1.1980 | train/val/test=1.000/0.658/0.649 | c=0.998347
[Epoch 0023] loss=12.0177 cls=0.0182 smmd=0.2385 ct=9.3677 rec=1.1966 | train/val/test=1.000/0.668/0.661 | c=0.998347
[Epoch 0024] loss=12.0200 cls=0.0229 smmd=0.2388 ct=9.3696 rec=1.1943 | train/val/test=1.000/0.668/0.666 | c=0.998347
[Epoch 0025] loss=11.9854 cls=0.0280 smmd=0.2085 ct=9.3668 rec=1.1911 | train/val/test=1.000/0.674/0.668 | c=0.998347
[Epoch 0026] loss=11.9469 cls=0.0317 smmd=0.1812 ct=9.3618 rec=1.1861 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0027] loss=11.9534 cls=0.0374 smmd=0.1713 ct=9.3767 rec=1.1840 | train/val/test=1.000/0.684/0.654 | c=0.998347
[Epoch 0028] loss=11.9807 cls=0.0474 smmd=0.1995 ct=9.3676 rec=1.1831 | train/val/test=1.000/0.684/0.694 | c=0.998347
[Epoch 0029] loss=11.9850 cls=0.0463 smmd=0.1805 ct=9.3941 rec=1.1820 | train/val/test=1.000/0.686/0.667 | c=0.998347
[Epoch 0030] loss=11.8540 cls=0.0197 smmd=0.1463 ct=9.3536 rec=1.1672 | train/val/test=1.000/0.680/0.670 | c=0.998347
[Epoch 0031] loss=11.8406 cls=0.0180 smmd=0.1367 ct=9.3542 rec=1.1659 | train/val/test=1.000/0.684/0.693 | c=0.998347
[Epoch 0032] loss=11.8482 cls=0.0124 smmd=0.1308 ct=9.3741 rec=1.1655 | train/val/test=1.000/0.672/0.685 | c=0.998347
[Epoch 0033] loss=11.7948 cls=0.0079 smmd=0.1093 ct=9.3560 rec=1.1608 | train/val/test=1.000/0.684/0.668 | c=0.998347
[Epoch 0034] loss=11.8087 cls=0.0102 smmd=0.1263 ct=9.3446 rec=1.1638 | train/val/test=1.000/0.674/0.684 | c=0.998347
[Epoch 0035] loss=11.7587 cls=0.0064 smmd=0.0903 ct=9.3409 rec=1.1606 | train/val/test=1.000/0.668/0.688 | c=0.998347
[Epoch 0036] loss=11.7735 cls=0.0072 smmd=0.0837 ct=9.3553 rec=1.1637 | train/val/test=1.000/0.676/0.669 | c=0.998347
[Epoch 0037] loss=11.7522 cls=0.0071 smmd=0.0733 ct=9.3442 rec=1.1638 | train/val/test=1.000/0.680/0.673 | c=0.998347
[Epoch 0038] loss=11.7338 cls=0.0080 smmd=0.0525 ct=9.3417 rec=1.1658 | train/val/test=1.000/0.666/0.689 | c=0.998347
[Epoch 0039] loss=11.7482 cls=0.0096 smmd=0.0515 ct=9.3492 rec=1.1689 | train/val/test=1.000/0.676/0.672 | c=0.998347
[Epoch 0040] loss=11.7280 cls=0.0098 smmd=0.0469 ct=9.3352 rec=1.1680 | train/val/test=1.000/0.678/0.677 | c=0.998347
[Epoch 0041] loss=11.7361 cls=0.0111 smmd=0.0524 ct=9.3343 rec=1.1691 | train/val/test=1.000/0.672/0.687 | c=0.998347
[Epoch 0042] loss=11.7410 cls=0.0149 smmd=0.0375 ct=9.3464 rec=1.1711 | train/val/test=1.000/0.678/0.672 | c=0.998347
[Epoch 0043] loss=11.7205 cls=0.0137 smmd=0.0333 ct=9.3383 rec=1.1676 | train/val/test=1.000/0.674/0.680 | c=0.998347
[Epoch 0044] loss=11.7045 cls=0.0138 smmd=0.0306 ct=9.3306 rec=1.1648 | train/val/test=1.000/0.678/0.686 | c=0.998347
[Epoch 0045] loss=11.7032 cls=0.0154 smmd=0.0258 ct=9.3331 rec=1.1645 | train/val/test=1.000/0.680/0.676 | c=0.998347
[Epoch 0046] loss=11.7008 cls=0.0156 smmd=0.0317 ct=9.3277 rec=1.1629 | train/val/test=1.000/0.674/0.685 | c=0.998347
[Epoch 0047] loss=11.6787 cls=0.0147 smmd=0.0146 ct=9.3267 rec=1.1614 | train/val/test=1.000/0.678/0.681 | c=0.998347
[Epoch 0048] loss=11.6774 cls=0.0148 smmd=0.0148 ct=9.3252 rec=1.1613 | train/val/test=1.000/0.676/0.681 | c=0.998347
[Epoch 0049] loss=11.6706 cls=0.0148 smmd=0.0138 ct=9.3187 rec=1.1616 | train/val/test=1.000/0.676/0.686 | c=0.998347
[Epoch 0050] loss=11.6642 cls=0.0135 smmd=0.0081 ct=9.3197 rec=1.1614 | train/val/test=1.000/0.676/0.688 | c=0.998347
[Epoch 0051] loss=11.6598 cls=0.0133 smmd=0.0027 ct=9.3194 rec=1.1622 | train/val/test=1.000/0.676/0.680 | c=0.998347
[Epoch 0052] loss=11.6629 cls=0.0133 smmd=0.0090 ct=9.3134 rec=1.1636 | train/val/test=1.000/0.670/0.686 | c=0.998347
[Epoch 0053] loss=11.6553 cls=0.0132 smmd=0.0015 ct=9.3124 rec=1.1641 | train/val/test=1.000/0.674/0.684 | c=0.998347
[Epoch 0054] loss=11.6551 cls=0.0128 smmd=0.0036 ct=9.3094 rec=1.1646 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0055] loss=11.6513 cls=0.0131 smmd=-0.0010 ct=9.3083 rec=1.1654 | train/val/test=1.000/0.666/0.682 | c=0.998347
[Epoch 0056] loss=11.6466 cls=0.0134 smmd=-0.0093 ct=9.3110 rec=1.1657 | train/val/test=1.000/0.672/0.678 | c=0.998347
[Epoch 0057] loss=11.6437 cls=0.0133 smmd=-0.0059 ct=9.3050 rec=1.1657 | train/val/test=1.000/0.670/0.680 | c=0.998347
[Epoch 0058] loss=11.6382 cls=0.0132 smmd=-0.0087 ct=9.3034 rec=1.1652 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0059] loss=11.6368 cls=0.0133 smmd=-0.0129 ct=9.3062 rec=1.1652 | train/val/test=1.000/0.672/0.677 | c=0.998347
[Epoch 0060] loss=11.6359 cls=0.0136 smmd=-0.0072 ct=9.2994 rec=1.1650 | train/val/test=1.000/0.670/0.685 | c=0.998347
[Epoch 0061] loss=11.6328 cls=0.0134 smmd=-0.0129 ct=9.3022 rec=1.1650 | train/val/test=1.000/0.672/0.681 | c=0.998347
[Epoch 0062] loss=11.6293 cls=0.0139 smmd=-0.0131 ct=9.2989 rec=1.1648 | train/val/test=1.000/0.676/0.685 | c=0.998347
[Epoch 0063] loss=11.6291 cls=0.0136 smmd=-0.0148 ct=9.3004 rec=1.1649 | train/val/test=1.000/0.670/0.679 | c=0.998347
[Epoch 0064] loss=11.6278 cls=0.0140 smmd=-0.0112 ct=9.2958 rec=1.1646 | train/val/test=1.000/0.670/0.683 | c=0.998347
[Epoch 0065] loss=11.6245 cls=0.0136 smmd=-0.0122 ct=9.2933 rec=1.1649 | train/val/test=1.000/0.670/0.681 | c=0.998347
[Epoch 0066] loss=11.6179 cls=0.0139 smmd=-0.0176 ct=9.2921 rec=1.1648 | train/val/test=1.000/0.668/0.679 | c=0.998347
[Epoch 0067] loss=11.6173 cls=0.0137 smmd=-0.0170 ct=9.2904 rec=1.1651 | train/val/test=1.000/0.672/0.682 | c=0.998347
[Epoch 0068] loss=11.6132 cls=0.0137 smmd=-0.0232 ct=9.2917 rec=1.1655 | train/val/test=1.000/0.666/0.679 | c=0.998347
[Epoch 0069] loss=11.6132 cls=0.0141 smmd=-0.0201 ct=9.2878 rec=1.1657 | train/val/test=1.000/0.668/0.680 | c=0.998347
[Epoch 0070] loss=11.6169 cls=0.0138 smmd=-0.0158 ct=9.2860 rec=1.1665 | train/val/test=1.000/0.668/0.676 | c=0.998347
[Epoch 0071] loss=11.6171 cls=0.0146 smmd=-0.0131 ct=9.2828 rec=1.1664 | train/val/test=1.000/0.668/0.681 | c=0.998347
[Epoch 0072] loss=11.6142 cls=0.0139 smmd=-0.0211 ct=9.2876 rec=1.1669 | train/val/test=1.000/0.666/0.676 | c=0.998347
[Epoch 0073] loss=11.6165 cls=0.0148 smmd=-0.0167 ct=9.2852 rec=1.1666 | train/val/test=1.000/0.676/0.683 | c=0.998347
[Epoch 0074] loss=11.6124 cls=0.0142 smmd=-0.0193 ct=9.2840 rec=1.1668 | train/val/test=1.000/0.664/0.677 | c=0.998347
[Epoch 0075] loss=11.6205 cls=0.0149 smmd=-0.0094 ct=9.2816 rec=1.1667 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0076] loss=11.6167 cls=0.0142 smmd=-0.0137 ct=9.2827 rec=1.1667 | train/val/test=1.000/0.664/0.677 | c=0.998347
[Epoch 0077] loss=11.6131 cls=0.0143 smmd=-0.0149 ct=9.2818 rec=1.1660 | train/val/test=1.000/0.676/0.684 | c=0.998347
[Epoch 0078] loss=11.6050 cls=0.0131 smmd=-0.0175 ct=9.2790 rec=1.1652 | train/val/test=1.000/0.668/0.678 | c=0.998347
[Epoch 0079] loss=11.5953 cls=0.0131 smmd=-0.0212 ct=9.2746 rec=1.1644 | train/val/test=1.000/0.674/0.681 | c=0.998347
[Epoch 0080] loss=11.5918 cls=0.0129 smmd=-0.0254 ct=9.2753 rec=1.1645 | train/val/test=1.000/0.668/0.680 | c=0.998347
[Epoch 0081] loss=11.5940 cls=0.0132 smmd=-0.0230 ct=9.2736 rec=1.1651 | train/val/test=1.000/0.664/0.679 | c=0.998347
[Epoch 0082] loss=11.5960 cls=0.0139 smmd=-0.0211 ct=9.2718 rec=1.1657 | train/val/test=1.000/0.676/0.677 | c=0.998347
[Epoch 0083] loss=11.5958 cls=0.0140 smmd=-0.0268 ct=9.2745 rec=1.1671 | train/val/test=1.000/0.670/0.682 | c=0.998347
[Epoch 0084] loss=11.6012 cls=0.0152 smmd=-0.0212 ct=9.2722 rec=1.1675 | train/val/test=1.000/0.670/0.677 | c=0.998347
[Epoch 0085] loss=11.6057 cls=0.0150 smmd=-0.0178 ct=9.2724 rec=1.1681 | train/val/test=1.000/0.664/0.685 | c=0.998347
[Epoch 0086] loss=11.6152 cls=0.0163 smmd=-0.0165 ct=9.2771 rec=1.1691 | train/val/test=1.000/0.670/0.659 | c=0.998347
[Epoch 0087] loss=11.6471 cls=0.0200 smmd=0.0044 ct=9.2773 rec=1.1727 | train/val/test=1.000/0.674/0.702 | c=0.998347
[Epoch 0088] loss=11.7301 cls=0.0282 smmd=0.0208 ct=9.3134 rec=1.1839 | train/val/test=1.000/0.652/0.632 | c=0.998347
[Epoch 0089] loss=11.8282 cls=0.0402 smmd=0.0970 ct=9.3102 rec=1.1904 | train/val/test=1.000/0.676/0.695 | c=0.998347
[Epoch 0090] loss=11.7400 cls=0.0186 smmd=0.0438 ct=9.3257 rec=1.1759 | train/val/test=1.000/0.672/0.691 | c=0.998347
[Epoch 0091] loss=11.5998 cls=0.0062 smmd=0.0017 ct=9.2806 rec=1.1557 | train/val/test=1.000/0.672/0.662 | c=0.998347
[Epoch 0092] loss=11.7039 cls=0.0081 smmd=0.0745 ct=9.2936 rec=1.1638 | train/val/test=1.000/0.666/0.685 | c=0.998347
[Epoch 0093] loss=11.6048 cls=0.0047 smmd=0.0007 ct=9.2907 rec=1.1544 | train/val/test=1.000/0.680/0.703 | c=0.998347
[Epoch 0094] loss=11.6692 cls=0.0090 smmd=0.0307 ct=9.3057 rec=1.1619 | train/val/test=1.000/0.668/0.675 | c=0.998347
[Epoch 0095] loss=11.6211 cls=0.0058 smmd=0.0211 ct=9.2764 rec=1.1590 | train/val/test=1.000/0.660/0.659 | c=0.998347
[Epoch 0096] loss=11.6296 cls=0.0078 smmd=0.0079 ct=9.2842 rec=1.1648 | train/val/test=1.000/0.672/0.697 | c=0.998347
[Epoch 0097] loss=11.6600 cls=0.0130 smmd=0.0090 ct=9.2968 rec=1.1706 | train/val/test=1.000/0.662/0.676 | c=0.998347
[Epoch 0098] loss=11.6003 cls=0.0115 smmd=-0.0172 ct=9.2669 rec=1.1696 | train/val/test=1.000/0.654/0.657 | c=0.998347
[Epoch 0099] loss=11.6427 cls=0.0177 smmd=-0.0059 ct=9.2752 rec=1.1779 | train/val/test=1.000/0.664/0.691 | c=0.998347
=== Best @ epoch 29: val=0.6860, test=0.6670 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 completed in 24.09 seconds.
==================================================
