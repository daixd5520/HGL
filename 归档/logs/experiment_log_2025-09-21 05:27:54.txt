Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1 - 2025-09-21 05:27:54:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.0503 cls=1.0875 smmd=5.6276 ct=7.2616 rec=1.4137 | train/val/test=0.462/0.475/0.467 | c=0.998347
[Epoch 0001] loss=52.2677 cls=1.0561 smmd=3.6596 ct=7.2179 rec=1.4165 | train/val/test=0.425/0.405/0.417 | c=0.998347
[Epoch 0002] loss=39.1758 cls=1.0858 smmd=2.3571 ct=7.1774 rec=1.4137 | train/val/test=0.609/0.605/0.605 | c=0.998347
[Epoch 0003] loss=40.7272 cls=1.0410 smmd=2.5269 ct=7.1154 rec=1.4141 | train/val/test=0.587/0.599/0.599 | c=0.998347
[Epoch 0004] loss=40.0553 cls=1.0068 smmd=2.4652 ct=7.0957 rec=1.4167 | train/val/test=0.582/0.572/0.572 | c=0.998347
[Epoch 0005] loss=36.2326 cls=0.9732 smmd=2.0988 ct=7.0249 rec=1.4171 | train/val/test=0.594/0.584/0.587 | c=0.998347
[Epoch 0006] loss=30.7355 cls=0.9353 smmd=1.5582 ct=6.9891 rec=1.4160 | train/val/test=0.607/0.601/0.596 | c=0.998347
[Epoch 0007] loss=33.7133 cls=0.8816 smmd=1.8637 ct=6.9658 rec=1.4083 | train/val/test=0.667/0.661/0.660 | c=0.998347
[Epoch 0008] loss=34.4192 cls=0.8218 smmd=1.9457 ct=6.9268 rec=1.3964 | train/val/test=0.713/0.709/0.704 | c=0.998347
[Epoch 0009] loss=28.1761 cls=0.7669 smmd=1.3321 ct=6.8901 rec=1.3824 | train/val/test=0.714/0.716/0.705 | c=0.998347
[Epoch 0010] loss=26.8743 cls=0.7344 smmd=1.2041 ct=6.8895 rec=1.3740 | train/val/test=0.719/0.722/0.710 | c=0.998347
[Epoch 0011] loss=29.2324 cls=0.7076 smmd=1.4406 ct=6.8949 rec=1.3665 | train/val/test=0.751/0.753/0.751 | c=0.998347
[Epoch 0012] loss=26.0278 cls=0.6589 smmd=1.1280 ct=6.8720 rec=1.3491 | train/val/test=0.757/0.753/0.756 | c=0.998347
[Epoch 0013] loss=25.5476 cls=0.6407 smmd=1.0828 ct=6.8652 rec=1.3373 | train/val/test=0.762/0.760/0.762 | c=0.998347
[Epoch 0014] loss=24.4079 cls=0.6149 smmd=0.9705 ct=6.8645 rec=1.3335 | train/val/test=0.783/0.785/0.785 | c=0.998347
[Epoch 0015] loss=24.2699 cls=0.5889 smmd=0.9565 ct=6.8718 rec=1.3330 | train/val/test=0.791/0.794/0.794 | c=0.998347
[Epoch 0016] loss=23.8312 cls=0.5721 smmd=0.9143 ct=6.8690 rec=1.3291 | train/val/test=0.775/0.772/0.773 | c=0.998347
[Epoch 0017] loss=21.8964 cls=0.5695 smmd=0.7234 ct=6.8576 rec=1.3244 | train/val/test=0.772/0.768/0.769 | c=0.998347
[Epoch 0018] loss=22.4452 cls=0.5702 smmd=0.7760 ct=6.8683 rec=1.3274 | train/val/test=0.799/0.803/0.802 | c=0.998347
[Epoch 0019] loss=21.4983 cls=0.5271 smmd=0.6816 ct=6.8789 rec=1.3222 | train/val/test=0.807/0.810/0.811 | c=0.998347
[Epoch 0020] loss=20.9267 cls=0.5186 smmd=0.6269 ct=6.8692 rec=1.3211 | train/val/test=0.806/0.806/0.807 | c=0.998347
[Epoch 0021] loss=20.7264 cls=0.5156 smmd=0.6093 ct=6.8582 rec=1.3186 | train/val/test=0.802/0.802/0.802 | c=0.998347
[Epoch 0022] loss=20.3011 cls=0.5090 smmd=0.5663 ct=6.8617 rec=1.3201 | train/val/test=0.801/0.798/0.794 | c=0.998347
[Epoch 0023] loss=19.6749 cls=0.5107 smmd=0.5005 ct=6.8767 rec=1.3227 | train/val/test=0.820/0.821/0.820 | c=0.998347
[Epoch 0024] loss=19.4810 cls=0.4873 smmd=0.4838 ct=6.8703 rec=1.3184 | train/val/test=0.820/0.825/0.827 | c=0.998347
[Epoch 0025] loss=19.1922 cls=0.4897 smmd=0.4564 ct=6.8623 rec=1.3183 | train/val/test=0.819/0.821/0.818 | c=0.998347
[Epoch 0026] loss=18.7349 cls=0.4770 smmd=0.4113 ct=6.8637 rec=1.3127 | train/val/test=0.823/0.825/0.825 | c=0.998347
[Epoch 0027] loss=18.7083 cls=0.4686 smmd=0.4084 ct=6.8671 rec=1.3110 | train/val/test=0.826/0.827/0.827 | c=0.998347
[Epoch 0028] loss=18.2653 cls=0.4651 smmd=0.3651 ct=6.8637 rec=1.3086 | train/val/test=0.831/0.833/0.833 | c=0.998347
[Epoch 0029] loss=18.1421 cls=0.4608 smmd=0.3530 ct=6.8628 rec=1.3118 | train/val/test=0.833/0.836/0.835 | c=0.998347
[Epoch 0030] loss=17.8721 cls=0.4581 smmd=0.3268 ct=6.8599 rec=1.3099 | train/val/test=0.826/0.827/0.827 | c=0.998347
[Epoch 0031] loss=17.7018 cls=0.4596 smmd=0.3095 ct=6.8628 rec=1.3026 | train/val/test=0.833/0.836/0.833 | c=0.998347
[Epoch 0032] loss=17.5278 cls=0.4512 smmd=0.2931 ct=6.8597 rec=1.3041 | train/val/test=0.833/0.834/0.830 | c=0.998347
[Epoch 0033] loss=17.4063 cls=0.4454 smmd=0.2819 ct=6.8567 rec=1.3023 | train/val/test=0.836/0.840/0.837 | c=0.998347
[Epoch 0034] loss=17.2250 cls=0.4429 smmd=0.2635 ct=6.8583 rec=1.3039 | train/val/test=0.836/0.837/0.835 | c=0.998347
[Epoch 0035] loss=17.0672 cls=0.4410 smmd=0.2483 ct=6.8564 rec=1.3026 | train/val/test=0.829/0.829/0.827 | c=0.998347
[Epoch 0036] loss=16.9309 cls=0.4486 smmd=0.2341 ct=6.8576 rec=1.3006 | train/val/test=0.838/0.843/0.838 | c=0.998347
[Epoch 0037] loss=16.8994 cls=0.4419 smmd=0.2304 ct=6.8612 rec=1.3052 | train/val/test=0.833/0.836/0.833 | c=0.998347
[Epoch 0038] loss=16.7277 cls=0.4395 smmd=0.2142 ct=6.8570 rec=1.3035 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0039] loss=16.6414 cls=0.4394 smmd=0.2055 ct=6.8571 rec=1.3042 | train/val/test=0.837/0.837/0.833 | c=0.998347
[Epoch 0040] loss=16.5473 cls=0.4388 smmd=0.1955 ct=6.8601 rec=1.3062 | train/val/test=0.833/0.834/0.831 | c=0.998347
[Epoch 0041] loss=16.4342 cls=0.4426 smmd=0.1846 ct=6.8569 rec=1.3063 | train/val/test=0.840/0.840/0.839 | c=0.998347
[Epoch 0042] loss=16.3945 cls=0.4426 smmd=0.1793 ct=6.8627 rec=1.3091 | train/val/test=0.832/0.834/0.827 | c=0.998347
[Epoch 0043] loss=16.3081 cls=0.4457 smmd=0.1703 ct=6.8638 rec=1.3089 | train/val/test=0.839/0.839/0.838 | c=0.998347
[Epoch 0044] loss=16.3241 cls=0.4438 smmd=0.1732 ct=6.8573 rec=1.3104 | train/val/test=0.833/0.834/0.827 | c=0.998347
[Epoch 0045] loss=16.2037 cls=0.4463 smmd=0.1598 ct=6.8634 rec=1.3109 | train/val/test=0.840/0.843/0.837 | c=0.998347
[Epoch 0046] loss=16.1106 cls=0.4453 smmd=0.1510 ct=6.8614 rec=1.3101 | train/val/test=0.838/0.840/0.838 | c=0.998347
[Epoch 0047] loss=16.1251 cls=0.4460 smmd=0.1530 ct=6.8584 rec=1.3107 | train/val/test=0.838/0.840/0.837 | c=0.998347
[Epoch 0048] loss=16.0341 cls=0.4469 smmd=0.1429 ct=6.8632 rec=1.3110 | train/val/test=0.839/0.842/0.838 | c=0.998347
[Epoch 0049] loss=16.0153 cls=0.4482 smmd=0.1416 ct=6.8601 rec=1.3108 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0050] loss=15.9903 cls=0.4491 smmd=0.1385 ct=6.8623 rec=1.3113 | train/val/test=0.837/0.840/0.833 | c=0.998347
[Epoch 0051] loss=15.9582 cls=0.4518 smmd=0.1360 ct=6.8585 rec=1.3106 | train/val/test=0.840/0.841/0.840 | c=0.998347
[Epoch 0052] loss=15.9262 cls=0.4526 smmd=0.1321 ct=6.8613 rec=1.3119 | train/val/test=0.833/0.834/0.830 | c=0.998347
[Epoch 0053] loss=15.9090 cls=0.4538 smmd=0.1298 ct=6.8644 rec=1.3114 | train/val/test=0.838/0.843/0.840 | c=0.998347
[Epoch 0054] loss=15.9046 cls=0.4569 smmd=0.1298 ct=6.8610 rec=1.3124 | train/val/test=0.832/0.834/0.830 | c=0.998347
[Epoch 0055] loss=15.8990 cls=0.4573 smmd=0.1296 ct=6.8595 rec=1.3112 | train/val/test=0.839/0.841/0.840 | c=0.998347
[Epoch 0056] loss=15.8514 cls=0.4569 smmd=0.1242 ct=6.8621 rec=1.3129 | train/val/test=0.835/0.837/0.832 | c=0.998347
[Epoch 0057] loss=17.2114 cls=0.4573 smmd=0.1192 ct=7.5677 rec=1.3108 | train/val/test=0.839/0.840/0.838 | c=0.998347
[Epoch 0058] loss=17.2307 cls=0.4584 smmd=0.1297 ct=7.5244 rec=1.3114 | train/val/test=0.833/0.836/0.831 | c=0.998347
[Epoch 0059] loss=17.2937 cls=0.4559 smmd=0.1246 ct=7.5818 rec=1.3120 | train/val/test=0.839/0.840/0.837 | c=0.998347
[Epoch 0060] loss=17.2389 cls=0.4577 smmd=0.1297 ct=7.5284 rec=1.3121 | train/val/test=0.835/0.836/0.831 | c=0.998347
[Epoch 0061] loss=17.1948 cls=0.4567 smmd=0.1191 ct=7.5599 rec=1.3123 | train/val/test=0.838/0.839/0.836 | c=0.998347
[Epoch 0062] loss=17.1334 cls=0.4568 smmd=0.1165 ct=7.5419 rec=1.3132 | train/val/test=0.837/0.838/0.832 | c=0.998347
[Epoch 0063] loss=17.1568 cls=0.4570 smmd=0.1187 ct=7.5423 rec=1.3139 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0064] loss=17.1664 cls=0.4571 smmd=0.1185 ct=7.5479 rec=1.3148 | train/val/test=0.835/0.836/0.832 | c=0.998347
[Epoch 0065] loss=17.1817 cls=0.4581 smmd=0.1230 ct=7.5325 rec=1.3147 | train/val/test=0.839/0.840/0.836 | c=0.998347
[Epoch 0066] loss=17.1993 cls=0.4575 smmd=0.1221 ct=7.5457 rec=1.3154 | train/val/test=0.835/0.835/0.831 | c=0.998347
[Epoch 0067] loss=17.2473 cls=0.4581 smmd=0.1295 ct=7.5327 rec=1.3156 | train/val/test=0.840/0.842/0.840 | c=0.998347
[Epoch 0068] loss=17.1910 cls=0.4577 smmd=0.1233 ct=7.5356 rec=1.3154 | train/val/test=0.832/0.833/0.829 | c=0.998347
[Epoch 0069] loss=17.1832 cls=0.4580 smmd=0.1230 ct=7.5337 rec=1.3143 | train/val/test=0.842/0.844/0.839 | c=0.998347
[Epoch 0070] loss=17.2506 cls=0.4568 smmd=0.1309 ct=7.5276 rec=1.3159 | train/val/test=0.831/0.833/0.827 | c=0.998347
[Epoch 0071] loss=17.1889 cls=0.4597 smmd=0.1246 ct=7.5285 rec=1.3131 | train/val/test=0.842/0.845/0.840 | c=0.998347
[Epoch 0072] loss=17.1730 cls=0.4594 smmd=0.1226 ct=7.5295 rec=1.3173 | train/val/test=0.824/0.825/0.823 | c=0.998347
[Epoch 0073] loss=17.3023 cls=0.4666 smmd=0.1369 ct=7.5217 rec=1.3136 | train/val/test=0.841/0.843/0.839 | c=0.998347
[Epoch 0074] loss=17.3245 cls=0.4694 smmd=0.1358 ct=7.5357 rec=1.3211 | train/val/test=0.817/0.813/0.813 | c=0.998347
[Epoch 0075] loss=17.2616 cls=0.4778 smmd=0.1330 ct=7.5172 rec=1.3162 | train/val/test=0.834/0.837/0.832 | c=0.998347
[Epoch 0076] loss=17.3295 cls=0.4796 smmd=0.1365 ct=7.5318 rec=1.3231 | train/val/test=0.813/0.808/0.810 | c=0.998347
[Epoch 0077] loss=17.3615 cls=0.4831 smmd=0.1417 ct=7.5221 rec=1.3183 | train/val/test=0.836/0.839/0.837 | c=0.998347
[Epoch 0078] loss=17.3685 cls=0.4770 smmd=0.1426 ct=7.5217 rec=1.3211 | train/val/test=0.817/0.812/0.814 | c=0.998347
[Epoch 0079] loss=17.3137 cls=0.4741 smmd=0.1384 ct=7.5171 rec=1.3163 | train/val/test=0.842/0.844/0.840 | c=0.998347
[Epoch 0080] loss=17.1854 cls=0.4593 smmd=0.1265 ct=7.5163 rec=1.3158 | train/val/test=0.831/0.833/0.830 | c=0.998347
[Epoch 0081] loss=17.1235 cls=0.4560 smmd=0.1229 ct=7.5054 rec=1.3118 | train/val/test=0.841/0.844/0.840 | c=0.998347
[Epoch 0082] loss=17.0914 cls=0.4493 smmd=0.1179 ct=7.5156 rec=1.3135 | train/val/test=0.838/0.841/0.837 | c=0.998347
[Epoch 0083] loss=17.0674 cls=0.4526 smmd=0.1173 ct=7.5058 rec=1.3135 | train/val/test=0.839/0.843/0.837 | c=0.998347
[Epoch 0084] loss=17.0790 cls=0.4528 smmd=0.1172 ct=7.5113 rec=1.3157 | train/val/test=0.841/0.843/0.839 | c=0.998347
[Epoch 0085] loss=17.0777 cls=0.4550 smmd=0.1161 ct=7.5154 rec=1.3171 | train/val/test=0.837/0.841/0.836 | c=0.998347
[Epoch 0086] loss=17.1442 cls=0.4584 smmd=0.1254 ct=7.5009 rec=1.3178 | train/val/test=0.841/0.845/0.842 | c=0.998347
[Epoch 0087] loss=17.2231 cls=0.4570 smmd=0.1291 ct=7.5220 rec=1.3199 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0088] loss=17.1873 cls=0.4598 smmd=0.1297 ct=7.5009 rec=1.3181 | train/val/test=0.843/0.848/0.841 | c=0.998347
[Epoch 0089] loss=17.1111 cls=0.4559 smmd=0.1218 ct=7.5026 rec=1.3191 | train/val/test=0.836/0.839/0.836 | c=0.998347
[Epoch 0090] loss=17.1369 cls=0.4577 smmd=0.1231 ct=7.5094 rec=1.3168 | train/val/test=0.844/0.849/0.842 | c=0.998347
[Epoch 0091] loss=17.1137 cls=0.4564 smmd=0.1244 ct=7.4910 rec=1.3183 | train/val/test=0.833/0.833/0.830 | c=0.998347
[Epoch 0092] loss=17.0566 cls=0.4581 smmd=0.1170 ct=7.4999 rec=1.3159 | train/val/test=0.843/0.846/0.845 | c=0.998347
[Epoch 0093] loss=17.0619 cls=0.4608 smmd=0.1180 ct=7.4958 rec=1.3194 | train/val/test=0.825/0.826/0.823 | c=0.998347
[Epoch 0094] loss=17.0946 cls=0.4653 smmd=0.1219 ct=7.4921 rec=1.3183 | train/val/test=0.841/0.841/0.836 | c=0.998347
[Epoch 0095] loss=17.0876 cls=0.4725 smmd=0.1190 ct=7.4998 rec=1.3234 | train/val/test=0.811/0.806/0.806 | c=0.998347
[Epoch 0096] loss=17.2322 cls=0.4902 smmd=0.1335 ct=7.4945 rec=1.3269 | train/val/test=0.817/0.818/0.814 | c=0.998347
[Epoch 0097] loss=17.4197 cls=0.5116 smmd=0.1475 ct=7.5102 rec=1.3362 | train/val/test=0.779/0.775/0.770 | c=0.998347
[Epoch 0098] loss=17.6432 cls=0.5566 smmd=0.1694 ct=7.4994 rec=1.3447 | train/val/test=0.773/0.781/0.778 | c=0.998347
[Epoch 0099] loss=17.6895 cls=0.5595 smmd=0.1685 ct=7.5246 rec=1.3507 | train/val/test=0.788/0.781/0.780 | c=0.998347
=== Best @ epoch 90: val=0.8491, test=0.8415 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1 - 2025-09-21 05:27:54:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.0503 cls=1.0875 smmd=5.6276 ct=7.2616 rec=1.4137 | train/val/test=0.462/0.475/0.467 | c=0.998347
[Epoch 0001] loss=52.2677 cls=1.0561 smmd=3.6596 ct=7.2179 rec=1.4165 | train/val/test=0.425/0.405/0.417 | c=0.998347
[Epoch 0002] loss=39.1758 cls=1.0858 smmd=2.3571 ct=7.1774 rec=1.4137 | train/val/test=0.609/0.605/0.605 | c=0.998347
[Epoch 0003] loss=40.7272 cls=1.0410 smmd=2.5269 ct=7.1154 rec=1.4141 | train/val/test=0.587/0.599/0.599 | c=0.998347
[Epoch 0004] loss=40.0553 cls=1.0068 smmd=2.4652 ct=7.0957 rec=1.4167 | train/val/test=0.582/0.572/0.572 | c=0.998347
[Epoch 0005] loss=36.2326 cls=0.9732 smmd=2.0988 ct=7.0249 rec=1.4171 | train/val/test=0.594/0.584/0.587 | c=0.998347
[Epoch 0006] loss=30.7355 cls=0.9353 smmd=1.5582 ct=6.9891 rec=1.4160 | train/val/test=0.607/0.601/0.596 | c=0.998347
[Epoch 0007] loss=33.7133 cls=0.8816 smmd=1.8637 ct=6.9658 rec=1.4083 | train/val/test=0.667/0.661/0.660 | c=0.998347
[Epoch 0008] loss=34.4192 cls=0.8218 smmd=1.9457 ct=6.9268 rec=1.3964 | train/val/test=0.713/0.709/0.704 | c=0.998347
[Epoch 0009] loss=28.1761 cls=0.7669 smmd=1.3321 ct=6.8901 rec=1.3824 | train/val/test=0.714/0.716/0.705 | c=0.998347
[Epoch 0010] loss=26.8743 cls=0.7344 smmd=1.2041 ct=6.8895 rec=1.3740 | train/val/test=0.719/0.722/0.710 | c=0.998347
[Epoch 0011] loss=29.2324 cls=0.7076 smmd=1.4406 ct=6.8949 rec=1.3665 | train/val/test=0.751/0.753/0.751 | c=0.998347
[Epoch 0012] loss=26.0278 cls=0.6589 smmd=1.1280 ct=6.8720 rec=1.3491 | train/val/test=0.757/0.753/0.756 | c=0.998347
[Epoch 0013] loss=25.5476 cls=0.6407 smmd=1.0828 ct=6.8652 rec=1.3373 | train/val/test=0.762/0.760/0.762 | c=0.998347
[Epoch 0014] loss=24.4079 cls=0.6149 smmd=0.9705 ct=6.8645 rec=1.3335 | train/val/test=0.783/0.785/0.785 | c=0.998347
[Epoch 0015] loss=24.2699 cls=0.5889 smmd=0.9565 ct=6.8718 rec=1.3330 | train/val/test=0.791/0.794/0.794 | c=0.998347
[Epoch 0016] loss=23.8312 cls=0.5721 smmd=0.9143 ct=6.8690 rec=1.3291 | train/val/test=0.775/0.772/0.773 | c=0.998347
[Epoch 0017] loss=21.8964 cls=0.5695 smmd=0.7234 ct=6.8576 rec=1.3244 | train/val/test=0.772/0.768/0.769 | c=0.998347
[Epoch 0018] loss=22.4452 cls=0.5702 smmd=0.7760 ct=6.8683 rec=1.3274 | train/val/test=0.799/0.803/0.802 | c=0.998347
[Epoch 0019] loss=21.4983 cls=0.5271 smmd=0.6816 ct=6.8789 rec=1.3222 | train/val/test=0.807/0.810/0.811 | c=0.998347
[Epoch 0020] loss=20.9267 cls=0.5186 smmd=0.6269 ct=6.8692 rec=1.3211 | train/val/test=0.806/0.806/0.807 | c=0.998347
[Epoch 0021] loss=20.7264 cls=0.5156 smmd=0.6093 ct=6.8582 rec=1.3186 | train/val/test=0.802/0.802/0.802 | c=0.998347
[Epoch 0022] loss=20.3011 cls=0.5090 smmd=0.5663 ct=6.8617 rec=1.3201 | train/val/test=0.801/0.798/0.794 | c=0.998347
[Epoch 0023] loss=19.6749 cls=0.5107 smmd=0.5005 ct=6.8767 rec=1.3227 | train/val/test=0.820/0.821/0.820 | c=0.998347
[Epoch 0024] loss=19.4810 cls=0.4873 smmd=0.4838 ct=6.8703 rec=1.3184 | train/val/test=0.820/0.825/0.827 | c=0.998347
[Epoch 0025] loss=19.1922 cls=0.4897 smmd=0.4564 ct=6.8623 rec=1.3183 | train/val/test=0.819/0.821/0.818 | c=0.998347
[Epoch 0026] loss=18.7349 cls=0.4770 smmd=0.4113 ct=6.8637 rec=1.3127 | train/val/test=0.823/0.825/0.825 | c=0.998347
[Epoch 0027] loss=18.7083 cls=0.4686 smmd=0.4084 ct=6.8671 rec=1.3110 | train/val/test=0.826/0.827/0.827 | c=0.998347
[Epoch 0028] loss=18.2653 cls=0.4651 smmd=0.3651 ct=6.8637 rec=1.3086 | train/val/test=0.831/0.833/0.833 | c=0.998347
[Epoch 0029] loss=18.1421 cls=0.4608 smmd=0.3530 ct=6.8628 rec=1.3118 | train/val/test=0.833/0.836/0.835 | c=0.998347
[Epoch 0030] loss=17.8721 cls=0.4581 smmd=0.3268 ct=6.8599 rec=1.3099 | train/val/test=0.826/0.827/0.827 | c=0.998347
[Epoch 0031] loss=17.7018 cls=0.4596 smmd=0.3095 ct=6.8628 rec=1.3026 | train/val/test=0.833/0.836/0.833 | c=0.998347
[Epoch 0032] loss=17.5278 cls=0.4512 smmd=0.2931 ct=6.8597 rec=1.3041 | train/val/test=0.833/0.834/0.830 | c=0.998347
[Epoch 0033] loss=17.4063 cls=0.4454 smmd=0.2819 ct=6.8567 rec=1.3023 | train/val/test=0.836/0.840/0.837 | c=0.998347
[Epoch 0034] loss=17.2250 cls=0.4429 smmd=0.2635 ct=6.8583 rec=1.3039 | train/val/test=0.836/0.837/0.835 | c=0.998347
[Epoch 0035] loss=17.0672 cls=0.4410 smmd=0.2483 ct=6.8564 rec=1.3026 | train/val/test=0.829/0.829/0.827 | c=0.998347
[Epoch 0036] loss=16.9309 cls=0.4486 smmd=0.2341 ct=6.8576 rec=1.3006 | train/val/test=0.838/0.843/0.838 | c=0.998347
[Epoch 0037] loss=16.8994 cls=0.4419 smmd=0.2304 ct=6.8612 rec=1.3052 | train/val/test=0.833/0.836/0.833 | c=0.998347
[Epoch 0038] loss=16.7277 cls=0.4395 smmd=0.2142 ct=6.8570 rec=1.3035 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0039] loss=16.6414 cls=0.4394 smmd=0.2055 ct=6.8571 rec=1.3042 | train/val/test=0.837/0.837/0.833 | c=0.998347
[Epoch 0040] loss=16.5473 cls=0.4388 smmd=0.1955 ct=6.8601 rec=1.3062 | train/val/test=0.833/0.834/0.831 | c=0.998347
[Epoch 0041] loss=16.4342 cls=0.4426 smmd=0.1846 ct=6.8569 rec=1.3063 | train/val/test=0.840/0.840/0.839 | c=0.998347
[Epoch 0042] loss=16.3945 cls=0.4426 smmd=0.1793 ct=6.8627 rec=1.3091 | train/val/test=0.832/0.834/0.827 | c=0.998347
[Epoch 0043] loss=16.3081 cls=0.4457 smmd=0.1703 ct=6.8638 rec=1.3089 | train/val/test=0.839/0.839/0.838 | c=0.998347
[Epoch 0044] loss=16.3241 cls=0.4438 smmd=0.1732 ct=6.8573 rec=1.3104 | train/val/test=0.833/0.834/0.827 | c=0.998347
[Epoch 0045] loss=16.2037 cls=0.4463 smmd=0.1598 ct=6.8634 rec=1.3109 | train/val/test=0.840/0.843/0.837 | c=0.998347
[Epoch 0046] loss=16.1106 cls=0.4453 smmd=0.1510 ct=6.8614 rec=1.3101 | train/val/test=0.838/0.840/0.838 | c=0.998347
[Epoch 0047] loss=16.1251 cls=0.4460 smmd=0.1530 ct=6.8584 rec=1.3107 | train/val/test=0.838/0.840/0.837 | c=0.998347
[Epoch 0048] loss=16.0341 cls=0.4469 smmd=0.1429 ct=6.8632 rec=1.3110 | train/val/test=0.839/0.842/0.838 | c=0.998347
[Epoch 0049] loss=16.0153 cls=0.4482 smmd=0.1416 ct=6.8601 rec=1.3108 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0050] loss=15.9903 cls=0.4491 smmd=0.1385 ct=6.8623 rec=1.3113 | train/val/test=0.837/0.840/0.833 | c=0.998347
[Epoch 0051] loss=15.9582 cls=0.4518 smmd=0.1360 ct=6.8585 rec=1.3106 | train/val/test=0.840/0.841/0.840 | c=0.998347
[Epoch 0052] loss=15.9262 cls=0.4526 smmd=0.1321 ct=6.8613 rec=1.3119 | train/val/test=0.833/0.834/0.830 | c=0.998347
[Epoch 0053] loss=15.9090 cls=0.4538 smmd=0.1298 ct=6.8644 rec=1.3114 | train/val/test=0.838/0.843/0.840 | c=0.998347
[Epoch 0054] loss=15.9046 cls=0.4569 smmd=0.1298 ct=6.8610 rec=1.3124 | train/val/test=0.832/0.834/0.830 | c=0.998347
[Epoch 0055] loss=15.8990 cls=0.4573 smmd=0.1296 ct=6.8595 rec=1.3112 | train/val/test=0.839/0.841/0.840 | c=0.998347
[Epoch 0056] loss=15.8514 cls=0.4569 smmd=0.1242 ct=6.8621 rec=1.3129 | train/val/test=0.835/0.837/0.832 | c=0.998347
[Epoch 0057] loss=17.2114 cls=0.4573 smmd=0.1192 ct=7.5677 rec=1.3108 | train/val/test=0.839/0.840/0.838 | c=0.998347
[Epoch 0058] loss=17.2307 cls=0.4584 smmd=0.1297 ct=7.5244 rec=1.3114 | train/val/test=0.833/0.836/0.831 | c=0.998347
[Epoch 0059] loss=17.2937 cls=0.4559 smmd=0.1246 ct=7.5818 rec=1.3120 | train/val/test=0.839/0.840/0.837 | c=0.998347
[Epoch 0060] loss=17.2389 cls=0.4577 smmd=0.1297 ct=7.5284 rec=1.3121 | train/val/test=0.835/0.836/0.831 | c=0.998347
[Epoch 0061] loss=17.1948 cls=0.4567 smmd=0.1191 ct=7.5599 rec=1.3123 | train/val/test=0.838/0.839/0.836 | c=0.998347
[Epoch 0062] loss=17.1334 cls=0.4568 smmd=0.1165 ct=7.5419 rec=1.3132 | train/val/test=0.837/0.838/0.832 | c=0.998347
[Epoch 0063] loss=17.1568 cls=0.4570 smmd=0.1187 ct=7.5423 rec=1.3139 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0064] loss=17.1664 cls=0.4571 smmd=0.1185 ct=7.5479 rec=1.3148 | train/val/test=0.835/0.836/0.832 | c=0.998347
[Epoch 0065] loss=17.1817 cls=0.4581 smmd=0.1230 ct=7.5325 rec=1.3147 | train/val/test=0.839/0.840/0.836 | c=0.998347
[Epoch 0066] loss=17.1993 cls=0.4575 smmd=0.1221 ct=7.5457 rec=1.3154 | train/val/test=0.835/0.835/0.831 | c=0.998347
[Epoch 0067] loss=17.2473 cls=0.4581 smmd=0.1295 ct=7.5327 rec=1.3156 | train/val/test=0.840/0.842/0.840 | c=0.998347
[Epoch 0068] loss=17.1910 cls=0.4577 smmd=0.1233 ct=7.5356 rec=1.3154 | train/val/test=0.832/0.833/0.829 | c=0.998347
[Epoch 0069] loss=17.1832 cls=0.4580 smmd=0.1230 ct=7.5337 rec=1.3143 | train/val/test=0.842/0.844/0.839 | c=0.998347
[Epoch 0070] loss=17.2506 cls=0.4568 smmd=0.1309 ct=7.5276 rec=1.3159 | train/val/test=0.831/0.833/0.827 | c=0.998347
[Epoch 0071] loss=17.1889 cls=0.4597 smmd=0.1246 ct=7.5285 rec=1.3131 | train/val/test=0.842/0.845/0.840 | c=0.998347
[Epoch 0072] loss=17.1730 cls=0.4594 smmd=0.1226 ct=7.5295 rec=1.3173 | train/val/test=0.824/0.825/0.823 | c=0.998347
[Epoch 0073] loss=17.3023 cls=0.4666 smmd=0.1369 ct=7.5217 rec=1.3136 | train/val/test=0.841/0.843/0.839 | c=0.998347
[Epoch 0074] loss=17.3245 cls=0.4694 smmd=0.1358 ct=7.5357 rec=1.3211 | train/val/test=0.817/0.813/0.813 | c=0.998347
[Epoch 0075] loss=17.2616 cls=0.4778 smmd=0.1330 ct=7.5172 rec=1.3162 | train/val/test=0.834/0.837/0.832 | c=0.998347
[Epoch 0076] loss=17.3295 cls=0.4796 smmd=0.1365 ct=7.5318 rec=1.3231 | train/val/test=0.813/0.808/0.810 | c=0.998347
[Epoch 0077] loss=17.3615 cls=0.4831 smmd=0.1417 ct=7.5221 rec=1.3183 | train/val/test=0.836/0.839/0.837 | c=0.998347
[Epoch 0078] loss=17.3685 cls=0.4770 smmd=0.1426 ct=7.5217 rec=1.3211 | train/val/test=0.817/0.812/0.814 | c=0.998347
[Epoch 0079] loss=17.3137 cls=0.4741 smmd=0.1384 ct=7.5171 rec=1.3163 | train/val/test=0.842/0.844/0.840 | c=0.998347
[Epoch 0080] loss=17.1854 cls=0.4593 smmd=0.1265 ct=7.5163 rec=1.3158 | train/val/test=0.831/0.833/0.830 | c=0.998347
[Epoch 0081] loss=17.1235 cls=0.4560 smmd=0.1229 ct=7.5054 rec=1.3118 | train/val/test=0.841/0.844/0.840 | c=0.998347
[Epoch 0082] loss=17.0914 cls=0.4493 smmd=0.1179 ct=7.5156 rec=1.3135 | train/val/test=0.838/0.841/0.837 | c=0.998347
[Epoch 0083] loss=17.0674 cls=0.4526 smmd=0.1173 ct=7.5058 rec=1.3135 | train/val/test=0.839/0.843/0.837 | c=0.998347
[Epoch 0084] loss=17.0790 cls=0.4528 smmd=0.1172 ct=7.5113 rec=1.3157 | train/val/test=0.841/0.843/0.839 | c=0.998347
[Epoch 0085] loss=17.0777 cls=0.4550 smmd=0.1161 ct=7.5154 rec=1.3171 | train/val/test=0.837/0.841/0.836 | c=0.998347
[Epoch 0086] loss=17.1442 cls=0.4584 smmd=0.1254 ct=7.5009 rec=1.3178 | train/val/test=0.841/0.845/0.842 | c=0.998347
[Epoch 0087] loss=17.2231 cls=0.4570 smmd=0.1291 ct=7.5220 rec=1.3199 | train/val/test=0.837/0.840/0.837 | c=0.998347
[Epoch 0088] loss=17.1873 cls=0.4598 smmd=0.1297 ct=7.5009 rec=1.3181 | train/val/test=0.843/0.848/0.841 | c=0.998347
[Epoch 0089] loss=17.1111 cls=0.4559 smmd=0.1218 ct=7.5026 rec=1.3191 | train/val/test=0.836/0.839/0.836 | c=0.998347
[Epoch 0090] loss=17.1369 cls=0.4577 smmd=0.1231 ct=7.5094 rec=1.3168 | train/val/test=0.844/0.849/0.842 | c=0.998347
[Epoch 0091] loss=17.1137 cls=0.4564 smmd=0.1244 ct=7.4910 rec=1.3183 | train/val/test=0.833/0.833/0.830 | c=0.998347
[Epoch 0092] loss=17.0566 cls=0.4581 smmd=0.1170 ct=7.4999 rec=1.3159 | train/val/test=0.843/0.846/0.845 | c=0.998347
[Epoch 0093] loss=17.0619 cls=0.4608 smmd=0.1180 ct=7.4958 rec=1.3194 | train/val/test=0.825/0.826/0.823 | c=0.998347
[Epoch 0094] loss=17.0946 cls=0.4653 smmd=0.1219 ct=7.4921 rec=1.3183 | train/val/test=0.841/0.841/0.836 | c=0.998347
[Epoch 0095] loss=17.0876 cls=0.4725 smmd=0.1190 ct=7.4998 rec=1.3234 | train/val/test=0.811/0.806/0.806 | c=0.998347
[Epoch 0096] loss=17.2322 cls=0.4902 smmd=0.1335 ct=7.4945 rec=1.3269 | train/val/test=0.817/0.818/0.814 | c=0.998347
[Epoch 0097] loss=17.4197 cls=0.5116 smmd=0.1475 ct=7.5102 rec=1.3362 | train/val/test=0.779/0.775/0.770 | c=0.998347
[Epoch 0098] loss=17.6432 cls=0.5566 smmd=0.1694 ct=7.4994 rec=1.3447 | train/val/test=0.773/0.781/0.778 | c=0.998347
[Epoch 0099] loss=17.6895 cls=0.5595 smmd=0.1685 ct=7.5246 rec=1.3507 | train/val/test=0.788/0.781/0.780 | c=0.998347
=== Best @ epoch 90: val=0.8491, test=0.8415 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-1 completed in 187.61 seconds.
==================================================
