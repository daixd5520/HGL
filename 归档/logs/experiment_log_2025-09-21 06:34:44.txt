Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 - 2025-09-21 06:34:44:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1909 cls=1.9442 smmd=4.1983 ct=9.2706 rec=1.3889 | train/val/test=0.276/0.172/0.227 | c=0.998347
[Epoch 0001] loss=16.6879 cls=1.8943 smmd=2.7974 ct=9.2171 rec=1.3895 | train/val/test=0.552/0.390/0.416 | c=0.998347
[Epoch 0002] loss=15.1050 cls=1.7772 smmd=1.4480 ct=9.1020 rec=1.3889 | train/val/test=0.897/0.378/0.430 | c=0.998347
[Epoch 0003] loss=14.8714 cls=1.5354 smmd=1.5182 ct=9.0423 rec=1.3878 | train/val/test=0.793/0.538/0.552 | c=0.998347
[Epoch 0004] loss=15.5330 cls=1.1970 smmd=1.7988 ct=9.7712 rec=1.3830 | train/val/test=0.897/0.418/0.441 | c=0.998347
[Epoch 0005] loss=14.6738 cls=0.7963 smmd=1.7291 ct=9.4149 rec=1.3668 | train/val/test=0.931/0.408/0.441 | c=0.998347
[Epoch 0006] loss=14.0454 cls=0.5183 smmd=1.5145 ct=9.3270 rec=1.3428 | train/val/test=0.897/0.536/0.557 | c=0.998347
[Epoch 0007] loss=13.5055 cls=0.3120 smmd=1.2934 ct=9.2795 rec=1.3103 | train/val/test=0.931/0.574/0.598 | c=0.998347
[Epoch 0008] loss=13.2999 cls=0.2060 smmd=1.1971 ct=9.3242 rec=1.2863 | train/val/test=1.000/0.548/0.577 | c=0.998347
[Epoch 0009] loss=13.2308 cls=0.1040 smmd=1.2341 ct=9.3542 rec=1.2693 | train/val/test=1.000/0.442/0.482 | c=0.998347
[Epoch 0010] loss=13.1705 cls=0.0630 smmd=1.1887 ct=9.4020 rec=1.2585 | train/val/test=1.000/0.458/0.498 | c=0.998347
[Epoch 0011] loss=13.0308 cls=0.0407 smmd=1.0785 ct=9.4225 rec=1.2445 | train/val/test=1.000/0.568/0.577 | c=0.998347
[Epoch 0012] loss=12.8194 cls=0.0222 smmd=0.9187 ct=9.4223 rec=1.2281 | train/val/test=1.000/0.600/0.618 | c=0.998347
[Epoch 0013] loss=12.6350 cls=0.0143 smmd=0.7704 ct=9.4158 rec=1.2173 | train/val/test=1.000/0.588/0.610 | c=0.998347
[Epoch 0014] loss=12.5553 cls=0.0090 smmd=0.7417 ct=9.3833 rec=1.2107 | train/val/test=1.000/0.552/0.566 | c=0.998347
[Epoch 0015] loss=12.5491 cls=0.0070 smmd=0.7686 ct=9.3569 rec=1.2082 | train/val/test=1.000/0.538/0.557 | c=0.998347
[Epoch 0016] loss=12.4919 cls=0.0063 smmd=0.7184 ct=9.3547 rec=1.2063 | train/val/test=1.000/0.566/0.576 | c=0.998347
[Epoch 0017] loss=12.3935 cls=0.0049 smmd=0.6191 ct=9.3649 rec=1.2023 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0018] loss=12.3114 cls=0.0051 smmd=0.5218 ct=9.3846 rec=1.2000 | train/val/test=1.000/0.596/0.600 | c=0.998347
[Epoch 0019] loss=12.2273 cls=0.0062 smmd=0.4336 ct=9.3864 rec=1.2006 | train/val/test=1.000/0.578/0.584 | c=0.998347
[Epoch 0020] loss=12.2350 cls=0.0085 smmd=0.4333 ct=9.3856 rec=1.2038 | train/val/test=1.000/0.586/0.584 | c=0.998347
[Epoch 0021] loss=12.2019 cls=0.0106 smmd=0.3932 ct=9.3890 rec=1.2045 | train/val/test=1.000/0.600/0.601 | c=0.998347
[Epoch 0022] loss=12.1272 cls=0.0134 smmd=0.3136 ct=9.3926 rec=1.2038 | train/val/test=1.000/0.598/0.595 | c=0.998347
[Epoch 0023] loss=12.0741 cls=0.0164 smmd=0.2649 ct=9.3848 rec=1.2040 | train/val/test=1.000/0.596/0.588 | c=0.998347
[Epoch 0024] loss=12.0558 cls=0.0193 smmd=0.2466 ct=9.3828 rec=1.2036 | train/val/test=1.000/0.614/0.603 | c=0.998347
[Epoch 0025] loss=12.0359 cls=0.0212 smmd=0.2282 ct=9.3882 rec=1.1991 | train/val/test=1.000/0.612/0.605 | c=0.998347
[Epoch 0026] loss=12.0047 cls=0.0225 smmd=0.2090 ct=9.3827 rec=1.1953 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0027] loss=11.9570 cls=0.0242 smmd=0.1652 ct=9.3829 rec=1.1924 | train/val/test=1.000/0.612/0.611 | c=0.998347
[Epoch 0028] loss=11.9594 cls=0.0253 smmd=0.1645 ct=9.3946 rec=1.1875 | train/val/test=1.000/0.612/0.604 | c=0.998347
[Epoch 0029] loss=11.9188 cls=0.0239 smmd=0.1418 ct=9.3862 rec=1.1835 | train/val/test=1.000/0.614/0.603 | c=0.998347
[Epoch 0030] loss=11.8813 cls=0.0211 smmd=0.1284 ct=9.3725 rec=1.1796 | train/val/test=1.000/0.606/0.598 | c=0.998347
[Epoch 0031] loss=11.8683 cls=0.0187 smmd=0.1333 ct=9.3644 rec=1.1760 | train/val/test=1.000/0.608/0.598 | c=0.998347
[Epoch 0032] loss=11.8370 cls=0.0169 smmd=0.1074 ct=9.3656 rec=1.1735 | train/val/test=1.000/0.608/0.604 | c=0.998347
[Epoch 0033] loss=11.8209 cls=0.0149 smmd=0.0930 ct=9.3700 rec=1.1715 | train/val/test=1.000/0.602/0.604 | c=0.998347
[Epoch 0034] loss=11.8077 cls=0.0135 smmd=0.0887 ct=9.3653 rec=1.1701 | train/val/test=1.000/0.604/0.606 | c=0.998347
[Epoch 0035] loss=11.7873 cls=0.0124 smmd=0.0741 ct=9.3615 rec=1.1696 | train/val/test=1.000/0.604/0.605 | c=0.998347
[Epoch 0036] loss=11.7914 cls=0.0117 smmd=0.0826 ct=9.3579 rec=1.1696 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0037] loss=11.7681 cls=0.0114 smmd=0.0639 ct=9.3526 rec=1.1701 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0038] loss=11.7541 cls=0.0114 smmd=0.0484 ct=9.3536 rec=1.1703 | train/val/test=1.000/0.604/0.603 | c=0.998347
[Epoch 0039] loss=11.7504 cls=0.0114 smmd=0.0427 ct=9.3556 rec=1.1703 | train/val/test=1.000/0.598/0.600 | c=0.998347
[Epoch 0040] loss=11.7389 cls=0.0116 smmd=0.0315 ct=9.3548 rec=1.1705 | train/val/test=1.000/0.602/0.601 | c=0.998347
[Epoch 0041] loss=11.7385 cls=0.0117 smmd=0.0367 ct=9.3507 rec=1.1697 | train/val/test=1.000/0.596/0.604 | c=0.998347
[Epoch 0042] loss=11.7254 cls=0.0119 smmd=0.0274 ct=9.3468 rec=1.1697 | train/val/test=1.000/0.598/0.601 | c=0.998347
[Epoch 0043] loss=11.7222 cls=0.0125 smmd=0.0308 ct=9.3425 rec=1.1683 | train/val/test=1.000/0.596/0.603 | c=0.998347
[Epoch 0044] loss=11.7193 cls=0.0129 smmd=0.0230 ct=9.3470 rec=1.1682 | train/val/test=1.000/0.596/0.600 | c=0.998347
[Epoch 0045] loss=11.7132 cls=0.0141 smmd=0.0235 ct=9.3410 rec=1.1673 | train/val/test=1.000/0.602/0.608 | c=0.998347
[Epoch 0046] loss=11.7161 cls=0.0140 smmd=0.0179 ct=9.3490 rec=1.1676 | train/val/test=1.000/0.590/0.596 | c=0.998347
[Epoch 0047] loss=11.7200 cls=0.0165 smmd=0.0378 ct=9.3297 rec=1.1680 | train/val/test=1.000/0.602/0.617 | c=0.998347
[Epoch 0048] loss=11.7362 cls=0.0159 smmd=0.0225 ct=9.3558 rec=1.1710 | train/val/test=1.000/0.588/0.587 | c=0.998347
[Epoch 0049] loss=11.7522 cls=0.0194 smmd=0.0607 ct=9.3289 rec=1.1716 | train/val/test=1.000/0.602/0.616 | c=0.998347
[Epoch 0050] loss=11.7327 cls=0.0137 smmd=0.0225 ct=9.3577 rec=1.1694 | train/val/test=1.000/0.598/0.601 | c=0.998347
[Epoch 0051] loss=11.6859 cls=0.0103 smmd=0.0235 ct=9.3254 rec=1.1634 | train/val/test=1.000/0.594/0.604 | c=0.998347
[Epoch 0052] loss=11.6747 cls=0.0093 smmd=0.0166 ct=9.3234 rec=1.1627 | train/val/test=1.000/0.604/0.614 | c=0.998347
[Epoch 0053] loss=11.6959 cls=0.0107 smmd=0.0116 ct=9.3411 rec=1.1663 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0054] loss=11.6891 cls=0.0102 smmd=0.0301 ct=9.3163 rec=1.1662 | train/val/test=1.000/0.598/0.603 | c=0.998347
[Epoch 0055] loss=11.6680 cls=0.0098 smmd=0.0026 ct=9.3262 rec=1.1647 | train/val/test=1.000/0.602/0.608 | c=0.998347
[Epoch 0056] loss=11.6722 cls=0.0107 smmd=-0.0034 ct=9.3315 rec=1.1667 | train/val/test=1.000/0.588/0.601 | c=0.998347
[Epoch 0057] loss=11.6870 cls=0.0131 smmd=0.0170 ct=9.3163 rec=1.1703 | train/val/test=1.000/0.602/0.611 | c=0.998347
[Epoch 0058] loss=11.6808 cls=0.0127 smmd=-0.0040 ct=9.3334 rec=1.1693 | train/val/test=1.000/0.594/0.602 | c=0.998347
[Epoch 0059] loss=11.6630 cls=0.0130 smmd=0.0031 ct=9.3104 rec=1.1682 | train/val/test=1.000/0.596/0.602 | c=0.998347
[Epoch 0060] loss=11.6590 cls=0.0125 smmd=-0.0027 ct=9.3157 rec=1.1668 | train/val/test=1.000/0.600/0.606 | c=0.998347
[Epoch 0061] loss=11.6568 cls=0.0131 smmd=-0.0097 ct=9.3186 rec=1.1674 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0062] loss=11.6647 cls=0.0145 smmd=0.0059 ct=9.3070 rec=1.1686 | train/val/test=1.000/0.600/0.610 | c=0.998347
[Epoch 0063] loss=11.6637 cls=0.0138 smmd=-0.0080 ct=9.3205 rec=1.1686 | train/val/test=1.000/0.596/0.602 | c=0.998347
[Epoch 0064] loss=11.6544 cls=0.0138 smmd=0.0030 ct=9.3015 rec=1.1681 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0065] loss=11.6443 cls=0.0125 smmd=-0.0114 ct=9.3093 rec=1.1670 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0066] loss=11.6387 cls=0.0126 smmd=-0.0146 ct=9.3060 rec=1.1673 | train/val/test=1.000/0.594/0.603 | c=0.998347
[Epoch 0067] loss=11.6442 cls=0.0129 smmd=-0.0029 ct=9.2969 rec=1.1686 | train/val/test=1.000/0.608/0.608 | c=0.998347
[Epoch 0068] loss=11.6497 cls=0.0133 smmd=-0.0127 ct=9.3102 rec=1.1694 | train/val/test=1.000/0.594/0.601 | c=0.998347
[Epoch 0069] loss=11.6456 cls=0.0138 smmd=-0.0046 ct=9.2947 rec=1.1709 | train/val/test=1.000/0.604/0.607 | c=0.998347
[Epoch 0070] loss=11.6435 cls=0.0135 smmd=-0.0206 ct=9.3100 rec=1.1703 | train/val/test=1.000/0.594/0.599 | c=0.998347
[Epoch 0071] loss=11.6460 cls=0.0140 smmd=-0.0049 ct=9.2937 rec=1.1716 | train/val/test=1.000/0.606/0.606 | c=0.998347
[Epoch 0072] loss=11.6470 cls=0.0138 smmd=-0.0101 ct=9.3019 rec=1.1707 | train/val/test=1.000/0.594/0.600 | c=0.998347
[Epoch 0073] loss=11.6558 cls=0.0140 smmd=-0.0025 ct=9.2996 rec=1.1723 | train/val/test=1.000/0.602/0.597 | c=0.998347
[Epoch 0074] loss=11.6582 cls=0.0150 smmd=-0.0024 ct=9.3012 rec=1.1722 | train/val/test=1.000/0.598/0.604 | c=0.998347
[Epoch 0075] loss=11.6580 cls=0.0133 smmd=-0.0042 ct=9.3061 rec=1.1714 | train/val/test=1.000/0.600/0.601 | c=0.998347
[Epoch 0076] loss=11.6322 cls=0.0125 smmd=-0.0042 ct=9.2886 rec=1.1676 | train/val/test=1.000/0.604/0.608 | c=0.998347
[Epoch 0077] loss=11.6196 cls=0.0111 smmd=-0.0205 ct=9.2976 rec=1.1657 | train/val/test=1.000/0.594/0.600 | c=0.998347
[Epoch 0078] loss=11.6334 cls=0.0117 smmd=-0.0059 ct=9.2931 rec=1.1673 | train/val/test=1.000/0.604/0.601 | c=0.998347
[Epoch 0079] loss=11.6452 cls=0.0131 smmd=-0.0039 ct=9.2974 rec=1.1693 | train/val/test=1.000/0.592/0.604 | c=0.998347
[Epoch 0080] loss=11.6328 cls=0.0127 smmd=-0.0115 ct=9.2922 rec=1.1697 | train/val/test=1.000/0.598/0.604 | c=0.998347
[Epoch 0081] loss=11.6197 cls=0.0127 smmd=-0.0178 ct=9.2870 rec=1.1689 | train/val/test=1.000/0.598/0.605 | c=0.998347
[Epoch 0082] loss=11.6115 cls=0.0132 smmd=-0.0286 ct=9.2874 rec=1.1698 | train/val/test=1.000/0.594/0.603 | c=0.998347
[Epoch 0083] loss=11.6253 cls=0.0141 smmd=-0.0188 ct=9.2856 rec=1.1721 | train/val/test=1.000/0.602/0.602 | c=0.998347
[Epoch 0084] loss=11.6354 cls=0.0153 smmd=-0.0179 ct=9.2913 rec=1.1734 | train/val/test=1.000/0.590/0.601 | c=0.998347
[Epoch 0085] loss=11.6424 cls=0.0154 smmd=-0.0112 ct=9.2887 rec=1.1747 | train/val/test=1.000/0.608/0.606 | c=0.998347
[Epoch 0086] loss=11.6386 cls=0.0152 smmd=-0.0143 ct=9.2929 rec=1.1724 | train/val/test=1.000/0.590/0.600 | c=0.998347
[Epoch 0087] loss=11.6224 cls=0.0130 smmd=-0.0158 ct=9.2856 rec=1.1698 | train/val/test=1.000/0.602/0.606 | c=0.998347
[Epoch 0088] loss=11.6047 cls=0.0117 smmd=-0.0242 ct=9.2836 rec=1.1667 | train/val/test=1.000/0.602/0.607 | c=0.998347
[Epoch 0089] loss=11.6035 cls=0.0115 smmd=-0.0239 ct=9.2829 rec=1.1665 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0090] loss=11.6182 cls=0.0119 smmd=-0.0140 ct=9.2843 rec=1.1680 | train/val/test=1.000/0.608/0.607 | c=0.998347
[Epoch 0091] loss=11.6108 cls=0.0122 smmd=-0.0222 ct=9.2850 rec=1.1679 | train/val/test=1.000/0.594/0.605 | c=0.998347
[Epoch 0092] loss=11.6019 cls=0.0120 smmd=-0.0256 ct=9.2792 rec=1.1681 | train/val/test=1.000/0.602/0.603 | c=0.998347
[Epoch 0093] loss=11.6025 cls=0.0125 smmd=-0.0271 ct=9.2794 rec=1.1689 | train/val/test=1.000/0.604/0.606 | c=0.998347
[Epoch 0094] loss=11.6101 cls=0.0136 smmd=-0.0227 ct=9.2779 rec=1.1707 | train/val/test=1.000/0.600/0.606 | c=0.998347
[Epoch 0095] loss=11.6229 cls=0.0145 smmd=-0.0229 ct=9.2850 rec=1.1732 | train/val/test=1.000/0.596/0.597 | c=0.998347
[Epoch 0096] loss=11.6364 cls=0.0163 smmd=-0.0083 ct=9.2781 rec=1.1751 | train/val/test=1.000/0.596/0.615 | c=0.998347
[Epoch 0097] loss=11.6651 cls=0.0169 smmd=-0.0150 ct=9.3051 rec=1.1790 | train/val/test=1.000/0.574/0.577 | c=0.998347
[Epoch 0098] loss=11.7226 cls=0.0233 smmd=0.0469 ct=9.2839 rec=1.1843 | train/val/test=1.000/0.610/0.642 | c=0.998347
[Epoch 0099] loss=11.7674 cls=0.0239 smmd=0.0197 ct=9.3515 rec=1.1862 | train/val/test=1.000/0.580/0.581 | c=0.998347
=== Best @ epoch 24: val=0.6140, test=0.6030 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 - 2025-09-21 06:34:44:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1909 cls=1.9442 smmd=4.1983 ct=9.2706 rec=1.3889 | train/val/test=0.276/0.172/0.227 | c=0.998347
[Epoch 0001] loss=16.6879 cls=1.8943 smmd=2.7974 ct=9.2171 rec=1.3895 | train/val/test=0.552/0.390/0.416 | c=0.998347
[Epoch 0002] loss=15.1050 cls=1.7772 smmd=1.4480 ct=9.1020 rec=1.3889 | train/val/test=0.897/0.378/0.430 | c=0.998347
[Epoch 0003] loss=14.8714 cls=1.5354 smmd=1.5182 ct=9.0423 rec=1.3878 | train/val/test=0.793/0.538/0.552 | c=0.998347
[Epoch 0004] loss=15.5330 cls=1.1970 smmd=1.7988 ct=9.7712 rec=1.3830 | train/val/test=0.897/0.418/0.441 | c=0.998347
[Epoch 0005] loss=14.6738 cls=0.7963 smmd=1.7291 ct=9.4149 rec=1.3668 | train/val/test=0.931/0.408/0.441 | c=0.998347
[Epoch 0006] loss=14.0454 cls=0.5183 smmd=1.5145 ct=9.3270 rec=1.3428 | train/val/test=0.897/0.536/0.557 | c=0.998347
[Epoch 0007] loss=13.5055 cls=0.3120 smmd=1.2934 ct=9.2795 rec=1.3103 | train/val/test=0.931/0.574/0.598 | c=0.998347
[Epoch 0008] loss=13.2999 cls=0.2060 smmd=1.1971 ct=9.3242 rec=1.2863 | train/val/test=1.000/0.548/0.577 | c=0.998347
[Epoch 0009] loss=13.2308 cls=0.1040 smmd=1.2341 ct=9.3542 rec=1.2693 | train/val/test=1.000/0.442/0.482 | c=0.998347
[Epoch 0010] loss=13.1705 cls=0.0630 smmd=1.1887 ct=9.4020 rec=1.2585 | train/val/test=1.000/0.458/0.498 | c=0.998347
[Epoch 0011] loss=13.0308 cls=0.0407 smmd=1.0785 ct=9.4225 rec=1.2445 | train/val/test=1.000/0.568/0.577 | c=0.998347
[Epoch 0012] loss=12.8194 cls=0.0222 smmd=0.9187 ct=9.4223 rec=1.2281 | train/val/test=1.000/0.600/0.618 | c=0.998347
[Epoch 0013] loss=12.6350 cls=0.0143 smmd=0.7704 ct=9.4158 rec=1.2173 | train/val/test=1.000/0.588/0.610 | c=0.998347
[Epoch 0014] loss=12.5553 cls=0.0090 smmd=0.7417 ct=9.3833 rec=1.2107 | train/val/test=1.000/0.552/0.566 | c=0.998347
[Epoch 0015] loss=12.5491 cls=0.0070 smmd=0.7686 ct=9.3569 rec=1.2082 | train/val/test=1.000/0.538/0.557 | c=0.998347
[Epoch 0016] loss=12.4919 cls=0.0063 smmd=0.7184 ct=9.3547 rec=1.2063 | train/val/test=1.000/0.566/0.576 | c=0.998347
[Epoch 0017] loss=12.3935 cls=0.0049 smmd=0.6191 ct=9.3649 rec=1.2023 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0018] loss=12.3114 cls=0.0051 smmd=0.5218 ct=9.3846 rec=1.2000 | train/val/test=1.000/0.596/0.600 | c=0.998347
[Epoch 0019] loss=12.2273 cls=0.0062 smmd=0.4336 ct=9.3864 rec=1.2006 | train/val/test=1.000/0.578/0.584 | c=0.998347
[Epoch 0020] loss=12.2350 cls=0.0085 smmd=0.4333 ct=9.3856 rec=1.2038 | train/val/test=1.000/0.586/0.584 | c=0.998347
[Epoch 0021] loss=12.2019 cls=0.0106 smmd=0.3932 ct=9.3890 rec=1.2045 | train/val/test=1.000/0.600/0.601 | c=0.998347
[Epoch 0022] loss=12.1272 cls=0.0134 smmd=0.3136 ct=9.3926 rec=1.2038 | train/val/test=1.000/0.598/0.595 | c=0.998347
[Epoch 0023] loss=12.0741 cls=0.0164 smmd=0.2649 ct=9.3848 rec=1.2040 | train/val/test=1.000/0.596/0.588 | c=0.998347
[Epoch 0024] loss=12.0558 cls=0.0193 smmd=0.2466 ct=9.3828 rec=1.2036 | train/val/test=1.000/0.614/0.603 | c=0.998347
[Epoch 0025] loss=12.0359 cls=0.0212 smmd=0.2282 ct=9.3882 rec=1.1991 | train/val/test=1.000/0.612/0.605 | c=0.998347
[Epoch 0026] loss=12.0047 cls=0.0225 smmd=0.2090 ct=9.3827 rec=1.1953 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0027] loss=11.9570 cls=0.0242 smmd=0.1652 ct=9.3829 rec=1.1924 | train/val/test=1.000/0.612/0.611 | c=0.998347
[Epoch 0028] loss=11.9594 cls=0.0253 smmd=0.1645 ct=9.3946 rec=1.1875 | train/val/test=1.000/0.612/0.604 | c=0.998347
[Epoch 0029] loss=11.9188 cls=0.0239 smmd=0.1418 ct=9.3862 rec=1.1835 | train/val/test=1.000/0.614/0.603 | c=0.998347
[Epoch 0030] loss=11.8813 cls=0.0211 smmd=0.1284 ct=9.3725 rec=1.1796 | train/val/test=1.000/0.606/0.598 | c=0.998347
[Epoch 0031] loss=11.8683 cls=0.0187 smmd=0.1333 ct=9.3644 rec=1.1760 | train/val/test=1.000/0.608/0.598 | c=0.998347
[Epoch 0032] loss=11.8370 cls=0.0169 smmd=0.1074 ct=9.3656 rec=1.1735 | train/val/test=1.000/0.608/0.604 | c=0.998347
[Epoch 0033] loss=11.8209 cls=0.0149 smmd=0.0930 ct=9.3700 rec=1.1715 | train/val/test=1.000/0.602/0.604 | c=0.998347
[Epoch 0034] loss=11.8077 cls=0.0135 smmd=0.0887 ct=9.3653 rec=1.1701 | train/val/test=1.000/0.604/0.606 | c=0.998347
[Epoch 0035] loss=11.7873 cls=0.0124 smmd=0.0741 ct=9.3615 rec=1.1696 | train/val/test=1.000/0.604/0.605 | c=0.998347
[Epoch 0036] loss=11.7914 cls=0.0117 smmd=0.0826 ct=9.3579 rec=1.1696 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0037] loss=11.7681 cls=0.0114 smmd=0.0639 ct=9.3526 rec=1.1701 | train/val/test=1.000/0.604/0.602 | c=0.998347
[Epoch 0038] loss=11.7541 cls=0.0114 smmd=0.0484 ct=9.3536 rec=1.1703 | train/val/test=1.000/0.604/0.603 | c=0.998347
[Epoch 0039] loss=11.7504 cls=0.0114 smmd=0.0427 ct=9.3556 rec=1.1703 | train/val/test=1.000/0.598/0.600 | c=0.998347
[Epoch 0040] loss=11.7389 cls=0.0116 smmd=0.0315 ct=9.3548 rec=1.1705 | train/val/test=1.000/0.602/0.601 | c=0.998347
[Epoch 0041] loss=11.7385 cls=0.0117 smmd=0.0367 ct=9.3507 rec=1.1697 | train/val/test=1.000/0.596/0.604 | c=0.998347
[Epoch 0042] loss=11.7254 cls=0.0119 smmd=0.0274 ct=9.3468 rec=1.1697 | train/val/test=1.000/0.598/0.601 | c=0.998347
[Epoch 0043] loss=11.7222 cls=0.0125 smmd=0.0308 ct=9.3425 rec=1.1683 | train/val/test=1.000/0.596/0.603 | c=0.998347
[Epoch 0044] loss=11.7193 cls=0.0129 smmd=0.0230 ct=9.3470 rec=1.1682 | train/val/test=1.000/0.596/0.600 | c=0.998347
[Epoch 0045] loss=11.7132 cls=0.0141 smmd=0.0235 ct=9.3410 rec=1.1673 | train/val/test=1.000/0.602/0.608 | c=0.998347
[Epoch 0046] loss=11.7161 cls=0.0140 smmd=0.0179 ct=9.3490 rec=1.1676 | train/val/test=1.000/0.590/0.596 | c=0.998347
[Epoch 0047] loss=11.7200 cls=0.0165 smmd=0.0378 ct=9.3297 rec=1.1680 | train/val/test=1.000/0.602/0.617 | c=0.998347
[Epoch 0048] loss=11.7362 cls=0.0159 smmd=0.0225 ct=9.3558 rec=1.1710 | train/val/test=1.000/0.588/0.587 | c=0.998347
[Epoch 0049] loss=11.7522 cls=0.0194 smmd=0.0607 ct=9.3289 rec=1.1716 | train/val/test=1.000/0.602/0.616 | c=0.998347
[Epoch 0050] loss=11.7327 cls=0.0137 smmd=0.0225 ct=9.3577 rec=1.1694 | train/val/test=1.000/0.598/0.601 | c=0.998347
[Epoch 0051] loss=11.6859 cls=0.0103 smmd=0.0235 ct=9.3254 rec=1.1634 | train/val/test=1.000/0.594/0.604 | c=0.998347
[Epoch 0052] loss=11.6747 cls=0.0093 smmd=0.0166 ct=9.3234 rec=1.1627 | train/val/test=1.000/0.604/0.614 | c=0.998347
[Epoch 0053] loss=11.6959 cls=0.0107 smmd=0.0116 ct=9.3411 rec=1.1663 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0054] loss=11.6891 cls=0.0102 smmd=0.0301 ct=9.3163 rec=1.1662 | train/val/test=1.000/0.598/0.603 | c=0.998347
[Epoch 0055] loss=11.6680 cls=0.0098 smmd=0.0026 ct=9.3262 rec=1.1647 | train/val/test=1.000/0.602/0.608 | c=0.998347
[Epoch 0056] loss=11.6722 cls=0.0107 smmd=-0.0034 ct=9.3315 rec=1.1667 | train/val/test=1.000/0.588/0.601 | c=0.998347
[Epoch 0057] loss=11.6870 cls=0.0131 smmd=0.0170 ct=9.3163 rec=1.1703 | train/val/test=1.000/0.602/0.611 | c=0.998347
[Epoch 0058] loss=11.6808 cls=0.0127 smmd=-0.0040 ct=9.3334 rec=1.1693 | train/val/test=1.000/0.594/0.602 | c=0.998347
[Epoch 0059] loss=11.6630 cls=0.0130 smmd=0.0031 ct=9.3104 rec=1.1682 | train/val/test=1.000/0.596/0.602 | c=0.998347
[Epoch 0060] loss=11.6590 cls=0.0125 smmd=-0.0027 ct=9.3157 rec=1.1668 | train/val/test=1.000/0.600/0.606 | c=0.998347
[Epoch 0061] loss=11.6568 cls=0.0131 smmd=-0.0097 ct=9.3186 rec=1.1674 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0062] loss=11.6647 cls=0.0145 smmd=0.0059 ct=9.3070 rec=1.1686 | train/val/test=1.000/0.600/0.610 | c=0.998347
[Epoch 0063] loss=11.6637 cls=0.0138 smmd=-0.0080 ct=9.3205 rec=1.1686 | train/val/test=1.000/0.596/0.602 | c=0.998347
[Epoch 0064] loss=11.6544 cls=0.0138 smmd=0.0030 ct=9.3015 rec=1.1681 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0065] loss=11.6443 cls=0.0125 smmd=-0.0114 ct=9.3093 rec=1.1670 | train/val/test=1.000/0.600/0.603 | c=0.998347
[Epoch 0066] loss=11.6387 cls=0.0126 smmd=-0.0146 ct=9.3060 rec=1.1673 | train/val/test=1.000/0.594/0.603 | c=0.998347
[Epoch 0067] loss=11.6442 cls=0.0129 smmd=-0.0029 ct=9.2969 rec=1.1686 | train/val/test=1.000/0.608/0.608 | c=0.998347
[Epoch 0068] loss=11.6497 cls=0.0133 smmd=-0.0127 ct=9.3102 rec=1.1694 | train/val/test=1.000/0.594/0.601 | c=0.998347
[Epoch 0069] loss=11.6456 cls=0.0138 smmd=-0.0046 ct=9.2947 rec=1.1709 | train/val/test=1.000/0.604/0.607 | c=0.998347
[Epoch 0070] loss=11.6435 cls=0.0135 smmd=-0.0206 ct=9.3100 rec=1.1703 | train/val/test=1.000/0.594/0.599 | c=0.998347
[Epoch 0071] loss=11.6460 cls=0.0140 smmd=-0.0049 ct=9.2937 rec=1.1716 | train/val/test=1.000/0.606/0.606 | c=0.998347
[Epoch 0072] loss=11.6470 cls=0.0138 smmd=-0.0101 ct=9.3019 rec=1.1707 | train/val/test=1.000/0.594/0.600 | c=0.998347
[Epoch 0073] loss=11.6558 cls=0.0140 smmd=-0.0025 ct=9.2996 rec=1.1723 | train/val/test=1.000/0.602/0.597 | c=0.998347
[Epoch 0074] loss=11.6582 cls=0.0150 smmd=-0.0024 ct=9.3012 rec=1.1722 | train/val/test=1.000/0.598/0.604 | c=0.998347
[Epoch 0075] loss=11.6580 cls=0.0133 smmd=-0.0042 ct=9.3061 rec=1.1714 | train/val/test=1.000/0.600/0.601 | c=0.998347
[Epoch 0076] loss=11.6322 cls=0.0125 smmd=-0.0042 ct=9.2886 rec=1.1676 | train/val/test=1.000/0.604/0.608 | c=0.998347
[Epoch 0077] loss=11.6196 cls=0.0111 smmd=-0.0205 ct=9.2976 rec=1.1657 | train/val/test=1.000/0.594/0.600 | c=0.998347
[Epoch 0078] loss=11.6334 cls=0.0117 smmd=-0.0059 ct=9.2931 rec=1.1673 | train/val/test=1.000/0.604/0.601 | c=0.998347
[Epoch 0079] loss=11.6452 cls=0.0131 smmd=-0.0039 ct=9.2974 rec=1.1693 | train/val/test=1.000/0.592/0.604 | c=0.998347
[Epoch 0080] loss=11.6328 cls=0.0127 smmd=-0.0115 ct=9.2922 rec=1.1697 | train/val/test=1.000/0.598/0.604 | c=0.998347
[Epoch 0081] loss=11.6197 cls=0.0127 smmd=-0.0178 ct=9.2870 rec=1.1689 | train/val/test=1.000/0.598/0.605 | c=0.998347
[Epoch 0082] loss=11.6115 cls=0.0132 smmd=-0.0286 ct=9.2874 rec=1.1698 | train/val/test=1.000/0.594/0.603 | c=0.998347
[Epoch 0083] loss=11.6253 cls=0.0141 smmd=-0.0188 ct=9.2856 rec=1.1721 | train/val/test=1.000/0.602/0.602 | c=0.998347
[Epoch 0084] loss=11.6354 cls=0.0153 smmd=-0.0179 ct=9.2913 rec=1.1734 | train/val/test=1.000/0.590/0.601 | c=0.998347
[Epoch 0085] loss=11.6424 cls=0.0154 smmd=-0.0112 ct=9.2887 rec=1.1747 | train/val/test=1.000/0.608/0.606 | c=0.998347
[Epoch 0086] loss=11.6386 cls=0.0152 smmd=-0.0143 ct=9.2929 rec=1.1724 | train/val/test=1.000/0.590/0.600 | c=0.998347
[Epoch 0087] loss=11.6224 cls=0.0130 smmd=-0.0158 ct=9.2856 rec=1.1698 | train/val/test=1.000/0.602/0.606 | c=0.998347
[Epoch 0088] loss=11.6047 cls=0.0117 smmd=-0.0242 ct=9.2836 rec=1.1667 | train/val/test=1.000/0.602/0.607 | c=0.998347
[Epoch 0089] loss=11.6035 cls=0.0115 smmd=-0.0239 ct=9.2829 rec=1.1665 | train/val/test=1.000/0.592/0.602 | c=0.998347
[Epoch 0090] loss=11.6182 cls=0.0119 smmd=-0.0140 ct=9.2843 rec=1.1680 | train/val/test=1.000/0.608/0.607 | c=0.998347
[Epoch 0091] loss=11.6108 cls=0.0122 smmd=-0.0222 ct=9.2850 rec=1.1679 | train/val/test=1.000/0.594/0.605 | c=0.998347
[Epoch 0092] loss=11.6019 cls=0.0120 smmd=-0.0256 ct=9.2792 rec=1.1681 | train/val/test=1.000/0.602/0.603 | c=0.998347
[Epoch 0093] loss=11.6025 cls=0.0125 smmd=-0.0271 ct=9.2794 rec=1.1689 | train/val/test=1.000/0.604/0.606 | c=0.998347
[Epoch 0094] loss=11.6101 cls=0.0136 smmd=-0.0227 ct=9.2779 rec=1.1707 | train/val/test=1.000/0.600/0.606 | c=0.998347
[Epoch 0095] loss=11.6229 cls=0.0145 smmd=-0.0229 ct=9.2850 rec=1.1732 | train/val/test=1.000/0.596/0.597 | c=0.998347
[Epoch 0096] loss=11.6364 cls=0.0163 smmd=-0.0083 ct=9.2781 rec=1.1751 | train/val/test=1.000/0.596/0.615 | c=0.998347
[Epoch 0097] loss=11.6651 cls=0.0169 smmd=-0.0150 ct=9.3051 rec=1.1790 | train/val/test=1.000/0.574/0.577 | c=0.998347
[Epoch 0098] loss=11.7226 cls=0.0233 smmd=0.0469 ct=9.2839 rec=1.1843 | train/val/test=1.000/0.610/0.642 | c=0.998347
[Epoch 0099] loss=11.7674 cls=0.0239 smmd=0.0197 ct=9.3515 rec=1.1862 | train/val/test=1.000/0.580/0.581 | c=0.998347
=== Best @ epoch 24: val=0.6140, test=0.6030 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-4 completed in 25.77 seconds.
==================================================
