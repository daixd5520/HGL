Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 - 2025-09-21 06:35:50:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2052 cls=1.9445 smmd=4.2271 ct=9.2558 rec=1.3889 | train/val/test=0.448/0.264/0.268 | c=0.998347
[Epoch 0001] loss=16.6972 cls=1.9003 smmd=2.8325 ct=9.1858 rec=1.3893 | train/val/test=0.707/0.394/0.397 | c=0.998347
[Epoch 0002] loss=15.2006 cls=1.7666 smmd=1.5775 ct=9.0796 rec=1.3885 | train/val/test=0.724/0.428/0.433 | c=0.998347
[Epoch 0003] loss=14.8285 cls=1.5166 smmd=1.5106 ct=9.0291 rec=1.3861 | train/val/test=0.914/0.564/0.565 | c=0.998347
[Epoch 0004] loss=14.4908 cls=1.1618 smmd=1.6814 ct=8.8969 rec=1.3754 | train/val/test=0.845/0.530/0.528 | c=0.998347
[Epoch 0005] loss=14.0066 cls=0.8036 smmd=1.6578 ct=8.8445 rec=1.3504 | train/val/test=0.914/0.588/0.572 | c=0.998347
[Epoch 0006] loss=13.3128 cls=0.5149 smmd=1.3606 ct=8.8099 rec=1.3137 | train/val/test=0.966/0.638/0.624 | c=0.998347
[Epoch 0007] loss=12.7453 cls=0.3094 smmd=1.0762 ct=8.7995 rec=1.2801 | train/val/test=0.983/0.658/0.653 | c=0.998347
[Epoch 0008] loss=12.5087 cls=0.1778 smmd=1.0379 ct=8.7909 rec=1.2510 | train/val/test=0.983/0.658/0.647 | c=0.998347
[Epoch 0009] loss=12.4584 cls=0.1014 smmd=1.1245 ct=8.7805 rec=1.2261 | train/val/test=0.983/0.668/0.646 | c=0.998347
[Epoch 0010] loss=12.3875 cls=0.0598 smmd=1.1376 ct=8.7749 rec=1.2076 | train/val/test=1.000/0.672/0.666 | c=0.998347
[Epoch 0011] loss=12.1819 cls=0.0353 smmd=0.9863 ct=8.7709 rec=1.1947 | train/val/test=1.000/0.688/0.673 | c=0.998347
[Epoch 0012] loss=12.0070 cls=0.0217 smmd=0.8420 ct=8.7708 rec=1.1863 | train/val/test=1.000/0.700/0.684 | c=0.998347
[Epoch 0013] loss=11.9038 cls=0.0149 smmd=0.7572 ct=8.7702 rec=1.1807 | train/val/test=1.000/0.704/0.679 | c=0.998347
[Epoch 0014] loss=12.5341 cls=0.0116 smmd=0.6966 ct=9.4712 rec=1.1773 | train/val/test=1.000/0.714/0.701 | c=0.998347
[Epoch 0015] loss=12.4840 cls=0.0098 smmd=0.7132 ct=9.4106 rec=1.1752 | train/val/test=1.000/0.720/0.708 | c=0.998347
[Epoch 0016] loss=12.4053 cls=0.0096 smmd=0.6860 ct=9.3578 rec=1.1759 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0017] loss=12.3299 cls=0.0087 smmd=0.6146 ct=9.3546 rec=1.1760 | train/val/test=1.000/0.706/0.703 | c=0.998347
[Epoch 0018] loss=12.2351 cls=0.0090 smmd=0.4752 ct=9.3965 rec=1.1772 | train/val/test=1.000/0.722/0.708 | c=0.998347
[Epoch 0019] loss=12.2097 cls=0.0109 smmd=0.4106 ct=9.4285 rec=1.1799 | train/val/test=1.000/0.716/0.709 | c=0.998347
[Epoch 0020] loss=12.1910 cls=0.0140 smmd=0.3869 ct=9.4223 rec=1.1838 | train/val/test=1.000/0.716/0.711 | c=0.998347
[Epoch 0021] loss=12.1310 cls=0.0167 smmd=0.3367 ct=9.4066 rec=1.1856 | train/val/test=1.000/0.720/0.718 | c=0.998347
[Epoch 0022] loss=12.1047 cls=0.0204 smmd=0.3099 ct=9.4007 rec=1.1868 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0023] loss=12.0657 cls=0.0228 smmd=0.2583 ct=9.4111 rec=1.1867 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0024] loss=12.0519 cls=0.0270 smmd=0.2425 ct=9.4136 rec=1.1844 | train/val/test=1.000/0.720/0.719 | c=0.998347
[Epoch 0025] loss=12.0141 cls=0.0303 smmd=0.2098 ct=9.4080 rec=1.1830 | train/val/test=1.000/0.726/0.726 | c=0.998347
[Epoch 0026] loss=12.0120 cls=0.0350 smmd=0.2106 ct=9.4084 rec=1.1791 | train/val/test=1.000/0.728/0.728 | c=0.998347
[Epoch 0027] loss=11.9779 cls=0.0362 smmd=0.1875 ct=9.3947 rec=1.1797 | train/val/test=1.000/0.712/0.702 | c=0.998347
[Epoch 0028] loss=11.9834 cls=0.0425 smmd=0.1729 ct=9.4135 rec=1.1773 | train/val/test=1.000/0.726/0.739 | c=0.998347
[Epoch 0029] loss=11.9544 cls=0.0327 smmd=0.1805 ct=9.3899 rec=1.1757 | train/val/test=1.000/0.728/0.718 | c=0.998347
[Epoch 0030] loss=11.8934 cls=0.0250 smmd=0.1388 ct=9.3940 rec=1.1678 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0031] loss=11.8698 cls=0.0214 smmd=0.1304 ct=9.3863 rec=1.1659 | train/val/test=1.000/0.730/0.732 | c=0.998347
[Epoch 0032] loss=11.8478 cls=0.0183 smmd=0.1319 ct=9.3667 rec=1.1655 | train/val/test=1.000/0.728/0.730 | c=0.998347
[Epoch 0033] loss=11.8270 cls=0.0159 smmd=0.1208 ct=9.3648 rec=1.1628 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0034] loss=11.8201 cls=0.0178 smmd=0.0989 ct=9.3745 rec=1.1645 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0035] loss=11.7998 cls=0.0157 smmd=0.0855 ct=9.3683 rec=1.1651 | train/val/test=1.000/0.726/0.729 | c=0.998347
[Epoch 0036] loss=11.7860 cls=0.0152 smmd=0.0786 ct=9.3634 rec=1.1644 | train/val/test=1.000/0.714/0.725 | c=0.998347
[Epoch 0037] loss=11.7871 cls=0.0173 smmd=0.0743 ct=9.3618 rec=1.1669 | train/val/test=1.000/0.718/0.720 | c=0.998347
[Epoch 0038] loss=11.7814 cls=0.0167 smmd=0.0829 ct=9.3446 rec=1.1686 | train/val/test=1.000/0.714/0.728 | c=0.998347
[Epoch 0039] loss=11.7706 cls=0.0174 smmd=0.0667 ct=9.3506 rec=1.1679 | train/val/test=1.000/0.712/0.721 | c=0.998347
[Epoch 0040] loss=11.7500 cls=0.0176 smmd=0.0428 ct=9.3531 rec=1.1682 | train/val/test=1.000/0.722/0.727 | c=0.998347
[Epoch 0041] loss=11.7581 cls=0.0189 smmd=0.0530 ct=9.3458 rec=1.1702 | train/val/test=1.000/0.712/0.719 | c=0.998347
[Epoch 0042] loss=11.7548 cls=0.0215 smmd=0.0399 ct=9.3533 rec=1.1700 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0043] loss=11.7462 cls=0.0203 smmd=0.0543 ct=9.3335 rec=1.1691 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0044] loss=11.7278 cls=0.0215 smmd=0.0360 ct=9.3367 rec=1.1668 | train/val/test=1.000/0.718/0.730 | c=0.998347
[Epoch 0045] loss=11.7171 cls=0.0203 smmd=0.0337 ct=9.3318 rec=1.1657 | train/val/test=1.000/0.718/0.732 | c=0.998347
[Epoch 0046] loss=11.7146 cls=0.0208 smmd=0.0335 ct=9.3283 rec=1.1661 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0047] loss=11.7113 cls=0.0212 smmd=0.0227 ct=9.3347 rec=1.1664 | train/val/test=1.000/0.718/0.730 | c=0.998347
[Epoch 0048] loss=11.7040 cls=0.0206 smmd=0.0273 ct=9.3227 rec=1.1667 | train/val/test=1.000/0.716/0.723 | c=0.998347
[Epoch 0049] loss=11.6955 cls=0.0196 smmd=0.0212 ct=9.3225 rec=1.1661 | train/val/test=1.000/0.720/0.733 | c=0.998347
[Epoch 0050] loss=11.6821 cls=0.0191 smmd=0.0143 ct=9.3165 rec=1.1661 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0051] loss=11.6879 cls=0.0191 smmd=0.0204 ct=9.3149 rec=1.1667 | train/val/test=1.000/0.720/0.731 | c=0.998347
[Epoch 0052] loss=11.6907 cls=0.0200 smmd=0.0166 ct=9.3190 rec=1.1676 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0053] loss=11.6964 cls=0.0207 smmd=0.0286 ct=9.3098 rec=1.1687 | train/val/test=1.000/0.718/0.729 | c=0.998347
[Epoch 0054] loss=11.7003 cls=0.0221 smmd=0.0208 ct=9.3183 rec=1.1696 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0055] loss=11.7192 cls=0.0245 smmd=0.0382 ct=9.3138 rec=1.1713 | train/val/test=1.000/0.718/0.731 | c=0.998347
[Epoch 0056] loss=11.7264 cls=0.0256 smmd=0.0363 ct=9.3211 rec=1.1717 | train/val/test=1.000/0.702/0.708 | c=0.998347
[Epoch 0057] loss=11.7095 cls=0.0239 smmd=0.0324 ct=9.3126 rec=1.1703 | train/val/test=1.000/0.720/0.735 | c=0.998347
[Epoch 0058] loss=11.6708 cls=0.0175 smmd=0.0242 ct=9.2987 rec=1.1652 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0059] loss=11.6537 cls=0.0167 smmd=0.0077 ct=9.3020 rec=1.1637 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0060] loss=11.6882 cls=0.0186 smmd=0.0317 ct=9.3043 rec=1.1668 | train/val/test=1.000/0.720/0.728 | c=0.998347
[Epoch 0061] loss=11.6808 cls=0.0203 smmd=0.0182 ct=9.3081 rec=1.1671 | train/val/test=1.000/0.714/0.722 | c=0.998347
[Epoch 0062] loss=11.6569 cls=0.0164 smmd=0.0153 ct=9.2947 rec=1.1652 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0063] loss=11.6553 cls=0.0174 smmd=0.0113 ct=9.2933 rec=1.1667 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0064] loss=11.6730 cls=0.0225 smmd=0.0125 ct=9.2964 rec=1.1708 | train/val/test=1.000/0.710/0.707 | c=0.998347
[Epoch 0065] loss=11.6905 cls=0.0236 smmd=0.0240 ct=9.2971 rec=1.1729 | train/val/test=1.000/0.718/0.727 | c=0.998347
[Epoch 0066] loss=11.6745 cls=0.0232 smmd=0.0105 ct=9.2970 rec=1.1719 | train/val/test=1.000/0.712/0.719 | c=0.998347
[Epoch 0067] loss=11.6493 cls=0.0198 smmd=0.0065 ct=9.2866 rec=1.1682 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0068] loss=11.6370 cls=0.0191 smmd=-0.0006 ct=9.2840 rec=1.1672 | train/val/test=1.000/0.722/0.728 | c=0.998347
[Epoch 0069] loss=11.6489 cls=0.0202 smmd=0.0038 ct=9.2885 rec=1.1682 | train/val/test=1.000/0.712/0.715 | c=0.998347
[Epoch 0070] loss=11.6571 cls=0.0213 smmd=0.0169 ct=9.2819 rec=1.1685 | train/val/test=1.000/0.716/0.728 | c=0.998347
[Epoch 0071] loss=11.6376 cls=0.0197 smmd=-0.0011 ct=9.2843 rec=1.1673 | train/val/test=1.000/0.718/0.726 | c=0.998347
[Epoch 0072] loss=11.6298 cls=0.0184 smmd=0.0010 ct=9.2776 rec=1.1664 | train/val/test=1.000/0.710/0.716 | c=0.998347
[Epoch 0073] loss=11.6303 cls=0.0195 smmd=-0.0033 ct=9.2794 rec=1.1674 | train/val/test=1.000/0.716/0.730 | c=0.998347
[Epoch 0074] loss=11.6391 cls=0.0213 smmd=0.0020 ct=9.2773 rec=1.1692 | train/val/test=1.000/0.710/0.716 | c=0.998347
[Epoch 0075] loss=11.6400 cls=0.0213 smmd=0.0002 ct=9.2795 rec=1.1695 | train/val/test=1.000/0.718/0.731 | c=0.998347
[Epoch 0076] loss=11.6372 cls=0.0213 smmd=0.0071 ct=9.2686 rec=1.1702 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0077] loss=11.6508 cls=0.0228 smmd=-0.0006 ct=9.2860 rec=1.1713 | train/val/test=1.000/0.716/0.724 | c=0.998347
[Epoch 0078] loss=11.6865 cls=0.0264 smmd=0.0320 ct=9.2736 rec=1.1773 | train/val/test=1.000/0.694/0.702 | c=0.998347
[Epoch 0079] loss=11.7593 cls=0.0375 smmd=0.0320 ct=9.3221 rec=1.1839 | train/val/test=1.000/0.702/0.724 | c=0.998347
[Epoch 0080] loss=11.7959 cls=0.0372 smmd=0.0906 ct=9.2909 rec=1.1886 | train/val/test=1.000/0.720/0.712 | c=0.998347
[Epoch 0081] loss=11.6690 cls=0.0189 smmd=0.0183 ct=9.2958 rec=1.1680 | train/val/test=1.000/0.718/0.726 | c=0.998347
[Epoch 0082] loss=11.6291 cls=0.0141 smmd=0.0067 ct=9.2844 rec=1.1620 | train/val/test=1.000/0.708/0.724 | c=0.998347
[Epoch 0083] loss=11.6991 cls=0.0158 smmd=0.0651 ct=9.2804 rec=1.1689 | train/val/test=1.000/0.718/0.723 | c=0.998347
[Epoch 0084] loss=11.6078 cls=0.0114 smmd=0.0011 ct=9.2759 rec=1.1597 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0085] loss=11.6717 cls=0.0160 smmd=0.0240 ct=9.2978 rec=1.1669 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0086] loss=11.6512 cls=0.0139 smmd=0.0378 ct=9.2675 rec=1.1660 | train/val/test=1.000/0.724/0.725 | c=0.998347
[Epoch 0087] loss=11.6127 cls=0.0140 smmd=0.0032 ct=9.2643 rec=1.1656 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0088] loss=11.6792 cls=0.0225 smmd=0.0086 ct=9.2958 rec=1.1761 | train/val/test=1.000/0.718/0.722 | c=0.998347
[Epoch 0089] loss=11.6669 cls=0.0213 smmd=0.0281 ct=9.2647 rec=1.1764 | train/val/test=1.000/0.720/0.720 | c=0.998347
[Epoch 0090] loss=11.6111 cls=0.0195 smmd=-0.0165 ct=9.2656 rec=1.1713 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0091] loss=11.6382 cls=0.0235 smmd=-0.0084 ct=9.2738 rec=1.1747 | train/val/test=1.000/0.714/0.725 | c=0.998347
[Epoch 0092] loss=11.6805 cls=0.0275 smmd=0.0253 ct=9.2661 rec=1.1808 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0093] loss=11.6779 cls=0.0295 smmd=0.0026 ct=9.2896 rec=1.1781 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0094] loss=11.6193 cls=0.0206 smmd=0.0012 ct=9.2576 rec=1.1700 | train/val/test=1.000/0.720/0.724 | c=0.998347
[Epoch 0095] loss=11.6020 cls=0.0179 smmd=-0.0072 ct=9.2584 rec=1.1664 | train/val/test=1.000/0.718/0.716 | c=0.998347
[Epoch 0096] loss=11.6321 cls=0.0205 smmd=-0.0071 ct=9.2804 rec=1.1691 | train/val/test=1.000/0.722/0.723 | c=0.998347
[Epoch 0097] loss=11.6105 cls=0.0176 smmd=0.0031 ct=9.2573 rec=1.1663 | train/val/test=1.000/0.724/0.720 | c=0.998347
[Epoch 0098] loss=11.5958 cls=0.0163 smmd=-0.0063 ct=9.2559 rec=1.1649 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0099] loss=11.6168 cls=0.0181 smmd=-0.0081 ct=9.2704 rec=1.1682 | train/val/test=1.000/0.722/0.723 | c=0.998347
=== Best @ epoch 31: val=0.7300, test=0.7320 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 - 2025-09-21 06:35:50:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2052 cls=1.9445 smmd=4.2271 ct=9.2558 rec=1.3889 | train/val/test=0.448/0.264/0.268 | c=0.998347
[Epoch 0001] loss=16.6972 cls=1.9003 smmd=2.8325 ct=9.1858 rec=1.3893 | train/val/test=0.707/0.394/0.397 | c=0.998347
[Epoch 0002] loss=15.2006 cls=1.7666 smmd=1.5775 ct=9.0796 rec=1.3885 | train/val/test=0.724/0.428/0.433 | c=0.998347
[Epoch 0003] loss=14.8285 cls=1.5166 smmd=1.5106 ct=9.0291 rec=1.3861 | train/val/test=0.914/0.564/0.565 | c=0.998347
[Epoch 0004] loss=14.4908 cls=1.1618 smmd=1.6814 ct=8.8969 rec=1.3754 | train/val/test=0.845/0.530/0.528 | c=0.998347
[Epoch 0005] loss=14.0066 cls=0.8036 smmd=1.6578 ct=8.8445 rec=1.3504 | train/val/test=0.914/0.588/0.572 | c=0.998347
[Epoch 0006] loss=13.3128 cls=0.5149 smmd=1.3606 ct=8.8099 rec=1.3137 | train/val/test=0.966/0.638/0.624 | c=0.998347
[Epoch 0007] loss=12.7453 cls=0.3094 smmd=1.0762 ct=8.7995 rec=1.2801 | train/val/test=0.983/0.658/0.653 | c=0.998347
[Epoch 0008] loss=12.5087 cls=0.1778 smmd=1.0379 ct=8.7909 rec=1.2510 | train/val/test=0.983/0.658/0.647 | c=0.998347
[Epoch 0009] loss=12.4584 cls=0.1014 smmd=1.1245 ct=8.7805 rec=1.2261 | train/val/test=0.983/0.668/0.646 | c=0.998347
[Epoch 0010] loss=12.3875 cls=0.0598 smmd=1.1376 ct=8.7749 rec=1.2076 | train/val/test=1.000/0.672/0.666 | c=0.998347
[Epoch 0011] loss=12.1819 cls=0.0353 smmd=0.9863 ct=8.7709 rec=1.1947 | train/val/test=1.000/0.688/0.673 | c=0.998347
[Epoch 0012] loss=12.0070 cls=0.0217 smmd=0.8420 ct=8.7708 rec=1.1863 | train/val/test=1.000/0.700/0.684 | c=0.998347
[Epoch 0013] loss=11.9038 cls=0.0149 smmd=0.7572 ct=8.7702 rec=1.1807 | train/val/test=1.000/0.704/0.679 | c=0.998347
[Epoch 0014] loss=12.5341 cls=0.0116 smmd=0.6966 ct=9.4712 rec=1.1773 | train/val/test=1.000/0.714/0.701 | c=0.998347
[Epoch 0015] loss=12.4840 cls=0.0098 smmd=0.7132 ct=9.4106 rec=1.1752 | train/val/test=1.000/0.720/0.708 | c=0.998347
[Epoch 0016] loss=12.4053 cls=0.0096 smmd=0.6860 ct=9.3578 rec=1.1759 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0017] loss=12.3299 cls=0.0087 smmd=0.6146 ct=9.3546 rec=1.1760 | train/val/test=1.000/0.706/0.703 | c=0.998347
[Epoch 0018] loss=12.2351 cls=0.0090 smmd=0.4752 ct=9.3965 rec=1.1772 | train/val/test=1.000/0.722/0.708 | c=0.998347
[Epoch 0019] loss=12.2097 cls=0.0109 smmd=0.4106 ct=9.4285 rec=1.1799 | train/val/test=1.000/0.716/0.709 | c=0.998347
[Epoch 0020] loss=12.1910 cls=0.0140 smmd=0.3869 ct=9.4223 rec=1.1838 | train/val/test=1.000/0.716/0.711 | c=0.998347
[Epoch 0021] loss=12.1310 cls=0.0167 smmd=0.3367 ct=9.4066 rec=1.1856 | train/val/test=1.000/0.720/0.718 | c=0.998347
[Epoch 0022] loss=12.1047 cls=0.0204 smmd=0.3099 ct=9.4007 rec=1.1868 | train/val/test=1.000/0.710/0.709 | c=0.998347
[Epoch 0023] loss=12.0657 cls=0.0228 smmd=0.2583 ct=9.4111 rec=1.1867 | train/val/test=1.000/0.724/0.718 | c=0.998347
[Epoch 0024] loss=12.0519 cls=0.0270 smmd=0.2425 ct=9.4136 rec=1.1844 | train/val/test=1.000/0.720/0.719 | c=0.998347
[Epoch 0025] loss=12.0141 cls=0.0303 smmd=0.2098 ct=9.4080 rec=1.1830 | train/val/test=1.000/0.726/0.726 | c=0.998347
[Epoch 0026] loss=12.0120 cls=0.0350 smmd=0.2106 ct=9.4084 rec=1.1791 | train/val/test=1.000/0.728/0.728 | c=0.998347
[Epoch 0027] loss=11.9779 cls=0.0362 smmd=0.1875 ct=9.3947 rec=1.1797 | train/val/test=1.000/0.712/0.702 | c=0.998347
[Epoch 0028] loss=11.9834 cls=0.0425 smmd=0.1729 ct=9.4135 rec=1.1773 | train/val/test=1.000/0.726/0.739 | c=0.998347
[Epoch 0029] loss=11.9544 cls=0.0327 smmd=0.1805 ct=9.3899 rec=1.1757 | train/val/test=1.000/0.728/0.718 | c=0.998347
[Epoch 0030] loss=11.8934 cls=0.0250 smmd=0.1388 ct=9.3940 rec=1.1678 | train/val/test=1.000/0.728/0.729 | c=0.998347
[Epoch 0031] loss=11.8698 cls=0.0214 smmd=0.1304 ct=9.3863 rec=1.1659 | train/val/test=1.000/0.730/0.732 | c=0.998347
[Epoch 0032] loss=11.8478 cls=0.0183 smmd=0.1319 ct=9.3667 rec=1.1655 | train/val/test=1.000/0.728/0.730 | c=0.998347
[Epoch 0033] loss=11.8270 cls=0.0159 smmd=0.1208 ct=9.3648 rec=1.1628 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0034] loss=11.8201 cls=0.0178 smmd=0.0989 ct=9.3745 rec=1.1645 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0035] loss=11.7998 cls=0.0157 smmd=0.0855 ct=9.3683 rec=1.1651 | train/val/test=1.000/0.726/0.729 | c=0.998347
[Epoch 0036] loss=11.7860 cls=0.0152 smmd=0.0786 ct=9.3634 rec=1.1644 | train/val/test=1.000/0.714/0.725 | c=0.998347
[Epoch 0037] loss=11.7871 cls=0.0173 smmd=0.0743 ct=9.3618 rec=1.1669 | train/val/test=1.000/0.718/0.720 | c=0.998347
[Epoch 0038] loss=11.7814 cls=0.0167 smmd=0.0829 ct=9.3446 rec=1.1686 | train/val/test=1.000/0.714/0.728 | c=0.998347
[Epoch 0039] loss=11.7706 cls=0.0174 smmd=0.0667 ct=9.3506 rec=1.1679 | train/val/test=1.000/0.712/0.721 | c=0.998347
[Epoch 0040] loss=11.7500 cls=0.0176 smmd=0.0428 ct=9.3531 rec=1.1682 | train/val/test=1.000/0.722/0.727 | c=0.998347
[Epoch 0041] loss=11.7581 cls=0.0189 smmd=0.0530 ct=9.3458 rec=1.1702 | train/val/test=1.000/0.712/0.719 | c=0.998347
[Epoch 0042] loss=11.7548 cls=0.0215 smmd=0.0399 ct=9.3533 rec=1.1700 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0043] loss=11.7462 cls=0.0203 smmd=0.0543 ct=9.3335 rec=1.1691 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0044] loss=11.7278 cls=0.0215 smmd=0.0360 ct=9.3367 rec=1.1668 | train/val/test=1.000/0.718/0.730 | c=0.998347
[Epoch 0045] loss=11.7171 cls=0.0203 smmd=0.0337 ct=9.3318 rec=1.1657 | train/val/test=1.000/0.718/0.732 | c=0.998347
[Epoch 0046] loss=11.7146 cls=0.0208 smmd=0.0335 ct=9.3283 rec=1.1661 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0047] loss=11.7113 cls=0.0212 smmd=0.0227 ct=9.3347 rec=1.1664 | train/val/test=1.000/0.718/0.730 | c=0.998347
[Epoch 0048] loss=11.7040 cls=0.0206 smmd=0.0273 ct=9.3227 rec=1.1667 | train/val/test=1.000/0.716/0.723 | c=0.998347
[Epoch 0049] loss=11.6955 cls=0.0196 smmd=0.0212 ct=9.3225 rec=1.1661 | train/val/test=1.000/0.720/0.733 | c=0.998347
[Epoch 0050] loss=11.6821 cls=0.0191 smmd=0.0143 ct=9.3165 rec=1.1661 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0051] loss=11.6879 cls=0.0191 smmd=0.0204 ct=9.3149 rec=1.1667 | train/val/test=1.000/0.720/0.731 | c=0.998347
[Epoch 0052] loss=11.6907 cls=0.0200 smmd=0.0166 ct=9.3190 rec=1.1676 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0053] loss=11.6964 cls=0.0207 smmd=0.0286 ct=9.3098 rec=1.1687 | train/val/test=1.000/0.718/0.729 | c=0.998347
[Epoch 0054] loss=11.7003 cls=0.0221 smmd=0.0208 ct=9.3183 rec=1.1696 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0055] loss=11.7192 cls=0.0245 smmd=0.0382 ct=9.3138 rec=1.1713 | train/val/test=1.000/0.718/0.731 | c=0.998347
[Epoch 0056] loss=11.7264 cls=0.0256 smmd=0.0363 ct=9.3211 rec=1.1717 | train/val/test=1.000/0.702/0.708 | c=0.998347
[Epoch 0057] loss=11.7095 cls=0.0239 smmd=0.0324 ct=9.3126 rec=1.1703 | train/val/test=1.000/0.720/0.735 | c=0.998347
[Epoch 0058] loss=11.6708 cls=0.0175 smmd=0.0242 ct=9.2987 rec=1.1652 | train/val/test=1.000/0.720/0.726 | c=0.998347
[Epoch 0059] loss=11.6537 cls=0.0167 smmd=0.0077 ct=9.3020 rec=1.1637 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0060] loss=11.6882 cls=0.0186 smmd=0.0317 ct=9.3043 rec=1.1668 | train/val/test=1.000/0.720/0.728 | c=0.998347
[Epoch 0061] loss=11.6808 cls=0.0203 smmd=0.0182 ct=9.3081 rec=1.1671 | train/val/test=1.000/0.714/0.722 | c=0.998347
[Epoch 0062] loss=11.6569 cls=0.0164 smmd=0.0153 ct=9.2947 rec=1.1652 | train/val/test=1.000/0.712/0.718 | c=0.998347
[Epoch 0063] loss=11.6553 cls=0.0174 smmd=0.0113 ct=9.2933 rec=1.1667 | train/val/test=1.000/0.720/0.729 | c=0.998347
[Epoch 0064] loss=11.6730 cls=0.0225 smmd=0.0125 ct=9.2964 rec=1.1708 | train/val/test=1.000/0.710/0.707 | c=0.998347
[Epoch 0065] loss=11.6905 cls=0.0236 smmd=0.0240 ct=9.2971 rec=1.1729 | train/val/test=1.000/0.718/0.727 | c=0.998347
[Epoch 0066] loss=11.6745 cls=0.0232 smmd=0.0105 ct=9.2970 rec=1.1719 | train/val/test=1.000/0.712/0.719 | c=0.998347
[Epoch 0067] loss=11.6493 cls=0.0198 smmd=0.0065 ct=9.2866 rec=1.1682 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0068] loss=11.6370 cls=0.0191 smmd=-0.0006 ct=9.2840 rec=1.1672 | train/val/test=1.000/0.722/0.728 | c=0.998347
[Epoch 0069] loss=11.6489 cls=0.0202 smmd=0.0038 ct=9.2885 rec=1.1682 | train/val/test=1.000/0.712/0.715 | c=0.998347
[Epoch 0070] loss=11.6571 cls=0.0213 smmd=0.0169 ct=9.2819 rec=1.1685 | train/val/test=1.000/0.716/0.728 | c=0.998347
[Epoch 0071] loss=11.6376 cls=0.0197 smmd=-0.0011 ct=9.2843 rec=1.1673 | train/val/test=1.000/0.718/0.726 | c=0.998347
[Epoch 0072] loss=11.6298 cls=0.0184 smmd=0.0010 ct=9.2776 rec=1.1664 | train/val/test=1.000/0.710/0.716 | c=0.998347
[Epoch 0073] loss=11.6303 cls=0.0195 smmd=-0.0033 ct=9.2794 rec=1.1674 | train/val/test=1.000/0.716/0.730 | c=0.998347
[Epoch 0074] loss=11.6391 cls=0.0213 smmd=0.0020 ct=9.2773 rec=1.1692 | train/val/test=1.000/0.710/0.716 | c=0.998347
[Epoch 0075] loss=11.6400 cls=0.0213 smmd=0.0002 ct=9.2795 rec=1.1695 | train/val/test=1.000/0.718/0.731 | c=0.998347
[Epoch 0076] loss=11.6372 cls=0.0213 smmd=0.0071 ct=9.2686 rec=1.1702 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0077] loss=11.6508 cls=0.0228 smmd=-0.0006 ct=9.2860 rec=1.1713 | train/val/test=1.000/0.716/0.724 | c=0.998347
[Epoch 0078] loss=11.6865 cls=0.0264 smmd=0.0320 ct=9.2736 rec=1.1773 | train/val/test=1.000/0.694/0.702 | c=0.998347
[Epoch 0079] loss=11.7593 cls=0.0375 smmd=0.0320 ct=9.3221 rec=1.1839 | train/val/test=1.000/0.702/0.724 | c=0.998347
[Epoch 0080] loss=11.7959 cls=0.0372 smmd=0.0906 ct=9.2909 rec=1.1886 | train/val/test=1.000/0.720/0.712 | c=0.998347
[Epoch 0081] loss=11.6690 cls=0.0189 smmd=0.0183 ct=9.2958 rec=1.1680 | train/val/test=1.000/0.718/0.726 | c=0.998347
[Epoch 0082] loss=11.6291 cls=0.0141 smmd=0.0067 ct=9.2844 rec=1.1620 | train/val/test=1.000/0.708/0.724 | c=0.998347
[Epoch 0083] loss=11.6991 cls=0.0158 smmd=0.0651 ct=9.2804 rec=1.1689 | train/val/test=1.000/0.718/0.723 | c=0.998347
[Epoch 0084] loss=11.6078 cls=0.0114 smmd=0.0011 ct=9.2759 rec=1.1597 | train/val/test=1.000/0.718/0.715 | c=0.998347
[Epoch 0085] loss=11.6717 cls=0.0160 smmd=0.0240 ct=9.2978 rec=1.1669 | train/val/test=1.000/0.714/0.723 | c=0.998347
[Epoch 0086] loss=11.6512 cls=0.0139 smmd=0.0378 ct=9.2675 rec=1.1660 | train/val/test=1.000/0.724/0.725 | c=0.998347
[Epoch 0087] loss=11.6127 cls=0.0140 smmd=0.0032 ct=9.2643 rec=1.1656 | train/val/test=1.000/0.708/0.708 | c=0.998347
[Epoch 0088] loss=11.6792 cls=0.0225 smmd=0.0086 ct=9.2958 rec=1.1761 | train/val/test=1.000/0.718/0.722 | c=0.998347
[Epoch 0089] loss=11.6669 cls=0.0213 smmd=0.0281 ct=9.2647 rec=1.1764 | train/val/test=1.000/0.720/0.720 | c=0.998347
[Epoch 0090] loss=11.6111 cls=0.0195 smmd=-0.0165 ct=9.2656 rec=1.1713 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0091] loss=11.6382 cls=0.0235 smmd=-0.0084 ct=9.2738 rec=1.1747 | train/val/test=1.000/0.714/0.725 | c=0.998347
[Epoch 0092] loss=11.6805 cls=0.0275 smmd=0.0253 ct=9.2661 rec=1.1808 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0093] loss=11.6779 cls=0.0295 smmd=0.0026 ct=9.2896 rec=1.1781 | train/val/test=1.000/0.724/0.726 | c=0.998347
[Epoch 0094] loss=11.6193 cls=0.0206 smmd=0.0012 ct=9.2576 rec=1.1700 | train/val/test=1.000/0.720/0.724 | c=0.998347
[Epoch 0095] loss=11.6020 cls=0.0179 smmd=-0.0072 ct=9.2584 rec=1.1664 | train/val/test=1.000/0.718/0.716 | c=0.998347
[Epoch 0096] loss=11.6321 cls=0.0205 smmd=-0.0071 ct=9.2804 rec=1.1691 | train/val/test=1.000/0.722/0.723 | c=0.998347
[Epoch 0097] loss=11.6105 cls=0.0176 smmd=0.0031 ct=9.2573 rec=1.1663 | train/val/test=1.000/0.724/0.720 | c=0.998347
[Epoch 0098] loss=11.5958 cls=0.0163 smmd=-0.0063 ct=9.2559 rec=1.1649 | train/val/test=1.000/0.716/0.715 | c=0.998347
[Epoch 0099] loss=11.6168 cls=0.0181 smmd=-0.0081 ct=9.2704 rec=1.1682 | train/val/test=1.000/0.722/0.723 | c=0.998347
=== Best @ epoch 31: val=0.7300, test=0.7320 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 completed in 22.52 seconds.
==================================================
