Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5 - 2025-09-21 05:38:49:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.6446 cls=1.0937 smmd=5.6883 ct=7.2537 rec=1.4138 | train/val/test=0.473/0.481/0.470 | c=0.998347
[Epoch 0001] loss=53.4912 cls=1.0611 smmd=3.7854 ct=7.1992 rec=1.4156 | train/val/test=0.401/0.387/0.408 | c=0.998347
[Epoch 0002] loss=38.1992 cls=1.0664 smmd=2.2660 ct=7.1493 rec=1.4139 | train/val/test=0.406/0.392/0.413 | c=0.998347
[Epoch 0003] loss=40.9336 cls=1.0569 smmd=2.5485 ct=7.1067 rec=1.4135 | train/val/test=0.566/0.557/0.587 | c=0.998347
[Epoch 0004] loss=40.4725 cls=1.0134 smmd=2.5154 ct=7.0522 rec=1.4146 | train/val/test=0.560/0.567/0.573 | c=0.998347
[Epoch 0005] loss=35.7745 cls=0.9779 smmd=2.0611 ct=6.9830 rec=1.4180 | train/val/test=0.539/0.547/0.554 | c=0.998347
[Epoch 0006] loss=30.7510 cls=0.9451 smmd=1.5699 ct=6.9354 rec=1.4176 | train/val/test=0.532/0.536/0.550 | c=0.998347
[Epoch 0007] loss=35.2573 cls=0.9086 smmd=1.8684 ct=7.7059 rec=1.4143 | train/val/test=0.594/0.605/0.606 | c=0.998347
[Epoch 0008] loss=36.0081 cls=0.8512 smmd=1.9795 ct=7.5425 rec=1.4043 | train/val/test=0.671/0.678/0.677 | c=0.998347
[Epoch 0009] loss=29.5192 cls=0.8061 smmd=1.3508 ct=7.4562 rec=1.3918 | train/val/test=0.670/0.680/0.677 | c=0.998347
[Epoch 0010] loss=28.8719 cls=0.7704 smmd=1.2808 ct=7.4934 rec=1.3831 | train/val/test=0.671/0.677/0.674 | c=0.998347
[Epoch 0011] loss=30.7634 cls=0.7446 smmd=1.4665 ct=7.5177 rec=1.3806 | train/val/test=0.687/0.691/0.694 | c=0.998347
[Epoch 0012] loss=27.4487 cls=0.7255 smmd=1.1341 ct=7.5283 rec=1.3763 | train/val/test=0.696/0.704/0.704 | c=0.998347
[Epoch 0013] loss=27.1090 cls=0.6991 smmd=1.0989 ct=7.5449 rec=1.3613 | train/val/test=0.692/0.694/0.706 | c=0.998347
[Epoch 0014] loss=25.7969 cls=0.7203 smmd=0.9667 ct=7.5463 rec=1.3546 | train/val/test=0.732/0.736/0.738 | c=0.998347
[Epoch 0015] loss=25.6606 cls=0.6649 smmd=0.9663 ct=7.4946 rec=1.3512 | train/val/test=0.739/0.741/0.739 | c=0.998347
[Epoch 0016] loss=25.4108 cls=0.6403 smmd=0.9372 ct=7.5201 rec=1.3575 | train/val/test=0.766/0.769/0.768 | c=0.998347
[Epoch 0017] loss=23.5731 cls=0.6125 smmd=0.7475 ct=7.5580 rec=1.3517 | train/val/test=0.787/0.781/0.787 | c=0.998347
[Epoch 0018] loss=23.4920 cls=0.6082 smmd=0.7428 ct=7.5448 rec=1.3418 | train/val/test=0.791/0.784/0.791 | c=0.998347
[Epoch 0019] loss=23.1153 cls=0.5976 smmd=0.7135 ct=7.5063 rec=1.3367 | train/val/test=0.794/0.783/0.789 | c=0.998347
[Epoch 0020] loss=22.3753 cls=0.5616 smmd=0.6427 ct=7.5001 rec=1.3340 | train/val/test=0.799/0.794/0.798 | c=0.998347
[Epoch 0021] loss=21.9213 cls=0.5429 smmd=0.5943 ct=7.5197 rec=1.3354 | train/val/test=0.809/0.799/0.808 | c=0.998347
[Epoch 0022] loss=21.7575 cls=0.5282 smmd=0.5777 ct=7.5254 rec=1.3315 | train/val/test=0.813/0.802/0.810 | c=0.998347
[Epoch 0023] loss=21.1943 cls=0.5182 smmd=0.5206 ct=7.5337 rec=1.3243 | train/val/test=0.818/0.809/0.813 | c=0.998347
[Epoch 0024] loss=20.7256 cls=0.5110 smmd=0.4752 ct=7.5287 rec=1.3216 | train/val/test=0.821/0.808/0.818 | c=0.998347
[Epoch 0025] loss=20.6382 cls=0.4999 smmd=0.4736 ct=7.4959 rec=1.3218 | train/val/test=0.818/0.801/0.816 | c=0.998347
[Epoch 0026] loss=20.1748 cls=0.4949 smmd=0.4275 ct=7.4956 rec=1.3213 | train/val/test=0.816/0.802/0.812 | c=0.998347
[Epoch 0027] loss=20.0817 cls=0.4923 smmd=0.4115 ct=7.5299 rec=1.3215 | train/val/test=0.827/0.811/0.817 | c=0.998347
[Epoch 0028] loss=19.7839 cls=0.4813 smmd=0.3818 ct=7.5337 rec=1.3165 | train/val/test=0.833/0.817/0.819 | c=0.998347
[Epoch 0029] loss=19.5304 cls=0.4787 smmd=0.3635 ct=7.4991 rec=1.3146 | train/val/test=0.833/0.817/0.823 | c=0.998347
[Epoch 0030] loss=19.3430 cls=0.4695 smmd=0.3449 ct=7.5010 rec=1.3149 | train/val/test=0.836/0.818/0.822 | c=0.998347
[Epoch 0031] loss=19.0803 cls=0.4646 smmd=0.3191 ct=7.5006 rec=1.3127 | train/val/test=0.838/0.819/0.825 | c=0.998347
[Epoch 0032] loss=18.9930 cls=0.4596 smmd=0.3101 ct=7.5031 rec=1.3122 | train/val/test=0.836/0.821/0.825 | c=0.998347
[Epoch 0033] loss=18.7711 cls=0.4594 smmd=0.2860 ct=7.5135 rec=1.3099 | train/val/test=0.838/0.824/0.828 | c=0.998347
[Epoch 0034] loss=18.6155 cls=0.4572 smmd=0.2748 ct=7.4923 rec=1.3095 | train/val/test=0.841/0.827/0.829 | c=0.998347
[Epoch 0035] loss=18.3883 cls=0.4562 smmd=0.2528 ct=7.4884 rec=1.3106 | train/val/test=0.838/0.825/0.828 | c=0.998347
[Epoch 0036] loss=18.3320 cls=0.4588 smmd=0.2451 ct=7.4987 rec=1.3086 | train/val/test=0.840/0.822/0.834 | c=0.998347
[Epoch 0037] loss=18.2039 cls=0.4517 smmd=0.2314 ct=7.5040 rec=1.3113 | train/val/test=0.840/0.823/0.833 | c=0.998347
[Epoch 0038] loss=18.0825 cls=0.4539 smmd=0.2217 ct=7.4916 rec=1.3104 | train/val/test=0.840/0.824/0.830 | c=0.998347
[Epoch 0039] loss=18.0587 cls=0.4589 smmd=0.2184 ct=7.4953 rec=1.3099 | train/val/test=0.839/0.827/0.836 | c=0.998347
[Epoch 0040] loss=17.9075 cls=0.4549 smmd=0.2029 ct=7.4974 rec=1.3128 | train/val/test=0.839/0.821/0.832 | c=0.998347
[Epoch 0041] loss=17.8666 cls=0.4568 smmd=0.2000 ct=7.4914 rec=1.3120 | train/val/test=0.841/0.828/0.836 | c=0.998347
[Epoch 0042] loss=17.6348 cls=0.4554 smmd=0.1746 ct=7.5022 rec=1.3142 | train/val/test=0.838/0.820/0.831 | c=0.998347
[Epoch 0043] loss=17.7256 cls=0.4567 smmd=0.1854 ct=7.4932 rec=1.3135 | train/val/test=0.841/0.825/0.832 | c=0.998347
[Epoch 0044] loss=17.5748 cls=0.4569 smmd=0.1706 ct=7.4920 rec=1.3131 | train/val/test=0.841/0.827/0.834 | c=0.998347
[Epoch 0045] loss=17.5538 cls=0.4550 smmd=0.1685 ct=7.4923 rec=1.3143 | train/val/test=0.840/0.825/0.833 | c=0.998347
[Epoch 0046] loss=17.4282 cls=0.4543 smmd=0.1567 ct=7.4883 rec=1.3146 | train/val/test=0.843/0.832/0.838 | c=0.998347
[Epoch 0047] loss=17.3602 cls=0.4546 smmd=0.1474 ct=7.5005 rec=1.3150 | train/val/test=0.840/0.823/0.831 | c=0.998347
[Epoch 0048] loss=17.3195 cls=0.4559 smmd=0.1467 ct=7.4838 rec=1.3145 | train/val/test=0.842/0.828/0.838 | c=0.998347
[Epoch 0049] loss=17.2451 cls=0.4549 smmd=0.1382 ct=7.4889 rec=1.3158 | train/val/test=0.842/0.829/0.837 | c=0.998347
[Epoch 0050] loss=17.2118 cls=0.4570 smmd=0.1349 ct=7.4882 rec=1.3155 | train/val/test=0.842/0.828/0.836 | c=0.998347
[Epoch 0051] loss=17.2339 cls=0.4581 smmd=0.1367 ct=7.4895 rec=1.3168 | train/val/test=0.842/0.827/0.836 | c=0.998347
[Epoch 0052] loss=17.2209 cls=0.4592 smmd=0.1340 ct=7.4960 rec=1.3179 | train/val/test=0.841/0.824/0.834 | c=0.998347
[Epoch 0053] loss=17.1830 cls=0.4616 smmd=0.1328 ct=7.4828 rec=1.3172 | train/val/test=0.842/0.830/0.835 | c=0.998347
[Epoch 0054] loss=17.0831 cls=0.4614 smmd=0.1214 ct=7.4895 rec=1.3183 | train/val/test=0.841/0.826/0.834 | c=0.998347
[Epoch 0055] loss=17.1181 cls=0.4632 smmd=0.1254 ct=7.4870 rec=1.3171 | train/val/test=0.840/0.827/0.834 | c=0.998347
[Epoch 0056] loss=17.1553 cls=0.4620 smmd=0.1284 ct=7.4906 rec=1.3181 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0057] loss=17.1508 cls=0.4625 smmd=0.1294 ct=7.4833 rec=1.3174 | train/val/test=0.840/0.824/0.834 | c=0.998347
[Epoch 0058] loss=17.0625 cls=0.4626 smmd=0.1216 ct=7.4785 rec=1.3164 | train/val/test=0.841/0.825/0.834 | c=0.998347
[Epoch 0059] loss=17.0451 cls=0.4617 smmd=0.1194 ct=7.4812 rec=1.3162 | train/val/test=0.841/0.829/0.836 | c=0.998347
[Epoch 0060] loss=17.0223 cls=0.4616 smmd=0.1170 ct=7.4818 rec=1.3169 | train/val/test=0.840/0.823/0.834 | c=0.998347
[Epoch 0061] loss=17.0102 cls=0.4617 smmd=0.1173 ct=7.4740 rec=1.3164 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0062] loss=16.9921 cls=0.4621 smmd=0.1156 ct=7.4733 rec=1.3169 | train/val/test=0.840/0.823/0.835 | c=0.998347
[Epoch 0063] loss=17.0147 cls=0.4629 smmd=0.1169 ct=7.4777 rec=1.3178 | train/val/test=0.842/0.827/0.836 | c=0.998347
[Epoch 0064] loss=17.0756 cls=0.4648 smmd=0.1241 ct=7.4714 rec=1.3185 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0065] loss=17.1191 cls=0.4646 smmd=0.1274 ct=7.4764 rec=1.3196 | train/val/test=0.838/0.818/0.828 | c=0.998347
[Epoch 0066] loss=17.1339 cls=0.4659 smmd=0.1301 ct=7.4698 rec=1.3198 | train/val/test=0.842/0.831/0.836 | c=0.998347
[Epoch 0067] loss=17.0859 cls=0.4646 smmd=0.1259 ct=7.4673 rec=1.3202 | train/val/test=0.839/0.819/0.829 | c=0.998347
[Epoch 0068] loss=17.1490 cls=0.4634 smmd=0.1320 ct=7.4687 rec=1.3189 | train/val/test=0.844/0.832/0.836 | c=0.998347
[Epoch 0069] loss=17.2278 cls=0.4621 smmd=0.1417 ct=7.4603 rec=1.3190 | train/val/test=0.837/0.819/0.827 | c=0.998347
[Epoch 0070] loss=17.1197 cls=0.4613 smmd=0.1313 ct=7.4586 rec=1.3183 | train/val/test=0.844/0.832/0.835 | c=0.998347
[Epoch 0071] loss=17.0488 cls=0.4626 smmd=0.1245 ct=7.4564 rec=1.3193 | train/val/test=0.835/0.816/0.825 | c=0.998347
[Epoch 0072] loss=17.0110 cls=0.4631 smmd=0.1225 ct=7.4478 rec=1.3180 | train/val/test=0.843/0.833/0.833 | c=0.998347
[Epoch 0073] loss=17.0177 cls=0.4634 smmd=0.1215 ct=7.4556 rec=1.3204 | train/val/test=0.832/0.816/0.823 | c=0.998347
[Epoch 0074] loss=17.0380 cls=0.4668 smmd=0.1268 ct=7.4383 rec=1.3200 | train/val/test=0.841/0.827/0.831 | c=0.998347
[Epoch 0075] loss=17.0645 cls=0.4700 smmd=0.1258 ct=7.4546 rec=1.3236 | train/val/test=0.824/0.808/0.815 | c=0.998347
[Epoch 0076] loss=17.0880 cls=0.4804 smmd=0.1307 ct=7.4391 rec=1.3247 | train/val/test=0.831/0.814/0.823 | c=0.998347
[Epoch 0077] loss=17.1588 cls=0.4885 smmd=0.1344 ct=7.4530 rec=1.3299 | train/val/test=0.808/0.796/0.801 | c=0.998347
[Epoch 0078] loss=17.2757 cls=0.5047 smmd=0.1474 ct=7.4415 rec=1.3325 | train/val/test=0.820/0.802/0.811 | c=0.998347
[Epoch 0079] loss=17.2310 cls=0.5020 smmd=0.1404 ct=7.4544 rec=1.3338 | train/val/test=0.814/0.801/0.807 | c=0.998347
[Epoch 0080] loss=17.1764 cls=0.4962 smmd=0.1405 ct=7.4298 rec=1.3284 | train/val/test=0.835/0.819/0.828 | c=0.998347
[Epoch 0081] loss=17.1506 cls=0.4778 smmd=0.1364 ct=7.4422 rec=1.3262 | train/val/test=0.832/0.815/0.825 | c=0.998347
[Epoch 0082] loss=16.9748 cls=0.4672 smmd=0.1240 ct=7.4208 rec=1.3185 | train/val/test=0.842/0.831/0.837 | c=0.998347
[Epoch 0083] loss=16.8891 cls=0.4548 smmd=0.1148 ct=7.4270 rec=1.3194 | train/val/test=0.843/0.831/0.835 | c=0.998347
[Epoch 0084] loss=16.9168 cls=0.4603 smmd=0.1184 ct=7.4220 rec=1.3179 | train/val/test=0.838/0.822/0.831 | c=0.998347
[Epoch 0085] loss=16.8698 cls=0.4625 smmd=0.1135 ct=7.4212 rec=1.3224 | train/val/test=0.845/0.833/0.833 | c=0.998347
[Epoch 0086] loss=16.8798 cls=0.4694 smmd=0.1128 ct=7.4274 rec=1.3243 | train/val/test=0.837/0.817/0.826 | c=0.998347
[Epoch 0087] loss=16.9566 cls=0.4756 smmd=0.1211 ct=7.4221 rec=1.3280 | train/val/test=0.843/0.832/0.831 | c=0.998347
[Epoch 0088] loss=17.0683 cls=0.4782 smmd=0.1306 ct=7.4291 rec=1.3293 | train/val/test=0.832/0.814/0.823 | c=0.998347
[Epoch 0089] loss=17.1026 cls=0.4843 smmd=0.1344 ct=7.4255 rec=1.3314 | train/val/test=0.839/0.828/0.831 | c=0.998347
[Epoch 0090] loss=17.1205 cls=0.4827 smmd=0.1365 ct=7.4246 rec=1.3303 | train/val/test=0.825/0.811/0.821 | c=0.998347
[Epoch 0091] loss=17.1377 cls=0.4860 smmd=0.1381 ct=7.4245 rec=1.3305 | train/val/test=0.835/0.821/0.828 | c=0.998347
[Epoch 0092] loss=17.0882 cls=0.4849 smmd=0.1335 ct=7.4234 rec=1.3285 | train/val/test=0.823/0.810/0.817 | c=0.998347
[Epoch 0093] loss=17.0795 cls=0.4813 smmd=0.1346 ct=7.4146 rec=1.3266 | train/val/test=0.837/0.825/0.830 | c=0.998347
[Epoch 0094] loss=17.0156 cls=0.4762 smmd=0.1271 ct=7.4224 rec=1.3244 | train/val/test=0.829/0.813/0.823 | c=0.998347
[Epoch 0095] loss=16.9590 cls=0.4705 smmd=0.1245 ct=7.4088 rec=1.3218 | train/val/test=0.841/0.832/0.833 | c=0.998347
[Epoch 0096] loss=16.9487 cls=0.4662 smmd=0.1223 ct=7.4158 rec=1.3226 | train/val/test=0.836/0.820/0.827 | c=0.998347
[Epoch 0097] loss=16.9022 cls=0.4675 smmd=0.1182 ct=7.4127 rec=1.3222 | train/val/test=0.841/0.831/0.836 | c=0.998347
[Epoch 0098] loss=16.8493 cls=0.4665 smmd=0.1130 ct=7.4117 rec=1.3249 | train/val/test=0.839/0.826/0.835 | c=0.998347
[Epoch 0099] loss=16.8369 cls=0.4713 smmd=0.1112 ct=7.4132 rec=1.3257 | train/val/test=0.840/0.827/0.836 | c=0.998347
=== Best @ epoch 72: val=0.8331, test=0.8332 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5 - 2025-09-21 05:38:49:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.6446 cls=1.0937 smmd=5.6883 ct=7.2537 rec=1.4138 | train/val/test=0.473/0.481/0.470 | c=0.998347
[Epoch 0001] loss=53.4912 cls=1.0611 smmd=3.7854 ct=7.1992 rec=1.4156 | train/val/test=0.401/0.387/0.408 | c=0.998347
[Epoch 0002] loss=38.1992 cls=1.0664 smmd=2.2660 ct=7.1493 rec=1.4139 | train/val/test=0.406/0.392/0.413 | c=0.998347
[Epoch 0003] loss=40.9336 cls=1.0569 smmd=2.5485 ct=7.1067 rec=1.4135 | train/val/test=0.566/0.557/0.587 | c=0.998347
[Epoch 0004] loss=40.4725 cls=1.0134 smmd=2.5154 ct=7.0522 rec=1.4146 | train/val/test=0.560/0.567/0.573 | c=0.998347
[Epoch 0005] loss=35.7745 cls=0.9779 smmd=2.0611 ct=6.9830 rec=1.4180 | train/val/test=0.539/0.547/0.554 | c=0.998347
[Epoch 0006] loss=30.7510 cls=0.9451 smmd=1.5699 ct=6.9354 rec=1.4176 | train/val/test=0.532/0.536/0.550 | c=0.998347
[Epoch 0007] loss=35.2573 cls=0.9086 smmd=1.8684 ct=7.7059 rec=1.4143 | train/val/test=0.594/0.605/0.606 | c=0.998347
[Epoch 0008] loss=36.0081 cls=0.8512 smmd=1.9795 ct=7.5425 rec=1.4043 | train/val/test=0.671/0.678/0.677 | c=0.998347
[Epoch 0009] loss=29.5192 cls=0.8061 smmd=1.3508 ct=7.4562 rec=1.3918 | train/val/test=0.670/0.680/0.677 | c=0.998347
[Epoch 0010] loss=28.8719 cls=0.7704 smmd=1.2808 ct=7.4934 rec=1.3831 | train/val/test=0.671/0.677/0.674 | c=0.998347
[Epoch 0011] loss=30.7634 cls=0.7446 smmd=1.4665 ct=7.5177 rec=1.3806 | train/val/test=0.687/0.691/0.694 | c=0.998347
[Epoch 0012] loss=27.4487 cls=0.7255 smmd=1.1341 ct=7.5283 rec=1.3763 | train/val/test=0.696/0.704/0.704 | c=0.998347
[Epoch 0013] loss=27.1090 cls=0.6991 smmd=1.0989 ct=7.5449 rec=1.3613 | train/val/test=0.692/0.694/0.706 | c=0.998347
[Epoch 0014] loss=25.7969 cls=0.7203 smmd=0.9667 ct=7.5463 rec=1.3546 | train/val/test=0.732/0.736/0.738 | c=0.998347
[Epoch 0015] loss=25.6606 cls=0.6649 smmd=0.9663 ct=7.4946 rec=1.3512 | train/val/test=0.739/0.741/0.739 | c=0.998347
[Epoch 0016] loss=25.4108 cls=0.6403 smmd=0.9372 ct=7.5201 rec=1.3575 | train/val/test=0.766/0.769/0.768 | c=0.998347
[Epoch 0017] loss=23.5731 cls=0.6125 smmd=0.7475 ct=7.5580 rec=1.3517 | train/val/test=0.787/0.781/0.787 | c=0.998347
[Epoch 0018] loss=23.4920 cls=0.6082 smmd=0.7428 ct=7.5448 rec=1.3418 | train/val/test=0.791/0.784/0.791 | c=0.998347
[Epoch 0019] loss=23.1153 cls=0.5976 smmd=0.7135 ct=7.5063 rec=1.3367 | train/val/test=0.794/0.783/0.789 | c=0.998347
[Epoch 0020] loss=22.3753 cls=0.5616 smmd=0.6427 ct=7.5001 rec=1.3340 | train/val/test=0.799/0.794/0.798 | c=0.998347
[Epoch 0021] loss=21.9213 cls=0.5429 smmd=0.5943 ct=7.5197 rec=1.3354 | train/val/test=0.809/0.799/0.808 | c=0.998347
[Epoch 0022] loss=21.7575 cls=0.5282 smmd=0.5777 ct=7.5254 rec=1.3315 | train/val/test=0.813/0.802/0.810 | c=0.998347
[Epoch 0023] loss=21.1943 cls=0.5182 smmd=0.5206 ct=7.5337 rec=1.3243 | train/val/test=0.818/0.809/0.813 | c=0.998347
[Epoch 0024] loss=20.7256 cls=0.5110 smmd=0.4752 ct=7.5287 rec=1.3216 | train/val/test=0.821/0.808/0.818 | c=0.998347
[Epoch 0025] loss=20.6382 cls=0.4999 smmd=0.4736 ct=7.4959 rec=1.3218 | train/val/test=0.818/0.801/0.816 | c=0.998347
[Epoch 0026] loss=20.1748 cls=0.4949 smmd=0.4275 ct=7.4956 rec=1.3213 | train/val/test=0.816/0.802/0.812 | c=0.998347
[Epoch 0027] loss=20.0817 cls=0.4923 smmd=0.4115 ct=7.5299 rec=1.3215 | train/val/test=0.827/0.811/0.817 | c=0.998347
[Epoch 0028] loss=19.7839 cls=0.4813 smmd=0.3818 ct=7.5337 rec=1.3165 | train/val/test=0.833/0.817/0.819 | c=0.998347
[Epoch 0029] loss=19.5304 cls=0.4787 smmd=0.3635 ct=7.4991 rec=1.3146 | train/val/test=0.833/0.817/0.823 | c=0.998347
[Epoch 0030] loss=19.3430 cls=0.4695 smmd=0.3449 ct=7.5010 rec=1.3149 | train/val/test=0.836/0.818/0.822 | c=0.998347
[Epoch 0031] loss=19.0803 cls=0.4646 smmd=0.3191 ct=7.5006 rec=1.3127 | train/val/test=0.838/0.819/0.825 | c=0.998347
[Epoch 0032] loss=18.9930 cls=0.4596 smmd=0.3101 ct=7.5031 rec=1.3122 | train/val/test=0.836/0.821/0.825 | c=0.998347
[Epoch 0033] loss=18.7711 cls=0.4594 smmd=0.2860 ct=7.5135 rec=1.3099 | train/val/test=0.838/0.824/0.828 | c=0.998347
[Epoch 0034] loss=18.6155 cls=0.4572 smmd=0.2748 ct=7.4923 rec=1.3095 | train/val/test=0.841/0.827/0.829 | c=0.998347
[Epoch 0035] loss=18.3883 cls=0.4562 smmd=0.2528 ct=7.4884 rec=1.3106 | train/val/test=0.838/0.825/0.828 | c=0.998347
[Epoch 0036] loss=18.3320 cls=0.4588 smmd=0.2451 ct=7.4987 rec=1.3086 | train/val/test=0.840/0.822/0.834 | c=0.998347
[Epoch 0037] loss=18.2039 cls=0.4517 smmd=0.2314 ct=7.5040 rec=1.3113 | train/val/test=0.840/0.823/0.833 | c=0.998347
[Epoch 0038] loss=18.0825 cls=0.4539 smmd=0.2217 ct=7.4916 rec=1.3104 | train/val/test=0.840/0.824/0.830 | c=0.998347
[Epoch 0039] loss=18.0587 cls=0.4589 smmd=0.2184 ct=7.4953 rec=1.3099 | train/val/test=0.839/0.827/0.836 | c=0.998347
[Epoch 0040] loss=17.9075 cls=0.4549 smmd=0.2029 ct=7.4974 rec=1.3128 | train/val/test=0.839/0.821/0.832 | c=0.998347
[Epoch 0041] loss=17.8666 cls=0.4568 smmd=0.2000 ct=7.4914 rec=1.3120 | train/val/test=0.841/0.828/0.836 | c=0.998347
[Epoch 0042] loss=17.6348 cls=0.4554 smmd=0.1746 ct=7.5022 rec=1.3142 | train/val/test=0.838/0.820/0.831 | c=0.998347
[Epoch 0043] loss=17.7256 cls=0.4567 smmd=0.1854 ct=7.4932 rec=1.3135 | train/val/test=0.841/0.825/0.832 | c=0.998347
[Epoch 0044] loss=17.5748 cls=0.4569 smmd=0.1706 ct=7.4920 rec=1.3131 | train/val/test=0.841/0.827/0.834 | c=0.998347
[Epoch 0045] loss=17.5538 cls=0.4550 smmd=0.1685 ct=7.4923 rec=1.3143 | train/val/test=0.840/0.825/0.833 | c=0.998347
[Epoch 0046] loss=17.4282 cls=0.4543 smmd=0.1567 ct=7.4883 rec=1.3146 | train/val/test=0.843/0.832/0.838 | c=0.998347
[Epoch 0047] loss=17.3602 cls=0.4546 smmd=0.1474 ct=7.5005 rec=1.3150 | train/val/test=0.840/0.823/0.831 | c=0.998347
[Epoch 0048] loss=17.3195 cls=0.4559 smmd=0.1467 ct=7.4838 rec=1.3145 | train/val/test=0.842/0.828/0.838 | c=0.998347
[Epoch 0049] loss=17.2451 cls=0.4549 smmd=0.1382 ct=7.4889 rec=1.3158 | train/val/test=0.842/0.829/0.837 | c=0.998347
[Epoch 0050] loss=17.2118 cls=0.4570 smmd=0.1349 ct=7.4882 rec=1.3155 | train/val/test=0.842/0.828/0.836 | c=0.998347
[Epoch 0051] loss=17.2339 cls=0.4581 smmd=0.1367 ct=7.4895 rec=1.3168 | train/val/test=0.842/0.827/0.836 | c=0.998347
[Epoch 0052] loss=17.2209 cls=0.4592 smmd=0.1340 ct=7.4960 rec=1.3179 | train/val/test=0.841/0.824/0.834 | c=0.998347
[Epoch 0053] loss=17.1830 cls=0.4616 smmd=0.1328 ct=7.4828 rec=1.3172 | train/val/test=0.842/0.830/0.835 | c=0.998347
[Epoch 0054] loss=17.0831 cls=0.4614 smmd=0.1214 ct=7.4895 rec=1.3183 | train/val/test=0.841/0.826/0.834 | c=0.998347
[Epoch 0055] loss=17.1181 cls=0.4632 smmd=0.1254 ct=7.4870 rec=1.3171 | train/val/test=0.840/0.827/0.834 | c=0.998347
[Epoch 0056] loss=17.1553 cls=0.4620 smmd=0.1284 ct=7.4906 rec=1.3181 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0057] loss=17.1508 cls=0.4625 smmd=0.1294 ct=7.4833 rec=1.3174 | train/val/test=0.840/0.824/0.834 | c=0.998347
[Epoch 0058] loss=17.0625 cls=0.4626 smmd=0.1216 ct=7.4785 rec=1.3164 | train/val/test=0.841/0.825/0.834 | c=0.998347
[Epoch 0059] loss=17.0451 cls=0.4617 smmd=0.1194 ct=7.4812 rec=1.3162 | train/val/test=0.841/0.829/0.836 | c=0.998347
[Epoch 0060] loss=17.0223 cls=0.4616 smmd=0.1170 ct=7.4818 rec=1.3169 | train/val/test=0.840/0.823/0.834 | c=0.998347
[Epoch 0061] loss=17.0102 cls=0.4617 smmd=0.1173 ct=7.4740 rec=1.3164 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0062] loss=16.9921 cls=0.4621 smmd=0.1156 ct=7.4733 rec=1.3169 | train/val/test=0.840/0.823/0.835 | c=0.998347
[Epoch 0063] loss=17.0147 cls=0.4629 smmd=0.1169 ct=7.4777 rec=1.3178 | train/val/test=0.842/0.827/0.836 | c=0.998347
[Epoch 0064] loss=17.0756 cls=0.4648 smmd=0.1241 ct=7.4714 rec=1.3185 | train/val/test=0.841/0.826/0.835 | c=0.998347
[Epoch 0065] loss=17.1191 cls=0.4646 smmd=0.1274 ct=7.4764 rec=1.3196 | train/val/test=0.838/0.818/0.828 | c=0.998347
[Epoch 0066] loss=17.1339 cls=0.4659 smmd=0.1301 ct=7.4698 rec=1.3198 | train/val/test=0.842/0.831/0.836 | c=0.998347
[Epoch 0067] loss=17.0859 cls=0.4646 smmd=0.1259 ct=7.4673 rec=1.3202 | train/val/test=0.839/0.819/0.829 | c=0.998347
[Epoch 0068] loss=17.1490 cls=0.4634 smmd=0.1320 ct=7.4687 rec=1.3189 | train/val/test=0.844/0.832/0.836 | c=0.998347
[Epoch 0069] loss=17.2278 cls=0.4621 smmd=0.1417 ct=7.4603 rec=1.3190 | train/val/test=0.837/0.819/0.827 | c=0.998347
[Epoch 0070] loss=17.1197 cls=0.4613 smmd=0.1313 ct=7.4586 rec=1.3183 | train/val/test=0.844/0.832/0.835 | c=0.998347
[Epoch 0071] loss=17.0488 cls=0.4626 smmd=0.1245 ct=7.4564 rec=1.3193 | train/val/test=0.835/0.816/0.825 | c=0.998347
[Epoch 0072] loss=17.0110 cls=0.4631 smmd=0.1225 ct=7.4478 rec=1.3180 | train/val/test=0.843/0.833/0.833 | c=0.998347
[Epoch 0073] loss=17.0177 cls=0.4634 smmd=0.1215 ct=7.4556 rec=1.3204 | train/val/test=0.832/0.816/0.823 | c=0.998347
[Epoch 0074] loss=17.0380 cls=0.4668 smmd=0.1268 ct=7.4383 rec=1.3200 | train/val/test=0.841/0.827/0.831 | c=0.998347
[Epoch 0075] loss=17.0645 cls=0.4700 smmd=0.1258 ct=7.4546 rec=1.3236 | train/val/test=0.824/0.808/0.815 | c=0.998347
[Epoch 0076] loss=17.0880 cls=0.4804 smmd=0.1307 ct=7.4391 rec=1.3247 | train/val/test=0.831/0.814/0.823 | c=0.998347
[Epoch 0077] loss=17.1588 cls=0.4885 smmd=0.1344 ct=7.4530 rec=1.3299 | train/val/test=0.808/0.796/0.801 | c=0.998347
[Epoch 0078] loss=17.2757 cls=0.5047 smmd=0.1474 ct=7.4415 rec=1.3325 | train/val/test=0.820/0.802/0.811 | c=0.998347
[Epoch 0079] loss=17.2310 cls=0.5020 smmd=0.1404 ct=7.4544 rec=1.3338 | train/val/test=0.814/0.801/0.807 | c=0.998347
[Epoch 0080] loss=17.1764 cls=0.4962 smmd=0.1405 ct=7.4298 rec=1.3284 | train/val/test=0.835/0.819/0.828 | c=0.998347
[Epoch 0081] loss=17.1506 cls=0.4778 smmd=0.1364 ct=7.4422 rec=1.3262 | train/val/test=0.832/0.815/0.825 | c=0.998347
[Epoch 0082] loss=16.9748 cls=0.4672 smmd=0.1240 ct=7.4208 rec=1.3185 | train/val/test=0.842/0.831/0.837 | c=0.998347
[Epoch 0083] loss=16.8891 cls=0.4548 smmd=0.1148 ct=7.4270 rec=1.3194 | train/val/test=0.843/0.831/0.835 | c=0.998347
[Epoch 0084] loss=16.9168 cls=0.4603 smmd=0.1184 ct=7.4220 rec=1.3179 | train/val/test=0.838/0.822/0.831 | c=0.998347
[Epoch 0085] loss=16.8698 cls=0.4625 smmd=0.1135 ct=7.4212 rec=1.3224 | train/val/test=0.845/0.833/0.833 | c=0.998347
[Epoch 0086] loss=16.8798 cls=0.4694 smmd=0.1128 ct=7.4274 rec=1.3243 | train/val/test=0.837/0.817/0.826 | c=0.998347
[Epoch 0087] loss=16.9566 cls=0.4756 smmd=0.1211 ct=7.4221 rec=1.3280 | train/val/test=0.843/0.832/0.831 | c=0.998347
[Epoch 0088] loss=17.0683 cls=0.4782 smmd=0.1306 ct=7.4291 rec=1.3293 | train/val/test=0.832/0.814/0.823 | c=0.998347
[Epoch 0089] loss=17.1026 cls=0.4843 smmd=0.1344 ct=7.4255 rec=1.3314 | train/val/test=0.839/0.828/0.831 | c=0.998347
[Epoch 0090] loss=17.1205 cls=0.4827 smmd=0.1365 ct=7.4246 rec=1.3303 | train/val/test=0.825/0.811/0.821 | c=0.998347
[Epoch 0091] loss=17.1377 cls=0.4860 smmd=0.1381 ct=7.4245 rec=1.3305 | train/val/test=0.835/0.821/0.828 | c=0.998347
[Epoch 0092] loss=17.0882 cls=0.4849 smmd=0.1335 ct=7.4234 rec=1.3285 | train/val/test=0.823/0.810/0.817 | c=0.998347
[Epoch 0093] loss=17.0795 cls=0.4813 smmd=0.1346 ct=7.4146 rec=1.3266 | train/val/test=0.837/0.825/0.830 | c=0.998347
[Epoch 0094] loss=17.0156 cls=0.4762 smmd=0.1271 ct=7.4224 rec=1.3244 | train/val/test=0.829/0.813/0.823 | c=0.998347
[Epoch 0095] loss=16.9590 cls=0.4705 smmd=0.1245 ct=7.4088 rec=1.3218 | train/val/test=0.841/0.832/0.833 | c=0.998347
[Epoch 0096] loss=16.9487 cls=0.4662 smmd=0.1223 ct=7.4158 rec=1.3226 | train/val/test=0.836/0.820/0.827 | c=0.998347
[Epoch 0097] loss=16.9022 cls=0.4675 smmd=0.1182 ct=7.4127 rec=1.3222 | train/val/test=0.841/0.831/0.836 | c=0.998347
[Epoch 0098] loss=16.8493 cls=0.4665 smmd=0.1130 ct=7.4117 rec=1.3249 | train/val/test=0.839/0.826/0.835 | c=0.998347
[Epoch 0099] loss=16.8369 cls=0.4713 smmd=0.1112 ct=7.4132 rec=1.3257 | train/val/test=0.840/0.827/0.836 | c=0.998347
=== Best @ epoch 72: val=0.8331, test=0.8332 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-5 completed in 190.72 seconds.
==================================================
