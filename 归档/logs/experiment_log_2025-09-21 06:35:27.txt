Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 - 2025-09-21 06:35:27:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2591 cls=1.9471 smmd=4.2712 ct=9.2630 rec=1.3889 | train/val/test=0.224/0.080/0.099 | c=0.998347
[Epoch 0001] loss=16.7863 cls=1.9080 smmd=2.8868 ct=9.2125 rec=1.3895 | train/val/test=0.569/0.272/0.258 | c=0.998347
[Epoch 0002] loss=15.2937 cls=1.7885 smmd=1.5777 ct=9.1501 rec=1.3888 | train/val/test=0.483/0.188/0.210 | c=0.998347
[Epoch 0003] loss=14.9678 cls=1.5858 smmd=1.4860 ct=9.1202 rec=1.3878 | train/val/test=0.845/0.468/0.467 | c=0.998347
[Epoch 0004] loss=14.7375 cls=1.2766 smmd=1.7312 ct=8.9639 rec=1.3829 | train/val/test=0.897/0.544/0.518 | c=0.998347
[Epoch 0005] loss=14.1838 cls=0.9225 smmd=1.6632 ct=8.8619 rec=1.3681 | train/val/test=0.897/0.548/0.520 | c=0.998347
[Epoch 0006] loss=13.5476 cls=0.6291 smmd=1.4051 ct=8.8315 rec=1.3410 | train/val/test=0.931/0.610/0.584 | c=0.998347
[Epoch 0007] loss=12.9146 cls=0.4018 smmd=1.0972 ct=8.8000 rec=1.3078 | train/val/test=0.931/0.620/0.610 | c=0.998347
[Epoch 0008] loss=12.5996 cls=0.2545 smmd=1.0031 ct=8.7938 rec=1.2741 | train/val/test=0.966/0.638/0.625 | c=0.998347
[Epoch 0009] loss=12.4935 cls=0.1594 smmd=1.0556 ct=8.7902 rec=1.2442 | train/val/test=0.966/0.654/0.634 | c=0.998347
[Epoch 0010] loss=12.4679 cls=0.1028 smmd=1.1308 ct=8.7874 rec=1.2234 | train/val/test=0.983/0.668/0.654 | c=0.998347
[Epoch 0011] loss=12.3200 cls=0.0611 smmd=1.0619 ct=8.7779 rec=1.2096 | train/val/test=1.000/0.674/0.675 | c=0.998347
[Epoch 0012] loss=12.0700 cls=0.0389 smmd=0.8586 ct=8.7707 rec=1.2009 | train/val/test=1.000/0.678/0.673 | c=0.998347
[Epoch 0013] loss=11.9208 cls=0.0278 smmd=0.7408 ct=8.7650 rec=1.1936 | train/val/test=1.000/0.690/0.682 | c=0.998347
[Epoch 0014] loss=11.8678 cls=0.0210 smmd=0.7081 ct=8.7627 rec=1.1880 | train/val/test=1.000/0.698/0.687 | c=0.998347
[Epoch 0015] loss=11.8476 cls=0.0153 smmd=0.7002 ct=8.7631 rec=1.1845 | train/val/test=1.000/0.702/0.689 | c=0.998347
[Epoch 0016] loss=11.8116 cls=0.0115 smmd=0.6693 ct=8.7659 rec=1.1824 | train/val/test=1.000/0.700/0.691 | c=0.998347
[Epoch 0017] loss=11.7087 cls=0.0095 smmd=0.5654 ct=8.7713 rec=1.1813 | train/val/test=1.000/0.696/0.691 | c=0.998347
[Epoch 0018] loss=11.5846 cls=0.0090 smmd=0.4355 ct=8.7787 rec=1.1807 | train/val/test=1.000/0.696/0.688 | c=0.998347
[Epoch 0019] loss=11.5427 cls=0.0095 smmd=0.3848 ct=8.7864 rec=1.1811 | train/val/test=1.000/0.692/0.687 | c=0.998347
[Epoch 0020] loss=11.5632 cls=0.0109 smmd=0.3947 ct=8.7934 rec=1.1821 | train/val/test=1.000/0.694/0.683 | c=0.998347
[Epoch 0021] loss=11.5051 cls=0.0138 smmd=0.3212 ct=8.8018 rec=1.1841 | train/val/test=1.000/0.702/0.691 | c=0.998347
[Epoch 0022] loss=11.4591 cls=0.0175 smmd=0.2609 ct=8.8091 rec=1.1858 | train/val/test=1.000/0.698/0.691 | c=0.998347
[Epoch 0023] loss=11.4167 cls=0.0220 smmd=0.2065 ct=8.8161 rec=1.1860 | train/val/test=1.000/0.708/0.695 | c=0.998347
[Epoch 0024] loss=11.4219 cls=0.0282 smmd=0.2002 ct=8.8218 rec=1.1859 | train/val/test=1.000/0.708/0.689 | c=0.998347
[Epoch 0025] loss=11.4159 cls=0.0326 smmd=0.1917 ct=8.8246 rec=1.1835 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0026] loss=11.3627 cls=0.0367 smmd=0.1403 ct=8.8235 rec=1.1810 | train/val/test=1.000/0.708/0.688 | c=0.998347
[Epoch 0027] loss=11.3579 cls=0.0369 smmd=0.1470 ct=8.8211 rec=1.1764 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0028] loss=11.3559 cls=0.0395 smmd=0.1486 ct=8.8182 rec=1.1748 | train/val/test=1.000/0.706/0.689 | c=0.998347
[Epoch 0029] loss=11.3295 cls=0.0328 smmd=0.1429 ct=8.8145 rec=1.1696 | train/val/test=1.000/0.722/0.703 | c=0.998347
[Epoch 0030] loss=11.2765 cls=0.0311 smmd=0.1014 ct=8.8122 rec=1.1659 | train/val/test=1.000/0.720/0.712 | c=0.998347
[Epoch 0031] loss=11.2625 cls=0.0221 smmd=0.1113 ct=8.8035 rec=1.1628 | train/val/test=1.000/0.720/0.697 | c=0.998347
[Epoch 0032] loss=11.2528 cls=0.0200 smmd=0.1128 ct=8.8000 rec=1.1599 | train/val/test=1.000/0.722/0.705 | c=0.998347
[Epoch 0033] loss=11.2179 cls=0.0174 smmd=0.0852 ct=8.7978 rec=1.1587 | train/val/test=1.000/0.714/0.710 | c=0.998347
[Epoch 0034] loss=11.2150 cls=0.0159 smmd=0.0833 ct=8.7979 rec=1.1589 | train/val/test=1.000/0.718/0.696 | c=0.998347
[Epoch 0035] loss=11.2114 cls=0.0166 smmd=0.0770 ct=8.8010 rec=1.1584 | train/val/test=1.000/0.714/0.704 | c=0.998347
[Epoch 0036] loss=11.1840 cls=0.0155 smmd=0.0484 ct=8.8025 rec=1.1588 | train/val/test=1.000/0.708/0.705 | c=0.998347
[Epoch 0037] loss=11.1946 cls=0.0160 smmd=0.0546 ct=8.8030 rec=1.1604 | train/val/test=1.000/0.712/0.695 | c=0.998347
[Epoch 0038] loss=11.1896 cls=0.0184 smmd=0.0407 ct=8.8065 rec=1.1620 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0039] loss=11.1946 cls=0.0186 smmd=0.0419 ct=8.8084 rec=1.1628 | train/val/test=1.000/0.714/0.703 | c=0.998347
[Epoch 0040] loss=11.1784 cls=0.0196 smmd=0.0229 ct=8.8099 rec=1.1630 | train/val/test=1.000/0.708/0.697 | c=0.998347
[Epoch 0041] loss=11.1712 cls=0.0195 smmd=0.0171 ct=8.8088 rec=1.1629 | train/val/test=1.000/0.718/0.703 | c=0.998347
[Epoch 0042] loss=11.1726 cls=0.0214 smmd=0.0177 ct=8.8087 rec=1.1624 | train/val/test=1.000/0.710/0.695 | c=0.998347
[Epoch 0043] loss=11.1647 cls=0.0215 smmd=0.0107 ct=8.8085 rec=1.1620 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0044] loss=11.1599 cls=0.0218 smmd=0.0105 ct=8.8053 rec=1.1611 | train/val/test=1.000/0.714/0.698 | c=0.998347
[Epoch 0045] loss=11.1469 cls=0.0215 smmd=0.0036 ct=8.8022 rec=1.1598 | train/val/test=1.000/0.710/0.701 | c=0.998347
[Epoch 0046] loss=11.1458 cls=0.0201 smmd=0.0061 ct=8.8008 rec=1.1594 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0047] loss=11.1453 cls=0.0206 smmd=0.0056 ct=8.8012 rec=1.1589 | train/val/test=1.000/0.710/0.700 | c=0.998347
[Epoch 0048] loss=11.1466 cls=0.0195 smmd=0.0103 ct=8.7990 rec=1.1589 | train/val/test=1.000/0.714/0.709 | c=0.998347
[Epoch 0049] loss=11.1417 cls=0.0202 smmd=0.0050 ct=8.7991 rec=1.1587 | train/val/test=1.000/0.708/0.703 | c=0.998347
[Epoch 0050] loss=11.1344 cls=0.0197 smmd=-0.0024 ct=8.7985 rec=1.1593 | train/val/test=1.000/0.718/0.704 | c=0.998347
[Epoch 0051] loss=11.1398 cls=0.0213 smmd=-0.0008 ct=8.8007 rec=1.1593 | train/val/test=1.000/0.710/0.705 | c=0.998347
[Epoch 0052] loss=11.1399 cls=0.0205 smmd=-0.0025 ct=8.8005 rec=1.1607 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0053] loss=11.1441 cls=0.0237 smmd=-0.0047 ct=8.8029 rec=1.1611 | train/val/test=1.000/0.706/0.705 | c=0.998347
[Epoch 0054] loss=11.1501 cls=0.0217 smmd=0.0017 ct=8.8016 rec=1.1626 | train/val/test=1.000/0.722/0.708 | c=0.998347
[Epoch 0055] loss=11.1597 cls=0.0254 smmd=0.0049 ct=8.8041 rec=1.1627 | train/val/test=1.000/0.704/0.700 | c=0.998347
[Epoch 0056] loss=11.1643 cls=0.0226 smmd=0.0112 ct=8.8022 rec=1.1641 | train/val/test=1.000/0.720/0.708 | c=0.998347
[Epoch 0057] loss=11.1554 cls=0.0238 smmd=0.0046 ct=8.8026 rec=1.1622 | train/val/test=1.000/0.706/0.700 | c=0.998347
[Epoch 0058] loss=11.1344 cls=0.0182 smmd=0.0008 ct=8.7951 rec=1.1601 | train/val/test=1.000/0.714/0.705 | c=0.998347
[Epoch 0059] loss=11.1191 cls=0.0173 smmd=-0.0084 ct=8.7932 rec=1.1585 | train/val/test=1.000/0.720/0.706 | c=0.998347
[Epoch 0060] loss=11.1274 cls=0.0191 smmd=-0.0061 ct=8.7956 rec=1.1594 | train/val/test=1.000/0.706/0.700 | c=0.998347
[Epoch 0061] loss=11.1347 cls=0.0190 smmd=-0.0012 ct=8.7944 rec=1.1613 | train/val/test=1.000/0.722/0.709 | c=0.998347
[Epoch 0062] loss=11.1332 cls=0.0205 smmd=-0.0059 ct=8.7972 rec=1.1608 | train/val/test=1.000/0.710/0.703 | c=0.998347
[Epoch 0063] loss=11.1174 cls=0.0192 smmd=-0.0176 ct=8.7945 rec=1.1607 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0064] loss=11.1246 cls=0.0200 smmd=-0.0128 ct=8.7948 rec=1.1613 | train/val/test=1.000/0.720/0.709 | c=0.998347
[Epoch 0065] loss=11.1356 cls=0.0237 smmd=-0.0129 ct=8.7997 rec=1.1626 | train/val/test=1.000/0.706/0.699 | c=0.998347
[Epoch 0066] loss=11.1465 cls=0.0236 smmd=-0.0062 ct=8.7987 rec=1.1652 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0067] loss=11.1533 cls=0.0272 smmd=-0.0051 ct=8.8025 rec=1.1643 | train/val/test=1.000/0.706/0.702 | c=0.998347
[Epoch 0068] loss=11.1492 cls=0.0237 smmd=-0.0013 ct=8.7974 rec=1.1647 | train/val/test=1.000/0.714/0.705 | c=0.998347
[Epoch 0069] loss=11.1389 cls=0.0234 smmd=-0.0049 ct=8.7968 rec=1.1618 | train/val/test=1.000/0.720/0.707 | c=0.998347
[Epoch 0070] loss=11.1252 cls=0.0202 smmd=-0.0094 ct=8.7926 rec=1.1609 | train/val/test=1.000/0.708/0.700 | c=0.998347
[Epoch 0071] loss=11.1191 cls=0.0192 smmd=-0.0116 ct=8.7912 rec=1.1601 | train/val/test=1.000/0.718/0.709 | c=0.998347
[Epoch 0072] loss=11.1221 cls=0.0203 smmd=-0.0106 ct=8.7920 rec=1.1602 | train/val/test=1.000/0.712/0.704 | c=0.998347
[Epoch 0073] loss=11.1243 cls=0.0194 smmd=-0.0068 ct=8.7897 rec=1.1610 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0074] loss=11.1207 cls=0.0202 smmd=-0.0124 ct=8.7922 rec=1.1604 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0075] loss=11.1189 cls=0.0194 smmd=-0.0129 ct=8.7908 rec=1.1608 | train/val/test=1.000/0.710/0.701 | c=0.998347
[Epoch 0076] loss=11.1182 cls=0.0201 smmd=-0.0158 ct=8.7914 rec=1.1613 | train/val/test=1.000/0.718/0.707 | c=0.998347
[Epoch 0077] loss=11.1268 cls=0.0212 smmd=-0.0119 ct=8.7936 rec=1.1619 | train/val/test=1.000/0.710/0.699 | c=0.998347
[Epoch 0078] loss=11.1203 cls=0.0213 smmd=-0.0180 ct=8.7922 rec=1.1624 | train/val/test=1.000/0.720/0.706 | c=0.998347
[Epoch 0079] loss=11.1235 cls=0.0219 smmd=-0.0164 ct=8.7937 rec=1.1622 | train/val/test=1.000/0.708/0.702 | c=0.998347
[Epoch 0080] loss=11.1158 cls=0.0216 smmd=-0.0229 ct=8.7923 rec=1.1624 | train/val/test=1.000/0.716/0.705 | c=0.998347
[Epoch 0081] loss=11.1199 cls=0.0221 smmd=-0.0199 ct=8.7936 rec=1.1620 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0082] loss=11.1214 cls=0.0214 smmd=-0.0171 ct=8.7927 rec=1.1623 | train/val/test=1.000/0.716/0.702 | c=0.998347
[Epoch 0083] loss=11.1205 cls=0.0218 smmd=-0.0171 ct=8.7926 rec=1.1616 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0084] loss=11.1185 cls=0.0207 smmd=-0.0170 ct=8.7918 rec=1.1615 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0085] loss=11.1135 cls=0.0206 smmd=-0.0197 ct=8.7913 rec=1.1607 | train/val/test=1.000/0.714/0.700 | c=0.998347
[Epoch 0086] loss=11.1115 cls=0.0195 smmd=-0.0187 ct=8.7899 rec=1.1604 | train/val/test=1.000/0.720/0.703 | c=0.998347
[Epoch 0087] loss=11.1115 cls=0.0196 smmd=-0.0184 ct=8.7905 rec=1.1599 | train/val/test=1.000/0.712/0.699 | c=0.998347
[Epoch 0088] loss=11.1117 cls=0.0190 smmd=-0.0161 ct=8.7889 rec=1.1599 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0089] loss=11.1087 cls=0.0193 smmd=-0.0211 ct=8.7906 rec=1.1600 | train/val/test=1.000/0.714/0.700 | c=0.998347
[Epoch 0090] loss=11.1141 cls=0.0195 smmd=-0.0167 ct=8.7907 rec=1.1603 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0091] loss=11.1149 cls=0.0199 smmd=-0.0180 ct=8.7914 rec=1.1608 | train/val/test=1.000/0.716/0.700 | c=0.998347
[Epoch 0092] loss=11.1181 cls=0.0211 smmd=-0.0188 ct=8.7932 rec=1.1613 | train/val/test=1.000/0.712/0.702 | c=0.998347
[Epoch 0093] loss=11.1238 cls=0.0209 smmd=-0.0148 ct=8.7935 rec=1.1621 | train/val/test=1.000/0.720/0.698 | c=0.998347
[Epoch 0094] loss=11.1348 cls=0.0230 smmd=-0.0087 ct=8.7962 rec=1.1621 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0095] loss=11.1395 cls=0.0229 smmd=-0.0058 ct=8.7950 rec=1.1637 | train/val/test=1.000/0.714/0.696 | c=0.998347
[Epoch 0096] loss=11.1656 cls=0.0276 smmd=0.0091 ct=8.8010 rec=1.1640 | train/val/test=1.000/0.702/0.701 | c=0.998347
[Epoch 0097] loss=11.1930 cls=0.0279 smmd=0.0304 ct=8.8012 rec=1.1667 | train/val/test=1.000/0.706/0.698 | c=0.998347
[Epoch 0098] loss=11.1983 cls=0.0316 smmd=0.0275 ct=8.8071 rec=1.1661 | train/val/test=1.000/0.698/0.692 | c=0.998347
[Epoch 0099] loss=11.1738 cls=0.0214 smmd=0.0321 ct=8.7940 rec=1.1632 | train/val/test=1.000/0.718/0.710 | c=0.998347
=== Best @ epoch 29: val=0.7220, test=0.7030 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 - 2025-09-21 06:35:27:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.2591 cls=1.9471 smmd=4.2712 ct=9.2630 rec=1.3889 | train/val/test=0.224/0.080/0.099 | c=0.998347
[Epoch 0001] loss=16.7863 cls=1.9080 smmd=2.8868 ct=9.2125 rec=1.3895 | train/val/test=0.569/0.272/0.258 | c=0.998347
[Epoch 0002] loss=15.2937 cls=1.7885 smmd=1.5777 ct=9.1501 rec=1.3888 | train/val/test=0.483/0.188/0.210 | c=0.998347
[Epoch 0003] loss=14.9678 cls=1.5858 smmd=1.4860 ct=9.1202 rec=1.3878 | train/val/test=0.845/0.468/0.467 | c=0.998347
[Epoch 0004] loss=14.7375 cls=1.2766 smmd=1.7312 ct=8.9639 rec=1.3829 | train/val/test=0.897/0.544/0.518 | c=0.998347
[Epoch 0005] loss=14.1838 cls=0.9225 smmd=1.6632 ct=8.8619 rec=1.3681 | train/val/test=0.897/0.548/0.520 | c=0.998347
[Epoch 0006] loss=13.5476 cls=0.6291 smmd=1.4051 ct=8.8315 rec=1.3410 | train/val/test=0.931/0.610/0.584 | c=0.998347
[Epoch 0007] loss=12.9146 cls=0.4018 smmd=1.0972 ct=8.8000 rec=1.3078 | train/val/test=0.931/0.620/0.610 | c=0.998347
[Epoch 0008] loss=12.5996 cls=0.2545 smmd=1.0031 ct=8.7938 rec=1.2741 | train/val/test=0.966/0.638/0.625 | c=0.998347
[Epoch 0009] loss=12.4935 cls=0.1594 smmd=1.0556 ct=8.7902 rec=1.2442 | train/val/test=0.966/0.654/0.634 | c=0.998347
[Epoch 0010] loss=12.4679 cls=0.1028 smmd=1.1308 ct=8.7874 rec=1.2234 | train/val/test=0.983/0.668/0.654 | c=0.998347
[Epoch 0011] loss=12.3200 cls=0.0611 smmd=1.0619 ct=8.7779 rec=1.2096 | train/val/test=1.000/0.674/0.675 | c=0.998347
[Epoch 0012] loss=12.0700 cls=0.0389 smmd=0.8586 ct=8.7707 rec=1.2009 | train/val/test=1.000/0.678/0.673 | c=0.998347
[Epoch 0013] loss=11.9208 cls=0.0278 smmd=0.7408 ct=8.7650 rec=1.1936 | train/val/test=1.000/0.690/0.682 | c=0.998347
[Epoch 0014] loss=11.8678 cls=0.0210 smmd=0.7081 ct=8.7627 rec=1.1880 | train/val/test=1.000/0.698/0.687 | c=0.998347
[Epoch 0015] loss=11.8476 cls=0.0153 smmd=0.7002 ct=8.7631 rec=1.1845 | train/val/test=1.000/0.702/0.689 | c=0.998347
[Epoch 0016] loss=11.8116 cls=0.0115 smmd=0.6693 ct=8.7659 rec=1.1824 | train/val/test=1.000/0.700/0.691 | c=0.998347
[Epoch 0017] loss=11.7087 cls=0.0095 smmd=0.5654 ct=8.7713 rec=1.1813 | train/val/test=1.000/0.696/0.691 | c=0.998347
[Epoch 0018] loss=11.5846 cls=0.0090 smmd=0.4355 ct=8.7787 rec=1.1807 | train/val/test=1.000/0.696/0.688 | c=0.998347
[Epoch 0019] loss=11.5427 cls=0.0095 smmd=0.3848 ct=8.7864 rec=1.1811 | train/val/test=1.000/0.692/0.687 | c=0.998347
[Epoch 0020] loss=11.5632 cls=0.0109 smmd=0.3947 ct=8.7934 rec=1.1821 | train/val/test=1.000/0.694/0.683 | c=0.998347
[Epoch 0021] loss=11.5051 cls=0.0138 smmd=0.3212 ct=8.8018 rec=1.1841 | train/val/test=1.000/0.702/0.691 | c=0.998347
[Epoch 0022] loss=11.4591 cls=0.0175 smmd=0.2609 ct=8.8091 rec=1.1858 | train/val/test=1.000/0.698/0.691 | c=0.998347
[Epoch 0023] loss=11.4167 cls=0.0220 smmd=0.2065 ct=8.8161 rec=1.1860 | train/val/test=1.000/0.708/0.695 | c=0.998347
[Epoch 0024] loss=11.4219 cls=0.0282 smmd=0.2002 ct=8.8218 rec=1.1859 | train/val/test=1.000/0.708/0.689 | c=0.998347
[Epoch 0025] loss=11.4159 cls=0.0326 smmd=0.1917 ct=8.8246 rec=1.1835 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0026] loss=11.3627 cls=0.0367 smmd=0.1403 ct=8.8235 rec=1.1810 | train/val/test=1.000/0.708/0.688 | c=0.998347
[Epoch 0027] loss=11.3579 cls=0.0369 smmd=0.1470 ct=8.8211 rec=1.1764 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0028] loss=11.3559 cls=0.0395 smmd=0.1486 ct=8.8182 rec=1.1748 | train/val/test=1.000/0.706/0.689 | c=0.998347
[Epoch 0029] loss=11.3295 cls=0.0328 smmd=0.1429 ct=8.8145 rec=1.1696 | train/val/test=1.000/0.722/0.703 | c=0.998347
[Epoch 0030] loss=11.2765 cls=0.0311 smmd=0.1014 ct=8.8122 rec=1.1659 | train/val/test=1.000/0.720/0.712 | c=0.998347
[Epoch 0031] loss=11.2625 cls=0.0221 smmd=0.1113 ct=8.8035 rec=1.1628 | train/val/test=1.000/0.720/0.697 | c=0.998347
[Epoch 0032] loss=11.2528 cls=0.0200 smmd=0.1128 ct=8.8000 rec=1.1599 | train/val/test=1.000/0.722/0.705 | c=0.998347
[Epoch 0033] loss=11.2179 cls=0.0174 smmd=0.0852 ct=8.7978 rec=1.1587 | train/val/test=1.000/0.714/0.710 | c=0.998347
[Epoch 0034] loss=11.2150 cls=0.0159 smmd=0.0833 ct=8.7979 rec=1.1589 | train/val/test=1.000/0.718/0.696 | c=0.998347
[Epoch 0035] loss=11.2114 cls=0.0166 smmd=0.0770 ct=8.8010 rec=1.1584 | train/val/test=1.000/0.714/0.704 | c=0.998347
[Epoch 0036] loss=11.1840 cls=0.0155 smmd=0.0484 ct=8.8025 rec=1.1588 | train/val/test=1.000/0.708/0.705 | c=0.998347
[Epoch 0037] loss=11.1946 cls=0.0160 smmd=0.0546 ct=8.8030 rec=1.1604 | train/val/test=1.000/0.712/0.695 | c=0.998347
[Epoch 0038] loss=11.1896 cls=0.0184 smmd=0.0407 ct=8.8065 rec=1.1620 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0039] loss=11.1946 cls=0.0186 smmd=0.0419 ct=8.8084 rec=1.1628 | train/val/test=1.000/0.714/0.703 | c=0.998347
[Epoch 0040] loss=11.1784 cls=0.0196 smmd=0.0229 ct=8.8099 rec=1.1630 | train/val/test=1.000/0.708/0.697 | c=0.998347
[Epoch 0041] loss=11.1712 cls=0.0195 smmd=0.0171 ct=8.8088 rec=1.1629 | train/val/test=1.000/0.718/0.703 | c=0.998347
[Epoch 0042] loss=11.1726 cls=0.0214 smmd=0.0177 ct=8.8087 rec=1.1624 | train/val/test=1.000/0.710/0.695 | c=0.998347
[Epoch 0043] loss=11.1647 cls=0.0215 smmd=0.0107 ct=8.8085 rec=1.1620 | train/val/test=1.000/0.712/0.705 | c=0.998347
[Epoch 0044] loss=11.1599 cls=0.0218 smmd=0.0105 ct=8.8053 rec=1.1611 | train/val/test=1.000/0.714/0.698 | c=0.998347
[Epoch 0045] loss=11.1469 cls=0.0215 smmd=0.0036 ct=8.8022 rec=1.1598 | train/val/test=1.000/0.710/0.701 | c=0.998347
[Epoch 0046] loss=11.1458 cls=0.0201 smmd=0.0061 ct=8.8008 rec=1.1594 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0047] loss=11.1453 cls=0.0206 smmd=0.0056 ct=8.8012 rec=1.1589 | train/val/test=1.000/0.710/0.700 | c=0.998347
[Epoch 0048] loss=11.1466 cls=0.0195 smmd=0.0103 ct=8.7990 rec=1.1589 | train/val/test=1.000/0.714/0.709 | c=0.998347
[Epoch 0049] loss=11.1417 cls=0.0202 smmd=0.0050 ct=8.7991 rec=1.1587 | train/val/test=1.000/0.708/0.703 | c=0.998347
[Epoch 0050] loss=11.1344 cls=0.0197 smmd=-0.0024 ct=8.7985 rec=1.1593 | train/val/test=1.000/0.718/0.704 | c=0.998347
[Epoch 0051] loss=11.1398 cls=0.0213 smmd=-0.0008 ct=8.8007 rec=1.1593 | train/val/test=1.000/0.710/0.705 | c=0.998347
[Epoch 0052] loss=11.1399 cls=0.0205 smmd=-0.0025 ct=8.8005 rec=1.1607 | train/val/test=1.000/0.722/0.706 | c=0.998347
[Epoch 0053] loss=11.1441 cls=0.0237 smmd=-0.0047 ct=8.8029 rec=1.1611 | train/val/test=1.000/0.706/0.705 | c=0.998347
[Epoch 0054] loss=11.1501 cls=0.0217 smmd=0.0017 ct=8.8016 rec=1.1626 | train/val/test=1.000/0.722/0.708 | c=0.998347
[Epoch 0055] loss=11.1597 cls=0.0254 smmd=0.0049 ct=8.8041 rec=1.1627 | train/val/test=1.000/0.704/0.700 | c=0.998347
[Epoch 0056] loss=11.1643 cls=0.0226 smmd=0.0112 ct=8.8022 rec=1.1641 | train/val/test=1.000/0.720/0.708 | c=0.998347
[Epoch 0057] loss=11.1554 cls=0.0238 smmd=0.0046 ct=8.8026 rec=1.1622 | train/val/test=1.000/0.706/0.700 | c=0.998347
[Epoch 0058] loss=11.1344 cls=0.0182 smmd=0.0008 ct=8.7951 rec=1.1601 | train/val/test=1.000/0.714/0.705 | c=0.998347
[Epoch 0059] loss=11.1191 cls=0.0173 smmd=-0.0084 ct=8.7932 rec=1.1585 | train/val/test=1.000/0.720/0.706 | c=0.998347
[Epoch 0060] loss=11.1274 cls=0.0191 smmd=-0.0061 ct=8.7956 rec=1.1594 | train/val/test=1.000/0.706/0.700 | c=0.998347
[Epoch 0061] loss=11.1347 cls=0.0190 smmd=-0.0012 ct=8.7944 rec=1.1613 | train/val/test=1.000/0.722/0.709 | c=0.998347
[Epoch 0062] loss=11.1332 cls=0.0205 smmd=-0.0059 ct=8.7972 rec=1.1608 | train/val/test=1.000/0.710/0.703 | c=0.998347
[Epoch 0063] loss=11.1174 cls=0.0192 smmd=-0.0176 ct=8.7945 rec=1.1607 | train/val/test=1.000/0.710/0.704 | c=0.998347
[Epoch 0064] loss=11.1246 cls=0.0200 smmd=-0.0128 ct=8.7948 rec=1.1613 | train/val/test=1.000/0.720/0.709 | c=0.998347
[Epoch 0065] loss=11.1356 cls=0.0237 smmd=-0.0129 ct=8.7997 rec=1.1626 | train/val/test=1.000/0.706/0.699 | c=0.998347
[Epoch 0066] loss=11.1465 cls=0.0236 smmd=-0.0062 ct=8.7987 rec=1.1652 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0067] loss=11.1533 cls=0.0272 smmd=-0.0051 ct=8.8025 rec=1.1643 | train/val/test=1.000/0.706/0.702 | c=0.998347
[Epoch 0068] loss=11.1492 cls=0.0237 smmd=-0.0013 ct=8.7974 rec=1.1647 | train/val/test=1.000/0.714/0.705 | c=0.998347
[Epoch 0069] loss=11.1389 cls=0.0234 smmd=-0.0049 ct=8.7968 rec=1.1618 | train/val/test=1.000/0.720/0.707 | c=0.998347
[Epoch 0070] loss=11.1252 cls=0.0202 smmd=-0.0094 ct=8.7926 rec=1.1609 | train/val/test=1.000/0.708/0.700 | c=0.998347
[Epoch 0071] loss=11.1191 cls=0.0192 smmd=-0.0116 ct=8.7912 rec=1.1601 | train/val/test=1.000/0.718/0.709 | c=0.998347
[Epoch 0072] loss=11.1221 cls=0.0203 smmd=-0.0106 ct=8.7920 rec=1.1602 | train/val/test=1.000/0.712/0.704 | c=0.998347
[Epoch 0073] loss=11.1243 cls=0.0194 smmd=-0.0068 ct=8.7897 rec=1.1610 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0074] loss=11.1207 cls=0.0202 smmd=-0.0124 ct=8.7922 rec=1.1604 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0075] loss=11.1189 cls=0.0194 smmd=-0.0129 ct=8.7908 rec=1.1608 | train/val/test=1.000/0.710/0.701 | c=0.998347
[Epoch 0076] loss=11.1182 cls=0.0201 smmd=-0.0158 ct=8.7914 rec=1.1613 | train/val/test=1.000/0.718/0.707 | c=0.998347
[Epoch 0077] loss=11.1268 cls=0.0212 smmd=-0.0119 ct=8.7936 rec=1.1619 | train/val/test=1.000/0.710/0.699 | c=0.998347
[Epoch 0078] loss=11.1203 cls=0.0213 smmd=-0.0180 ct=8.7922 rec=1.1624 | train/val/test=1.000/0.720/0.706 | c=0.998347
[Epoch 0079] loss=11.1235 cls=0.0219 smmd=-0.0164 ct=8.7937 rec=1.1622 | train/val/test=1.000/0.708/0.702 | c=0.998347
[Epoch 0080] loss=11.1158 cls=0.0216 smmd=-0.0229 ct=8.7923 rec=1.1624 | train/val/test=1.000/0.716/0.705 | c=0.998347
[Epoch 0081] loss=11.1199 cls=0.0221 smmd=-0.0199 ct=8.7936 rec=1.1620 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0082] loss=11.1214 cls=0.0214 smmd=-0.0171 ct=8.7927 rec=1.1623 | train/val/test=1.000/0.716/0.702 | c=0.998347
[Epoch 0083] loss=11.1205 cls=0.0218 smmd=-0.0171 ct=8.7926 rec=1.1616 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0084] loss=11.1185 cls=0.0207 smmd=-0.0170 ct=8.7918 rec=1.1615 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0085] loss=11.1135 cls=0.0206 smmd=-0.0197 ct=8.7913 rec=1.1607 | train/val/test=1.000/0.714/0.700 | c=0.998347
[Epoch 0086] loss=11.1115 cls=0.0195 smmd=-0.0187 ct=8.7899 rec=1.1604 | train/val/test=1.000/0.720/0.703 | c=0.998347
[Epoch 0087] loss=11.1115 cls=0.0196 smmd=-0.0184 ct=8.7905 rec=1.1599 | train/val/test=1.000/0.712/0.699 | c=0.998347
[Epoch 0088] loss=11.1117 cls=0.0190 smmd=-0.0161 ct=8.7889 rec=1.1599 | train/val/test=1.000/0.716/0.706 | c=0.998347
[Epoch 0089] loss=11.1087 cls=0.0193 smmd=-0.0211 ct=8.7906 rec=1.1600 | train/val/test=1.000/0.714/0.700 | c=0.998347
[Epoch 0090] loss=11.1141 cls=0.0195 smmd=-0.0167 ct=8.7907 rec=1.1603 | train/val/test=1.000/0.716/0.703 | c=0.998347
[Epoch 0091] loss=11.1149 cls=0.0199 smmd=-0.0180 ct=8.7914 rec=1.1608 | train/val/test=1.000/0.716/0.700 | c=0.998347
[Epoch 0092] loss=11.1181 cls=0.0211 smmd=-0.0188 ct=8.7932 rec=1.1613 | train/val/test=1.000/0.712/0.702 | c=0.998347
[Epoch 0093] loss=11.1238 cls=0.0209 smmd=-0.0148 ct=8.7935 rec=1.1621 | train/val/test=1.000/0.720/0.698 | c=0.998347
[Epoch 0094] loss=11.1348 cls=0.0230 smmd=-0.0087 ct=8.7962 rec=1.1621 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0095] loss=11.1395 cls=0.0229 smmd=-0.0058 ct=8.7950 rec=1.1637 | train/val/test=1.000/0.714/0.696 | c=0.998347
[Epoch 0096] loss=11.1656 cls=0.0276 smmd=0.0091 ct=8.8010 rec=1.1640 | train/val/test=1.000/0.702/0.701 | c=0.998347
[Epoch 0097] loss=11.1930 cls=0.0279 smmd=0.0304 ct=8.8012 rec=1.1667 | train/val/test=1.000/0.706/0.698 | c=0.998347
[Epoch 0098] loss=11.1983 cls=0.0316 smmd=0.0275 ct=8.8071 rec=1.1661 | train/val/test=1.000/0.698/0.692 | c=0.998347
[Epoch 0099] loss=11.1738 cls=0.0214 smmd=0.0321 ct=8.7940 rec=1.1632 | train/val/test=1.000/0.718/0.710 | c=0.998347
=== Best @ epoch 29: val=0.7220, test=0.7030 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-1 completed in 22.60 seconds.
==================================================
