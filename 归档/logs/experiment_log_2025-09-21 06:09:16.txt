Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 - 2025-09-21 06:09:16:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.2949 cls=1.0999 smmd=5.6659 ct=11.2740 rec=1.4136 | train/val/test=0.462/0.230/0.221 | c=0.998347
[Epoch 0001] loss=30.0432 cls=1.0763 smmd=3.6245 ct=11.2410 rec=1.4138 | train/val/test=0.538/0.288/0.266 | c=0.998347
[Epoch 0002] loss=36.0610 cls=1.0425 smmd=4.8317 ct=11.2397 rec=1.4135 | train/val/test=0.654/0.596/0.583 | c=0.998347
[Epoch 0003] loss=35.3127 cls=0.9617 smmd=4.7044 ct=11.1688 rec=1.4121 | train/val/test=0.654/0.582/0.579 | c=0.998347
[Epoch 0004] loss=26.1347 cls=0.8342 smmd=2.9203 ct=10.9754 rec=1.4058 | train/val/test=0.654/0.654/0.638 | c=0.998347
[Epoch 0005] loss=25.7700 cls=0.7072 smmd=2.8640 ct=10.9574 rec=1.3925 | train/val/test=0.654/0.628/0.613 | c=0.998347
[Epoch 0006] loss=28.1023 cls=0.6203 smmd=3.3421 ct=10.9443 rec=1.3757 | train/val/test=0.731/0.654/0.639 | c=0.998347
[Epoch 0007] loss=26.5452 cls=0.5235 smmd=3.0485 ct=10.9051 rec=1.3592 | train/val/test=0.885/0.692/0.663 | c=0.998347
[Epoch 0008] loss=22.8436 cls=0.4298 smmd=2.1900 ct=11.5443 rec=1.3441 | train/val/test=0.885/0.640/0.619 | c=0.998347
[Epoch 0009] loss=22.8185 cls=0.4120 smmd=2.2121 ct=11.4173 rec=1.3461 | train/val/test=0.962/0.708/0.679 | c=0.998347
[Epoch 0010] loss=24.7308 cls=0.2947 smmd=2.6030 ct=11.4350 rec=1.3349 | train/val/test=1.000/0.762/0.736 | c=0.998347
[Epoch 0011] loss=23.3838 cls=0.2311 smmd=2.3337 ct=11.4664 rec=1.3357 | train/val/test=1.000/0.762/0.726 | c=0.998347
[Epoch 0012] loss=20.5350 cls=0.2073 smmd=1.7739 ct=11.4279 rec=1.3411 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0013] loss=22.9596 cls=0.1891 smmd=2.2387 ct=11.5373 rec=1.3420 | train/val/test=1.000/0.770/0.740 | c=0.998347
[Epoch 0014] loss=22.2749 cls=0.1584 smmd=2.1069 ct=11.5277 rec=1.3334 | train/val/test=1.000/0.794/0.745 | c=0.998347
[Epoch 0015] loss=19.5557 cls=0.1231 smmd=1.5768 ct=11.4777 rec=1.3259 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0016] loss=19.6423 cls=0.1011 smmd=1.6021 ct=11.4497 rec=1.3132 | train/val/test=1.000/0.780/0.758 | c=0.998347
[Epoch 0017] loss=20.0382 cls=0.0881 smmd=1.6870 ct=11.4281 rec=1.3094 | train/val/test=1.000/0.808/0.754 | c=0.998347
[Epoch 0018] loss=19.0339 cls=0.0726 smmd=1.4947 ct=11.3928 rec=1.3121 | train/val/test=1.000/0.806/0.760 | c=0.998347
[Epoch 0019] loss=17.6886 cls=0.0696 smmd=1.2167 ct=11.4397 rec=1.3070 | train/val/test=1.000/0.800/0.755 | c=0.998347
[Epoch 0020] loss=18.7807 cls=0.0811 smmd=1.4132 ct=11.5429 rec=1.3105 | train/val/test=1.000/0.812/0.757 | c=0.998347
[Epoch 0021] loss=18.2316 cls=0.0833 smmd=1.3231 ct=11.4435 rec=1.3110 | train/val/test=1.000/0.800/0.749 | c=0.998347
[Epoch 0022] loss=17.2690 cls=0.0881 smmd=1.1359 ct=11.4147 rec=1.3079 | train/val/test=1.000/0.822/0.767 | c=0.998347
[Epoch 0023] loss=18.1959 cls=0.0859 smmd=1.3051 ct=11.4959 rec=1.3136 | train/val/test=1.000/0.794/0.772 | c=0.998347
[Epoch 0024] loss=17.3153 cls=0.0827 smmd=1.1369 ct=11.4594 rec=1.3022 | train/val/test=1.000/0.822/0.773 | c=0.998347
[Epoch 0025] loss=16.9523 cls=0.0738 smmd=1.0702 ct=11.4340 rec=1.3042 | train/val/test=1.000/0.788/0.741 | c=0.998347
[Epoch 0026] loss=16.5812 cls=0.0795 smmd=0.9961 ct=11.4308 rec=1.3009 | train/val/test=1.000/0.790/0.767 | c=0.998347
[Epoch 0027] loss=16.5902 cls=0.0636 smmd=0.9897 ct=11.4804 rec=1.2962 | train/val/test=1.000/0.802/0.764 | c=0.998347
[Epoch 0028] loss=15.8298 cls=0.0617 smmd=0.8485 ct=11.4262 rec=1.2998 | train/val/test=1.000/0.800/0.760 | c=0.998347
[Epoch 0029] loss=16.0742 cls=0.0615 smmd=0.9001 ct=11.4130 rec=1.2998 | train/val/test=1.000/0.804/0.756 | c=0.998347
[Epoch 0030] loss=15.8456 cls=0.0718 smmd=0.8409 ct=11.4744 rec=1.3073 | train/val/test=1.000/0.808/0.753 | c=0.998347
[Epoch 0031] loss=15.8534 cls=0.0691 smmd=0.8465 ct=11.4553 rec=1.3093 | train/val/test=1.000/0.774/0.743 | c=0.998347
[Epoch 0032] loss=15.9333 cls=0.0710 smmd=0.8677 ct=11.4290 rec=1.3043 | train/val/test=1.000/0.812/0.754 | c=0.998347
[Epoch 0033] loss=16.0519 cls=0.0758 smmd=0.8767 ct=11.4990 rec=1.3150 | train/val/test=1.000/0.756/0.710 | c=0.998347
[Epoch 0034] loss=15.5875 cls=0.0686 smmd=0.7982 ct=11.4316 rec=1.3057 | train/val/test=1.000/0.796/0.761 | c=0.998347
[Epoch 0035] loss=15.3558 cls=0.0713 smmd=0.7424 ct=11.4773 rec=1.3081 | train/val/test=1.000/0.798/0.754 | c=0.998347
[Epoch 0036] loss=15.1191 cls=0.0519 smmd=0.7063 ct=11.4311 rec=1.3031 | train/val/test=1.000/0.804/0.761 | c=0.998347
[Epoch 0037] loss=14.8321 cls=0.0522 smmd=0.6504 ct=11.4246 rec=1.2964 | train/val/test=1.000/0.796/0.762 | c=0.998347
[Epoch 0038] loss=14.9144 cls=0.0574 smmd=0.6680 ct=11.4161 rec=1.2985 | train/val/test=1.000/0.818/0.772 | c=0.998347
[Epoch 0039] loss=14.9728 cls=0.0612 smmd=0.6715 ct=11.4541 rec=1.3066 | train/val/test=1.000/0.804/0.761 | c=0.998347
[Epoch 0040] loss=14.8736 cls=0.0743 smmd=0.6535 ct=11.4381 rec=1.3076 | train/val/test=1.000/0.804/0.764 | c=0.998347
[Epoch 0041] loss=15.4059 cls=0.0848 smmd=0.7515 ct=11.4744 rec=1.3154 | train/val/test=1.000/0.796/0.739 | c=0.998347
[Epoch 0042] loss=15.3245 cls=0.0925 smmd=0.7400 ct=11.4462 rec=1.3189 | train/val/test=1.000/0.782/0.758 | c=0.998347
[Epoch 0043] loss=15.0086 cls=0.0846 smmd=0.6725 ct=11.4728 rec=1.3127 | train/val/test=1.000/0.810/0.762 | c=0.998347
[Epoch 0044] loss=14.7762 cls=0.0801 smmd=0.6384 ct=11.4132 rec=1.3114 | train/val/test=1.000/0.810/0.776 | c=0.998347
[Epoch 0045] loss=14.4587 cls=0.0709 smmd=0.5674 ct=11.4556 rec=1.3072 | train/val/test=1.000/0.822/0.770 | c=0.998347
[Epoch 0046] loss=14.4918 cls=0.0723 smmd=0.5784 ct=11.4328 rec=1.3086 | train/val/test=1.000/0.818/0.780 | c=0.998347
[Epoch 0047] loss=14.2971 cls=0.0728 smmd=0.5369 ct=11.4455 rec=1.3091 | train/val/test=1.000/0.818/0.770 | c=0.998347
[Epoch 0048] loss=14.2476 cls=0.0793 smmd=0.5321 ct=11.4158 rec=1.3143 | train/val/test=1.000/0.788/0.765 | c=0.998347
[Epoch 0049] loss=14.4309 cls=0.0915 smmd=0.5547 ct=11.4794 rec=1.3192 | train/val/test=1.000/0.800/0.753 | c=0.998347
[Epoch 0050] loss=14.7921 cls=0.0976 smmd=0.6361 ct=11.4302 rec=1.3270 | train/val/test=1.000/0.768/0.744 | c=0.998347
[Epoch 0051] loss=14.7666 cls=0.1052 smmd=0.6220 ct=11.4715 rec=1.3275 | train/val/test=1.000/0.786/0.728 | c=0.998347
[Epoch 0052] loss=14.8501 cls=0.0960 smmd=0.6435 ct=11.4518 rec=1.3300 | train/val/test=1.000/0.772/0.742 | c=0.998347
[Epoch 0053] loss=14.6197 cls=0.0979 smmd=0.5984 ct=11.4464 rec=1.3239 | train/val/test=1.000/0.784/0.729 | c=0.998347
[Epoch 0054] loss=13.9145 cls=0.0828 smmd=0.4623 ct=11.4295 rec=1.3243 | train/val/test=1.000/0.784/0.747 | c=0.998347
[Epoch 0055] loss=14.2079 cls=0.0819 smmd=0.5200 ct=11.4352 rec=1.3160 | train/val/test=1.000/0.810/0.742 | c=0.998347
[Epoch 0056] loss=13.8870 cls=0.0758 smmd=0.4583 ct=11.4255 rec=1.3197 | train/val/test=1.000/0.810/0.769 | c=0.998347
[Epoch 0057] loss=13.9397 cls=0.0768 smmd=0.4667 ct=11.4362 rec=1.3162 | train/val/test=1.000/0.814/0.758 | c=0.998347
[Epoch 0058] loss=14.1957 cls=0.0845 smmd=0.5198 ct=11.4221 rec=1.3237 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0059] loss=14.5834 cls=0.0956 smmd=0.5841 ct=11.4821 rec=1.3281 | train/val/test=1.000/0.758/0.710 | c=0.998347
[Epoch 0060] loss=14.6563 cls=0.1088 smmd=0.6067 ct=11.4347 rec=1.3370 | train/val/test=1.000/0.744/0.729 | c=0.998347
[Epoch 0061] loss=14.6158 cls=0.1140 smmd=0.5876 ct=11.4866 rec=1.3399 | train/val/test=1.000/0.704/0.657 | c=0.998347
[Epoch 0062] loss=14.3670 cls=0.1175 smmd=0.5458 ct=11.4450 rec=1.3413 | train/val/test=1.000/0.766/0.735 | c=0.998347
[Epoch 0063] loss=14.0015 cls=0.0897 smmd=0.4755 ct=11.4468 rec=1.3256 | train/val/test=1.000/0.802/0.739 | c=0.998347
[Epoch 0064] loss=13.7848 cls=0.0739 smmd=0.4406 ct=11.4127 rec=1.3204 | train/val/test=1.000/0.820/0.770 | c=0.998347
[Epoch 0065] loss=13.6990 cls=0.0664 smmd=0.4234 ct=11.4172 rec=1.3149 | train/val/test=1.000/0.814/0.773 | c=0.998347
[Epoch 0066] loss=13.7462 cls=0.0733 smmd=0.4329 ct=11.4129 rec=1.3199 | train/val/test=1.000/0.814/0.769 | c=0.998347
[Epoch 0067] loss=14.0434 cls=0.0823 smmd=0.4835 ct=11.4519 rec=1.3276 | train/val/test=1.000/0.810/0.756 | c=0.998347
[Epoch 0068] loss=14.3700 cls=0.0933 smmd=0.5519 ct=11.4301 rec=1.3348 | train/val/test=1.000/0.778/0.760 | c=0.998347
[Epoch 0069] loss=14.8439 cls=0.1123 smmd=0.6336 ct=11.4853 rec=1.3435 | train/val/test=1.000/0.712/0.681 | c=0.998347
[Epoch 0070] loss=14.8384 cls=0.1208 smmd=0.6391 ct=11.4477 rec=1.3479 | train/val/test=1.000/0.742/0.727 | c=0.998347
[Epoch 0071] loss=14.0418 cls=0.1167 smmd=0.4708 ct=11.4944 rec=1.3475 | train/val/test=1.000/0.708/0.677 | c=0.998347
[Epoch 0072] loss=13.7613 cls=0.1060 smmd=0.4338 ct=11.4052 rec=1.3396 | train/val/test=1.000/0.790/0.754 | c=0.998347
[Epoch 0073] loss=13.7321 cls=0.0750 smmd=0.4258 ct=11.4335 rec=1.3236 | train/val/test=1.000/0.810/0.755 | c=0.998347
[Epoch 0074] loss=13.4307 cls=0.0670 smmd=0.3748 ct=11.3913 rec=1.3208 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0075] loss=13.4793 cls=0.0698 smmd=0.3807 ct=11.4087 rec=1.3232 | train/val/test=1.000/0.812/0.778 | c=0.998347
[Epoch 0076] loss=13.8381 cls=0.0814 smmd=0.4457 ct=11.4358 rec=1.3300 | train/val/test=1.000/0.804/0.760 | c=0.998347
[Epoch 0077] loss=14.4182 cls=0.0950 smmd=0.5603 ct=11.4354 rec=1.3393 | train/val/test=1.000/0.788/0.763 | c=0.998347
[Epoch 0078] loss=15.0605 cls=0.1002 smmd=0.6850 ct=11.4514 rec=1.3412 | train/val/test=1.000/0.780/0.736 | c=0.998347
[Epoch 0079] loss=14.7361 cls=0.0983 smmd=0.6222 ct=11.4416 rec=1.3427 | train/val/test=1.000/0.784/0.750 | c=0.998347
[Epoch 0080] loss=14.1540 cls=0.0872 smmd=0.5146 ct=11.4037 rec=1.3350 | train/val/test=1.000/0.772/0.732 | c=0.998347
[Epoch 0081] loss=13.7122 cls=0.0802 smmd=0.4214 ct=11.4318 rec=1.3347 | train/val/test=1.000/0.788/0.753 | c=0.998347
[Epoch 0082] loss=13.6314 cls=0.0776 smmd=0.4155 ct=11.3825 rec=1.3285 | train/val/test=1.000/0.796/0.745 | c=0.998347
[Epoch 0083] loss=13.4494 cls=0.0736 smmd=0.3752 ct=11.4034 rec=1.3306 | train/val/test=1.000/0.790/0.762 | c=0.998347
[Epoch 0084] loss=13.4248 cls=0.0805 smmd=0.3691 ct=11.4060 rec=1.3313 | train/val/test=1.000/0.794/0.747 | c=0.998347
[Epoch 0085] loss=13.6746 cls=0.0896 smmd=0.4141 ct=11.4253 rec=1.3396 | train/val/test=1.000/0.750/0.724 | c=0.998347
[Epoch 0086] loss=14.2139 cls=0.1080 smmd=0.5179 ct=11.4357 rec=1.3454 | train/val/test=1.000/0.718/0.678 | c=0.998347
[Epoch 0087] loss=14.8416 cls=0.1269 smmd=0.6304 ct=11.4900 rec=1.3609 | train/val/test=1.000/0.680/0.657 | c=0.998347
[Epoch 0088] loss=15.1745 cls=0.1565 smmd=0.7000 ct=11.4605 rec=1.3593 | train/val/test=1.000/0.668/0.635 | c=0.998347
[Epoch 0089] loss=14.3912 cls=0.1318 smmd=0.5361 ct=11.5075 rec=1.3716 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0090] loss=13.8916 cls=0.1043 smmd=0.4583 ct=11.4141 rec=1.3363 | train/val/test=1.000/0.792/0.736 | c=0.998347
[Epoch 0091] loss=13.6751 cls=0.0587 smmd=0.4224 ct=11.4019 rec=1.3185 | train/val/test=1.000/0.806/0.780 | c=0.998347
[Epoch 0092] loss=13.3024 cls=0.0458 smmd=0.3500 ct=11.3985 rec=1.3079 | train/val/test=1.000/0.800/0.774 | c=0.998347
[Epoch 0093] loss=13.3804 cls=0.0535 smmd=0.3664 ct=11.3902 rec=1.3145 | train/val/test=1.000/0.806/0.757 | c=0.998347
[Epoch 0094] loss=13.5217 cls=0.0612 smmd=0.3881 ct=11.4185 rec=1.3238 | train/val/test=1.000/0.752/0.743 | c=0.998347
[Epoch 0095] loss=13.9015 cls=0.0858 smmd=0.4555 ct=11.4470 rec=1.3397 | train/val/test=1.000/0.740/0.698 | c=0.998347
[Epoch 0096] loss=14.8939 cls=0.1117 smmd=0.6461 ct=11.4722 rec=1.3521 | train/val/test=1.000/0.626/0.626 | c=0.998347
[Epoch 0097] loss=15.6696 cls=0.1852 smmd=0.7854 ct=11.5116 rec=1.3855 | train/val/test=1.000/0.608/0.599 | c=0.998347
[Epoch 0098] loss=14.8298 cls=0.1773 smmd=0.6206 ct=11.5005 rec=1.3757 | train/val/test=0.962/0.648/0.631 | c=0.998347
[Epoch 0099] loss=14.1695 cls=0.1584 smmd=0.4925 ct=11.4906 rec=1.3727 | train/val/test=1.000/0.772/0.733 | c=0.998347
=== Best @ epoch 22: val=0.8220, test=0.7670 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 - 2025-09-21 06:09:16:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=40.2949 cls=1.0999 smmd=5.6659 ct=11.2740 rec=1.4136 | train/val/test=0.462/0.230/0.221 | c=0.998347
[Epoch 0001] loss=30.0432 cls=1.0763 smmd=3.6245 ct=11.2410 rec=1.4138 | train/val/test=0.538/0.288/0.266 | c=0.998347
[Epoch 0002] loss=36.0610 cls=1.0425 smmd=4.8317 ct=11.2397 rec=1.4135 | train/val/test=0.654/0.596/0.583 | c=0.998347
[Epoch 0003] loss=35.3127 cls=0.9617 smmd=4.7044 ct=11.1688 rec=1.4121 | train/val/test=0.654/0.582/0.579 | c=0.998347
[Epoch 0004] loss=26.1347 cls=0.8342 smmd=2.9203 ct=10.9754 rec=1.4058 | train/val/test=0.654/0.654/0.638 | c=0.998347
[Epoch 0005] loss=25.7700 cls=0.7072 smmd=2.8640 ct=10.9574 rec=1.3925 | train/val/test=0.654/0.628/0.613 | c=0.998347
[Epoch 0006] loss=28.1023 cls=0.6203 smmd=3.3421 ct=10.9443 rec=1.3757 | train/val/test=0.731/0.654/0.639 | c=0.998347
[Epoch 0007] loss=26.5452 cls=0.5235 smmd=3.0485 ct=10.9051 rec=1.3592 | train/val/test=0.885/0.692/0.663 | c=0.998347
[Epoch 0008] loss=22.8436 cls=0.4298 smmd=2.1900 ct=11.5443 rec=1.3441 | train/val/test=0.885/0.640/0.619 | c=0.998347
[Epoch 0009] loss=22.8185 cls=0.4120 smmd=2.2121 ct=11.4173 rec=1.3461 | train/val/test=0.962/0.708/0.679 | c=0.998347
[Epoch 0010] loss=24.7308 cls=0.2947 smmd=2.6030 ct=11.4350 rec=1.3349 | train/val/test=1.000/0.762/0.736 | c=0.998347
[Epoch 0011] loss=23.3838 cls=0.2311 smmd=2.3337 ct=11.4664 rec=1.3357 | train/val/test=1.000/0.762/0.726 | c=0.998347
[Epoch 0012] loss=20.5350 cls=0.2073 smmd=1.7739 ct=11.4279 rec=1.3411 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0013] loss=22.9596 cls=0.1891 smmd=2.2387 ct=11.5373 rec=1.3420 | train/val/test=1.000/0.770/0.740 | c=0.998347
[Epoch 0014] loss=22.2749 cls=0.1584 smmd=2.1069 ct=11.5277 rec=1.3334 | train/val/test=1.000/0.794/0.745 | c=0.998347
[Epoch 0015] loss=19.5557 cls=0.1231 smmd=1.5768 ct=11.4777 rec=1.3259 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0016] loss=19.6423 cls=0.1011 smmd=1.6021 ct=11.4497 rec=1.3132 | train/val/test=1.000/0.780/0.758 | c=0.998347
[Epoch 0017] loss=20.0382 cls=0.0881 smmd=1.6870 ct=11.4281 rec=1.3094 | train/val/test=1.000/0.808/0.754 | c=0.998347
[Epoch 0018] loss=19.0339 cls=0.0726 smmd=1.4947 ct=11.3928 rec=1.3121 | train/val/test=1.000/0.806/0.760 | c=0.998347
[Epoch 0019] loss=17.6886 cls=0.0696 smmd=1.2167 ct=11.4397 rec=1.3070 | train/val/test=1.000/0.800/0.755 | c=0.998347
[Epoch 0020] loss=18.7807 cls=0.0811 smmd=1.4132 ct=11.5429 rec=1.3105 | train/val/test=1.000/0.812/0.757 | c=0.998347
[Epoch 0021] loss=18.2316 cls=0.0833 smmd=1.3231 ct=11.4435 rec=1.3110 | train/val/test=1.000/0.800/0.749 | c=0.998347
[Epoch 0022] loss=17.2690 cls=0.0881 smmd=1.1359 ct=11.4147 rec=1.3079 | train/val/test=1.000/0.822/0.767 | c=0.998347
[Epoch 0023] loss=18.1959 cls=0.0859 smmd=1.3051 ct=11.4959 rec=1.3136 | train/val/test=1.000/0.794/0.772 | c=0.998347
[Epoch 0024] loss=17.3153 cls=0.0827 smmd=1.1369 ct=11.4594 rec=1.3022 | train/val/test=1.000/0.822/0.773 | c=0.998347
[Epoch 0025] loss=16.9523 cls=0.0738 smmd=1.0702 ct=11.4340 rec=1.3042 | train/val/test=1.000/0.788/0.741 | c=0.998347
[Epoch 0026] loss=16.5812 cls=0.0795 smmd=0.9961 ct=11.4308 rec=1.3009 | train/val/test=1.000/0.790/0.767 | c=0.998347
[Epoch 0027] loss=16.5902 cls=0.0636 smmd=0.9897 ct=11.4804 rec=1.2962 | train/val/test=1.000/0.802/0.764 | c=0.998347
[Epoch 0028] loss=15.8298 cls=0.0617 smmd=0.8485 ct=11.4262 rec=1.2998 | train/val/test=1.000/0.800/0.760 | c=0.998347
[Epoch 0029] loss=16.0742 cls=0.0615 smmd=0.9001 ct=11.4130 rec=1.2998 | train/val/test=1.000/0.804/0.756 | c=0.998347
[Epoch 0030] loss=15.8456 cls=0.0718 smmd=0.8409 ct=11.4744 rec=1.3073 | train/val/test=1.000/0.808/0.753 | c=0.998347
[Epoch 0031] loss=15.8534 cls=0.0691 smmd=0.8465 ct=11.4553 rec=1.3093 | train/val/test=1.000/0.774/0.743 | c=0.998347
[Epoch 0032] loss=15.9333 cls=0.0710 smmd=0.8677 ct=11.4290 rec=1.3043 | train/val/test=1.000/0.812/0.754 | c=0.998347
[Epoch 0033] loss=16.0519 cls=0.0758 smmd=0.8767 ct=11.4990 rec=1.3150 | train/val/test=1.000/0.756/0.710 | c=0.998347
[Epoch 0034] loss=15.5875 cls=0.0686 smmd=0.7982 ct=11.4316 rec=1.3057 | train/val/test=1.000/0.796/0.761 | c=0.998347
[Epoch 0035] loss=15.3558 cls=0.0713 smmd=0.7424 ct=11.4773 rec=1.3081 | train/val/test=1.000/0.798/0.754 | c=0.998347
[Epoch 0036] loss=15.1191 cls=0.0519 smmd=0.7063 ct=11.4311 rec=1.3031 | train/val/test=1.000/0.804/0.761 | c=0.998347
[Epoch 0037] loss=14.8321 cls=0.0522 smmd=0.6504 ct=11.4246 rec=1.2964 | train/val/test=1.000/0.796/0.762 | c=0.998347
[Epoch 0038] loss=14.9144 cls=0.0574 smmd=0.6680 ct=11.4161 rec=1.2985 | train/val/test=1.000/0.818/0.772 | c=0.998347
[Epoch 0039] loss=14.9728 cls=0.0612 smmd=0.6715 ct=11.4541 rec=1.3066 | train/val/test=1.000/0.804/0.761 | c=0.998347
[Epoch 0040] loss=14.8736 cls=0.0743 smmd=0.6535 ct=11.4381 rec=1.3076 | train/val/test=1.000/0.804/0.764 | c=0.998347
[Epoch 0041] loss=15.4059 cls=0.0848 smmd=0.7515 ct=11.4744 rec=1.3154 | train/val/test=1.000/0.796/0.739 | c=0.998347
[Epoch 0042] loss=15.3245 cls=0.0925 smmd=0.7400 ct=11.4462 rec=1.3189 | train/val/test=1.000/0.782/0.758 | c=0.998347
[Epoch 0043] loss=15.0086 cls=0.0846 smmd=0.6725 ct=11.4728 rec=1.3127 | train/val/test=1.000/0.810/0.762 | c=0.998347
[Epoch 0044] loss=14.7762 cls=0.0801 smmd=0.6384 ct=11.4132 rec=1.3114 | train/val/test=1.000/0.810/0.776 | c=0.998347
[Epoch 0045] loss=14.4587 cls=0.0709 smmd=0.5674 ct=11.4556 rec=1.3072 | train/val/test=1.000/0.822/0.770 | c=0.998347
[Epoch 0046] loss=14.4918 cls=0.0723 smmd=0.5784 ct=11.4328 rec=1.3086 | train/val/test=1.000/0.818/0.780 | c=0.998347
[Epoch 0047] loss=14.2971 cls=0.0728 smmd=0.5369 ct=11.4455 rec=1.3091 | train/val/test=1.000/0.818/0.770 | c=0.998347
[Epoch 0048] loss=14.2476 cls=0.0793 smmd=0.5321 ct=11.4158 rec=1.3143 | train/val/test=1.000/0.788/0.765 | c=0.998347
[Epoch 0049] loss=14.4309 cls=0.0915 smmd=0.5547 ct=11.4794 rec=1.3192 | train/val/test=1.000/0.800/0.753 | c=0.998347
[Epoch 0050] loss=14.7921 cls=0.0976 smmd=0.6361 ct=11.4302 rec=1.3270 | train/val/test=1.000/0.768/0.744 | c=0.998347
[Epoch 0051] loss=14.7666 cls=0.1052 smmd=0.6220 ct=11.4715 rec=1.3275 | train/val/test=1.000/0.786/0.728 | c=0.998347
[Epoch 0052] loss=14.8501 cls=0.0960 smmd=0.6435 ct=11.4518 rec=1.3300 | train/val/test=1.000/0.772/0.742 | c=0.998347
[Epoch 0053] loss=14.6197 cls=0.0979 smmd=0.5984 ct=11.4464 rec=1.3239 | train/val/test=1.000/0.784/0.729 | c=0.998347
[Epoch 0054] loss=13.9145 cls=0.0828 smmd=0.4623 ct=11.4295 rec=1.3243 | train/val/test=1.000/0.784/0.747 | c=0.998347
[Epoch 0055] loss=14.2079 cls=0.0819 smmd=0.5200 ct=11.4352 rec=1.3160 | train/val/test=1.000/0.810/0.742 | c=0.998347
[Epoch 0056] loss=13.8870 cls=0.0758 smmd=0.4583 ct=11.4255 rec=1.3197 | train/val/test=1.000/0.810/0.769 | c=0.998347
[Epoch 0057] loss=13.9397 cls=0.0768 smmd=0.4667 ct=11.4362 rec=1.3162 | train/val/test=1.000/0.814/0.758 | c=0.998347
[Epoch 0058] loss=14.1957 cls=0.0845 smmd=0.5198 ct=11.4221 rec=1.3237 | train/val/test=1.000/0.782/0.757 | c=0.998347
[Epoch 0059] loss=14.5834 cls=0.0956 smmd=0.5841 ct=11.4821 rec=1.3281 | train/val/test=1.000/0.758/0.710 | c=0.998347
[Epoch 0060] loss=14.6563 cls=0.1088 smmd=0.6067 ct=11.4347 rec=1.3370 | train/val/test=1.000/0.744/0.729 | c=0.998347
[Epoch 0061] loss=14.6158 cls=0.1140 smmd=0.5876 ct=11.4866 rec=1.3399 | train/val/test=1.000/0.704/0.657 | c=0.998347
[Epoch 0062] loss=14.3670 cls=0.1175 smmd=0.5458 ct=11.4450 rec=1.3413 | train/val/test=1.000/0.766/0.735 | c=0.998347
[Epoch 0063] loss=14.0015 cls=0.0897 smmd=0.4755 ct=11.4468 rec=1.3256 | train/val/test=1.000/0.802/0.739 | c=0.998347
[Epoch 0064] loss=13.7848 cls=0.0739 smmd=0.4406 ct=11.4127 rec=1.3204 | train/val/test=1.000/0.820/0.770 | c=0.998347
[Epoch 0065] loss=13.6990 cls=0.0664 smmd=0.4234 ct=11.4172 rec=1.3149 | train/val/test=1.000/0.814/0.773 | c=0.998347
[Epoch 0066] loss=13.7462 cls=0.0733 smmd=0.4329 ct=11.4129 rec=1.3199 | train/val/test=1.000/0.814/0.769 | c=0.998347
[Epoch 0067] loss=14.0434 cls=0.0823 smmd=0.4835 ct=11.4519 rec=1.3276 | train/val/test=1.000/0.810/0.756 | c=0.998347
[Epoch 0068] loss=14.3700 cls=0.0933 smmd=0.5519 ct=11.4301 rec=1.3348 | train/val/test=1.000/0.778/0.760 | c=0.998347
[Epoch 0069] loss=14.8439 cls=0.1123 smmd=0.6336 ct=11.4853 rec=1.3435 | train/val/test=1.000/0.712/0.681 | c=0.998347
[Epoch 0070] loss=14.8384 cls=0.1208 smmd=0.6391 ct=11.4477 rec=1.3479 | train/val/test=1.000/0.742/0.727 | c=0.998347
[Epoch 0071] loss=14.0418 cls=0.1167 smmd=0.4708 ct=11.4944 rec=1.3475 | train/val/test=1.000/0.708/0.677 | c=0.998347
[Epoch 0072] loss=13.7613 cls=0.1060 smmd=0.4338 ct=11.4052 rec=1.3396 | train/val/test=1.000/0.790/0.754 | c=0.998347
[Epoch 0073] loss=13.7321 cls=0.0750 smmd=0.4258 ct=11.4335 rec=1.3236 | train/val/test=1.000/0.810/0.755 | c=0.998347
[Epoch 0074] loss=13.4307 cls=0.0670 smmd=0.3748 ct=11.3913 rec=1.3208 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0075] loss=13.4793 cls=0.0698 smmd=0.3807 ct=11.4087 rec=1.3232 | train/val/test=1.000/0.812/0.778 | c=0.998347
[Epoch 0076] loss=13.8381 cls=0.0814 smmd=0.4457 ct=11.4358 rec=1.3300 | train/val/test=1.000/0.804/0.760 | c=0.998347
[Epoch 0077] loss=14.4182 cls=0.0950 smmd=0.5603 ct=11.4354 rec=1.3393 | train/val/test=1.000/0.788/0.763 | c=0.998347
[Epoch 0078] loss=15.0605 cls=0.1002 smmd=0.6850 ct=11.4514 rec=1.3412 | train/val/test=1.000/0.780/0.736 | c=0.998347
[Epoch 0079] loss=14.7361 cls=0.0983 smmd=0.6222 ct=11.4416 rec=1.3427 | train/val/test=1.000/0.784/0.750 | c=0.998347
[Epoch 0080] loss=14.1540 cls=0.0872 smmd=0.5146 ct=11.4037 rec=1.3350 | train/val/test=1.000/0.772/0.732 | c=0.998347
[Epoch 0081] loss=13.7122 cls=0.0802 smmd=0.4214 ct=11.4318 rec=1.3347 | train/val/test=1.000/0.788/0.753 | c=0.998347
[Epoch 0082] loss=13.6314 cls=0.0776 smmd=0.4155 ct=11.3825 rec=1.3285 | train/val/test=1.000/0.796/0.745 | c=0.998347
[Epoch 0083] loss=13.4494 cls=0.0736 smmd=0.3752 ct=11.4034 rec=1.3306 | train/val/test=1.000/0.790/0.762 | c=0.998347
[Epoch 0084] loss=13.4248 cls=0.0805 smmd=0.3691 ct=11.4060 rec=1.3313 | train/val/test=1.000/0.794/0.747 | c=0.998347
[Epoch 0085] loss=13.6746 cls=0.0896 smmd=0.4141 ct=11.4253 rec=1.3396 | train/val/test=1.000/0.750/0.724 | c=0.998347
[Epoch 0086] loss=14.2139 cls=0.1080 smmd=0.5179 ct=11.4357 rec=1.3454 | train/val/test=1.000/0.718/0.678 | c=0.998347
[Epoch 0087] loss=14.8416 cls=0.1269 smmd=0.6304 ct=11.4900 rec=1.3609 | train/val/test=1.000/0.680/0.657 | c=0.998347
[Epoch 0088] loss=15.1745 cls=0.1565 smmd=0.7000 ct=11.4605 rec=1.3593 | train/val/test=1.000/0.668/0.635 | c=0.998347
[Epoch 0089] loss=14.3912 cls=0.1318 smmd=0.5361 ct=11.5075 rec=1.3716 | train/val/test=1.000/0.714/0.706 | c=0.998347
[Epoch 0090] loss=13.8916 cls=0.1043 smmd=0.4583 ct=11.4141 rec=1.3363 | train/val/test=1.000/0.792/0.736 | c=0.998347
[Epoch 0091] loss=13.6751 cls=0.0587 smmd=0.4224 ct=11.4019 rec=1.3185 | train/val/test=1.000/0.806/0.780 | c=0.998347
[Epoch 0092] loss=13.3024 cls=0.0458 smmd=0.3500 ct=11.3985 rec=1.3079 | train/val/test=1.000/0.800/0.774 | c=0.998347
[Epoch 0093] loss=13.3804 cls=0.0535 smmd=0.3664 ct=11.3902 rec=1.3145 | train/val/test=1.000/0.806/0.757 | c=0.998347
[Epoch 0094] loss=13.5217 cls=0.0612 smmd=0.3881 ct=11.4185 rec=1.3238 | train/val/test=1.000/0.752/0.743 | c=0.998347
[Epoch 0095] loss=13.9015 cls=0.0858 smmd=0.4555 ct=11.4470 rec=1.3397 | train/val/test=1.000/0.740/0.698 | c=0.998347
[Epoch 0096] loss=14.8939 cls=0.1117 smmd=0.6461 ct=11.4722 rec=1.3521 | train/val/test=1.000/0.626/0.626 | c=0.998347
[Epoch 0097] loss=15.6696 cls=0.1852 smmd=0.7854 ct=11.5116 rec=1.3855 | train/val/test=1.000/0.608/0.599 | c=0.998347
[Epoch 0098] loss=14.8298 cls=0.1773 smmd=0.6206 ct=11.5005 rec=1.3757 | train/val/test=0.962/0.648/0.631 | c=0.998347
[Epoch 0099] loss=14.1695 cls=0.1584 smmd=0.4925 ct=11.4906 rec=1.3727 | train/val/test=1.000/0.772/0.733 | c=0.998347
=== Best @ epoch 22: val=0.8220, test=0.7670 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-5 completed in 142.91 seconds.
==================================================
