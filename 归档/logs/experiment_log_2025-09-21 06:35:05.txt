Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 - 2025-09-21 06:35:05:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1381 cls=1.9429 smmd=4.1410 ct=9.2765 rec=1.3889 | train/val/test=0.207/0.320/0.328 | c=0.998347
[Epoch 0001] loss=16.5795 cls=1.8797 smmd=2.7202 ct=9.2009 rec=1.3893 | train/val/test=0.552/0.210/0.208 | c=0.998347
[Epoch 0002] loss=15.0973 cls=1.7073 smmd=1.4794 ct=9.1334 rec=1.3885 | train/val/test=0.414/0.352/0.360 | c=0.998347
[Epoch 0003] loss=14.9096 cls=1.4440 smmd=1.5337 ct=9.1529 rec=1.3895 | train/val/test=0.897/0.420/0.412 | c=0.998347
[Epoch 0004] loss=14.5922 cls=1.1027 smmd=1.7549 ct=8.9764 rec=1.3791 | train/val/test=0.897/0.418/0.421 | c=0.998347
[Epoch 0005] loss=14.0546 cls=0.7428 smmd=1.6618 ct=8.9191 rec=1.3655 | train/val/test=1.000/0.456/0.462 | c=0.998347
[Epoch 0006] loss=13.3728 cls=0.4767 smmd=1.3369 ct=8.8770 rec=1.3411 | train/val/test=0.966/0.520/0.517 | c=0.998347
[Epoch 0007] loss=12.8261 cls=0.3167 smmd=1.0702 ct=8.8227 rec=1.3083 | train/val/test=1.000/0.524/0.523 | c=0.998347
[Epoch 0008] loss=12.5627 cls=0.1939 smmd=0.9929 ct=8.8123 rec=1.2818 | train/val/test=1.000/0.524/0.525 | c=0.998347
[Epoch 0009] loss=12.5059 cls=0.0950 smmd=1.0881 ct=8.8095 rec=1.2567 | train/val/test=1.000/0.544/0.539 | c=0.998347
[Epoch 0010] loss=13.1553 cls=0.0482 smmd=1.1179 ct=9.5161 rec=1.2365 | train/val/test=1.000/0.574/0.583 | c=0.998347
[Epoch 0011] loss=12.9136 cls=0.0253 smmd=1.0391 ct=9.4045 rec=1.2224 | train/val/test=1.000/0.602/0.602 | c=0.998347
[Epoch 0012] loss=12.7189 cls=0.0163 smmd=0.9473 ct=9.3215 rec=1.2169 | train/val/test=1.000/0.596/0.607 | c=0.998347
[Epoch 0013] loss=12.6284 cls=0.0099 smmd=0.9046 ct=9.2968 rec=1.2085 | train/val/test=1.000/0.610/0.610 | c=0.998347
[Epoch 0014] loss=12.5490 cls=0.0084 smmd=0.8209 ct=9.3236 rec=1.1981 | train/val/test=1.000/0.592/0.610 | c=0.998347
[Epoch 0015] loss=12.5277 cls=0.0121 smmd=0.7416 ct=9.3867 rec=1.1937 | train/val/test=1.000/0.604/0.616 | c=0.998347
[Epoch 0016] loss=12.4804 cls=0.0089 smmd=0.6749 ct=9.4143 rec=1.1912 | train/val/test=1.000/0.628/0.633 | c=0.998347
[Epoch 0017] loss=12.3668 cls=0.0052 smmd=0.5818 ct=9.3994 rec=1.1902 | train/val/test=1.000/0.636/0.635 | c=0.998347
[Epoch 0018] loss=12.2667 cls=0.0048 smmd=0.5061 ct=9.3746 rec=1.1905 | train/val/test=1.000/0.642/0.644 | c=0.998347
[Epoch 0019] loss=12.1896 cls=0.0054 smmd=0.4406 ct=9.3630 rec=1.1903 | train/val/test=1.000/0.640/0.649 | c=0.998347
[Epoch 0020] loss=12.1982 cls=0.0060 smmd=0.4366 ct=9.3698 rec=1.1929 | train/val/test=1.000/0.650/0.646 | c=0.998347
[Epoch 0021] loss=12.1540 cls=0.0069 smmd=0.3928 ct=9.3664 rec=1.1940 | train/val/test=1.000/0.654/0.661 | c=0.998347
[Epoch 0022] loss=12.0795 cls=0.0093 smmd=0.3083 ct=9.3711 rec=1.1955 | train/val/test=1.000/0.650/0.650 | c=0.998347
[Epoch 0023] loss=12.0275 cls=0.0124 smmd=0.2428 ct=9.3807 rec=1.1958 | train/val/test=1.000/0.658/0.645 | c=0.998347
[Epoch 0024] loss=12.0331 cls=0.0174 smmd=0.2608 ct=9.3669 rec=1.1940 | train/val/test=1.000/0.662/0.659 | c=0.998347
[Epoch 0025] loss=12.0023 cls=0.0237 smmd=0.2359 ct=9.3603 rec=1.1912 | train/val/test=1.000/0.650/0.653 | c=0.998347
[Epoch 0026] loss=11.9700 cls=0.0284 smmd=0.1985 ct=9.3656 rec=1.1887 | train/val/test=1.000/0.656/0.652 | c=0.998347
[Epoch 0027] loss=11.9206 cls=0.0332 smmd=0.1527 ct=9.3641 rec=1.1853 | train/val/test=1.000/0.652/0.656 | c=0.998347
[Epoch 0028] loss=11.9333 cls=0.0324 smmd=0.1696 ct=9.3703 rec=1.1805 | train/val/test=1.000/0.654/0.648 | c=0.998347
[Epoch 0029] loss=11.8847 cls=0.0315 smmd=0.1376 ct=9.3629 rec=1.1764 | train/val/test=1.000/0.654/0.657 | c=0.998347
[Epoch 0030] loss=11.8483 cls=0.0269 smmd=0.1241 ct=9.3537 rec=1.1718 | train/val/test=1.000/0.654/0.653 | c=0.998347
[Epoch 0031] loss=11.8185 cls=0.0234 smmd=0.1146 ct=9.3438 rec=1.1683 | train/val/test=1.000/0.648/0.655 | c=0.998347
[Epoch 0032] loss=11.8093 cls=0.0193 smmd=0.1201 ct=9.3384 rec=1.1657 | train/val/test=1.000/0.646/0.657 | c=0.998347
[Epoch 0033] loss=11.7898 cls=0.0161 smmd=0.1062 ct=9.3395 rec=1.1640 | train/val/test=1.000/0.642/0.655 | c=0.998347
[Epoch 0034] loss=11.7697 cls=0.0135 smmd=0.0899 ct=9.3403 rec=1.1630 | train/val/test=1.000/0.640/0.656 | c=0.998347
[Epoch 0035] loss=11.7613 cls=0.0122 smmd=0.0840 ct=9.3386 rec=1.1632 | train/val/test=1.000/0.646/0.651 | c=0.998347
[Epoch 0036] loss=11.7531 cls=0.0116 smmd=0.0735 ct=9.3400 rec=1.1640 | train/val/test=1.000/0.646/0.646 | c=0.998347
[Epoch 0037] loss=11.7333 cls=0.0118 smmd=0.0560 ct=9.3348 rec=1.1654 | train/val/test=1.000/0.648/0.648 | c=0.998347
[Epoch 0038] loss=11.7235 cls=0.0121 smmd=0.0502 ct=9.3268 rec=1.1672 | train/val/test=1.000/0.648/0.643 | c=0.998347
[Epoch 0039] loss=11.7264 cls=0.0128 smmd=0.0573 ct=9.3194 rec=1.1684 | train/val/test=1.000/0.644/0.644 | c=0.998347
[Epoch 0040] loss=11.7144 cls=0.0135 smmd=0.0427 ct=9.3198 rec=1.1692 | train/val/test=1.000/0.646/0.640 | c=0.998347
[Epoch 0041] loss=11.7142 cls=0.0140 smmd=0.0385 ct=9.3233 rec=1.1692 | train/val/test=1.000/0.642/0.639 | c=0.998347
[Epoch 0042] loss=11.7017 cls=0.0147 smmd=0.0247 ct=9.3252 rec=1.1685 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0043] loss=11.6961 cls=0.0150 smmd=0.0255 ct=9.3207 rec=1.1675 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0044] loss=11.6853 cls=0.0154 smmd=0.0251 ct=9.3124 rec=1.1662 | train/val/test=1.000/0.642/0.639 | c=0.998347
[Epoch 0045] loss=11.6791 cls=0.0156 smmd=0.0268 ct=9.3063 rec=1.1652 | train/val/test=1.000/0.646/0.641 | c=0.998347
[Epoch 0046] loss=11.6699 cls=0.0152 smmd=0.0173 ct=9.3091 rec=1.1641 | train/val/test=1.000/0.644/0.643 | c=0.998347
[Epoch 0047] loss=11.6692 cls=0.0155 smmd=0.0237 ct=9.3029 rec=1.1636 | train/val/test=1.000/0.648/0.642 | c=0.998347
[Epoch 0048] loss=11.6692 cls=0.0154 smmd=0.0169 ct=9.3099 rec=1.1635 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0049] loss=11.6686 cls=0.0161 smmd=0.0260 ct=9.2979 rec=1.1643 | train/val/test=1.000/0.646/0.642 | c=0.998347
[Epoch 0050] loss=11.6826 cls=0.0169 smmd=0.0171 ct=9.3160 rec=1.1663 | train/val/test=1.000/0.640/0.632 | c=0.998347
[Epoch 0051] loss=11.7034 cls=0.0203 smmd=0.0437 ct=9.2992 rec=1.1701 | train/val/test=1.000/0.638/0.642 | c=0.998347
[Epoch 0052] loss=11.7365 cls=0.0217 smmd=0.0387 ct=9.3279 rec=1.1741 | train/val/test=1.000/0.642/0.629 | c=0.998347
[Epoch 0053] loss=11.7488 cls=0.0229 smmd=0.0771 ct=9.3005 rec=1.1742 | train/val/test=1.000/0.652/0.644 | c=0.998347
[Epoch 0054] loss=11.6847 cls=0.0142 smmd=0.0231 ct=9.3115 rec=1.1680 | train/val/test=1.000/0.648/0.636 | c=0.998347
[Epoch 0055] loss=11.6446 cls=0.0105 smmd=0.0053 ct=9.3045 rec=1.1621 | train/val/test=1.000/0.642/0.637 | c=0.998347
[Epoch 0056] loss=11.6863 cls=0.0126 smmd=0.0500 ct=9.2916 rec=1.1660 | train/val/test=1.000/0.648/0.640 | c=0.998347
[Epoch 0057] loss=11.6564 cls=0.0111 smmd=0.0127 ct=9.3057 rec=1.1634 | train/val/test=1.000/0.644/0.639 | c=0.998347
[Epoch 0058] loss=11.6269 cls=0.0097 smmd=0.0044 ct=9.2903 rec=1.1613 | train/val/test=1.000/0.644/0.636 | c=0.998347
[Epoch 0059] loss=11.6584 cls=0.0124 smmd=0.0309 ct=9.2841 rec=1.1655 | train/val/test=1.000/0.650/0.637 | c=0.998347
[Epoch 0060] loss=11.6512 cls=0.0123 smmd=0.0042 ct=9.3044 rec=1.1652 | train/val/test=1.000/0.640/0.638 | c=0.998347
[Epoch 0061] loss=11.6260 cls=0.0123 smmd=0.0016 ct=9.2819 rec=1.1651 | train/val/test=1.000/0.640/0.631 | c=0.998347
[Epoch 0062] loss=11.6376 cls=0.0152 smmd=0.0048 ct=9.2808 rec=1.1684 | train/val/test=1.000/0.648/0.637 | c=0.998347
[Epoch 0063] loss=11.6606 cls=0.0167 smmd=0.0015 ct=9.3010 rec=1.1707 | train/val/test=1.000/0.642/0.630 | c=0.998347
[Epoch 0064] loss=11.6548 cls=0.0195 smmd=0.0108 ct=9.2797 rec=1.1724 | train/val/test=1.000/0.646/0.640 | c=0.998347
[Epoch 0065] loss=11.6357 cls=0.0168 smmd=-0.0084 ct=9.2892 rec=1.1690 | train/val/test=1.000/0.648/0.630 | c=0.998347
[Epoch 0066] loss=11.6233 cls=0.0164 smmd=-0.0052 ct=9.2769 rec=1.1676 | train/val/test=1.000/0.642/0.633 | c=0.998347
[Epoch 0067] loss=11.6248 cls=0.0166 smmd=0.0015 ct=9.2702 rec=1.1682 | train/val/test=1.000/0.648/0.641 | c=0.998347
[Epoch 0068] loss=11.6365 cls=0.0173 smmd=-0.0088 ct=9.2932 rec=1.1674 | train/val/test=1.000/0.642/0.633 | c=0.998347
[Epoch 0069] loss=11.6211 cls=0.0161 smmd=-0.0011 ct=9.2717 rec=1.1672 | train/val/test=1.000/0.644/0.635 | c=0.998347
[Epoch 0070] loss=11.6056 cls=0.0149 smmd=-0.0155 ct=9.2751 rec=1.1656 | train/val/test=1.000/0.648/0.641 | c=0.998347
[Epoch 0071] loss=11.6075 cls=0.0151 smmd=-0.0149 ct=9.2750 rec=1.1661 | train/val/test=1.000/0.642/0.632 | c=0.998347
[Epoch 0072] loss=11.6128 cls=0.0157 smmd=-0.0022 ct=9.2648 rec=1.1673 | train/val/test=1.000/0.646/0.639 | c=0.998347
[Epoch 0073] loss=11.6081 cls=0.0150 smmd=-0.0187 ct=9.2768 rec=1.1675 | train/val/test=1.000/0.646/0.634 | c=0.998347
[Epoch 0074] loss=11.6021 cls=0.0151 smmd=-0.0135 ct=9.2659 rec=1.1673 | train/val/test=1.000/0.640/0.633 | c=0.998347
[Epoch 0075] loss=11.5981 cls=0.0148 smmd=-0.0147 ct=9.2623 rec=1.1678 | train/val/test=1.000/0.650/0.634 | c=0.998347
[Epoch 0076] loss=11.6032 cls=0.0152 smmd=-0.0222 ct=9.2730 rec=1.1686 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0077] loss=11.6078 cls=0.0167 smmd=-0.0105 ct=9.2612 rec=1.1702 | train/val/test=1.000/0.648/0.637 | c=0.998347
[Epoch 0078] loss=11.6149 cls=0.0159 smmd=-0.0135 ct=9.2726 rec=1.1699 | train/val/test=1.000/0.648/0.630 | c=0.998347
[Epoch 0079] loss=11.6010 cls=0.0162 smmd=-0.0111 ct=9.2576 rec=1.1691 | train/val/test=1.000/0.648/0.633 | c=0.998347
[Epoch 0080] loss=11.5939 cls=0.0151 smmd=-0.0220 ct=9.2645 rec=1.1681 | train/val/test=1.000/0.646/0.630 | c=0.998347
[Epoch 0081] loss=11.5923 cls=0.0154 smmd=-0.0232 ct=9.2649 rec=1.1676 | train/val/test=1.000/0.640/0.637 | c=0.998347
[Epoch 0082] loss=11.6009 cls=0.0157 smmd=-0.0085 ct=9.2560 rec=1.1689 | train/val/test=1.000/0.648/0.627 | c=0.998347
[Epoch 0083] loss=11.6045 cls=0.0165 smmd=-0.0232 ct=9.2729 rec=1.1691 | train/val/test=1.000/0.642/0.642 | c=0.998347
[Epoch 0084] loss=11.6175 cls=0.0175 smmd=0.0006 ct=9.2559 rec=1.1717 | train/val/test=1.000/0.638/0.617 | c=0.998347
[Epoch 0085] loss=11.6333 cls=0.0192 smmd=-0.0089 ct=9.2788 rec=1.1722 | train/val/test=1.000/0.650/0.649 | c=0.998347
[Epoch 0086] loss=11.6569 cls=0.0206 smmd=0.0162 ct=9.2652 rec=1.1774 | train/val/test=1.000/0.638/0.608 | c=0.998347
[Epoch 0087] loss=11.6838 cls=0.0234 smmd=0.0209 ct=9.2860 rec=1.1767 | train/val/test=1.000/0.646/0.651 | c=0.998347
[Epoch 0088] loss=11.6495 cls=0.0183 smmd=0.0049 ct=9.2793 rec=1.1735 | train/val/test=1.000/0.644/0.635 | c=0.998347
[Epoch 0089] loss=11.6096 cls=0.0134 smmd=0.0060 ct=9.2602 rec=1.1650 | train/val/test=1.000/0.646/0.628 | c=0.998347
[Epoch 0090] loss=11.6078 cls=0.0113 smmd=-0.0080 ct=9.2788 rec=1.1628 | train/val/test=1.000/0.650/0.647 | c=0.998347
[Epoch 0091] loss=11.6008 cls=0.0112 smmd=-0.0012 ct=9.2618 rec=1.1645 | train/val/test=1.000/0.648/0.633 | c=0.998347
[Epoch 0092] loss=11.5940 cls=0.0118 smmd=-0.0045 ct=9.2596 rec=1.1636 | train/val/test=1.000/0.642/0.626 | c=0.998347
[Epoch 0093] loss=11.6059 cls=0.0120 smmd=-0.0160 ct=9.2792 rec=1.1654 | train/val/test=1.000/0.646/0.643 | c=0.998347
[Epoch 0094] loss=11.6022 cls=0.0129 smmd=-0.0022 ct=9.2544 rec=1.1685 | train/val/test=1.000/0.644/0.625 | c=0.998347
[Epoch 0095] loss=11.6004 cls=0.0152 smmd=-0.0123 ct=9.2598 rec=1.1688 | train/val/test=1.000/0.644/0.636 | c=0.998347
[Epoch 0096] loss=11.5995 cls=0.0148 smmd=-0.0229 ct=9.2669 rec=1.1704 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0097] loss=11.6080 cls=0.0177 smmd=-0.0067 ct=9.2518 rec=1.1726 | train/val/test=1.000/0.644/0.626 | c=0.998347
[Epoch 0098] loss=11.6151 cls=0.0184 smmd=-0.0225 ct=9.2744 rec=1.1724 | train/val/test=1.000/0.640/0.640 | c=0.998347
[Epoch 0099] loss=11.6275 cls=0.0185 smmd=0.0060 ct=9.2536 rec=1.1747 | train/val/test=1.000/0.642/0.626 | c=0.998347
=== Best @ epoch 24: val=0.6620, test=0.6590 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 - 2025-09-21 06:35:05:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1381 cls=1.9429 smmd=4.1410 ct=9.2765 rec=1.3889 | train/val/test=0.207/0.320/0.328 | c=0.998347
[Epoch 0001] loss=16.5795 cls=1.8797 smmd=2.7202 ct=9.2009 rec=1.3893 | train/val/test=0.552/0.210/0.208 | c=0.998347
[Epoch 0002] loss=15.0973 cls=1.7073 smmd=1.4794 ct=9.1334 rec=1.3885 | train/val/test=0.414/0.352/0.360 | c=0.998347
[Epoch 0003] loss=14.9096 cls=1.4440 smmd=1.5337 ct=9.1529 rec=1.3895 | train/val/test=0.897/0.420/0.412 | c=0.998347
[Epoch 0004] loss=14.5922 cls=1.1027 smmd=1.7549 ct=8.9764 rec=1.3791 | train/val/test=0.897/0.418/0.421 | c=0.998347
[Epoch 0005] loss=14.0546 cls=0.7428 smmd=1.6618 ct=8.9191 rec=1.3655 | train/val/test=1.000/0.456/0.462 | c=0.998347
[Epoch 0006] loss=13.3728 cls=0.4767 smmd=1.3369 ct=8.8770 rec=1.3411 | train/val/test=0.966/0.520/0.517 | c=0.998347
[Epoch 0007] loss=12.8261 cls=0.3167 smmd=1.0702 ct=8.8227 rec=1.3083 | train/val/test=1.000/0.524/0.523 | c=0.998347
[Epoch 0008] loss=12.5627 cls=0.1939 smmd=0.9929 ct=8.8123 rec=1.2818 | train/val/test=1.000/0.524/0.525 | c=0.998347
[Epoch 0009] loss=12.5059 cls=0.0950 smmd=1.0881 ct=8.8095 rec=1.2567 | train/val/test=1.000/0.544/0.539 | c=0.998347
[Epoch 0010] loss=13.1553 cls=0.0482 smmd=1.1179 ct=9.5161 rec=1.2365 | train/val/test=1.000/0.574/0.583 | c=0.998347
[Epoch 0011] loss=12.9136 cls=0.0253 smmd=1.0391 ct=9.4045 rec=1.2224 | train/val/test=1.000/0.602/0.602 | c=0.998347
[Epoch 0012] loss=12.7189 cls=0.0163 smmd=0.9473 ct=9.3215 rec=1.2169 | train/val/test=1.000/0.596/0.607 | c=0.998347
[Epoch 0013] loss=12.6284 cls=0.0099 smmd=0.9046 ct=9.2968 rec=1.2085 | train/val/test=1.000/0.610/0.610 | c=0.998347
[Epoch 0014] loss=12.5490 cls=0.0084 smmd=0.8209 ct=9.3236 rec=1.1981 | train/val/test=1.000/0.592/0.610 | c=0.998347
[Epoch 0015] loss=12.5277 cls=0.0121 smmd=0.7416 ct=9.3867 rec=1.1937 | train/val/test=1.000/0.604/0.616 | c=0.998347
[Epoch 0016] loss=12.4804 cls=0.0089 smmd=0.6749 ct=9.4143 rec=1.1912 | train/val/test=1.000/0.628/0.633 | c=0.998347
[Epoch 0017] loss=12.3668 cls=0.0052 smmd=0.5818 ct=9.3994 rec=1.1902 | train/val/test=1.000/0.636/0.635 | c=0.998347
[Epoch 0018] loss=12.2667 cls=0.0048 smmd=0.5061 ct=9.3746 rec=1.1905 | train/val/test=1.000/0.642/0.644 | c=0.998347
[Epoch 0019] loss=12.1896 cls=0.0054 smmd=0.4406 ct=9.3630 rec=1.1903 | train/val/test=1.000/0.640/0.649 | c=0.998347
[Epoch 0020] loss=12.1982 cls=0.0060 smmd=0.4366 ct=9.3698 rec=1.1929 | train/val/test=1.000/0.650/0.646 | c=0.998347
[Epoch 0021] loss=12.1540 cls=0.0069 smmd=0.3928 ct=9.3664 rec=1.1940 | train/val/test=1.000/0.654/0.661 | c=0.998347
[Epoch 0022] loss=12.0795 cls=0.0093 smmd=0.3083 ct=9.3711 rec=1.1955 | train/val/test=1.000/0.650/0.650 | c=0.998347
[Epoch 0023] loss=12.0275 cls=0.0124 smmd=0.2428 ct=9.3807 rec=1.1958 | train/val/test=1.000/0.658/0.645 | c=0.998347
[Epoch 0024] loss=12.0331 cls=0.0174 smmd=0.2608 ct=9.3669 rec=1.1940 | train/val/test=1.000/0.662/0.659 | c=0.998347
[Epoch 0025] loss=12.0023 cls=0.0237 smmd=0.2359 ct=9.3603 rec=1.1912 | train/val/test=1.000/0.650/0.653 | c=0.998347
[Epoch 0026] loss=11.9700 cls=0.0284 smmd=0.1985 ct=9.3656 rec=1.1887 | train/val/test=1.000/0.656/0.652 | c=0.998347
[Epoch 0027] loss=11.9206 cls=0.0332 smmd=0.1527 ct=9.3641 rec=1.1853 | train/val/test=1.000/0.652/0.656 | c=0.998347
[Epoch 0028] loss=11.9333 cls=0.0324 smmd=0.1696 ct=9.3703 rec=1.1805 | train/val/test=1.000/0.654/0.648 | c=0.998347
[Epoch 0029] loss=11.8847 cls=0.0315 smmd=0.1376 ct=9.3629 rec=1.1764 | train/val/test=1.000/0.654/0.657 | c=0.998347
[Epoch 0030] loss=11.8483 cls=0.0269 smmd=0.1241 ct=9.3537 rec=1.1718 | train/val/test=1.000/0.654/0.653 | c=0.998347
[Epoch 0031] loss=11.8185 cls=0.0234 smmd=0.1146 ct=9.3438 rec=1.1683 | train/val/test=1.000/0.648/0.655 | c=0.998347
[Epoch 0032] loss=11.8093 cls=0.0193 smmd=0.1201 ct=9.3384 rec=1.1657 | train/val/test=1.000/0.646/0.657 | c=0.998347
[Epoch 0033] loss=11.7898 cls=0.0161 smmd=0.1062 ct=9.3395 rec=1.1640 | train/val/test=1.000/0.642/0.655 | c=0.998347
[Epoch 0034] loss=11.7697 cls=0.0135 smmd=0.0899 ct=9.3403 rec=1.1630 | train/val/test=1.000/0.640/0.656 | c=0.998347
[Epoch 0035] loss=11.7613 cls=0.0122 smmd=0.0840 ct=9.3386 rec=1.1632 | train/val/test=1.000/0.646/0.651 | c=0.998347
[Epoch 0036] loss=11.7531 cls=0.0116 smmd=0.0735 ct=9.3400 rec=1.1640 | train/val/test=1.000/0.646/0.646 | c=0.998347
[Epoch 0037] loss=11.7333 cls=0.0118 smmd=0.0560 ct=9.3348 rec=1.1654 | train/val/test=1.000/0.648/0.648 | c=0.998347
[Epoch 0038] loss=11.7235 cls=0.0121 smmd=0.0502 ct=9.3268 rec=1.1672 | train/val/test=1.000/0.648/0.643 | c=0.998347
[Epoch 0039] loss=11.7264 cls=0.0128 smmd=0.0573 ct=9.3194 rec=1.1684 | train/val/test=1.000/0.644/0.644 | c=0.998347
[Epoch 0040] loss=11.7144 cls=0.0135 smmd=0.0427 ct=9.3198 rec=1.1692 | train/val/test=1.000/0.646/0.640 | c=0.998347
[Epoch 0041] loss=11.7142 cls=0.0140 smmd=0.0385 ct=9.3233 rec=1.1692 | train/val/test=1.000/0.642/0.639 | c=0.998347
[Epoch 0042] loss=11.7017 cls=0.0147 smmd=0.0247 ct=9.3252 rec=1.1685 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0043] loss=11.6961 cls=0.0150 smmd=0.0255 ct=9.3207 rec=1.1675 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0044] loss=11.6853 cls=0.0154 smmd=0.0251 ct=9.3124 rec=1.1662 | train/val/test=1.000/0.642/0.639 | c=0.998347
[Epoch 0045] loss=11.6791 cls=0.0156 smmd=0.0268 ct=9.3063 rec=1.1652 | train/val/test=1.000/0.646/0.641 | c=0.998347
[Epoch 0046] loss=11.6699 cls=0.0152 smmd=0.0173 ct=9.3091 rec=1.1641 | train/val/test=1.000/0.644/0.643 | c=0.998347
[Epoch 0047] loss=11.6692 cls=0.0155 smmd=0.0237 ct=9.3029 rec=1.1636 | train/val/test=1.000/0.648/0.642 | c=0.998347
[Epoch 0048] loss=11.6692 cls=0.0154 smmd=0.0169 ct=9.3099 rec=1.1635 | train/val/test=1.000/0.644/0.640 | c=0.998347
[Epoch 0049] loss=11.6686 cls=0.0161 smmd=0.0260 ct=9.2979 rec=1.1643 | train/val/test=1.000/0.646/0.642 | c=0.998347
[Epoch 0050] loss=11.6826 cls=0.0169 smmd=0.0171 ct=9.3160 rec=1.1663 | train/val/test=1.000/0.640/0.632 | c=0.998347
[Epoch 0051] loss=11.7034 cls=0.0203 smmd=0.0437 ct=9.2992 rec=1.1701 | train/val/test=1.000/0.638/0.642 | c=0.998347
[Epoch 0052] loss=11.7365 cls=0.0217 smmd=0.0387 ct=9.3279 rec=1.1741 | train/val/test=1.000/0.642/0.629 | c=0.998347
[Epoch 0053] loss=11.7488 cls=0.0229 smmd=0.0771 ct=9.3005 rec=1.1742 | train/val/test=1.000/0.652/0.644 | c=0.998347
[Epoch 0054] loss=11.6847 cls=0.0142 smmd=0.0231 ct=9.3115 rec=1.1680 | train/val/test=1.000/0.648/0.636 | c=0.998347
[Epoch 0055] loss=11.6446 cls=0.0105 smmd=0.0053 ct=9.3045 rec=1.1621 | train/val/test=1.000/0.642/0.637 | c=0.998347
[Epoch 0056] loss=11.6863 cls=0.0126 smmd=0.0500 ct=9.2916 rec=1.1660 | train/val/test=1.000/0.648/0.640 | c=0.998347
[Epoch 0057] loss=11.6564 cls=0.0111 smmd=0.0127 ct=9.3057 rec=1.1634 | train/val/test=1.000/0.644/0.639 | c=0.998347
[Epoch 0058] loss=11.6269 cls=0.0097 smmd=0.0044 ct=9.2903 rec=1.1613 | train/val/test=1.000/0.644/0.636 | c=0.998347
[Epoch 0059] loss=11.6584 cls=0.0124 smmd=0.0309 ct=9.2841 rec=1.1655 | train/val/test=1.000/0.650/0.637 | c=0.998347
[Epoch 0060] loss=11.6512 cls=0.0123 smmd=0.0042 ct=9.3044 rec=1.1652 | train/val/test=1.000/0.640/0.638 | c=0.998347
[Epoch 0061] loss=11.6260 cls=0.0123 smmd=0.0016 ct=9.2819 rec=1.1651 | train/val/test=1.000/0.640/0.631 | c=0.998347
[Epoch 0062] loss=11.6376 cls=0.0152 smmd=0.0048 ct=9.2808 rec=1.1684 | train/val/test=1.000/0.648/0.637 | c=0.998347
[Epoch 0063] loss=11.6606 cls=0.0167 smmd=0.0015 ct=9.3010 rec=1.1707 | train/val/test=1.000/0.642/0.630 | c=0.998347
[Epoch 0064] loss=11.6548 cls=0.0195 smmd=0.0108 ct=9.2797 rec=1.1724 | train/val/test=1.000/0.646/0.640 | c=0.998347
[Epoch 0065] loss=11.6357 cls=0.0168 smmd=-0.0084 ct=9.2892 rec=1.1690 | train/val/test=1.000/0.648/0.630 | c=0.998347
[Epoch 0066] loss=11.6233 cls=0.0164 smmd=-0.0052 ct=9.2769 rec=1.1676 | train/val/test=1.000/0.642/0.633 | c=0.998347
[Epoch 0067] loss=11.6248 cls=0.0166 smmd=0.0015 ct=9.2702 rec=1.1682 | train/val/test=1.000/0.648/0.641 | c=0.998347
[Epoch 0068] loss=11.6365 cls=0.0173 smmd=-0.0088 ct=9.2932 rec=1.1674 | train/val/test=1.000/0.642/0.633 | c=0.998347
[Epoch 0069] loss=11.6211 cls=0.0161 smmd=-0.0011 ct=9.2717 rec=1.1672 | train/val/test=1.000/0.644/0.635 | c=0.998347
[Epoch 0070] loss=11.6056 cls=0.0149 smmd=-0.0155 ct=9.2751 rec=1.1656 | train/val/test=1.000/0.648/0.641 | c=0.998347
[Epoch 0071] loss=11.6075 cls=0.0151 smmd=-0.0149 ct=9.2750 rec=1.1661 | train/val/test=1.000/0.642/0.632 | c=0.998347
[Epoch 0072] loss=11.6128 cls=0.0157 smmd=-0.0022 ct=9.2648 rec=1.1673 | train/val/test=1.000/0.646/0.639 | c=0.998347
[Epoch 0073] loss=11.6081 cls=0.0150 smmd=-0.0187 ct=9.2768 rec=1.1675 | train/val/test=1.000/0.646/0.634 | c=0.998347
[Epoch 0074] loss=11.6021 cls=0.0151 smmd=-0.0135 ct=9.2659 rec=1.1673 | train/val/test=1.000/0.640/0.633 | c=0.998347
[Epoch 0075] loss=11.5981 cls=0.0148 smmd=-0.0147 ct=9.2623 rec=1.1678 | train/val/test=1.000/0.650/0.634 | c=0.998347
[Epoch 0076] loss=11.6032 cls=0.0152 smmd=-0.0222 ct=9.2730 rec=1.1686 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0077] loss=11.6078 cls=0.0167 smmd=-0.0105 ct=9.2612 rec=1.1702 | train/val/test=1.000/0.648/0.637 | c=0.998347
[Epoch 0078] loss=11.6149 cls=0.0159 smmd=-0.0135 ct=9.2726 rec=1.1699 | train/val/test=1.000/0.648/0.630 | c=0.998347
[Epoch 0079] loss=11.6010 cls=0.0162 smmd=-0.0111 ct=9.2576 rec=1.1691 | train/val/test=1.000/0.648/0.633 | c=0.998347
[Epoch 0080] loss=11.5939 cls=0.0151 smmd=-0.0220 ct=9.2645 rec=1.1681 | train/val/test=1.000/0.646/0.630 | c=0.998347
[Epoch 0081] loss=11.5923 cls=0.0154 smmd=-0.0232 ct=9.2649 rec=1.1676 | train/val/test=1.000/0.640/0.637 | c=0.998347
[Epoch 0082] loss=11.6009 cls=0.0157 smmd=-0.0085 ct=9.2560 rec=1.1689 | train/val/test=1.000/0.648/0.627 | c=0.998347
[Epoch 0083] loss=11.6045 cls=0.0165 smmd=-0.0232 ct=9.2729 rec=1.1691 | train/val/test=1.000/0.642/0.642 | c=0.998347
[Epoch 0084] loss=11.6175 cls=0.0175 smmd=0.0006 ct=9.2559 rec=1.1717 | train/val/test=1.000/0.638/0.617 | c=0.998347
[Epoch 0085] loss=11.6333 cls=0.0192 smmd=-0.0089 ct=9.2788 rec=1.1722 | train/val/test=1.000/0.650/0.649 | c=0.998347
[Epoch 0086] loss=11.6569 cls=0.0206 smmd=0.0162 ct=9.2652 rec=1.1774 | train/val/test=1.000/0.638/0.608 | c=0.998347
[Epoch 0087] loss=11.6838 cls=0.0234 smmd=0.0209 ct=9.2860 rec=1.1767 | train/val/test=1.000/0.646/0.651 | c=0.998347
[Epoch 0088] loss=11.6495 cls=0.0183 smmd=0.0049 ct=9.2793 rec=1.1735 | train/val/test=1.000/0.644/0.635 | c=0.998347
[Epoch 0089] loss=11.6096 cls=0.0134 smmd=0.0060 ct=9.2602 rec=1.1650 | train/val/test=1.000/0.646/0.628 | c=0.998347
[Epoch 0090] loss=11.6078 cls=0.0113 smmd=-0.0080 ct=9.2788 rec=1.1628 | train/val/test=1.000/0.650/0.647 | c=0.998347
[Epoch 0091] loss=11.6008 cls=0.0112 smmd=-0.0012 ct=9.2618 rec=1.1645 | train/val/test=1.000/0.648/0.633 | c=0.998347
[Epoch 0092] loss=11.5940 cls=0.0118 smmd=-0.0045 ct=9.2596 rec=1.1636 | train/val/test=1.000/0.642/0.626 | c=0.998347
[Epoch 0093] loss=11.6059 cls=0.0120 smmd=-0.0160 ct=9.2792 rec=1.1654 | train/val/test=1.000/0.646/0.643 | c=0.998347
[Epoch 0094] loss=11.6022 cls=0.0129 smmd=-0.0022 ct=9.2544 rec=1.1685 | train/val/test=1.000/0.644/0.625 | c=0.998347
[Epoch 0095] loss=11.6004 cls=0.0152 smmd=-0.0123 ct=9.2598 rec=1.1688 | train/val/test=1.000/0.644/0.636 | c=0.998347
[Epoch 0096] loss=11.5995 cls=0.0148 smmd=-0.0229 ct=9.2669 rec=1.1704 | train/val/test=1.000/0.644/0.631 | c=0.998347
[Epoch 0097] loss=11.6080 cls=0.0177 smmd=-0.0067 ct=9.2518 rec=1.1726 | train/val/test=1.000/0.644/0.626 | c=0.998347
[Epoch 0098] loss=11.6151 cls=0.0184 smmd=-0.0225 ct=9.2744 rec=1.1724 | train/val/test=1.000/0.640/0.640 | c=0.998347
[Epoch 0099] loss=11.6275 cls=0.0185 smmd=0.0060 ct=9.2536 rec=1.1747 | train/val/test=1.000/0.642/0.626 | c=0.998347
=== Best @ epoch 24: val=0.6620, test=0.6590 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-5 completed in 20.90 seconds.
==================================================
