Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4 - 2025-09-21 05:35:39:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=71.9200 cls=1.0994 smmd=5.6158 ct=7.2528 rec=1.4136 | train/val/test=0.402/0.417/0.406 | c=0.998347
[Epoch 0001] loss=52.4432 cls=1.0637 smmd=3.6802 ct=7.2007 rec=1.4154 | train/val/test=0.565/0.568/0.558 | c=0.998347
[Epoch 0002] loss=38.1083 cls=1.0755 smmd=2.2583 ct=7.1403 rec=1.4138 | train/val/test=0.562/0.556/0.547 | c=0.998347
[Epoch 0003] loss=40.8594 cls=1.0559 smmd=2.5414 ct=7.1052 rec=1.4142 | train/val/test=0.570/0.565/0.556 | c=0.998347
[Epoch 0004] loss=40.0736 cls=1.0230 smmd=2.4754 ct=7.0502 rec=1.4164 | train/val/test=0.561/0.567/0.544 | c=0.998347
[Epoch 0005] loss=35.5585 cls=0.9922 smmd=2.0383 ct=6.9850 rec=1.4186 | train/val/test=0.538/0.546/0.527 | c=0.998347
[Epoch 0006] loss=30.3844 cls=0.9584 smmd=1.5316 ct=6.9399 rec=1.4188 | train/val/test=0.527/0.531/0.516 | c=0.998347
[Epoch 0007] loss=33.7723 cls=0.9139 smmd=1.8734 ct=6.9373 rec=1.4137 | train/val/test=0.555/0.558/0.545 | c=0.998347
[Epoch 0008] loss=35.8364 cls=0.8669 smmd=1.9669 ct=7.5154 rec=1.4058 | train/val/test=0.622/0.623/0.614 | c=0.998347
[Epoch 0009] loss=29.0375 cls=0.8227 smmd=1.3016 ct=7.4559 rec=1.3964 | train/val/test=0.645/0.650/0.634 | c=0.998347
[Epoch 0010] loss=27.9360 cls=0.7901 smmd=1.1850 ct=7.4987 rec=1.3877 | train/val/test=0.657/0.662/0.644 | c=0.998347
[Epoch 0011] loss=30.9233 cls=0.7644 smmd=1.4814 ct=7.5182 rec=1.3809 | train/val/test=0.667/0.668/0.654 | c=0.998347
[Epoch 0012] loss=27.2719 cls=0.7414 smmd=1.1236 ct=7.4892 rec=1.3736 | train/val/test=0.673/0.679/0.663 | c=0.998347
[Epoch 0013] loss=26.4533 cls=0.7179 smmd=1.0439 ct=7.4865 rec=1.3645 | train/val/test=0.680/0.690/0.670 | c=0.998347
[Epoch 0014] loss=25.9255 cls=0.7005 smmd=0.9905 ct=7.4956 rec=1.3579 | train/val/test=0.690/0.696/0.680 | c=0.998347
[Epoch 0015] loss=25.6452 cls=0.6820 smmd=0.9620 ct=7.5037 rec=1.3540 | train/val/test=0.707/0.709/0.689 | c=0.998347
[Epoch 0016] loss=24.7573 cls=0.6659 smmd=0.8714 ct=7.5177 rec=1.3499 | train/val/test=0.720/0.721/0.707 | c=0.998347
[Epoch 0017] loss=23.2333 cls=0.6493 smmd=0.7272 ct=7.4828 rec=1.3426 | train/val/test=0.748/0.744/0.735 | c=0.998347
[Epoch 0018] loss=23.7928 cls=0.6264 smmd=0.7834 ct=7.4880 rec=1.3393 | train/val/test=0.761/0.757/0.751 | c=0.998347
[Epoch 0019] loss=22.7176 cls=0.6057 smmd=0.6698 ct=7.5247 rec=1.3354 | train/val/test=0.778/0.776/0.767 | c=0.998347
[Epoch 0020] loss=22.1754 cls=0.5875 smmd=0.6169 ct=7.5233 rec=1.3315 | train/val/test=0.778/0.776/0.769 | c=0.998347
[Epoch 0021] loss=21.8986 cls=0.5715 smmd=0.6018 ct=7.4653 rec=1.3278 | train/val/test=0.793/0.795/0.786 | c=0.998347
[Epoch 0022] loss=21.5576 cls=0.5495 smmd=0.5684 ct=7.4681 rec=1.3258 | train/val/test=0.809/0.806/0.801 | c=0.998347
[Epoch 0023] loss=20.7689 cls=0.5315 smmd=0.4829 ct=7.5059 rec=1.3239 | train/val/test=0.807/0.804/0.791 | c=0.998347
[Epoch 0024] loss=20.7762 cls=0.5200 smmd=0.4838 ct=7.5089 rec=1.3218 | train/val/test=0.815/0.811/0.800 | c=0.998347
[Epoch 0025] loss=20.3631 cls=0.5068 smmd=0.4455 ct=7.4970 rec=1.3205 | train/val/test=0.824/0.823/0.814 | c=0.998347
[Epoch 0026] loss=19.9761 cls=0.4948 smmd=0.4133 ct=7.4679 rec=1.3194 | train/val/test=0.824/0.824/0.811 | c=0.998347
[Epoch 0027] loss=20.0423 cls=0.4869 smmd=0.4209 ct=7.4655 rec=1.3180 | train/val/test=0.828/0.824/0.817 | c=0.998347
[Epoch 0028] loss=19.4172 cls=0.4815 smmd=0.3526 ct=7.4966 rec=1.3152 | train/val/test=0.829/0.830/0.820 | c=0.998347
[Epoch 0029] loss=19.4148 cls=0.4730 smmd=0.3515 ct=7.5026 rec=1.3161 | train/val/test=0.830/0.829/0.820 | c=0.998347
[Epoch 0030] loss=19.2311 cls=0.4684 smmd=0.3397 ct=7.4713 rec=1.3154 | train/val/test=0.833/0.831/0.825 | c=0.998347
[Epoch 0031] loss=18.8833 cls=0.4660 smmd=0.3082 ct=7.4560 rec=1.3132 | train/val/test=0.833/0.834/0.825 | c=0.998347
[Epoch 0032] loss=18.8003 cls=0.4621 smmd=0.2963 ct=7.4755 rec=1.3114 | train/val/test=0.835/0.834/0.825 | c=0.998347
[Epoch 0033] loss=18.5479 cls=0.4597 smmd=0.2693 ct=7.4847 rec=1.3106 | train/val/test=0.837/0.835/0.828 | c=0.998347
[Epoch 0034] loss=18.5054 cls=0.4588 smmd=0.2684 ct=7.4684 rec=1.3113 | train/val/test=0.834/0.833/0.822 | c=0.998347
[Epoch 0035] loss=18.2740 cls=0.4585 smmd=0.2467 ct=7.4618 rec=1.3088 | train/val/test=0.835/0.835/0.824 | c=0.998347
[Epoch 0036] loss=18.0983 cls=0.4590 smmd=0.2280 ct=7.4673 rec=1.3087 | train/val/test=0.839/0.840/0.828 | c=0.998347
[Epoch 0037] loss=18.0075 cls=0.4595 smmd=0.2186 ct=7.4682 rec=1.3115 | train/val/test=0.832/0.826/0.819 | c=0.998347
[Epoch 0038] loss=17.9379 cls=0.4631 smmd=0.2137 ct=7.4574 rec=1.3087 | train/val/test=0.837/0.836/0.826 | c=0.998347
[Epoch 0039] loss=17.7860 cls=0.4587 smmd=0.1958 ct=7.4716 rec=1.3113 | train/val/test=0.835/0.832/0.822 | c=0.998347
[Epoch 0040] loss=17.7945 cls=0.4615 smmd=0.1979 ct=7.4645 rec=1.3109 | train/val/test=0.834/0.828/0.820 | c=0.998347
[Epoch 0041] loss=17.6587 cls=0.4679 smmd=0.1870 ct=7.4497 rec=1.3108 | train/val/test=0.839/0.838/0.826 | c=0.998347
[Epoch 0042] loss=17.5707 cls=0.4627 smmd=0.1740 ct=7.4707 rec=1.3159 | train/val/test=0.832/0.829/0.821 | c=0.998347
[Epoch 0043] loss=17.5151 cls=0.4685 smmd=0.1704 ct=7.4598 rec=1.3155 | train/val/test=0.837/0.837/0.829 | c=0.998347
[Epoch 0044] loss=17.4757 cls=0.4678 smmd=0.1661 ct=7.4612 rec=1.3165 | train/val/test=0.834/0.831/0.822 | c=0.998347
[Epoch 0045] loss=17.4710 cls=0.4698 smmd=0.1663 ct=7.4569 rec=1.3177 | train/val/test=0.839/0.840/0.829 | c=0.998347
[Epoch 0046] loss=17.3281 cls=0.4704 smmd=0.1526 ct=7.4541 rec=1.3182 | train/val/test=0.837/0.835/0.827 | c=0.998347
[Epoch 0047] loss=17.2488 cls=0.4691 smmd=0.1434 ct=7.4603 rec=1.3196 | train/val/test=0.840/0.839/0.828 | c=0.998347
[Epoch 0048] loss=17.2277 cls=0.4701 smmd=0.1430 ct=7.4512 rec=1.3197 | train/val/test=0.840/0.840/0.828 | c=0.998347
[Epoch 0049] loss=17.1482 cls=0.4708 smmd=0.1345 ct=7.4539 rec=1.3198 | train/val/test=0.836/0.836/0.825 | c=0.998347
[Epoch 0050] loss=17.1759 cls=0.4716 smmd=0.1384 ct=7.4483 rec=1.3189 | train/val/test=0.842/0.842/0.829 | c=0.998347
[Epoch 0051] loss=17.1186 cls=0.4722 smmd=0.1318 ct=7.4519 rec=1.3209 | train/val/test=0.837/0.836/0.825 | c=0.998347
[Epoch 0052] loss=17.1095 cls=0.4721 smmd=0.1320 ct=7.4468 rec=1.3197 | train/val/test=0.842/0.844/0.829 | c=0.998347
[Epoch 0053] loss=17.0642 cls=0.4745 smmd=0.1267 ct=7.4494 rec=1.3216 | train/val/test=0.835/0.833/0.826 | c=0.998347
[Epoch 0054] loss=17.0643 cls=0.4761 smmd=0.1277 ct=7.4449 rec=1.3196 | train/val/test=0.840/0.841/0.830 | c=0.998347
[Epoch 0055] loss=16.9998 cls=0.4740 smmd=0.1208 ct=7.4470 rec=1.3220 | train/val/test=0.836/0.836/0.827 | c=0.998347
[Epoch 0056] loss=17.0189 cls=0.4776 smmd=0.1235 ct=7.4427 rec=1.3193 | train/val/test=0.837/0.838/0.826 | c=0.998347
[Epoch 0057] loss=17.0115 cls=0.4742 smmd=0.1226 ct=7.4435 rec=1.3219 | train/val/test=0.836/0.837/0.826 | c=0.998347
[Epoch 0058] loss=17.0205 cls=0.4772 smmd=0.1240 ct=7.4409 rec=1.3192 | train/val/test=0.839/0.839/0.828 | c=0.998347
[Epoch 0059] loss=16.9397 cls=0.4726 smmd=0.1167 ct=7.4378 rec=1.3215 | train/val/test=0.834/0.835/0.825 | c=0.998347
[Epoch 0060] loss=16.9277 cls=0.4786 smmd=0.1173 ct=7.4280 rec=1.3186 | train/val/test=0.841/0.839/0.831 | c=0.998347
[Epoch 0061] loss=16.9629 cls=0.4718 smmd=0.1181 ct=7.4423 rec=1.3222 | train/val/test=0.834/0.834/0.827 | c=0.998347
[Epoch 0062] loss=16.9707 cls=0.4783 smmd=0.1234 ct=7.4188 rec=1.3193 | train/val/test=0.843/0.838/0.831 | c=0.998347
[Epoch 0063] loss=17.0023 cls=0.4742 smmd=0.1228 ct=7.4374 rec=1.3241 | train/val/test=0.833/0.834/0.824 | c=0.998347
[Epoch 0064] loss=17.0167 cls=0.4810 smmd=0.1280 ct=7.4177 rec=1.3223 | train/val/test=0.842/0.840/0.832 | c=0.998347
[Epoch 0065] loss=16.9974 cls=0.4781 smmd=0.1232 ct=7.4316 rec=1.3268 | train/val/test=0.832/0.830/0.823 | c=0.998347
[Epoch 0066] loss=17.0408 cls=0.4853 smmd=0.1309 ct=7.4136 rec=1.3247 | train/val/test=0.841/0.840/0.835 | c=0.998347
[Epoch 0067] loss=17.0877 cls=0.4856 smmd=0.1321 ct=7.4293 rec=1.3315 | train/val/test=0.826/0.820/0.813 | c=0.998347
[Epoch 0068] loss=17.1151 cls=0.4946 smmd=0.1393 ct=7.4055 rec=1.3285 | train/val/test=0.837/0.837/0.830 | c=0.998347
[Epoch 0069] loss=17.2313 cls=0.4933 smmd=0.1462 ct=7.4277 rec=1.3338 | train/val/test=0.818/0.815/0.806 | c=0.998347
[Epoch 0070] loss=17.2414 cls=0.5033 smmd=0.1533 ct=7.3956 rec=1.3314 | train/val/test=0.832/0.835/0.826 | c=0.998347
[Epoch 0071] loss=17.1038 cls=0.4962 smmd=0.1335 ct=7.4265 rec=1.3345 | train/val/test=0.821/0.817/0.809 | c=0.998347
[Epoch 0072] loss=17.1671 cls=0.4969 smmd=0.1479 ct=7.3877 rec=1.3286 | train/val/test=0.839/0.839/0.833 | c=0.998347
[Epoch 0073] loss=17.0299 cls=0.4822 smmd=0.1316 ct=7.4045 rec=1.3283 | train/val/test=0.830/0.830/0.820 | c=0.998347
[Epoch 0074] loss=16.8410 cls=0.4790 smmd=0.1173 ct=7.3830 rec=1.3247 | train/val/test=0.840/0.837/0.833 | c=0.998347
[Epoch 0075] loss=16.7745 cls=0.4737 smmd=0.1096 ct=7.3895 rec=1.3247 | train/val/test=0.840/0.839/0.833 | c=0.998347
[Epoch 0076] loss=16.7861 cls=0.4767 smmd=0.1120 ct=7.3824 rec=1.3260 | train/val/test=0.835/0.835/0.827 | c=0.998347
[Epoch 0077] loss=16.8388 cls=0.4817 smmd=0.1164 ct=7.3849 rec=1.3289 | train/val/test=0.842/0.841/0.836 | c=0.998347
[Epoch 0078] loss=16.8324 cls=0.4843 smmd=0.1139 ct=7.3931 rec=1.3303 | train/val/test=0.831/0.828/0.820 | c=0.998347
[Epoch 0079] loss=16.9232 cls=0.4917 smmd=0.1251 ct=7.3799 rec=1.3335 | train/val/test=0.844/0.841/0.835 | c=0.998347
[Epoch 0080] loss=16.9365 cls=0.4891 smmd=0.1232 ct=7.3969 rec=1.3318 | train/val/test=0.830/0.828/0.819 | c=0.998347
[Epoch 0081] loss=16.9332 cls=0.4905 smmd=0.1268 ct=7.3765 rec=1.3332 | train/val/test=0.842/0.841/0.835 | c=0.998347
[Epoch 0082] loss=16.9232 cls=0.4892 smmd=0.1238 ct=7.3880 rec=1.3299 | train/val/test=0.826/0.823/0.814 | c=0.998347
[Epoch 0083] loss=16.9336 cls=0.4886 smmd=0.1273 ct=7.3753 rec=1.3309 | train/val/test=0.840/0.840/0.830 | c=0.998347
[Epoch 0084] loss=16.9598 cls=0.4896 smmd=0.1281 ct=7.3850 rec=1.3276 | train/val/test=0.826/0.821/0.814 | c=0.998347
[Epoch 0085] loss=16.9335 cls=0.4848 smmd=0.1293 ct=7.3672 rec=1.3281 | train/val/test=0.841/0.840/0.831 | c=0.998347
[Epoch 0086] loss=16.8160 cls=0.4858 smmd=0.1139 ct=7.3855 rec=1.3255 | train/val/test=0.829/0.827/0.818 | c=0.998347
[Epoch 0087] loss=16.8143 cls=0.4822 smmd=0.1186 ct=7.3620 rec=1.3260 | train/val/test=0.840/0.840/0.833 | c=0.998347
[Epoch 0088] loss=16.8032 cls=0.4826 smmd=0.1138 ct=7.3803 rec=1.3260 | train/val/test=0.832/0.829/0.818 | c=0.998347
[Epoch 0089] loss=16.8254 cls=0.4850 smmd=0.1190 ct=7.3643 rec=1.3279 | train/val/test=0.840/0.839/0.834 | c=0.998347
[Epoch 0090] loss=16.8258 cls=0.4855 smmd=0.1163 ct=7.3780 rec=1.3288 | train/val/test=0.829/0.827/0.818 | c=0.998347
[Epoch 0091] loss=16.8603 cls=0.4905 smmd=0.1208 ct=7.3705 rec=1.3314 | train/val/test=0.840/0.841/0.834 | c=0.998347
[Epoch 0092] loss=16.8648 cls=0.4896 smmd=0.1206 ct=7.3743 rec=1.3316 | train/val/test=0.824/0.820/0.811 | c=0.998347
[Epoch 0093] loss=16.8748 cls=0.4981 smmd=0.1215 ct=7.3720 rec=1.3337 | train/val/test=0.840/0.840/0.831 | c=0.998347
[Epoch 0094] loss=16.9474 cls=0.4941 smmd=0.1269 ct=7.3821 rec=1.3342 | train/val/test=0.819/0.815/0.805 | c=0.998347
[Epoch 0095] loss=16.9971 cls=0.5068 smmd=0.1347 ct=7.3654 rec=1.3324 | train/val/test=0.833/0.832/0.823 | c=0.998347
[Epoch 0096] loss=17.0988 cls=0.5035 smmd=0.1379 ct=7.3993 rec=1.3379 | train/val/test=0.807/0.802/0.799 | c=0.998347
[Epoch 0097] loss=17.2233 cls=0.5249 smmd=0.1558 ct=7.3686 rec=1.3315 | train/val/test=0.827/0.828/0.818 | c=0.998347
[Epoch 0098] loss=17.2524 cls=0.5098 smmd=0.1516 ct=7.4059 rec=1.3400 | train/val/test=0.813/0.806/0.801 | c=0.998347
[Epoch 0099] loss=17.1316 cls=0.5199 smmd=0.1468 ct=7.3706 rec=1.3257 | train/val/test=0.841/0.845/0.828 | c=0.998347
=== Best @ epoch 99: val=0.8445, test=0.8281 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4 - 2025-09-21 05:35:39:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=71.9200 cls=1.0994 smmd=5.6158 ct=7.2528 rec=1.4136 | train/val/test=0.402/0.417/0.406 | c=0.998347
[Epoch 0001] loss=52.4432 cls=1.0637 smmd=3.6802 ct=7.2007 rec=1.4154 | train/val/test=0.565/0.568/0.558 | c=0.998347
[Epoch 0002] loss=38.1083 cls=1.0755 smmd=2.2583 ct=7.1403 rec=1.4138 | train/val/test=0.562/0.556/0.547 | c=0.998347
[Epoch 0003] loss=40.8594 cls=1.0559 smmd=2.5414 ct=7.1052 rec=1.4142 | train/val/test=0.570/0.565/0.556 | c=0.998347
[Epoch 0004] loss=40.0736 cls=1.0230 smmd=2.4754 ct=7.0502 rec=1.4164 | train/val/test=0.561/0.567/0.544 | c=0.998347
[Epoch 0005] loss=35.5585 cls=0.9922 smmd=2.0383 ct=6.9850 rec=1.4186 | train/val/test=0.538/0.546/0.527 | c=0.998347
[Epoch 0006] loss=30.3844 cls=0.9584 smmd=1.5316 ct=6.9399 rec=1.4188 | train/val/test=0.527/0.531/0.516 | c=0.998347
[Epoch 0007] loss=33.7723 cls=0.9139 smmd=1.8734 ct=6.9373 rec=1.4137 | train/val/test=0.555/0.558/0.545 | c=0.998347
[Epoch 0008] loss=35.8364 cls=0.8669 smmd=1.9669 ct=7.5154 rec=1.4058 | train/val/test=0.622/0.623/0.614 | c=0.998347
[Epoch 0009] loss=29.0375 cls=0.8227 smmd=1.3016 ct=7.4559 rec=1.3964 | train/val/test=0.645/0.650/0.634 | c=0.998347
[Epoch 0010] loss=27.9360 cls=0.7901 smmd=1.1850 ct=7.4987 rec=1.3877 | train/val/test=0.657/0.662/0.644 | c=0.998347
[Epoch 0011] loss=30.9233 cls=0.7644 smmd=1.4814 ct=7.5182 rec=1.3809 | train/val/test=0.667/0.668/0.654 | c=0.998347
[Epoch 0012] loss=27.2719 cls=0.7414 smmd=1.1236 ct=7.4892 rec=1.3736 | train/val/test=0.673/0.679/0.663 | c=0.998347
[Epoch 0013] loss=26.4533 cls=0.7179 smmd=1.0439 ct=7.4865 rec=1.3645 | train/val/test=0.680/0.690/0.670 | c=0.998347
[Epoch 0014] loss=25.9255 cls=0.7005 smmd=0.9905 ct=7.4956 rec=1.3579 | train/val/test=0.690/0.696/0.680 | c=0.998347
[Epoch 0015] loss=25.6452 cls=0.6820 smmd=0.9620 ct=7.5037 rec=1.3540 | train/val/test=0.707/0.709/0.689 | c=0.998347
[Epoch 0016] loss=24.7573 cls=0.6659 smmd=0.8714 ct=7.5177 rec=1.3499 | train/val/test=0.720/0.721/0.707 | c=0.998347
[Epoch 0017] loss=23.2333 cls=0.6493 smmd=0.7272 ct=7.4828 rec=1.3426 | train/val/test=0.748/0.744/0.735 | c=0.998347
[Epoch 0018] loss=23.7928 cls=0.6264 smmd=0.7834 ct=7.4880 rec=1.3393 | train/val/test=0.761/0.757/0.751 | c=0.998347
[Epoch 0019] loss=22.7176 cls=0.6057 smmd=0.6698 ct=7.5247 rec=1.3354 | train/val/test=0.778/0.776/0.767 | c=0.998347
[Epoch 0020] loss=22.1754 cls=0.5875 smmd=0.6169 ct=7.5233 rec=1.3315 | train/val/test=0.778/0.776/0.769 | c=0.998347
[Epoch 0021] loss=21.8986 cls=0.5715 smmd=0.6018 ct=7.4653 rec=1.3278 | train/val/test=0.793/0.795/0.786 | c=0.998347
[Epoch 0022] loss=21.5576 cls=0.5495 smmd=0.5684 ct=7.4681 rec=1.3258 | train/val/test=0.809/0.806/0.801 | c=0.998347
[Epoch 0023] loss=20.7689 cls=0.5315 smmd=0.4829 ct=7.5059 rec=1.3239 | train/val/test=0.807/0.804/0.791 | c=0.998347
[Epoch 0024] loss=20.7762 cls=0.5200 smmd=0.4838 ct=7.5089 rec=1.3218 | train/val/test=0.815/0.811/0.800 | c=0.998347
[Epoch 0025] loss=20.3631 cls=0.5068 smmd=0.4455 ct=7.4970 rec=1.3205 | train/val/test=0.824/0.823/0.814 | c=0.998347
[Epoch 0026] loss=19.9761 cls=0.4948 smmd=0.4133 ct=7.4679 rec=1.3194 | train/val/test=0.824/0.824/0.811 | c=0.998347
[Epoch 0027] loss=20.0423 cls=0.4869 smmd=0.4209 ct=7.4655 rec=1.3180 | train/val/test=0.828/0.824/0.817 | c=0.998347
[Epoch 0028] loss=19.4172 cls=0.4815 smmd=0.3526 ct=7.4966 rec=1.3152 | train/val/test=0.829/0.830/0.820 | c=0.998347
[Epoch 0029] loss=19.4148 cls=0.4730 smmd=0.3515 ct=7.5026 rec=1.3161 | train/val/test=0.830/0.829/0.820 | c=0.998347
[Epoch 0030] loss=19.2311 cls=0.4684 smmd=0.3397 ct=7.4713 rec=1.3154 | train/val/test=0.833/0.831/0.825 | c=0.998347
[Epoch 0031] loss=18.8833 cls=0.4660 smmd=0.3082 ct=7.4560 rec=1.3132 | train/val/test=0.833/0.834/0.825 | c=0.998347
[Epoch 0032] loss=18.8003 cls=0.4621 smmd=0.2963 ct=7.4755 rec=1.3114 | train/val/test=0.835/0.834/0.825 | c=0.998347
[Epoch 0033] loss=18.5479 cls=0.4597 smmd=0.2693 ct=7.4847 rec=1.3106 | train/val/test=0.837/0.835/0.828 | c=0.998347
[Epoch 0034] loss=18.5054 cls=0.4588 smmd=0.2684 ct=7.4684 rec=1.3113 | train/val/test=0.834/0.833/0.822 | c=0.998347
[Epoch 0035] loss=18.2740 cls=0.4585 smmd=0.2467 ct=7.4618 rec=1.3088 | train/val/test=0.835/0.835/0.824 | c=0.998347
[Epoch 0036] loss=18.0983 cls=0.4590 smmd=0.2280 ct=7.4673 rec=1.3087 | train/val/test=0.839/0.840/0.828 | c=0.998347
[Epoch 0037] loss=18.0075 cls=0.4595 smmd=0.2186 ct=7.4682 rec=1.3115 | train/val/test=0.832/0.826/0.819 | c=0.998347
[Epoch 0038] loss=17.9379 cls=0.4631 smmd=0.2137 ct=7.4574 rec=1.3087 | train/val/test=0.837/0.836/0.826 | c=0.998347
[Epoch 0039] loss=17.7860 cls=0.4587 smmd=0.1958 ct=7.4716 rec=1.3113 | train/val/test=0.835/0.832/0.822 | c=0.998347
[Epoch 0040] loss=17.7945 cls=0.4615 smmd=0.1979 ct=7.4645 rec=1.3109 | train/val/test=0.834/0.828/0.820 | c=0.998347
[Epoch 0041] loss=17.6587 cls=0.4679 smmd=0.1870 ct=7.4497 rec=1.3108 | train/val/test=0.839/0.838/0.826 | c=0.998347
[Epoch 0042] loss=17.5707 cls=0.4627 smmd=0.1740 ct=7.4707 rec=1.3159 | train/val/test=0.832/0.829/0.821 | c=0.998347
[Epoch 0043] loss=17.5151 cls=0.4685 smmd=0.1704 ct=7.4598 rec=1.3155 | train/val/test=0.837/0.837/0.829 | c=0.998347
[Epoch 0044] loss=17.4757 cls=0.4678 smmd=0.1661 ct=7.4612 rec=1.3165 | train/val/test=0.834/0.831/0.822 | c=0.998347
[Epoch 0045] loss=17.4710 cls=0.4698 smmd=0.1663 ct=7.4569 rec=1.3177 | train/val/test=0.839/0.840/0.829 | c=0.998347
[Epoch 0046] loss=17.3281 cls=0.4704 smmd=0.1526 ct=7.4541 rec=1.3182 | train/val/test=0.837/0.835/0.827 | c=0.998347
[Epoch 0047] loss=17.2488 cls=0.4691 smmd=0.1434 ct=7.4603 rec=1.3196 | train/val/test=0.840/0.839/0.828 | c=0.998347
[Epoch 0048] loss=17.2277 cls=0.4701 smmd=0.1430 ct=7.4512 rec=1.3197 | train/val/test=0.840/0.840/0.828 | c=0.998347
[Epoch 0049] loss=17.1482 cls=0.4708 smmd=0.1345 ct=7.4539 rec=1.3198 | train/val/test=0.836/0.836/0.825 | c=0.998347
[Epoch 0050] loss=17.1759 cls=0.4716 smmd=0.1384 ct=7.4483 rec=1.3189 | train/val/test=0.842/0.842/0.829 | c=0.998347
[Epoch 0051] loss=17.1186 cls=0.4722 smmd=0.1318 ct=7.4519 rec=1.3209 | train/val/test=0.837/0.836/0.825 | c=0.998347
[Epoch 0052] loss=17.1095 cls=0.4721 smmd=0.1320 ct=7.4468 rec=1.3197 | train/val/test=0.842/0.844/0.829 | c=0.998347
[Epoch 0053] loss=17.0642 cls=0.4745 smmd=0.1267 ct=7.4494 rec=1.3216 | train/val/test=0.835/0.833/0.826 | c=0.998347
[Epoch 0054] loss=17.0643 cls=0.4761 smmd=0.1277 ct=7.4449 rec=1.3196 | train/val/test=0.840/0.841/0.830 | c=0.998347
[Epoch 0055] loss=16.9998 cls=0.4740 smmd=0.1208 ct=7.4470 rec=1.3220 | train/val/test=0.836/0.836/0.827 | c=0.998347
[Epoch 0056] loss=17.0189 cls=0.4776 smmd=0.1235 ct=7.4427 rec=1.3193 | train/val/test=0.837/0.838/0.826 | c=0.998347
[Epoch 0057] loss=17.0115 cls=0.4742 smmd=0.1226 ct=7.4435 rec=1.3219 | train/val/test=0.836/0.837/0.826 | c=0.998347
[Epoch 0058] loss=17.0205 cls=0.4772 smmd=0.1240 ct=7.4409 rec=1.3192 | train/val/test=0.839/0.839/0.828 | c=0.998347
[Epoch 0059] loss=16.9397 cls=0.4726 smmd=0.1167 ct=7.4378 rec=1.3215 | train/val/test=0.834/0.835/0.825 | c=0.998347
[Epoch 0060] loss=16.9277 cls=0.4786 smmd=0.1173 ct=7.4280 rec=1.3186 | train/val/test=0.841/0.839/0.831 | c=0.998347
[Epoch 0061] loss=16.9629 cls=0.4718 smmd=0.1181 ct=7.4423 rec=1.3222 | train/val/test=0.834/0.834/0.827 | c=0.998347
[Epoch 0062] loss=16.9707 cls=0.4783 smmd=0.1234 ct=7.4188 rec=1.3193 | train/val/test=0.843/0.838/0.831 | c=0.998347
[Epoch 0063] loss=17.0023 cls=0.4742 smmd=0.1228 ct=7.4374 rec=1.3241 | train/val/test=0.833/0.834/0.824 | c=0.998347
[Epoch 0064] loss=17.0167 cls=0.4810 smmd=0.1280 ct=7.4177 rec=1.3223 | train/val/test=0.842/0.840/0.832 | c=0.998347
[Epoch 0065] loss=16.9974 cls=0.4781 smmd=0.1232 ct=7.4316 rec=1.3268 | train/val/test=0.832/0.830/0.823 | c=0.998347
[Epoch 0066] loss=17.0408 cls=0.4853 smmd=0.1309 ct=7.4136 rec=1.3247 | train/val/test=0.841/0.840/0.835 | c=0.998347
[Epoch 0067] loss=17.0877 cls=0.4856 smmd=0.1321 ct=7.4293 rec=1.3315 | train/val/test=0.826/0.820/0.813 | c=0.998347
[Epoch 0068] loss=17.1151 cls=0.4946 smmd=0.1393 ct=7.4055 rec=1.3285 | train/val/test=0.837/0.837/0.830 | c=0.998347
[Epoch 0069] loss=17.2313 cls=0.4933 smmd=0.1462 ct=7.4277 rec=1.3338 | train/val/test=0.818/0.815/0.806 | c=0.998347
[Epoch 0070] loss=17.2414 cls=0.5033 smmd=0.1533 ct=7.3956 rec=1.3314 | train/val/test=0.832/0.835/0.826 | c=0.998347
[Epoch 0071] loss=17.1038 cls=0.4962 smmd=0.1335 ct=7.4265 rec=1.3345 | train/val/test=0.821/0.817/0.809 | c=0.998347
[Epoch 0072] loss=17.1671 cls=0.4969 smmd=0.1479 ct=7.3877 rec=1.3286 | train/val/test=0.839/0.839/0.833 | c=0.998347
[Epoch 0073] loss=17.0299 cls=0.4822 smmd=0.1316 ct=7.4045 rec=1.3283 | train/val/test=0.830/0.830/0.820 | c=0.998347
[Epoch 0074] loss=16.8410 cls=0.4790 smmd=0.1173 ct=7.3830 rec=1.3247 | train/val/test=0.840/0.837/0.833 | c=0.998347
[Epoch 0075] loss=16.7745 cls=0.4737 smmd=0.1096 ct=7.3895 rec=1.3247 | train/val/test=0.840/0.839/0.833 | c=0.998347
[Epoch 0076] loss=16.7861 cls=0.4767 smmd=0.1120 ct=7.3824 rec=1.3260 | train/val/test=0.835/0.835/0.827 | c=0.998347
[Epoch 0077] loss=16.8388 cls=0.4817 smmd=0.1164 ct=7.3849 rec=1.3289 | train/val/test=0.842/0.841/0.836 | c=0.998347
[Epoch 0078] loss=16.8324 cls=0.4843 smmd=0.1139 ct=7.3931 rec=1.3303 | train/val/test=0.831/0.828/0.820 | c=0.998347
[Epoch 0079] loss=16.9232 cls=0.4917 smmd=0.1251 ct=7.3799 rec=1.3335 | train/val/test=0.844/0.841/0.835 | c=0.998347
[Epoch 0080] loss=16.9365 cls=0.4891 smmd=0.1232 ct=7.3969 rec=1.3318 | train/val/test=0.830/0.828/0.819 | c=0.998347
[Epoch 0081] loss=16.9332 cls=0.4905 smmd=0.1268 ct=7.3765 rec=1.3332 | train/val/test=0.842/0.841/0.835 | c=0.998347
[Epoch 0082] loss=16.9232 cls=0.4892 smmd=0.1238 ct=7.3880 rec=1.3299 | train/val/test=0.826/0.823/0.814 | c=0.998347
[Epoch 0083] loss=16.9336 cls=0.4886 smmd=0.1273 ct=7.3753 rec=1.3309 | train/val/test=0.840/0.840/0.830 | c=0.998347
[Epoch 0084] loss=16.9598 cls=0.4896 smmd=0.1281 ct=7.3850 rec=1.3276 | train/val/test=0.826/0.821/0.814 | c=0.998347
[Epoch 0085] loss=16.9335 cls=0.4848 smmd=0.1293 ct=7.3672 rec=1.3281 | train/val/test=0.841/0.840/0.831 | c=0.998347
[Epoch 0086] loss=16.8160 cls=0.4858 smmd=0.1139 ct=7.3855 rec=1.3255 | train/val/test=0.829/0.827/0.818 | c=0.998347
[Epoch 0087] loss=16.8143 cls=0.4822 smmd=0.1186 ct=7.3620 rec=1.3260 | train/val/test=0.840/0.840/0.833 | c=0.998347
[Epoch 0088] loss=16.8032 cls=0.4826 smmd=0.1138 ct=7.3803 rec=1.3260 | train/val/test=0.832/0.829/0.818 | c=0.998347
[Epoch 0089] loss=16.8254 cls=0.4850 smmd=0.1190 ct=7.3643 rec=1.3279 | train/val/test=0.840/0.839/0.834 | c=0.998347
[Epoch 0090] loss=16.8258 cls=0.4855 smmd=0.1163 ct=7.3780 rec=1.3288 | train/val/test=0.829/0.827/0.818 | c=0.998347
[Epoch 0091] loss=16.8603 cls=0.4905 smmd=0.1208 ct=7.3705 rec=1.3314 | train/val/test=0.840/0.841/0.834 | c=0.998347
[Epoch 0092] loss=16.8648 cls=0.4896 smmd=0.1206 ct=7.3743 rec=1.3316 | train/val/test=0.824/0.820/0.811 | c=0.998347
[Epoch 0093] loss=16.8748 cls=0.4981 smmd=0.1215 ct=7.3720 rec=1.3337 | train/val/test=0.840/0.840/0.831 | c=0.998347
[Epoch 0094] loss=16.9474 cls=0.4941 smmd=0.1269 ct=7.3821 rec=1.3342 | train/val/test=0.819/0.815/0.805 | c=0.998347
[Epoch 0095] loss=16.9971 cls=0.5068 smmd=0.1347 ct=7.3654 rec=1.3324 | train/val/test=0.833/0.832/0.823 | c=0.998347
[Epoch 0096] loss=17.0988 cls=0.5035 smmd=0.1379 ct=7.3993 rec=1.3379 | train/val/test=0.807/0.802/0.799 | c=0.998347
[Epoch 0097] loss=17.2233 cls=0.5249 smmd=0.1558 ct=7.3686 rec=1.3315 | train/val/test=0.827/0.828/0.818 | c=0.998347
[Epoch 0098] loss=17.2524 cls=0.5098 smmd=0.1516 ct=7.4059 rec=1.3400 | train/val/test=0.813/0.806/0.801 | c=0.998347
[Epoch 0099] loss=17.1316 cls=0.5199 smmd=0.1468 ct=7.3706 rec=1.3257 | train/val/test=0.841/0.845/0.828 | c=0.998347
=== Best @ epoch 99: val=0.8445, test=0.8281 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-10-4 completed in 179.54 seconds.
==================================================
