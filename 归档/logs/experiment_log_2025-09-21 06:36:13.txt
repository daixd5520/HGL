Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 - 2025-09-21 06:36:13:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1920 cls=1.9463 smmd=4.2145 ct=9.2534 rec=1.3889 | train/val/test=0.379/0.236/0.240 | c=0.998347
[Epoch 0001] loss=16.6782 cls=1.8948 smmd=2.8385 ct=9.1662 rec=1.3893 | train/val/test=0.655/0.402/0.427 | c=0.998347
[Epoch 0002] loss=15.2047 cls=1.7431 smmd=1.5981 ct=9.0867 rec=1.3884 | train/val/test=0.500/0.350/0.337 | c=0.998347
[Epoch 0003] loss=14.8176 cls=1.4938 smmd=1.4911 ct=9.0629 rec=1.3849 | train/val/test=0.759/0.418/0.408 | c=0.998347
[Epoch 0004] loss=14.5537 cls=1.1372 smmd=1.7104 ct=8.9592 rec=1.3735 | train/val/test=0.948/0.656/0.670 | c=0.998347
[Epoch 0005] loss=14.0062 cls=0.8093 smmd=1.6088 ct=8.8873 rec=1.3504 | train/val/test=0.931/0.628/0.630 | c=0.998347
[Epoch 0006] loss=13.4013 cls=0.5562 smmd=1.3852 ct=8.8268 rec=1.3165 | train/val/test=0.931/0.608/0.589 | c=0.998347
[Epoch 0007] loss=12.8685 cls=0.3659 smmd=1.1338 ct=8.8027 rec=1.2830 | train/val/test=0.931/0.626/0.604 | c=0.998347
[Epoch 0008] loss=12.5655 cls=0.2310 smmd=1.0347 ct=8.7893 rec=1.2552 | train/val/test=0.983/0.648/0.643 | c=0.998347
[Epoch 0009] loss=13.1646 cls=0.1373 smmd=1.0814 ct=9.4813 rec=1.2322 | train/val/test=0.983/0.660/0.659 | c=0.998347
[Epoch 0010] loss=13.0711 cls=0.0909 smmd=1.1522 ct=9.3959 rec=1.2161 | train/val/test=1.000/0.668/0.658 | c=0.998347
[Epoch 0011] loss=12.8944 cls=0.0550 smmd=1.1119 ct=9.3219 rec=1.2028 | train/val/test=1.000/0.684/0.668 | c=0.998347
[Epoch 0012] loss=12.7275 cls=0.0323 smmd=1.0144 ct=9.2970 rec=1.1919 | train/val/test=1.000/0.696/0.676 | c=0.998347
[Epoch 0013] loss=12.5713 cls=0.0214 smmd=0.8703 ct=9.3117 rec=1.1840 | train/val/test=1.000/0.700/0.680 | c=0.998347
[Epoch 0014] loss=12.5295 cls=0.0156 smmd=0.8119 ct=9.3440 rec=1.1790 | train/val/test=1.000/0.706/0.686 | c=0.998347
[Epoch 0015] loss=12.4919 cls=0.0120 smmd=0.7504 ct=9.3753 rec=1.1771 | train/val/test=1.000/0.698/0.682 | c=0.998347
[Epoch 0016] loss=12.4047 cls=0.0100 smmd=0.6562 ct=9.3852 rec=1.1766 | train/val/test=1.000/0.698/0.677 | c=0.998347
[Epoch 0017] loss=12.3060 cls=0.0094 smmd=0.5726 ct=9.3713 rec=1.1764 | train/val/test=1.000/0.700/0.672 | c=0.998347
[Epoch 0018] loss=12.2434 cls=0.0102 smmd=0.5228 ct=9.3541 rec=1.1782 | train/val/test=1.000/0.698/0.674 | c=0.998347
[Epoch 0019] loss=12.1823 cls=0.0102 smmd=0.4679 ct=9.3441 rec=1.1800 | train/val/test=1.000/0.696/0.680 | c=0.998347
[Epoch 0020] loss=12.1518 cls=0.0115 smmd=0.4263 ct=9.3489 rec=1.1825 | train/val/test=1.000/0.698/0.685 | c=0.998347
[Epoch 0021] loss=12.1108 cls=0.0148 smmd=0.3684 ct=9.3567 rec=1.1855 | train/val/test=1.000/0.696/0.681 | c=0.998347
[Epoch 0022] loss=12.0587 cls=0.0174 smmd=0.3202 ct=9.3481 rec=1.1865 | train/val/test=1.000/0.700/0.680 | c=0.998347
[Epoch 0023] loss=12.0354 cls=0.0223 smmd=0.2950 ct=9.3437 rec=1.1872 | train/val/test=1.000/0.700/0.691 | c=0.998347
[Epoch 0024] loss=11.9905 cls=0.0278 smmd=0.2386 ct=9.3512 rec=1.1865 | train/val/test=1.000/0.694/0.688 | c=0.998347
[Epoch 0025] loss=11.9791 cls=0.0323 smmd=0.2257 ct=9.3543 rec=1.1834 | train/val/test=1.000/0.706/0.691 | c=0.998347
[Epoch 0026] loss=11.9498 cls=0.0378 smmd=0.1856 ct=9.3639 rec=1.1813 | train/val/test=1.000/0.694/0.696 | c=0.998347
[Epoch 0027] loss=11.9434 cls=0.0388 smmd=0.1956 ct=9.3528 rec=1.1781 | train/val/test=1.000/0.704/0.707 | c=0.998347
[Epoch 0028] loss=11.9450 cls=0.0501 smmd=0.1804 ct=9.3605 rec=1.1770 | train/val/test=1.000/0.690/0.678 | c=0.998347
[Epoch 0029] loss=11.9362 cls=0.0450 smmd=0.1954 ct=9.3457 rec=1.1750 | train/val/test=1.000/0.704/0.705 | c=0.998347
[Epoch 0030] loss=11.8597 cls=0.0298 smmd=0.1604 ct=9.3396 rec=1.1649 | train/val/test=1.000/0.700/0.701 | c=0.998347
[Epoch 0031] loss=11.8268 cls=0.0244 smmd=0.1403 ct=9.3386 rec=1.1618 | train/val/test=1.000/0.686/0.689 | c=0.998347
[Epoch 0032] loss=11.8223 cls=0.0272 smmd=0.1342 ct=9.3340 rec=1.1635 | train/val/test=1.000/0.702/0.707 | c=0.998347
[Epoch 0033] loss=11.7918 cls=0.0182 smmd=0.1223 ct=9.3333 rec=1.1590 | train/val/test=1.000/0.700/0.705 | c=0.998347
[Epoch 0034] loss=11.7696 cls=0.0176 smmd=0.0939 ct=9.3378 rec=1.1602 | train/val/test=1.000/0.694/0.687 | c=0.998347
[Epoch 0035] loss=11.7633 cls=0.0168 smmd=0.1045 ct=9.3195 rec=1.1613 | train/val/test=1.000/0.700/0.698 | c=0.998347
[Epoch 0036] loss=11.7315 cls=0.0144 smmd=0.0820 ct=9.3143 rec=1.1604 | train/val/test=1.000/0.700/0.710 | c=0.998347
[Epoch 0037] loss=11.7414 cls=0.0155 smmd=0.0778 ct=9.3223 rec=1.1629 | train/val/test=1.000/0.694/0.694 | c=0.998347
[Epoch 0038] loss=11.7298 cls=0.0160 smmd=0.0738 ct=9.3115 rec=1.1642 | train/val/test=1.000/0.698/0.706 | c=0.998347
[Epoch 0039] loss=11.7142 cls=0.0158 smmd=0.0542 ct=9.3171 rec=1.1636 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0040] loss=11.7159 cls=0.0184 smmd=0.0404 ct=9.3261 rec=1.1655 | train/val/test=1.000/0.694/0.704 | c=0.998347
[Epoch 0041] loss=11.7167 cls=0.0196 smmd=0.0556 ct=9.3083 rec=1.1666 | train/val/test=1.000/0.706/0.710 | c=0.998347
[Epoch 0042] loss=11.7057 cls=0.0211 smmd=0.0487 ct=9.3061 rec=1.1649 | train/val/test=1.000/0.704/0.716 | c=0.998347
[Epoch 0043] loss=11.6916 cls=0.0204 smmd=0.0387 ct=9.3046 rec=1.1639 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0044] loss=11.6860 cls=0.0215 smmd=0.0344 ct=9.3041 rec=1.1630 | train/val/test=1.000/0.708/0.709 | c=0.998347
[Epoch 0045] loss=11.6801 cls=0.0229 smmd=0.0236 ct=9.3100 rec=1.1618 | train/val/test=1.000/0.704/0.717 | c=0.998347
[Epoch 0046] loss=11.6648 cls=0.0214 smmd=0.0256 ct=9.2957 rec=1.1610 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0047] loss=11.6664 cls=0.0219 smmd=0.0307 ct=9.2935 rec=1.1601 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0048] loss=11.6570 cls=0.0215 smmd=0.0145 ct=9.3003 rec=1.1604 | train/val/test=1.000/0.702/0.714 | c=0.998347
[Epoch 0049] loss=11.6576 cls=0.0216 smmd=0.0245 ct=9.2901 rec=1.1607 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0050] loss=11.6488 cls=0.0205 smmd=0.0160 ct=9.2917 rec=1.1603 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0051] loss=11.6442 cls=0.0200 smmd=0.0156 ct=9.2877 rec=1.1605 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0052] loss=11.6526 cls=0.0199 smmd=0.0237 ct=9.2862 rec=1.1614 | train/val/test=1.000/0.706/0.710 | c=0.998347
[Epoch 0053] loss=11.6463 cls=0.0203 smmd=0.0059 ct=9.2954 rec=1.1624 | train/val/test=1.000/0.702/0.717 | c=0.998347
[Epoch 0054] loss=11.6479 cls=0.0202 smmd=0.0219 ct=9.2797 rec=1.1630 | train/val/test=1.000/0.706/0.711 | c=0.998347
[Epoch 0055] loss=11.6421 cls=0.0198 smmd=0.0108 ct=9.2861 rec=1.1627 | train/val/test=1.000/0.710/0.714 | c=0.998347
[Epoch 0056] loss=11.6340 cls=0.0207 smmd=0.0059 ct=9.2820 rec=1.1627 | train/val/test=1.000/0.706/0.715 | c=0.998347
[Epoch 0057] loss=11.6403 cls=0.0203 smmd=0.0115 ct=9.2818 rec=1.1634 | train/val/test=1.000/0.704/0.710 | c=0.998347
[Epoch 0058] loss=11.6441 cls=0.0229 smmd=0.0082 ct=9.2858 rec=1.1636 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0059] loss=11.6462 cls=0.0217 smmd=0.0155 ct=9.2795 rec=1.1648 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0060] loss=11.6534 cls=0.0243 smmd=0.0178 ct=9.2825 rec=1.1644 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0061] loss=11.6531 cls=0.0230 smmd=0.0133 ct=9.2857 rec=1.1656 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0062] loss=11.6563 cls=0.0236 smmd=0.0221 ct=9.2816 rec=1.1645 | train/val/test=1.000/0.702/0.706 | c=0.998347
[Epoch 0063] loss=11.6611 cls=0.0223 smmd=0.0186 ct=9.2910 rec=1.1645 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0064] loss=11.6448 cls=0.0204 smmd=0.0247 ct=9.2740 rec=1.1628 | train/val/test=1.000/0.708/0.706 | c=0.998347
[Epoch 0065] loss=11.6263 cls=0.0177 smmd=0.0051 ct=9.2829 rec=1.1603 | train/val/test=1.000/0.710/0.714 | c=0.998347
[Epoch 0066] loss=11.6157 cls=0.0169 smmd=0.0054 ct=9.2741 rec=1.1596 | train/val/test=1.000/0.708/0.716 | c=0.998347
[Epoch 0067] loss=11.6248 cls=0.0186 smmd=0.0123 ct=9.2717 rec=1.1611 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0068] loss=11.6358 cls=0.0200 smmd=0.0061 ct=9.2829 rec=1.1634 | train/val/test=1.000/0.712/0.716 | c=0.998347
[Epoch 0069] loss=11.6331 cls=0.0212 smmd=0.0150 ct=9.2682 rec=1.1644 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0070] loss=11.6216 cls=0.0201 smmd=-0.0010 ct=9.2758 rec=1.1634 | train/val/test=1.000/0.716/0.711 | c=0.998347
[Epoch 0071] loss=11.6141 cls=0.0206 smmd=-0.0039 ct=9.2701 rec=1.1636 | train/val/test=1.000/0.712/0.714 | c=0.998347
[Epoch 0072] loss=11.6182 cls=0.0217 smmd=0.0016 ct=9.2651 rec=1.1649 | train/val/test=1.000/0.710/0.712 | c=0.998347
[Epoch 0073] loss=11.6336 cls=0.0234 smmd=0.0037 ct=9.2741 rec=1.1662 | train/val/test=1.000/0.708/0.712 | c=0.998347
[Epoch 0074] loss=11.6301 cls=0.0240 smmd=0.0106 ct=9.2630 rec=1.1663 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0075] loss=11.6263 cls=0.0232 smmd=-0.0065 ct=9.2785 rec=1.1656 | train/val/test=1.000/0.716/0.713 | c=0.998347
[Epoch 0076] loss=11.6247 cls=0.0231 smmd=0.0100 ct=9.2639 rec=1.1639 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0077] loss=11.6291 cls=0.0218 smmd=0.0075 ct=9.2716 rec=1.1641 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0078] loss=11.6348 cls=0.0240 smmd=0.0116 ct=9.2711 rec=1.1641 | train/val/test=1.000/0.712/0.709 | c=0.998347
[Epoch 0079] loss=11.6471 cls=0.0227 smmd=0.0281 ct=9.2663 rec=1.1650 | train/val/test=1.000/0.714/0.718 | c=0.998347
[Epoch 0080] loss=11.6218 cls=0.0202 smmd=0.0038 ct=9.2738 rec=1.1620 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0081] loss=11.5992 cls=0.0167 smmd=0.0026 ct=9.2596 rec=1.1602 | train/val/test=1.000/0.708/0.713 | c=0.998347
[Epoch 0082] loss=11.6014 cls=0.0168 smmd=0.0068 ct=9.2567 rec=1.1605 | train/val/test=1.000/0.714/0.717 | c=0.998347
[Epoch 0083] loss=11.6210 cls=0.0200 smmd=0.0060 ct=9.2682 rec=1.1634 | train/val/test=1.000/0.704/0.712 | c=0.998347
[Epoch 0084] loss=11.6278 cls=0.0218 smmd=0.0117 ct=9.2614 rec=1.1665 | train/val/test=1.000/0.712/0.715 | c=0.998347
[Epoch 0085] loss=11.6158 cls=0.0216 smmd=0.0004 ct=9.2637 rec=1.1651 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0086] loss=11.6046 cls=0.0200 smmd=-0.0020 ct=9.2576 rec=1.1645 | train/val/test=1.000/0.710/0.712 | c=0.998347
[Epoch 0087] loss=11.6043 cls=0.0201 smmd=0.0073 ct=9.2487 rec=1.1641 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0088] loss=11.6087 cls=0.0212 smmd=-0.0074 ct=9.2667 rec=1.1641 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0089] loss=11.6104 cls=0.0206 smmd=0.0108 ct=9.2507 rec=1.1642 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0090] loss=11.5929 cls=0.0199 smmd=-0.0069 ct=9.2556 rec=1.1622 | train/val/test=1.000/0.714/0.717 | c=0.998347
[Epoch 0091] loss=11.5887 cls=0.0189 smmd=-0.0061 ct=9.2529 rec=1.1616 | train/val/test=1.000/0.712/0.717 | c=0.998347
[Epoch 0092] loss=11.5922 cls=0.0200 smmd=0.0015 ct=9.2451 rec=1.1629 | train/val/test=1.000/0.710/0.715 | c=0.998347
[Epoch 0093] loss=11.6051 cls=0.0213 smmd=-0.0045 ct=9.2593 rec=1.1646 | train/val/test=1.000/0.712/0.717 | c=0.998347
[Epoch 0094] loss=11.6096 cls=0.0230 smmd=0.0069 ct=9.2463 rec=1.1667 | train/val/test=1.000/0.700/0.711 | c=0.998347
[Epoch 0095] loss=11.6289 cls=0.0253 smmd=0.0024 ct=9.2630 rec=1.1691 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0096] loss=11.6754 cls=0.0335 smmd=0.0297 ct=9.2647 rec=1.1737 | train/val/test=1.000/0.702/0.698 | c=0.998347
[Epoch 0097] loss=11.7987 cls=0.0529 smmd=0.0688 ct=9.2995 rec=1.1888 | train/val/test=0.983/0.690/0.666 | c=0.998347
[Epoch 0098] loss=11.9517 cls=0.0856 smmd=0.1513 ct=9.3179 rec=1.1985 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0099] loss=11.7889 cls=0.0304 smmd=0.1014 ct=9.3063 rec=1.1754 | train/val/test=1.000/0.712/0.714 | c=0.998347
=== Best @ epoch 70: val=0.7160, test=0.7110 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 - 2025-09-21 06:36:13:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1920 cls=1.9463 smmd=4.2145 ct=9.2534 rec=1.3889 | train/val/test=0.379/0.236/0.240 | c=0.998347
[Epoch 0001] loss=16.6782 cls=1.8948 smmd=2.8385 ct=9.1662 rec=1.3893 | train/val/test=0.655/0.402/0.427 | c=0.998347
[Epoch 0002] loss=15.2047 cls=1.7431 smmd=1.5981 ct=9.0867 rec=1.3884 | train/val/test=0.500/0.350/0.337 | c=0.998347
[Epoch 0003] loss=14.8176 cls=1.4938 smmd=1.4911 ct=9.0629 rec=1.3849 | train/val/test=0.759/0.418/0.408 | c=0.998347
[Epoch 0004] loss=14.5537 cls=1.1372 smmd=1.7104 ct=8.9592 rec=1.3735 | train/val/test=0.948/0.656/0.670 | c=0.998347
[Epoch 0005] loss=14.0062 cls=0.8093 smmd=1.6088 ct=8.8873 rec=1.3504 | train/val/test=0.931/0.628/0.630 | c=0.998347
[Epoch 0006] loss=13.4013 cls=0.5562 smmd=1.3852 ct=8.8268 rec=1.3165 | train/val/test=0.931/0.608/0.589 | c=0.998347
[Epoch 0007] loss=12.8685 cls=0.3659 smmd=1.1338 ct=8.8027 rec=1.2830 | train/val/test=0.931/0.626/0.604 | c=0.998347
[Epoch 0008] loss=12.5655 cls=0.2310 smmd=1.0347 ct=8.7893 rec=1.2552 | train/val/test=0.983/0.648/0.643 | c=0.998347
[Epoch 0009] loss=13.1646 cls=0.1373 smmd=1.0814 ct=9.4813 rec=1.2322 | train/val/test=0.983/0.660/0.659 | c=0.998347
[Epoch 0010] loss=13.0711 cls=0.0909 smmd=1.1522 ct=9.3959 rec=1.2161 | train/val/test=1.000/0.668/0.658 | c=0.998347
[Epoch 0011] loss=12.8944 cls=0.0550 smmd=1.1119 ct=9.3219 rec=1.2028 | train/val/test=1.000/0.684/0.668 | c=0.998347
[Epoch 0012] loss=12.7275 cls=0.0323 smmd=1.0144 ct=9.2970 rec=1.1919 | train/val/test=1.000/0.696/0.676 | c=0.998347
[Epoch 0013] loss=12.5713 cls=0.0214 smmd=0.8703 ct=9.3117 rec=1.1840 | train/val/test=1.000/0.700/0.680 | c=0.998347
[Epoch 0014] loss=12.5295 cls=0.0156 smmd=0.8119 ct=9.3440 rec=1.1790 | train/val/test=1.000/0.706/0.686 | c=0.998347
[Epoch 0015] loss=12.4919 cls=0.0120 smmd=0.7504 ct=9.3753 rec=1.1771 | train/val/test=1.000/0.698/0.682 | c=0.998347
[Epoch 0016] loss=12.4047 cls=0.0100 smmd=0.6562 ct=9.3852 rec=1.1766 | train/val/test=1.000/0.698/0.677 | c=0.998347
[Epoch 0017] loss=12.3060 cls=0.0094 smmd=0.5726 ct=9.3713 rec=1.1764 | train/val/test=1.000/0.700/0.672 | c=0.998347
[Epoch 0018] loss=12.2434 cls=0.0102 smmd=0.5228 ct=9.3541 rec=1.1782 | train/val/test=1.000/0.698/0.674 | c=0.998347
[Epoch 0019] loss=12.1823 cls=0.0102 smmd=0.4679 ct=9.3441 rec=1.1800 | train/val/test=1.000/0.696/0.680 | c=0.998347
[Epoch 0020] loss=12.1518 cls=0.0115 smmd=0.4263 ct=9.3489 rec=1.1825 | train/val/test=1.000/0.698/0.685 | c=0.998347
[Epoch 0021] loss=12.1108 cls=0.0148 smmd=0.3684 ct=9.3567 rec=1.1855 | train/val/test=1.000/0.696/0.681 | c=0.998347
[Epoch 0022] loss=12.0587 cls=0.0174 smmd=0.3202 ct=9.3481 rec=1.1865 | train/val/test=1.000/0.700/0.680 | c=0.998347
[Epoch 0023] loss=12.0354 cls=0.0223 smmd=0.2950 ct=9.3437 rec=1.1872 | train/val/test=1.000/0.700/0.691 | c=0.998347
[Epoch 0024] loss=11.9905 cls=0.0278 smmd=0.2386 ct=9.3512 rec=1.1865 | train/val/test=1.000/0.694/0.688 | c=0.998347
[Epoch 0025] loss=11.9791 cls=0.0323 smmd=0.2257 ct=9.3543 rec=1.1834 | train/val/test=1.000/0.706/0.691 | c=0.998347
[Epoch 0026] loss=11.9498 cls=0.0378 smmd=0.1856 ct=9.3639 rec=1.1813 | train/val/test=1.000/0.694/0.696 | c=0.998347
[Epoch 0027] loss=11.9434 cls=0.0388 smmd=0.1956 ct=9.3528 rec=1.1781 | train/val/test=1.000/0.704/0.707 | c=0.998347
[Epoch 0028] loss=11.9450 cls=0.0501 smmd=0.1804 ct=9.3605 rec=1.1770 | train/val/test=1.000/0.690/0.678 | c=0.998347
[Epoch 0029] loss=11.9362 cls=0.0450 smmd=0.1954 ct=9.3457 rec=1.1750 | train/val/test=1.000/0.704/0.705 | c=0.998347
[Epoch 0030] loss=11.8597 cls=0.0298 smmd=0.1604 ct=9.3396 rec=1.1649 | train/val/test=1.000/0.700/0.701 | c=0.998347
[Epoch 0031] loss=11.8268 cls=0.0244 smmd=0.1403 ct=9.3386 rec=1.1618 | train/val/test=1.000/0.686/0.689 | c=0.998347
[Epoch 0032] loss=11.8223 cls=0.0272 smmd=0.1342 ct=9.3340 rec=1.1635 | train/val/test=1.000/0.702/0.707 | c=0.998347
[Epoch 0033] loss=11.7918 cls=0.0182 smmd=0.1223 ct=9.3333 rec=1.1590 | train/val/test=1.000/0.700/0.705 | c=0.998347
[Epoch 0034] loss=11.7696 cls=0.0176 smmd=0.0939 ct=9.3378 rec=1.1602 | train/val/test=1.000/0.694/0.687 | c=0.998347
[Epoch 0035] loss=11.7633 cls=0.0168 smmd=0.1045 ct=9.3195 rec=1.1613 | train/val/test=1.000/0.700/0.698 | c=0.998347
[Epoch 0036] loss=11.7315 cls=0.0144 smmd=0.0820 ct=9.3143 rec=1.1604 | train/val/test=1.000/0.700/0.710 | c=0.998347
[Epoch 0037] loss=11.7414 cls=0.0155 smmd=0.0778 ct=9.3223 rec=1.1629 | train/val/test=1.000/0.694/0.694 | c=0.998347
[Epoch 0038] loss=11.7298 cls=0.0160 smmd=0.0738 ct=9.3115 rec=1.1642 | train/val/test=1.000/0.698/0.706 | c=0.998347
[Epoch 0039] loss=11.7142 cls=0.0158 smmd=0.0542 ct=9.3171 rec=1.1636 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0040] loss=11.7159 cls=0.0184 smmd=0.0404 ct=9.3261 rec=1.1655 | train/val/test=1.000/0.694/0.704 | c=0.998347
[Epoch 0041] loss=11.7167 cls=0.0196 smmd=0.0556 ct=9.3083 rec=1.1666 | train/val/test=1.000/0.706/0.710 | c=0.998347
[Epoch 0042] loss=11.7057 cls=0.0211 smmd=0.0487 ct=9.3061 rec=1.1649 | train/val/test=1.000/0.704/0.716 | c=0.998347
[Epoch 0043] loss=11.6916 cls=0.0204 smmd=0.0387 ct=9.3046 rec=1.1639 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0044] loss=11.6860 cls=0.0215 smmd=0.0344 ct=9.3041 rec=1.1630 | train/val/test=1.000/0.708/0.709 | c=0.998347
[Epoch 0045] loss=11.6801 cls=0.0229 smmd=0.0236 ct=9.3100 rec=1.1618 | train/val/test=1.000/0.704/0.717 | c=0.998347
[Epoch 0046] loss=11.6648 cls=0.0214 smmd=0.0256 ct=9.2957 rec=1.1610 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0047] loss=11.6664 cls=0.0219 smmd=0.0307 ct=9.2935 rec=1.1601 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0048] loss=11.6570 cls=0.0215 smmd=0.0145 ct=9.3003 rec=1.1604 | train/val/test=1.000/0.702/0.714 | c=0.998347
[Epoch 0049] loss=11.6576 cls=0.0216 smmd=0.0245 ct=9.2901 rec=1.1607 | train/val/test=1.000/0.704/0.709 | c=0.998347
[Epoch 0050] loss=11.6488 cls=0.0205 smmd=0.0160 ct=9.2917 rec=1.1603 | train/val/test=1.000/0.708/0.714 | c=0.998347
[Epoch 0051] loss=11.6442 cls=0.0200 smmd=0.0156 ct=9.2877 rec=1.1605 | train/val/test=1.000/0.702/0.709 | c=0.998347
[Epoch 0052] loss=11.6526 cls=0.0199 smmd=0.0237 ct=9.2862 rec=1.1614 | train/val/test=1.000/0.706/0.710 | c=0.998347
[Epoch 0053] loss=11.6463 cls=0.0203 smmd=0.0059 ct=9.2954 rec=1.1624 | train/val/test=1.000/0.702/0.717 | c=0.998347
[Epoch 0054] loss=11.6479 cls=0.0202 smmd=0.0219 ct=9.2797 rec=1.1630 | train/val/test=1.000/0.706/0.711 | c=0.998347
[Epoch 0055] loss=11.6421 cls=0.0198 smmd=0.0108 ct=9.2861 rec=1.1627 | train/val/test=1.000/0.710/0.714 | c=0.998347
[Epoch 0056] loss=11.6340 cls=0.0207 smmd=0.0059 ct=9.2820 rec=1.1627 | train/val/test=1.000/0.706/0.715 | c=0.998347
[Epoch 0057] loss=11.6403 cls=0.0203 smmd=0.0115 ct=9.2818 rec=1.1634 | train/val/test=1.000/0.704/0.710 | c=0.998347
[Epoch 0058] loss=11.6441 cls=0.0229 smmd=0.0082 ct=9.2858 rec=1.1636 | train/val/test=1.000/0.706/0.708 | c=0.998347
[Epoch 0059] loss=11.6462 cls=0.0217 smmd=0.0155 ct=9.2795 rec=1.1648 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0060] loss=11.6534 cls=0.0243 smmd=0.0178 ct=9.2825 rec=1.1644 | train/val/test=1.000/0.710/0.710 | c=0.998347
[Epoch 0061] loss=11.6531 cls=0.0230 smmd=0.0133 ct=9.2857 rec=1.1656 | train/val/test=1.000/0.702/0.713 | c=0.998347
[Epoch 0062] loss=11.6563 cls=0.0236 smmd=0.0221 ct=9.2816 rec=1.1645 | train/val/test=1.000/0.702/0.706 | c=0.998347
[Epoch 0063] loss=11.6611 cls=0.0223 smmd=0.0186 ct=9.2910 rec=1.1645 | train/val/test=1.000/0.704/0.711 | c=0.998347
[Epoch 0064] loss=11.6448 cls=0.0204 smmd=0.0247 ct=9.2740 rec=1.1628 | train/val/test=1.000/0.708/0.706 | c=0.998347
[Epoch 0065] loss=11.6263 cls=0.0177 smmd=0.0051 ct=9.2829 rec=1.1603 | train/val/test=1.000/0.710/0.714 | c=0.998347
[Epoch 0066] loss=11.6157 cls=0.0169 smmd=0.0054 ct=9.2741 rec=1.1596 | train/val/test=1.000/0.708/0.716 | c=0.998347
[Epoch 0067] loss=11.6248 cls=0.0186 smmd=0.0123 ct=9.2717 rec=1.1611 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0068] loss=11.6358 cls=0.0200 smmd=0.0061 ct=9.2829 rec=1.1634 | train/val/test=1.000/0.712/0.716 | c=0.998347
[Epoch 0069] loss=11.6331 cls=0.0212 smmd=0.0150 ct=9.2682 rec=1.1644 | train/val/test=1.000/0.712/0.711 | c=0.998347
[Epoch 0070] loss=11.6216 cls=0.0201 smmd=-0.0010 ct=9.2758 rec=1.1634 | train/val/test=1.000/0.716/0.711 | c=0.998347
[Epoch 0071] loss=11.6141 cls=0.0206 smmd=-0.0039 ct=9.2701 rec=1.1636 | train/val/test=1.000/0.712/0.714 | c=0.998347
[Epoch 0072] loss=11.6182 cls=0.0217 smmd=0.0016 ct=9.2651 rec=1.1649 | train/val/test=1.000/0.710/0.712 | c=0.998347
[Epoch 0073] loss=11.6336 cls=0.0234 smmd=0.0037 ct=9.2741 rec=1.1662 | train/val/test=1.000/0.708/0.712 | c=0.998347
[Epoch 0074] loss=11.6301 cls=0.0240 smmd=0.0106 ct=9.2630 rec=1.1663 | train/val/test=1.000/0.714/0.714 | c=0.998347
[Epoch 0075] loss=11.6263 cls=0.0232 smmd=-0.0065 ct=9.2785 rec=1.1656 | train/val/test=1.000/0.716/0.713 | c=0.998347
[Epoch 0076] loss=11.6247 cls=0.0231 smmd=0.0100 ct=9.2639 rec=1.1639 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0077] loss=11.6291 cls=0.0218 smmd=0.0075 ct=9.2716 rec=1.1641 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0078] loss=11.6348 cls=0.0240 smmd=0.0116 ct=9.2711 rec=1.1641 | train/val/test=1.000/0.712/0.709 | c=0.998347
[Epoch 0079] loss=11.6471 cls=0.0227 smmd=0.0281 ct=9.2663 rec=1.1650 | train/val/test=1.000/0.714/0.718 | c=0.998347
[Epoch 0080] loss=11.6218 cls=0.0202 smmd=0.0038 ct=9.2738 rec=1.1620 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0081] loss=11.5992 cls=0.0167 smmd=0.0026 ct=9.2596 rec=1.1602 | train/val/test=1.000/0.708/0.713 | c=0.998347
[Epoch 0082] loss=11.6014 cls=0.0168 smmd=0.0068 ct=9.2567 rec=1.1605 | train/val/test=1.000/0.714/0.717 | c=0.998347
[Epoch 0083] loss=11.6210 cls=0.0200 smmd=0.0060 ct=9.2682 rec=1.1634 | train/val/test=1.000/0.704/0.712 | c=0.998347
[Epoch 0084] loss=11.6278 cls=0.0218 smmd=0.0117 ct=9.2614 rec=1.1665 | train/val/test=1.000/0.712/0.715 | c=0.998347
[Epoch 0085] loss=11.6158 cls=0.0216 smmd=0.0004 ct=9.2637 rec=1.1651 | train/val/test=1.000/0.714/0.716 | c=0.998347
[Epoch 0086] loss=11.6046 cls=0.0200 smmd=-0.0020 ct=9.2576 rec=1.1645 | train/val/test=1.000/0.710/0.712 | c=0.998347
[Epoch 0087] loss=11.6043 cls=0.0201 smmd=0.0073 ct=9.2487 rec=1.1641 | train/val/test=1.000/0.716/0.716 | c=0.998347
[Epoch 0088] loss=11.6087 cls=0.0212 smmd=-0.0074 ct=9.2667 rec=1.1641 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0089] loss=11.6104 cls=0.0206 smmd=0.0108 ct=9.2507 rec=1.1642 | train/val/test=1.000/0.714/0.715 | c=0.998347
[Epoch 0090] loss=11.5929 cls=0.0199 smmd=-0.0069 ct=9.2556 rec=1.1622 | train/val/test=1.000/0.714/0.717 | c=0.998347
[Epoch 0091] loss=11.5887 cls=0.0189 smmd=-0.0061 ct=9.2529 rec=1.1616 | train/val/test=1.000/0.712/0.717 | c=0.998347
[Epoch 0092] loss=11.5922 cls=0.0200 smmd=0.0015 ct=9.2451 rec=1.1629 | train/val/test=1.000/0.710/0.715 | c=0.998347
[Epoch 0093] loss=11.6051 cls=0.0213 smmd=-0.0045 ct=9.2593 rec=1.1646 | train/val/test=1.000/0.712/0.717 | c=0.998347
[Epoch 0094] loss=11.6096 cls=0.0230 smmd=0.0069 ct=9.2463 rec=1.1667 | train/val/test=1.000/0.700/0.711 | c=0.998347
[Epoch 0095] loss=11.6289 cls=0.0253 smmd=0.0024 ct=9.2630 rec=1.1691 | train/val/test=1.000/0.712/0.713 | c=0.998347
[Epoch 0096] loss=11.6754 cls=0.0335 smmd=0.0297 ct=9.2647 rec=1.1737 | train/val/test=1.000/0.702/0.698 | c=0.998347
[Epoch 0097] loss=11.7987 cls=0.0529 smmd=0.0688 ct=9.2995 rec=1.1888 | train/val/test=0.983/0.690/0.666 | c=0.998347
[Epoch 0098] loss=11.9517 cls=0.0856 smmd=0.1513 ct=9.3179 rec=1.1985 | train/val/test=1.000/0.716/0.708 | c=0.998347
[Epoch 0099] loss=11.7889 cls=0.0304 smmd=0.1014 ct=9.3063 rec=1.1754 | train/val/test=1.000/0.712/0.714 | c=0.998347
=== Best @ epoch 70: val=0.7160, test=0.7110 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-3 completed in 23.25 seconds.
==================================================
