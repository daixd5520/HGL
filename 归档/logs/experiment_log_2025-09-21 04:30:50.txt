Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5 - 2025-09-21 04:30:50:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1474 cls=1.9436 smmd=4.1752 ct=9.2509 rec=1.3889 | train/val/test=0.241/0.074/0.101 | c=0.998437
[Epoch 0001] loss=16.6188 cls=1.8948 smmd=2.7562 ct=9.1893 rec=1.3892 | train/val/test=0.741/0.396/0.378 | c=0.998437
[Epoch 0002] loss=15.0422 cls=1.7579 smmd=1.4650 ct=9.0425 rec=1.3884 | train/val/test=0.810/0.422/0.425 | c=0.998437
[Epoch 0003] loss=14.7480 cls=1.5092 smmd=1.4791 ct=8.9882 rec=1.3858 | train/val/test=0.690/0.396/0.379 | c=0.998437
[Epoch 0004] loss=14.6590 cls=1.2254 smmd=1.7291 ct=8.9573 rec=1.3736 | train/val/test=0.845/0.544/0.506 | c=0.998437
[Epoch 0005] loss=14.1019 cls=0.8653 smmd=1.6724 ct=8.8662 rec=1.3490 | train/val/test=0.897/0.540/0.537 | c=0.998437
[Epoch 0006] loss=13.4137 cls=0.5960 smmd=1.3298 ct=8.8537 rec=1.3171 | train/val/test=0.948/0.576/0.557 | c=0.998437
[Epoch 0007] loss=13.5725 cls=0.4058 smmd=1.0647 ct=9.5208 rec=1.2905 | train/val/test=0.966/0.606/0.592 | c=0.998437
[Epoch 0008] loss=13.2484 cls=0.2671 smmd=1.0150 ct=9.4313 rec=1.2675 | train/val/test=0.983/0.626/0.618 | c=0.998437
[Epoch 0009] loss=13.1966 cls=0.1650 smmd=1.1897 ct=9.3521 rec=1.2449 | train/val/test=0.983/0.640/0.644 | c=0.998437
[Epoch 0010] loss=13.1521 cls=0.1003 smmd=1.2897 ct=9.3077 rec=1.2272 | train/val/test=1.000/0.666/0.667 | c=0.998437
[Epoch 0011] loss=12.9733 cls=0.0617 smmd=1.1824 ct=9.3004 rec=1.2144 | train/val/test=1.000/0.674/0.667 | c=0.998437
[Epoch 0012] loss=12.7932 cls=0.0410 smmd=1.0110 ct=9.3307 rec=1.2053 | train/val/test=1.000/0.672/0.671 | c=0.998437
[Epoch 0013] loss=12.6433 cls=0.0272 smmd=0.8457 ct=9.3770 rec=1.1967 | train/val/test=1.000/0.674/0.665 | c=0.998437
[Epoch 0014] loss=12.5234 cls=0.0195 smmd=0.7087 ct=9.4136 rec=1.1908 | train/val/test=1.000/0.678/0.670 | c=0.998437
[Epoch 0015] loss=12.4729 cls=0.0156 smmd=0.6670 ct=9.4155 rec=1.1874 | train/val/test=1.000/0.696/0.673 | c=0.998437
[Epoch 0016] loss=12.4740 cls=0.0136 smmd=0.6987 ct=9.3902 rec=1.1858 | train/val/test=1.000/0.704/0.682 | c=0.998437
[Epoch 0017] loss=12.3820 cls=0.0128 smmd=0.6325 ct=9.3649 rec=1.1859 | train/val/test=1.000/0.710/0.682 | c=0.998437
[Epoch 0018] loss=12.2856 cls=0.0128 smmd=0.5488 ct=9.3514 rec=1.1863 | train/val/test=1.000/0.714/0.685 | c=0.998437
[Epoch 0019] loss=12.2149 cls=0.0143 smmd=0.4686 ct=9.3551 rec=1.1884 | train/val/test=1.000/0.718/0.689 | c=0.998437
[Epoch 0020] loss=12.1685 cls=0.0165 smmd=0.4089 ct=9.3609 rec=1.1910 | train/val/test=1.000/0.714/0.696 | c=0.998437
[Epoch 0021] loss=12.1489 cls=0.0200 smmd=0.3799 ct=9.3607 rec=1.1942 | train/val/test=1.000/0.708/0.690 | c=0.998437
[Epoch 0022] loss=12.0959 cls=0.0240 smmd=0.3249 ct=9.3574 rec=1.1948 | train/val/test=1.000/0.716/0.709 | c=0.998437
[Epoch 0023] loss=12.0689 cls=0.0285 smmd=0.2909 ct=9.3605 rec=1.1945 | train/val/test=1.000/0.712/0.707 | c=0.998437
[Epoch 0024] loss=12.0155 cls=0.0333 smmd=0.2294 ct=9.3665 rec=1.1932 | train/val/test=1.000/0.714/0.710 | c=0.998437
[Epoch 0025] loss=11.9942 cls=0.0368 smmd=0.2104 ct=9.3689 rec=1.1890 | train/val/test=1.000/0.718/0.719 | c=0.998437
[Epoch 0026] loss=11.9904 cls=0.0394 smmd=0.2173 ct=9.3640 rec=1.1848 | train/val/test=1.000/0.718/0.710 | c=0.998437
[Epoch 0027] loss=11.9340 cls=0.0416 smmd=0.1701 ct=9.3590 rec=1.1817 | train/val/test=1.000/0.718/0.727 | c=0.998437
[Epoch 0028] loss=11.9247 cls=0.0417 smmd=0.1647 ct=9.3600 rec=1.1791 | train/val/test=1.000/0.714/0.700 | c=0.998437
[Epoch 0029] loss=11.9318 cls=0.0413 smmd=0.1766 ct=9.3589 rec=1.1775 | train/val/test=1.000/0.714/0.721 | c=0.998437
[Epoch 0030] loss=11.9064 cls=0.0381 smmd=0.1595 ct=9.3584 rec=1.1752 | train/val/test=1.000/0.718/0.711 | c=0.998437
[Epoch 0031] loss=11.8544 cls=0.0262 smmd=0.1462 ct=9.3445 rec=1.1688 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0032] loss=11.8009 cls=0.0207 smmd=0.1149 ct=9.3365 rec=1.1644 | train/val/test=1.000/0.720/0.718 | c=0.998437
[Epoch 0033] loss=11.8056 cls=0.0195 smmd=0.1188 ct=9.3387 rec=1.1643 | train/val/test=1.000/0.712/0.710 | c=0.998437
[Epoch 0034] loss=11.8007 cls=0.0186 smmd=0.1164 ct=9.3384 rec=1.1637 | train/val/test=1.000/0.714/0.703 | c=0.998437
[Epoch 0035] loss=11.7704 cls=0.0167 smmd=0.0951 ct=9.3342 rec=1.1622 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0036] loss=11.7566 cls=0.0172 smmd=0.0855 ct=9.3279 rec=1.1630 | train/val/test=1.000/0.710/0.710 | c=0.998437
[Epoch 0037] loss=11.7629 cls=0.0196 smmd=0.0914 ct=9.3218 rec=1.1651 | train/val/test=1.000/0.716/0.699 | c=0.998437
[Epoch 0038] loss=11.7550 cls=0.0209 smmd=0.0760 ct=9.3230 rec=1.1676 | train/val/test=1.000/0.716/0.713 | c=0.998437
[Epoch 0039] loss=11.7377 cls=0.0208 smmd=0.0632 ct=9.3209 rec=1.1664 | train/val/test=1.000/0.716/0.707 | c=0.998437
[Epoch 0040] loss=11.7332 cls=0.0216 smmd=0.0590 ct=9.3202 rec=1.1662 | train/val/test=1.000/0.714/0.703 | c=0.998437
[Epoch 0041] loss=11.7163 cls=0.0227 smmd=0.0451 ct=9.3155 rec=1.1665 | train/val/test=1.000/0.714/0.710 | c=0.998437
[Epoch 0042] loss=11.7256 cls=0.0241 smmd=0.0554 ct=9.3122 rec=1.1670 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0043] loss=11.7153 cls=0.0246 smmd=0.0398 ct=9.3180 rec=1.1665 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0044] loss=11.7076 cls=0.0258 smmd=0.0332 ct=9.3167 rec=1.1660 | train/val/test=1.000/0.718/0.715 | c=0.998437
[Epoch 0045] loss=11.7137 cls=0.0262 smmd=0.0432 ct=9.3119 rec=1.1663 | train/val/test=1.000/0.708/0.704 | c=0.998437
[Epoch 0046] loss=11.7108 cls=0.0278 smmd=0.0469 ct=9.3039 rec=1.1661 | train/val/test=1.000/0.718/0.716 | c=0.998437
[Epoch 0047] loss=11.7066 cls=0.0261 smmd=0.0437 ct=9.3056 rec=1.1656 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0048] loss=11.6868 cls=0.0248 smmd=0.0291 ct=9.3050 rec=1.1639 | train/val/test=1.000/0.708/0.707 | c=0.998437
[Epoch 0049] loss=11.6786 cls=0.0217 smmd=0.0268 ct=9.3058 rec=1.1621 | train/val/test=1.000/0.716/0.718 | c=0.998437
[Epoch 0050] loss=11.6633 cls=0.0201 smmd=0.0191 ct=9.3012 rec=1.1614 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0051] loss=11.6561 cls=0.0201 smmd=0.0204 ct=9.2927 rec=1.1614 | train/val/test=1.000/0.718/0.708 | c=0.998437
[Epoch 0052] loss=11.6605 cls=0.0204 smmd=0.0255 ct=9.2893 rec=1.1627 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0053] loss=11.6664 cls=0.0210 smmd=0.0272 ct=9.2904 rec=1.1639 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0054] loss=11.6618 cls=0.0220 smmd=0.0165 ct=9.2924 rec=1.1655 | train/val/test=1.000/0.708/0.706 | c=0.998437
[Epoch 0055] loss=11.6528 cls=0.0218 smmd=0.0119 ct=9.2892 rec=1.1649 | train/val/test=1.000/0.714/0.709 | c=0.998437
[Epoch 0056] loss=11.6554 cls=0.0223 smmd=0.0146 ct=9.2876 rec=1.1654 | train/val/test=1.000/0.708/0.702 | c=0.998437
[Epoch 0057] loss=11.6525 cls=0.0232 smmd=0.0139 ct=9.2840 rec=1.1657 | train/val/test=1.000/0.720/0.715 | c=0.998437
[Epoch 0058] loss=11.6588 cls=0.0242 smmd=0.0132 ct=9.2878 rec=1.1668 | train/val/test=1.000/0.708/0.701 | c=0.998437
[Epoch 0059] loss=11.6621 cls=0.0259 smmd=0.0182 ct=9.2834 rec=1.1673 | train/val/test=1.000/0.720/0.712 | c=0.998437
[Epoch 0060] loss=11.6648 cls=0.0254 smmd=0.0199 ct=9.2843 rec=1.1676 | train/val/test=1.000/0.708/0.708 | c=0.998437
[Epoch 0061] loss=11.6596 cls=0.0261 smmd=0.0168 ct=9.2826 rec=1.1670 | train/val/test=1.000/0.710/0.701 | c=0.998437
[Epoch 0062] loss=11.6642 cls=0.0245 smmd=0.0178 ct=9.2886 rec=1.1667 | train/val/test=1.000/0.704/0.715 | c=0.998437
[Epoch 0063] loss=11.6635 cls=0.0246 smmd=0.0221 ct=9.2849 rec=1.1659 | train/val/test=1.000/0.712/0.699 | c=0.998437
[Epoch 0064] loss=11.6424 cls=0.0219 smmd=0.0137 ct=9.2793 rec=1.1638 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0065] loss=11.6224 cls=0.0198 smmd=0.0060 ct=9.2739 rec=1.1614 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0066] loss=11.6200 cls=0.0200 smmd=0.0040 ct=9.2722 rec=1.1619 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0067] loss=11.6301 cls=0.0219 smmd=0.0061 ct=9.2726 rec=1.1648 | train/val/test=1.000/0.710/0.707 | c=0.998437
[Epoch 0068] loss=11.6328 cls=0.0225 smmd=0.0085 ct=9.2716 rec=1.1651 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0069] loss=11.6257 cls=0.0225 smmd=0.0029 ct=9.2686 rec=1.1658 | train/val/test=1.000/0.710/0.702 | c=0.998437
[Epoch 0070] loss=11.6234 cls=0.0238 smmd=-0.0030 ct=9.2693 rec=1.1667 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0071] loss=11.6320 cls=0.0249 smmd=-0.0002 ct=9.2705 rec=1.1683 | train/val/test=1.000/0.708/0.701 | c=0.998437
[Epoch 0072] loss=11.6357 cls=0.0259 smmd=0.0094 ct=9.2630 rec=1.1688 | train/val/test=1.000/0.718/0.713 | c=0.998437
[Epoch 0073] loss=11.6238 cls=0.0242 smmd=-0.0010 ct=9.2658 rec=1.1674 | train/val/test=1.000/0.714/0.704 | c=0.998437
[Epoch 0074] loss=11.6171 cls=0.0239 smmd=-0.0032 ct=9.2648 rec=1.1659 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0075] loss=11.6234 cls=0.0237 smmd=-0.0003 ct=9.2678 rec=1.1661 | train/val/test=1.000/0.706/0.713 | c=0.998437
[Epoch 0076] loss=11.6348 cls=0.0253 smmd=0.0077 ct=9.2673 rec=1.1672 | train/val/test=1.000/0.710/0.700 | c=0.998437
[Epoch 0077] loss=11.6269 cls=0.0240 smmd=0.0038 ct=9.2658 rec=1.1667 | train/val/test=1.000/0.710/0.707 | c=0.998437
[Epoch 0078] loss=11.6186 cls=0.0226 smmd=0.0030 ct=9.2624 rec=1.1653 | train/val/test=1.000/0.722/0.708 | c=0.998437
[Epoch 0079] loss=11.6179 cls=0.0225 smmd=0.0013 ct=9.2625 rec=1.1658 | train/val/test=1.000/0.708/0.699 | c=0.998437
[Epoch 0080] loss=11.6182 cls=0.0222 smmd=0.0046 ct=9.2601 rec=1.1657 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0081] loss=11.5976 cls=0.0211 smmd=-0.0107 ct=9.2583 rec=1.1645 | train/val/test=1.000/0.716/0.703 | c=0.998437
[Epoch 0082] loss=11.5901 cls=0.0208 smmd=-0.0111 ct=9.2532 rec=1.1636 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0083] loss=11.6015 cls=0.0215 smmd=-0.0028 ct=9.2531 rec=1.1648 | train/val/test=1.000/0.718/0.709 | c=0.998437
[Epoch 0084] loss=11.6031 cls=0.0225 smmd=-0.0097 ct=9.2578 rec=1.1663 | train/val/test=1.000/0.714/0.701 | c=0.998437
[Epoch 0085] loss=11.6031 cls=0.0236 smmd=-0.0063 ct=9.2520 rec=1.1669 | train/val/test=1.000/0.720/0.711 | c=0.998437
[Epoch 0086] loss=11.6015 cls=0.0234 smmd=-0.0069 ct=9.2508 rec=1.1671 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0087] loss=11.6182 cls=0.0268 smmd=-0.0013 ct=9.2541 rec=1.1693 | train/val/test=1.000/0.710/0.704 | c=0.998437
[Epoch 0088] loss=11.6468 cls=0.0296 smmd=0.0053 ct=9.2664 rec=1.1727 | train/val/test=1.000/0.702/0.698 | c=0.998437
[Epoch 0089] loss=11.7100 cls=0.0385 smmd=0.0372 ct=9.2769 rec=1.1787 | train/val/test=1.000/0.696/0.701 | c=0.998437
[Epoch 0090] loss=11.7560 cls=0.0429 smmd=0.0538 ct=9.2906 rec=1.1844 | train/val/test=1.000/0.694/0.695 | c=0.998437
[Epoch 0091] loss=11.7290 cls=0.0297 smmd=0.0629 ct=9.2874 rec=1.1745 | train/val/test=1.000/0.720/0.717 | c=0.998437
[Epoch 0092] loss=11.6093 cls=0.0157 smmd=0.0111 ct=9.2637 rec=1.1594 | train/val/test=1.000/0.704/0.698 | c=0.998437
[Epoch 0093] loss=11.6537 cls=0.0177 smmd=0.0355 ct=9.2738 rec=1.1634 | train/val/test=1.000/0.712/0.703 | c=0.998437
[Epoch 0094] loss=11.6570 cls=0.0166 smmd=0.0481 ct=9.2723 rec=1.1600 | train/val/test=1.000/0.712/0.710 | c=0.998437
[Epoch 0095] loss=11.6062 cls=0.0134 smmd=0.0132 ct=9.2639 rec=1.1578 | train/val/test=1.000/0.710/0.704 | c=0.998437
[Epoch 0096] loss=11.6508 cls=0.0166 smmd=0.0325 ct=9.2723 rec=1.1647 | train/val/test=1.000/0.714/0.701 | c=0.998437
[Epoch 0097] loss=11.6165 cls=0.0179 smmd=0.0128 ct=9.2604 rec=1.1627 | train/val/test=1.000/0.710/0.709 | c=0.998437
[Epoch 0098] loss=11.6047 cls=0.0194 smmd=-0.0040 ct=9.2583 rec=1.1655 | train/val/test=1.000/0.708/0.697 | c=0.998437
[Epoch 0099] loss=11.6574 cls=0.0243 smmd=0.0163 ct=9.2681 rec=1.1743 | train/val/test=1.000/0.702/0.699 | c=0.998437
=== Best @ epoch 78: val=0.7220, test=0.7080 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5 - 2025-09-21 04:30:50:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Photo
Split sizes | train=58, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=18.1474 cls=1.9436 smmd=4.1752 ct=9.2509 rec=1.3889 | train/val/test=0.241/0.074/0.101 | c=0.998437
[Epoch 0001] loss=16.6188 cls=1.8948 smmd=2.7562 ct=9.1893 rec=1.3892 | train/val/test=0.741/0.396/0.378 | c=0.998437
[Epoch 0002] loss=15.0422 cls=1.7579 smmd=1.4650 ct=9.0425 rec=1.3884 | train/val/test=0.810/0.422/0.425 | c=0.998437
[Epoch 0003] loss=14.7480 cls=1.5092 smmd=1.4791 ct=8.9882 rec=1.3858 | train/val/test=0.690/0.396/0.379 | c=0.998437
[Epoch 0004] loss=14.6590 cls=1.2254 smmd=1.7291 ct=8.9573 rec=1.3736 | train/val/test=0.845/0.544/0.506 | c=0.998437
[Epoch 0005] loss=14.1019 cls=0.8653 smmd=1.6724 ct=8.8662 rec=1.3490 | train/val/test=0.897/0.540/0.537 | c=0.998437
[Epoch 0006] loss=13.4137 cls=0.5960 smmd=1.3298 ct=8.8537 rec=1.3171 | train/val/test=0.948/0.576/0.557 | c=0.998437
[Epoch 0007] loss=13.5725 cls=0.4058 smmd=1.0647 ct=9.5208 rec=1.2905 | train/val/test=0.966/0.606/0.592 | c=0.998437
[Epoch 0008] loss=13.2484 cls=0.2671 smmd=1.0150 ct=9.4313 rec=1.2675 | train/val/test=0.983/0.626/0.618 | c=0.998437
[Epoch 0009] loss=13.1966 cls=0.1650 smmd=1.1897 ct=9.3521 rec=1.2449 | train/val/test=0.983/0.640/0.644 | c=0.998437
[Epoch 0010] loss=13.1521 cls=0.1003 smmd=1.2897 ct=9.3077 rec=1.2272 | train/val/test=1.000/0.666/0.667 | c=0.998437
[Epoch 0011] loss=12.9733 cls=0.0617 smmd=1.1824 ct=9.3004 rec=1.2144 | train/val/test=1.000/0.674/0.667 | c=0.998437
[Epoch 0012] loss=12.7932 cls=0.0410 smmd=1.0110 ct=9.3307 rec=1.2053 | train/val/test=1.000/0.672/0.671 | c=0.998437
[Epoch 0013] loss=12.6433 cls=0.0272 smmd=0.8457 ct=9.3770 rec=1.1967 | train/val/test=1.000/0.674/0.665 | c=0.998437
[Epoch 0014] loss=12.5234 cls=0.0195 smmd=0.7087 ct=9.4136 rec=1.1908 | train/val/test=1.000/0.678/0.670 | c=0.998437
[Epoch 0015] loss=12.4729 cls=0.0156 smmd=0.6670 ct=9.4155 rec=1.1874 | train/val/test=1.000/0.696/0.673 | c=0.998437
[Epoch 0016] loss=12.4740 cls=0.0136 smmd=0.6987 ct=9.3902 rec=1.1858 | train/val/test=1.000/0.704/0.682 | c=0.998437
[Epoch 0017] loss=12.3820 cls=0.0128 smmd=0.6325 ct=9.3649 rec=1.1859 | train/val/test=1.000/0.710/0.682 | c=0.998437
[Epoch 0018] loss=12.2856 cls=0.0128 smmd=0.5488 ct=9.3514 rec=1.1863 | train/val/test=1.000/0.714/0.685 | c=0.998437
[Epoch 0019] loss=12.2149 cls=0.0143 smmd=0.4686 ct=9.3551 rec=1.1884 | train/val/test=1.000/0.718/0.689 | c=0.998437
[Epoch 0020] loss=12.1685 cls=0.0165 smmd=0.4089 ct=9.3609 rec=1.1910 | train/val/test=1.000/0.714/0.696 | c=0.998437
[Epoch 0021] loss=12.1489 cls=0.0200 smmd=0.3799 ct=9.3607 rec=1.1942 | train/val/test=1.000/0.708/0.690 | c=0.998437
[Epoch 0022] loss=12.0959 cls=0.0240 smmd=0.3249 ct=9.3574 rec=1.1948 | train/val/test=1.000/0.716/0.709 | c=0.998437
[Epoch 0023] loss=12.0689 cls=0.0285 smmd=0.2909 ct=9.3605 rec=1.1945 | train/val/test=1.000/0.712/0.707 | c=0.998437
[Epoch 0024] loss=12.0155 cls=0.0333 smmd=0.2294 ct=9.3665 rec=1.1932 | train/val/test=1.000/0.714/0.710 | c=0.998437
[Epoch 0025] loss=11.9942 cls=0.0368 smmd=0.2104 ct=9.3689 rec=1.1890 | train/val/test=1.000/0.718/0.719 | c=0.998437
[Epoch 0026] loss=11.9904 cls=0.0394 smmd=0.2173 ct=9.3640 rec=1.1848 | train/val/test=1.000/0.718/0.710 | c=0.998437
[Epoch 0027] loss=11.9340 cls=0.0416 smmd=0.1701 ct=9.3590 rec=1.1817 | train/val/test=1.000/0.718/0.727 | c=0.998437
[Epoch 0028] loss=11.9247 cls=0.0417 smmd=0.1647 ct=9.3600 rec=1.1791 | train/val/test=1.000/0.714/0.700 | c=0.998437
[Epoch 0029] loss=11.9318 cls=0.0413 smmd=0.1766 ct=9.3589 rec=1.1775 | train/val/test=1.000/0.714/0.721 | c=0.998437
[Epoch 0030] loss=11.9064 cls=0.0381 smmd=0.1595 ct=9.3584 rec=1.1752 | train/val/test=1.000/0.718/0.711 | c=0.998437
[Epoch 0031] loss=11.8544 cls=0.0262 smmd=0.1462 ct=9.3445 rec=1.1688 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0032] loss=11.8009 cls=0.0207 smmd=0.1149 ct=9.3365 rec=1.1644 | train/val/test=1.000/0.720/0.718 | c=0.998437
[Epoch 0033] loss=11.8056 cls=0.0195 smmd=0.1188 ct=9.3387 rec=1.1643 | train/val/test=1.000/0.712/0.710 | c=0.998437
[Epoch 0034] loss=11.8007 cls=0.0186 smmd=0.1164 ct=9.3384 rec=1.1637 | train/val/test=1.000/0.714/0.703 | c=0.998437
[Epoch 0035] loss=11.7704 cls=0.0167 smmd=0.0951 ct=9.3342 rec=1.1622 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0036] loss=11.7566 cls=0.0172 smmd=0.0855 ct=9.3279 rec=1.1630 | train/val/test=1.000/0.710/0.710 | c=0.998437
[Epoch 0037] loss=11.7629 cls=0.0196 smmd=0.0914 ct=9.3218 rec=1.1651 | train/val/test=1.000/0.716/0.699 | c=0.998437
[Epoch 0038] loss=11.7550 cls=0.0209 smmd=0.0760 ct=9.3230 rec=1.1676 | train/val/test=1.000/0.716/0.713 | c=0.998437
[Epoch 0039] loss=11.7377 cls=0.0208 smmd=0.0632 ct=9.3209 rec=1.1664 | train/val/test=1.000/0.716/0.707 | c=0.998437
[Epoch 0040] loss=11.7332 cls=0.0216 smmd=0.0590 ct=9.3202 rec=1.1662 | train/val/test=1.000/0.714/0.703 | c=0.998437
[Epoch 0041] loss=11.7163 cls=0.0227 smmd=0.0451 ct=9.3155 rec=1.1665 | train/val/test=1.000/0.714/0.710 | c=0.998437
[Epoch 0042] loss=11.7256 cls=0.0241 smmd=0.0554 ct=9.3122 rec=1.1670 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0043] loss=11.7153 cls=0.0246 smmd=0.0398 ct=9.3180 rec=1.1665 | train/val/test=1.000/0.712/0.708 | c=0.998437
[Epoch 0044] loss=11.7076 cls=0.0258 smmd=0.0332 ct=9.3167 rec=1.1660 | train/val/test=1.000/0.718/0.715 | c=0.998437
[Epoch 0045] loss=11.7137 cls=0.0262 smmd=0.0432 ct=9.3119 rec=1.1663 | train/val/test=1.000/0.708/0.704 | c=0.998437
[Epoch 0046] loss=11.7108 cls=0.0278 smmd=0.0469 ct=9.3039 rec=1.1661 | train/val/test=1.000/0.718/0.716 | c=0.998437
[Epoch 0047] loss=11.7066 cls=0.0261 smmd=0.0437 ct=9.3056 rec=1.1656 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0048] loss=11.6868 cls=0.0248 smmd=0.0291 ct=9.3050 rec=1.1639 | train/val/test=1.000/0.708/0.707 | c=0.998437
[Epoch 0049] loss=11.6786 cls=0.0217 smmd=0.0268 ct=9.3058 rec=1.1621 | train/val/test=1.000/0.716/0.718 | c=0.998437
[Epoch 0050] loss=11.6633 cls=0.0201 smmd=0.0191 ct=9.3012 rec=1.1614 | train/val/test=1.000/0.710/0.705 | c=0.998437
[Epoch 0051] loss=11.6561 cls=0.0201 smmd=0.0204 ct=9.2927 rec=1.1614 | train/val/test=1.000/0.718/0.708 | c=0.998437
[Epoch 0052] loss=11.6605 cls=0.0204 smmd=0.0255 ct=9.2893 rec=1.1627 | train/val/test=1.000/0.710/0.706 | c=0.998437
[Epoch 0053] loss=11.6664 cls=0.0210 smmd=0.0272 ct=9.2904 rec=1.1639 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0054] loss=11.6618 cls=0.0220 smmd=0.0165 ct=9.2924 rec=1.1655 | train/val/test=1.000/0.708/0.706 | c=0.998437
[Epoch 0055] loss=11.6528 cls=0.0218 smmd=0.0119 ct=9.2892 rec=1.1649 | train/val/test=1.000/0.714/0.709 | c=0.998437
[Epoch 0056] loss=11.6554 cls=0.0223 smmd=0.0146 ct=9.2876 rec=1.1654 | train/val/test=1.000/0.708/0.702 | c=0.998437
[Epoch 0057] loss=11.6525 cls=0.0232 smmd=0.0139 ct=9.2840 rec=1.1657 | train/val/test=1.000/0.720/0.715 | c=0.998437
[Epoch 0058] loss=11.6588 cls=0.0242 smmd=0.0132 ct=9.2878 rec=1.1668 | train/val/test=1.000/0.708/0.701 | c=0.998437
[Epoch 0059] loss=11.6621 cls=0.0259 smmd=0.0182 ct=9.2834 rec=1.1673 | train/val/test=1.000/0.720/0.712 | c=0.998437
[Epoch 0060] loss=11.6648 cls=0.0254 smmd=0.0199 ct=9.2843 rec=1.1676 | train/val/test=1.000/0.708/0.708 | c=0.998437
[Epoch 0061] loss=11.6596 cls=0.0261 smmd=0.0168 ct=9.2826 rec=1.1670 | train/val/test=1.000/0.710/0.701 | c=0.998437
[Epoch 0062] loss=11.6642 cls=0.0245 smmd=0.0178 ct=9.2886 rec=1.1667 | train/val/test=1.000/0.704/0.715 | c=0.998437
[Epoch 0063] loss=11.6635 cls=0.0246 smmd=0.0221 ct=9.2849 rec=1.1659 | train/val/test=1.000/0.712/0.699 | c=0.998437
[Epoch 0064] loss=11.6424 cls=0.0219 smmd=0.0137 ct=9.2793 rec=1.1638 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0065] loss=11.6224 cls=0.0198 smmd=0.0060 ct=9.2739 rec=1.1614 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0066] loss=11.6200 cls=0.0200 smmd=0.0040 ct=9.2722 rec=1.1619 | train/val/test=1.000/0.714/0.699 | c=0.998437
[Epoch 0067] loss=11.6301 cls=0.0219 smmd=0.0061 ct=9.2726 rec=1.1648 | train/val/test=1.000/0.710/0.707 | c=0.998437
[Epoch 0068] loss=11.6328 cls=0.0225 smmd=0.0085 ct=9.2716 rec=1.1651 | train/val/test=1.000/0.714/0.707 | c=0.998437
[Epoch 0069] loss=11.6257 cls=0.0225 smmd=0.0029 ct=9.2686 rec=1.1658 | train/val/test=1.000/0.710/0.702 | c=0.998437
[Epoch 0070] loss=11.6234 cls=0.0238 smmd=-0.0030 ct=9.2693 rec=1.1667 | train/val/test=1.000/0.716/0.710 | c=0.998437
[Epoch 0071] loss=11.6320 cls=0.0249 smmd=-0.0002 ct=9.2705 rec=1.1683 | train/val/test=1.000/0.708/0.701 | c=0.998437
[Epoch 0072] loss=11.6357 cls=0.0259 smmd=0.0094 ct=9.2630 rec=1.1688 | train/val/test=1.000/0.718/0.713 | c=0.998437
[Epoch 0073] loss=11.6238 cls=0.0242 smmd=-0.0010 ct=9.2658 rec=1.1674 | train/val/test=1.000/0.714/0.704 | c=0.998437
[Epoch 0074] loss=11.6171 cls=0.0239 smmd=-0.0032 ct=9.2648 rec=1.1659 | train/val/test=1.000/0.716/0.702 | c=0.998437
[Epoch 0075] loss=11.6234 cls=0.0237 smmd=-0.0003 ct=9.2678 rec=1.1661 | train/val/test=1.000/0.706/0.713 | c=0.998437
[Epoch 0076] loss=11.6348 cls=0.0253 smmd=0.0077 ct=9.2673 rec=1.1672 | train/val/test=1.000/0.710/0.700 | c=0.998437
[Epoch 0077] loss=11.6269 cls=0.0240 smmd=0.0038 ct=9.2658 rec=1.1667 | train/val/test=1.000/0.710/0.707 | c=0.998437
[Epoch 0078] loss=11.6186 cls=0.0226 smmd=0.0030 ct=9.2624 rec=1.1653 | train/val/test=1.000/0.722/0.708 | c=0.998437
[Epoch 0079] loss=11.6179 cls=0.0225 smmd=0.0013 ct=9.2625 rec=1.1658 | train/val/test=1.000/0.708/0.699 | c=0.998437
[Epoch 0080] loss=11.6182 cls=0.0222 smmd=0.0046 ct=9.2601 rec=1.1657 | train/val/test=1.000/0.716/0.712 | c=0.998437
[Epoch 0081] loss=11.5976 cls=0.0211 smmd=-0.0107 ct=9.2583 rec=1.1645 | train/val/test=1.000/0.716/0.703 | c=0.998437
[Epoch 0082] loss=11.5901 cls=0.0208 smmd=-0.0111 ct=9.2532 rec=1.1636 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0083] loss=11.6015 cls=0.0215 smmd=-0.0028 ct=9.2531 rec=1.1648 | train/val/test=1.000/0.718/0.709 | c=0.998437
[Epoch 0084] loss=11.6031 cls=0.0225 smmd=-0.0097 ct=9.2578 rec=1.1663 | train/val/test=1.000/0.714/0.701 | c=0.998437
[Epoch 0085] loss=11.6031 cls=0.0236 smmd=-0.0063 ct=9.2520 rec=1.1669 | train/val/test=1.000/0.720/0.711 | c=0.998437
[Epoch 0086] loss=11.6015 cls=0.0234 smmd=-0.0069 ct=9.2508 rec=1.1671 | train/val/test=1.000/0.712/0.702 | c=0.998437
[Epoch 0087] loss=11.6182 cls=0.0268 smmd=-0.0013 ct=9.2541 rec=1.1693 | train/val/test=1.000/0.710/0.704 | c=0.998437
[Epoch 0088] loss=11.6468 cls=0.0296 smmd=0.0053 ct=9.2664 rec=1.1727 | train/val/test=1.000/0.702/0.698 | c=0.998437
[Epoch 0089] loss=11.7100 cls=0.0385 smmd=0.0372 ct=9.2769 rec=1.1787 | train/val/test=1.000/0.696/0.701 | c=0.998437
[Epoch 0090] loss=11.7560 cls=0.0429 smmd=0.0538 ct=9.2906 rec=1.1844 | train/val/test=1.000/0.694/0.695 | c=0.998437
[Epoch 0091] loss=11.7290 cls=0.0297 smmd=0.0629 ct=9.2874 rec=1.1745 | train/val/test=1.000/0.720/0.717 | c=0.998437
[Epoch 0092] loss=11.6093 cls=0.0157 smmd=0.0111 ct=9.2637 rec=1.1594 | train/val/test=1.000/0.704/0.698 | c=0.998437
[Epoch 0093] loss=11.6537 cls=0.0177 smmd=0.0355 ct=9.2738 rec=1.1634 | train/val/test=1.000/0.712/0.703 | c=0.998437
[Epoch 0094] loss=11.6570 cls=0.0166 smmd=0.0481 ct=9.2723 rec=1.1600 | train/val/test=1.000/0.712/0.710 | c=0.998437
[Epoch 0095] loss=11.6062 cls=0.0134 smmd=0.0132 ct=9.2639 rec=1.1578 | train/val/test=1.000/0.710/0.704 | c=0.998437
[Epoch 0096] loss=11.6508 cls=0.0166 smmd=0.0325 ct=9.2723 rec=1.1647 | train/val/test=1.000/0.714/0.701 | c=0.998437
[Epoch 0097] loss=11.6165 cls=0.0179 smmd=0.0128 ct=9.2604 rec=1.1627 | train/val/test=1.000/0.710/0.709 | c=0.998437
[Epoch 0098] loss=11.6047 cls=0.0194 smmd=-0.0040 ct=9.2583 rec=1.1655 | train/val/test=1.000/0.708/0.697 | c=0.998437
[Epoch 0099] loss=11.6574 cls=0.0243 smmd=0.0163 ct=9.2681 rec=1.1743 | train/val/test=1.000/0.702/0.699 | c=0.998437
=== Best @ epoch 78: val=0.7220, test=0.7080 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Photo.GRACE.GAT.hyp_True.True.20250912-232537.pth-True-10-5 completed in 30.73 seconds.
==================================================
