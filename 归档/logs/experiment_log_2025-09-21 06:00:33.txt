Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 - 2025-09-21 06:00:33:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=39.9088 cls=1.0999 smmd=5.5882 ct=11.2763 rec=1.4138 | train/val/test=0.346/0.198/0.185 | c=0.998347
[Epoch 0001] loss=29.7564 cls=1.0814 smmd=3.5679 ct=11.2350 rec=1.4137 | train/val/test=0.615/0.522/0.511 | c=0.998347
[Epoch 0002] loss=35.3851 cls=1.0613 smmd=4.6933 ct=11.2464 rec=1.4133 | train/val/test=0.615/0.424/0.447 | c=0.998347
[Epoch 0003] loss=34.1523 cls=1.0131 smmd=4.4629 ct=11.1902 rec=1.4126 | train/val/test=0.577/0.520/0.514 | c=0.998347
[Epoch 0004] loss=24.9301 cls=0.9371 smmd=2.6629 ct=11.0062 rec=1.4081 | train/val/test=0.654/0.588/0.606 | c=0.998347
[Epoch 0005] loss=26.3604 cls=0.8496 smmd=2.9620 ct=10.9855 rec=1.3990 | train/val/test=0.731/0.634/0.646 | c=0.998347
[Epoch 0006] loss=28.2939 cls=0.7536 smmd=3.3621 ct=10.9682 rec=1.3868 | train/val/test=0.769/0.650/0.664 | c=0.998347
[Epoch 0007] loss=26.4762 cls=0.6494 smmd=2.8962 ct=11.5336 rec=1.3685 | train/val/test=0.769/0.650/0.666 | c=0.998347
[Epoch 0008] loss=22.1677 cls=0.5479 smmd=2.0655 ct=11.4304 rec=1.3559 | train/val/test=0.769/0.690/0.681 | c=0.998347
[Epoch 0009] loss=23.1194 cls=0.4458 smmd=2.2525 ct=11.4989 rec=1.3498 | train/val/test=0.885/0.732/0.710 | c=0.998347
[Epoch 0010] loss=24.8623 cls=0.3720 smmd=2.5975 ct=11.5536 rec=1.3501 | train/val/test=1.000/0.764/0.746 | c=0.998347
[Epoch 0011] loss=22.3501 cls=0.2797 smmd=2.1142 ct=11.5043 rec=1.3512 | train/val/test=1.000/0.774/0.719 | c=0.998347
[Epoch 0012] loss=20.7856 cls=0.2455 smmd=1.7998 ct=11.5283 rec=1.3569 | train/val/test=1.000/0.738/0.702 | c=0.998347
[Epoch 0013] loss=23.1238 cls=0.2048 smmd=2.2802 ct=11.4851 rec=1.3526 | train/val/test=1.000/0.742/0.703 | c=0.998347
[Epoch 0014] loss=21.5417 cls=0.1629 smmd=1.9632 ct=11.5101 rec=1.3401 | train/val/test=1.000/0.730/0.693 | c=0.998347
[Epoch 0015] loss=19.5001 cls=0.1182 smmd=1.5631 ct=11.4928 rec=1.3270 | train/val/test=1.000/0.750/0.705 | c=0.998347
[Epoch 0016] loss=19.9798 cls=0.0888 smmd=1.6625 ct=11.4910 rec=1.3195 | train/val/test=1.000/0.794/0.729 | c=0.998347
[Epoch 0017] loss=20.0069 cls=0.0651 smmd=1.6694 ct=11.4965 rec=1.3072 | train/val/test=1.000/0.774/0.733 | c=0.998347
[Epoch 0018] loss=18.3797 cls=0.0536 smmd=1.3483 ct=11.4816 rec=1.3005 | train/val/test=1.000/0.760/0.724 | c=0.998347
[Epoch 0019] loss=18.2270 cls=0.0567 smmd=1.3209 ct=11.4639 rec=1.3014 | train/val/test=1.000/0.810/0.755 | c=0.998347
[Epoch 0020] loss=18.7832 cls=0.0609 smmd=1.4340 ct=11.4527 rec=1.2993 | train/val/test=1.000/0.798/0.749 | c=0.998347
[Epoch 0021] loss=17.9228 cls=0.0730 smmd=1.2574 ct=11.4694 rec=1.2999 | train/val/test=1.000/0.808/0.762 | c=0.998347
[Epoch 0022] loss=17.9219 cls=0.0862 smmd=1.2469 ct=11.5144 rec=1.2986 | train/val/test=1.000/0.812/0.767 | c=0.998347
[Epoch 0023] loss=17.9867 cls=0.0865 smmd=1.2616 ct=11.5056 rec=1.3003 | train/val/test=1.000/0.812/0.767 | c=0.998347
[Epoch 0024] loss=17.1368 cls=0.0808 smmd=1.0997 ct=11.4685 rec=1.2962 | train/val/test=1.000/0.798/0.756 | c=0.998347
[Epoch 0025] loss=16.9813 cls=0.0758 smmd=1.0728 ct=11.4500 rec=1.2944 | train/val/test=1.000/0.810/0.757 | c=0.998347
[Epoch 0026] loss=16.6719 cls=0.0647 smmd=1.0102 ct=11.4587 rec=1.2966 | train/val/test=1.000/0.812/0.756 | c=0.998347
[Epoch 0027] loss=16.2566 cls=0.0604 smmd=0.9235 ct=11.4796 rec=1.2950 | train/val/test=1.000/0.810/0.757 | c=0.998347
[Epoch 0028] loss=15.7997 cls=0.0601 smmd=0.8368 ct=11.4561 rec=1.2942 | train/val/test=1.000/0.780/0.732 | c=0.998347
[Epoch 0029] loss=16.0662 cls=0.0749 smmd=0.8870 ct=11.4631 rec=1.3042 | train/val/test=1.000/0.796/0.759 | c=0.998347
[Epoch 0030] loss=15.6088 cls=0.0724 smmd=0.7905 ct=11.4898 rec=1.3013 | train/val/test=1.000/0.794/0.738 | c=0.998347
[Epoch 0031] loss=16.3694 cls=0.0775 smmd=0.9479 ct=11.4601 rec=1.3093 | train/val/test=1.000/0.796/0.742 | c=0.998347
[Epoch 0032] loss=15.8277 cls=0.0739 smmd=0.8381 ct=11.4699 rec=1.3048 | train/val/test=1.000/0.806/0.758 | c=0.998347
[Epoch 0033] loss=16.1571 cls=0.0725 smmd=0.8953 ct=11.5135 rec=1.3067 | train/val/test=1.000/0.774/0.728 | c=0.998347
[Epoch 0034] loss=15.2483 cls=0.0642 smmd=0.7237 ct=11.4674 rec=1.3047 | train/val/test=1.000/0.778/0.757 | c=0.998347
[Epoch 0035] loss=15.2422 cls=0.0669 smmd=0.7211 ct=11.4734 rec=1.2965 | train/val/test=1.000/0.818/0.757 | c=0.998347
[Epoch 0036] loss=15.0056 cls=0.0491 smmd=0.6785 ct=11.4588 rec=1.3001 | train/val/test=1.000/0.794/0.760 | c=0.998347
[Epoch 0037] loss=14.6558 cls=0.0552 smmd=0.6111 ct=11.4436 rec=1.2897 | train/val/test=1.000/0.828/0.770 | c=0.998347
[Epoch 0038] loss=14.8065 cls=0.0521 smmd=0.6354 ct=11.4735 rec=1.2969 | train/val/test=1.000/0.828/0.769 | c=0.998347
[Epoch 0039] loss=14.6091 cls=0.0581 smmd=0.5964 ct=11.4680 rec=1.3010 | train/val/test=1.000/0.804/0.769 | c=0.998347
[Epoch 0040] loss=15.0786 cls=0.0703 smmd=0.6902 ct=11.4621 rec=1.3043 | train/val/test=1.000/0.828/0.761 | c=0.998347
[Epoch 0041] loss=15.3321 cls=0.0767 smmd=0.7367 ct=11.4788 rec=1.3132 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0042] loss=15.2070 cls=0.0793 smmd=0.7118 ct=11.4772 rec=1.3118 | train/val/test=1.000/0.792/0.743 | c=0.998347
[Epoch 0043] loss=15.2029 cls=0.0842 smmd=0.7133 ct=11.4627 rec=1.3157 | train/val/test=1.000/0.808/0.771 | c=0.998347
[Epoch 0044] loss=14.4190 cls=0.0758 smmd=0.5574 ct=11.4632 rec=1.3086 | train/val/test=1.000/0.804/0.747 | c=0.998347
[Epoch 0045] loss=14.4036 cls=0.0775 smmd=0.5549 ct=11.4591 rec=1.3125 | train/val/test=1.000/0.806/0.770 | c=0.998347
[Epoch 0046] loss=14.4412 cls=0.0762 smmd=0.5655 ct=11.4452 rec=1.3062 | train/val/test=1.000/0.812/0.753 | c=0.998347
[Epoch 0047] loss=13.9776 cls=0.0803 smmd=0.4700 ct=11.4563 rec=1.3137 | train/val/test=1.000/0.800/0.775 | c=0.998347
[Epoch 0048] loss=14.2783 cls=0.0860 smmd=0.5278 ct=11.4654 rec=1.3116 | train/val/test=1.000/0.794/0.758 | c=0.998347
[Epoch 0049] loss=14.4915 cls=0.0963 smmd=0.5691 ct=11.4658 rec=1.3210 | train/val/test=1.000/0.744/0.734 | c=0.998347
[Epoch 0050] loss=14.8820 cls=0.1267 smmd=0.6364 ct=11.5038 rec=1.3311 | train/val/test=1.000/0.710/0.685 | c=0.998347
[Epoch 0051] loss=15.1345 cls=0.1302 smmd=0.6862 ct=11.5050 rec=1.3361 | train/val/test=1.000/0.718/0.717 | c=0.998347
[Epoch 0052] loss=14.7959 cls=0.1456 smmd=0.6172 ct=11.5029 rec=1.3405 | train/val/test=1.000/0.754/0.725 | c=0.998347
[Epoch 0053] loss=14.2223 cls=0.0905 smmd=0.5132 ct=11.4791 rec=1.3208 | train/val/test=1.000/0.788/0.764 | c=0.998347
[Epoch 0054] loss=14.2421 cls=0.0660 smmd=0.5251 ct=11.4534 rec=1.3001 | train/val/test=1.000/0.820/0.789 | c=0.998347
[Epoch 0055] loss=13.7576 cls=0.0551 smmd=0.4322 ct=11.4394 rec=1.2991 | train/val/test=1.000/0.816/0.787 | c=0.998347
[Epoch 0056] loss=13.7165 cls=0.0574 smmd=0.4214 ct=11.4504 rec=1.3021 | train/val/test=1.000/0.814/0.774 | c=0.998347
[Epoch 0057] loss=14.0748 cls=0.0669 smmd=0.4903 ct=11.4590 rec=1.3097 | train/val/test=1.000/0.800/0.778 | c=0.998347
[Epoch 0058] loss=14.2122 cls=0.0775 smmd=0.5152 ct=11.4659 rec=1.3176 | train/val/test=1.000/0.782/0.745 | c=0.998347
[Epoch 0059] loss=14.6558 cls=0.0943 smmd=0.5995 ct=11.4785 rec=1.3270 | train/val/test=1.000/0.770/0.755 | c=0.998347
[Epoch 0060] loss=15.3112 cls=0.1032 smmd=0.7243 ct=11.5046 rec=1.3337 | train/val/test=1.000/0.710/0.697 | c=0.998347
[Epoch 0061] loss=14.5992 cls=0.1197 smmd=0.5878 ct=11.4665 rec=1.3376 | train/val/test=1.000/0.762/0.742 | c=0.998347
[Epoch 0062] loss=14.1662 cls=0.0990 smmd=0.5033 ct=11.4672 rec=1.3302 | train/val/test=1.000/0.714/0.691 | c=0.998347
[Epoch 0063] loss=14.0710 cls=0.1025 smmd=0.4823 ct=11.4751 rec=1.3313 | train/val/test=1.000/0.788/0.766 | c=0.998347
[Epoch 0064] loss=13.7528 cls=0.0811 smmd=0.4289 ct=11.4364 rec=1.3152 | train/val/test=1.000/0.758/0.739 | c=0.998347
[Epoch 0065] loss=13.7708 cls=0.0842 smmd=0.4296 ct=11.4487 rec=1.3204 | train/val/test=1.000/0.788/0.771 | c=0.998347
[Epoch 0066] loss=13.7075 cls=0.0865 smmd=0.4135 ct=11.4650 rec=1.3167 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0067] loss=13.9307 cls=0.0882 smmd=0.4602 ct=11.4531 rec=1.3240 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0068] loss=14.7133 cls=0.1038 smmd=0.6078 ct=11.4896 rec=1.3267 | train/val/test=1.000/0.802/0.768 | c=0.998347
[Epoch 0069] loss=14.9397 cls=0.0969 smmd=0.6559 ct=11.4788 rec=1.3316 | train/val/test=1.000/0.758/0.729 | c=0.998347
[Epoch 0070] loss=14.9730 cls=0.1150 smmd=0.6620 ct=11.4720 rec=1.3321 | train/val/test=1.000/0.738/0.711 | c=0.998347
[Epoch 0071] loss=14.1688 cls=0.0938 smmd=0.4996 ct=11.4903 rec=1.3375 | train/val/test=1.000/0.760/0.730 | c=0.998347
[Epoch 0072] loss=13.8427 cls=0.0963 smmd=0.4431 ct=11.4470 rec=1.3219 | train/val/test=1.000/0.778/0.751 | c=0.998347
[Epoch 0073] loss=13.7937 cls=0.0699 smmd=0.4370 ct=11.4419 rec=1.3208 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0074] loss=13.5504 cls=0.0589 smmd=0.3892 ct=11.4444 rec=1.3081 | train/val/test=1.000/0.814/0.765 | c=0.998347
[Epoch 0075] loss=13.4784 cls=0.0632 smmd=0.3771 ct=11.4299 rec=1.3142 | train/val/test=1.000/0.804/0.777 | c=0.998347
[Epoch 0076] loss=13.7410 cls=0.0714 smmd=0.4250 ct=11.4481 rec=1.3208 | train/val/test=1.000/0.792/0.760 | c=0.998347
[Epoch 0077] loss=14.3299 cls=0.0857 smmd=0.5341 ct=11.4836 rec=1.3297 | train/val/test=1.000/0.798/0.761 | c=0.998347
[Epoch 0078] loss=15.1871 cls=0.0979 smmd=0.7090 ct=11.4597 rec=1.3355 | train/val/test=1.000/0.796/0.756 | c=0.998347
[Epoch 0079] loss=14.8946 cls=0.0913 smmd=0.6498 ct=11.4668 rec=1.3322 | train/val/test=1.000/0.808/0.772 | c=0.998347
[Epoch 0080] loss=13.9378 cls=0.0784 smmd=0.4627 ct=11.4524 rec=1.3246 | train/val/test=1.000/0.814/0.773 | c=0.998347
[Epoch 0081] loss=13.6043 cls=0.0700 smmd=0.4047 ct=11.4139 rec=1.3180 | train/val/test=1.000/0.814/0.778 | c=0.998347
[Epoch 0082] loss=13.5308 cls=0.0682 smmd=0.3895 ct=11.4175 rec=1.3154 | train/val/test=1.000/0.812/0.781 | c=0.998347
[Epoch 0083] loss=13.3018 cls=0.0719 smmd=0.3421 ct=11.4239 rec=1.3165 | train/val/test=1.000/0.816/0.788 | c=0.998347
[Epoch 0084] loss=13.3315 cls=0.0817 smmd=0.3466 ct=11.4252 rec=1.3226 | train/val/test=1.000/0.792/0.764 | c=0.998347
[Epoch 0085] loss=13.8425 cls=0.1040 smmd=0.4423 ct=11.4458 rec=1.3335 | train/val/test=1.000/0.804/0.780 | c=0.998347
[Epoch 0086] loss=14.4002 cls=0.1240 smmd=0.5426 ct=11.4910 rec=1.3432 | train/val/test=1.000/0.760/0.729 | c=0.998347
[Epoch 0087] loss=15.1442 cls=0.1340 smmd=0.6921 ct=11.4819 rec=1.3483 | train/val/test=1.000/0.812/0.766 | c=0.998347
[Epoch 0088] loss=15.2334 cls=0.1291 smmd=0.7071 ct=11.4986 rec=1.3477 | train/val/test=1.000/0.762/0.731 | c=0.998347
[Epoch 0089] loss=13.8616 cls=0.0998 smmd=0.4447 ct=11.4551 rec=1.3325 | train/val/test=1.000/0.784/0.762 | c=0.998347
[Epoch 0090] loss=14.0144 cls=0.0759 smmd=0.4843 ct=11.4225 rec=1.3263 | train/val/test=1.000/0.780/0.754 | c=0.998347
[Epoch 0091] loss=13.6308 cls=0.0672 smmd=0.4095 ct=11.4177 rec=1.3182 | train/val/test=1.000/0.818/0.771 | c=0.998347
[Epoch 0092] loss=13.3382 cls=0.0612 smmd=0.3501 ct=11.4254 rec=1.3177 | train/val/test=1.000/0.794/0.776 | c=0.998347
[Epoch 0093] loss=13.5735 cls=0.0625 smmd=0.3989 ct=11.4155 rec=1.3197 | train/val/test=1.000/0.802/0.775 | c=0.998347
[Epoch 0094] loss=13.5988 cls=0.0740 smmd=0.3970 ct=11.4440 rec=1.3276 | train/val/test=1.000/0.792/0.765 | c=0.998347
[Epoch 0095] loss=14.3265 cls=0.1123 smmd=0.5267 ct=11.5023 rec=1.3453 | train/val/test=1.000/0.634/0.614 | c=0.998347
[Epoch 0096] loss=15.5568 cls=0.1417 smmd=0.7666 ct=11.5163 rec=1.3636 | train/val/test=1.000/0.758/0.740 | c=0.998347
[Epoch 0097] loss=15.3359 cls=0.1841 smmd=0.7051 ct=11.5824 rec=1.3608 | train/val/test=1.000/0.672/0.645 | c=0.998347
[Epoch 0098] loss=14.2905 cls=0.1081 smmd=0.5246 ct=11.4790 rec=1.3448 | train/val/test=1.000/0.780/0.783 | c=0.998347
[Epoch 0099] loss=13.8534 cls=0.0611 smmd=0.4502 ct=11.4398 rec=1.3189 | train/val/test=1.000/0.810/0.770 | c=0.998347
=== Best @ epoch 37: val=0.8280, test=0.7700 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 - 2025-09-21 06:00:33:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=26, val=500, test=1000 (mode=few, shot=10)
[Epoch 0000] loss=39.9088 cls=1.0999 smmd=5.5882 ct=11.2763 rec=1.4138 | train/val/test=0.346/0.198/0.185 | c=0.998347
[Epoch 0001] loss=29.7564 cls=1.0814 smmd=3.5679 ct=11.2350 rec=1.4137 | train/val/test=0.615/0.522/0.511 | c=0.998347
[Epoch 0002] loss=35.3851 cls=1.0613 smmd=4.6933 ct=11.2464 rec=1.4133 | train/val/test=0.615/0.424/0.447 | c=0.998347
[Epoch 0003] loss=34.1523 cls=1.0131 smmd=4.4629 ct=11.1902 rec=1.4126 | train/val/test=0.577/0.520/0.514 | c=0.998347
[Epoch 0004] loss=24.9301 cls=0.9371 smmd=2.6629 ct=11.0062 rec=1.4081 | train/val/test=0.654/0.588/0.606 | c=0.998347
[Epoch 0005] loss=26.3604 cls=0.8496 smmd=2.9620 ct=10.9855 rec=1.3990 | train/val/test=0.731/0.634/0.646 | c=0.998347
[Epoch 0006] loss=28.2939 cls=0.7536 smmd=3.3621 ct=10.9682 rec=1.3868 | train/val/test=0.769/0.650/0.664 | c=0.998347
[Epoch 0007] loss=26.4762 cls=0.6494 smmd=2.8962 ct=11.5336 rec=1.3685 | train/val/test=0.769/0.650/0.666 | c=0.998347
[Epoch 0008] loss=22.1677 cls=0.5479 smmd=2.0655 ct=11.4304 rec=1.3559 | train/val/test=0.769/0.690/0.681 | c=0.998347
[Epoch 0009] loss=23.1194 cls=0.4458 smmd=2.2525 ct=11.4989 rec=1.3498 | train/val/test=0.885/0.732/0.710 | c=0.998347
[Epoch 0010] loss=24.8623 cls=0.3720 smmd=2.5975 ct=11.5536 rec=1.3501 | train/val/test=1.000/0.764/0.746 | c=0.998347
[Epoch 0011] loss=22.3501 cls=0.2797 smmd=2.1142 ct=11.5043 rec=1.3512 | train/val/test=1.000/0.774/0.719 | c=0.998347
[Epoch 0012] loss=20.7856 cls=0.2455 smmd=1.7998 ct=11.5283 rec=1.3569 | train/val/test=1.000/0.738/0.702 | c=0.998347
[Epoch 0013] loss=23.1238 cls=0.2048 smmd=2.2802 ct=11.4851 rec=1.3526 | train/val/test=1.000/0.742/0.703 | c=0.998347
[Epoch 0014] loss=21.5417 cls=0.1629 smmd=1.9632 ct=11.5101 rec=1.3401 | train/val/test=1.000/0.730/0.693 | c=0.998347
[Epoch 0015] loss=19.5001 cls=0.1182 smmd=1.5631 ct=11.4928 rec=1.3270 | train/val/test=1.000/0.750/0.705 | c=0.998347
[Epoch 0016] loss=19.9798 cls=0.0888 smmd=1.6625 ct=11.4910 rec=1.3195 | train/val/test=1.000/0.794/0.729 | c=0.998347
[Epoch 0017] loss=20.0069 cls=0.0651 smmd=1.6694 ct=11.4965 rec=1.3072 | train/val/test=1.000/0.774/0.733 | c=0.998347
[Epoch 0018] loss=18.3797 cls=0.0536 smmd=1.3483 ct=11.4816 rec=1.3005 | train/val/test=1.000/0.760/0.724 | c=0.998347
[Epoch 0019] loss=18.2270 cls=0.0567 smmd=1.3209 ct=11.4639 rec=1.3014 | train/val/test=1.000/0.810/0.755 | c=0.998347
[Epoch 0020] loss=18.7832 cls=0.0609 smmd=1.4340 ct=11.4527 rec=1.2993 | train/val/test=1.000/0.798/0.749 | c=0.998347
[Epoch 0021] loss=17.9228 cls=0.0730 smmd=1.2574 ct=11.4694 rec=1.2999 | train/val/test=1.000/0.808/0.762 | c=0.998347
[Epoch 0022] loss=17.9219 cls=0.0862 smmd=1.2469 ct=11.5144 rec=1.2986 | train/val/test=1.000/0.812/0.767 | c=0.998347
[Epoch 0023] loss=17.9867 cls=0.0865 smmd=1.2616 ct=11.5056 rec=1.3003 | train/val/test=1.000/0.812/0.767 | c=0.998347
[Epoch 0024] loss=17.1368 cls=0.0808 smmd=1.0997 ct=11.4685 rec=1.2962 | train/val/test=1.000/0.798/0.756 | c=0.998347
[Epoch 0025] loss=16.9813 cls=0.0758 smmd=1.0728 ct=11.4500 rec=1.2944 | train/val/test=1.000/0.810/0.757 | c=0.998347
[Epoch 0026] loss=16.6719 cls=0.0647 smmd=1.0102 ct=11.4587 rec=1.2966 | train/val/test=1.000/0.812/0.756 | c=0.998347
[Epoch 0027] loss=16.2566 cls=0.0604 smmd=0.9235 ct=11.4796 rec=1.2950 | train/val/test=1.000/0.810/0.757 | c=0.998347
[Epoch 0028] loss=15.7997 cls=0.0601 smmd=0.8368 ct=11.4561 rec=1.2942 | train/val/test=1.000/0.780/0.732 | c=0.998347
[Epoch 0029] loss=16.0662 cls=0.0749 smmd=0.8870 ct=11.4631 rec=1.3042 | train/val/test=1.000/0.796/0.759 | c=0.998347
[Epoch 0030] loss=15.6088 cls=0.0724 smmd=0.7905 ct=11.4898 rec=1.3013 | train/val/test=1.000/0.794/0.738 | c=0.998347
[Epoch 0031] loss=16.3694 cls=0.0775 smmd=0.9479 ct=11.4601 rec=1.3093 | train/val/test=1.000/0.796/0.742 | c=0.998347
[Epoch 0032] loss=15.8277 cls=0.0739 smmd=0.8381 ct=11.4699 rec=1.3048 | train/val/test=1.000/0.806/0.758 | c=0.998347
[Epoch 0033] loss=16.1571 cls=0.0725 smmd=0.8953 ct=11.5135 rec=1.3067 | train/val/test=1.000/0.774/0.728 | c=0.998347
[Epoch 0034] loss=15.2483 cls=0.0642 smmd=0.7237 ct=11.4674 rec=1.3047 | train/val/test=1.000/0.778/0.757 | c=0.998347
[Epoch 0035] loss=15.2422 cls=0.0669 smmd=0.7211 ct=11.4734 rec=1.2965 | train/val/test=1.000/0.818/0.757 | c=0.998347
[Epoch 0036] loss=15.0056 cls=0.0491 smmd=0.6785 ct=11.4588 rec=1.3001 | train/val/test=1.000/0.794/0.760 | c=0.998347
[Epoch 0037] loss=14.6558 cls=0.0552 smmd=0.6111 ct=11.4436 rec=1.2897 | train/val/test=1.000/0.828/0.770 | c=0.998347
[Epoch 0038] loss=14.8065 cls=0.0521 smmd=0.6354 ct=11.4735 rec=1.2969 | train/val/test=1.000/0.828/0.769 | c=0.998347
[Epoch 0039] loss=14.6091 cls=0.0581 smmd=0.5964 ct=11.4680 rec=1.3010 | train/val/test=1.000/0.804/0.769 | c=0.998347
[Epoch 0040] loss=15.0786 cls=0.0703 smmd=0.6902 ct=11.4621 rec=1.3043 | train/val/test=1.000/0.828/0.761 | c=0.998347
[Epoch 0041] loss=15.3321 cls=0.0767 smmd=0.7367 ct=11.4788 rec=1.3132 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0042] loss=15.2070 cls=0.0793 smmd=0.7118 ct=11.4772 rec=1.3118 | train/val/test=1.000/0.792/0.743 | c=0.998347
[Epoch 0043] loss=15.2029 cls=0.0842 smmd=0.7133 ct=11.4627 rec=1.3157 | train/val/test=1.000/0.808/0.771 | c=0.998347
[Epoch 0044] loss=14.4190 cls=0.0758 smmd=0.5574 ct=11.4632 rec=1.3086 | train/val/test=1.000/0.804/0.747 | c=0.998347
[Epoch 0045] loss=14.4036 cls=0.0775 smmd=0.5549 ct=11.4591 rec=1.3125 | train/val/test=1.000/0.806/0.770 | c=0.998347
[Epoch 0046] loss=14.4412 cls=0.0762 smmd=0.5655 ct=11.4452 rec=1.3062 | train/val/test=1.000/0.812/0.753 | c=0.998347
[Epoch 0047] loss=13.9776 cls=0.0803 smmd=0.4700 ct=11.4563 rec=1.3137 | train/val/test=1.000/0.800/0.775 | c=0.998347
[Epoch 0048] loss=14.2783 cls=0.0860 smmd=0.5278 ct=11.4654 rec=1.3116 | train/val/test=1.000/0.794/0.758 | c=0.998347
[Epoch 0049] loss=14.4915 cls=0.0963 smmd=0.5691 ct=11.4658 rec=1.3210 | train/val/test=1.000/0.744/0.734 | c=0.998347
[Epoch 0050] loss=14.8820 cls=0.1267 smmd=0.6364 ct=11.5038 rec=1.3311 | train/val/test=1.000/0.710/0.685 | c=0.998347
[Epoch 0051] loss=15.1345 cls=0.1302 smmd=0.6862 ct=11.5050 rec=1.3361 | train/val/test=1.000/0.718/0.717 | c=0.998347
[Epoch 0052] loss=14.7959 cls=0.1456 smmd=0.6172 ct=11.5029 rec=1.3405 | train/val/test=1.000/0.754/0.725 | c=0.998347
[Epoch 0053] loss=14.2223 cls=0.0905 smmd=0.5132 ct=11.4791 rec=1.3208 | train/val/test=1.000/0.788/0.764 | c=0.998347
[Epoch 0054] loss=14.2421 cls=0.0660 smmd=0.5251 ct=11.4534 rec=1.3001 | train/val/test=1.000/0.820/0.789 | c=0.998347
[Epoch 0055] loss=13.7576 cls=0.0551 smmd=0.4322 ct=11.4394 rec=1.2991 | train/val/test=1.000/0.816/0.787 | c=0.998347
[Epoch 0056] loss=13.7165 cls=0.0574 smmd=0.4214 ct=11.4504 rec=1.3021 | train/val/test=1.000/0.814/0.774 | c=0.998347
[Epoch 0057] loss=14.0748 cls=0.0669 smmd=0.4903 ct=11.4590 rec=1.3097 | train/val/test=1.000/0.800/0.778 | c=0.998347
[Epoch 0058] loss=14.2122 cls=0.0775 smmd=0.5152 ct=11.4659 rec=1.3176 | train/val/test=1.000/0.782/0.745 | c=0.998347
[Epoch 0059] loss=14.6558 cls=0.0943 smmd=0.5995 ct=11.4785 rec=1.3270 | train/val/test=1.000/0.770/0.755 | c=0.998347
[Epoch 0060] loss=15.3112 cls=0.1032 smmd=0.7243 ct=11.5046 rec=1.3337 | train/val/test=1.000/0.710/0.697 | c=0.998347
[Epoch 0061] loss=14.5992 cls=0.1197 smmd=0.5878 ct=11.4665 rec=1.3376 | train/val/test=1.000/0.762/0.742 | c=0.998347
[Epoch 0062] loss=14.1662 cls=0.0990 smmd=0.5033 ct=11.4672 rec=1.3302 | train/val/test=1.000/0.714/0.691 | c=0.998347
[Epoch 0063] loss=14.0710 cls=0.1025 smmd=0.4823 ct=11.4751 rec=1.3313 | train/val/test=1.000/0.788/0.766 | c=0.998347
[Epoch 0064] loss=13.7528 cls=0.0811 smmd=0.4289 ct=11.4364 rec=1.3152 | train/val/test=1.000/0.758/0.739 | c=0.998347
[Epoch 0065] loss=13.7708 cls=0.0842 smmd=0.4296 ct=11.4487 rec=1.3204 | train/val/test=1.000/0.788/0.771 | c=0.998347
[Epoch 0066] loss=13.7075 cls=0.0865 smmd=0.4135 ct=11.4650 rec=1.3167 | train/val/test=1.000/0.810/0.768 | c=0.998347
[Epoch 0067] loss=13.9307 cls=0.0882 smmd=0.4602 ct=11.4531 rec=1.3240 | train/val/test=1.000/0.776/0.755 | c=0.998347
[Epoch 0068] loss=14.7133 cls=0.1038 smmd=0.6078 ct=11.4896 rec=1.3267 | train/val/test=1.000/0.802/0.768 | c=0.998347
[Epoch 0069] loss=14.9397 cls=0.0969 smmd=0.6559 ct=11.4788 rec=1.3316 | train/val/test=1.000/0.758/0.729 | c=0.998347
[Epoch 0070] loss=14.9730 cls=0.1150 smmd=0.6620 ct=11.4720 rec=1.3321 | train/val/test=1.000/0.738/0.711 | c=0.998347
[Epoch 0071] loss=14.1688 cls=0.0938 smmd=0.4996 ct=11.4903 rec=1.3375 | train/val/test=1.000/0.760/0.730 | c=0.998347
[Epoch 0072] loss=13.8427 cls=0.0963 smmd=0.4431 ct=11.4470 rec=1.3219 | train/val/test=1.000/0.778/0.751 | c=0.998347
[Epoch 0073] loss=13.7937 cls=0.0699 smmd=0.4370 ct=11.4419 rec=1.3208 | train/val/test=1.000/0.790/0.772 | c=0.998347
[Epoch 0074] loss=13.5504 cls=0.0589 smmd=0.3892 ct=11.4444 rec=1.3081 | train/val/test=1.000/0.814/0.765 | c=0.998347
[Epoch 0075] loss=13.4784 cls=0.0632 smmd=0.3771 ct=11.4299 rec=1.3142 | train/val/test=1.000/0.804/0.777 | c=0.998347
[Epoch 0076] loss=13.7410 cls=0.0714 smmd=0.4250 ct=11.4481 rec=1.3208 | train/val/test=1.000/0.792/0.760 | c=0.998347
[Epoch 0077] loss=14.3299 cls=0.0857 smmd=0.5341 ct=11.4836 rec=1.3297 | train/val/test=1.000/0.798/0.761 | c=0.998347
[Epoch 0078] loss=15.1871 cls=0.0979 smmd=0.7090 ct=11.4597 rec=1.3355 | train/val/test=1.000/0.796/0.756 | c=0.998347
[Epoch 0079] loss=14.8946 cls=0.0913 smmd=0.6498 ct=11.4668 rec=1.3322 | train/val/test=1.000/0.808/0.772 | c=0.998347
[Epoch 0080] loss=13.9378 cls=0.0784 smmd=0.4627 ct=11.4524 rec=1.3246 | train/val/test=1.000/0.814/0.773 | c=0.998347
[Epoch 0081] loss=13.6043 cls=0.0700 smmd=0.4047 ct=11.4139 rec=1.3180 | train/val/test=1.000/0.814/0.778 | c=0.998347
[Epoch 0082] loss=13.5308 cls=0.0682 smmd=0.3895 ct=11.4175 rec=1.3154 | train/val/test=1.000/0.812/0.781 | c=0.998347
[Epoch 0083] loss=13.3018 cls=0.0719 smmd=0.3421 ct=11.4239 rec=1.3165 | train/val/test=1.000/0.816/0.788 | c=0.998347
[Epoch 0084] loss=13.3315 cls=0.0817 smmd=0.3466 ct=11.4252 rec=1.3226 | train/val/test=1.000/0.792/0.764 | c=0.998347
[Epoch 0085] loss=13.8425 cls=0.1040 smmd=0.4423 ct=11.4458 rec=1.3335 | train/val/test=1.000/0.804/0.780 | c=0.998347
[Epoch 0086] loss=14.4002 cls=0.1240 smmd=0.5426 ct=11.4910 rec=1.3432 | train/val/test=1.000/0.760/0.729 | c=0.998347
[Epoch 0087] loss=15.1442 cls=0.1340 smmd=0.6921 ct=11.4819 rec=1.3483 | train/val/test=1.000/0.812/0.766 | c=0.998347
[Epoch 0088] loss=15.2334 cls=0.1291 smmd=0.7071 ct=11.4986 rec=1.3477 | train/val/test=1.000/0.762/0.731 | c=0.998347
[Epoch 0089] loss=13.8616 cls=0.0998 smmd=0.4447 ct=11.4551 rec=1.3325 | train/val/test=1.000/0.784/0.762 | c=0.998347
[Epoch 0090] loss=14.0144 cls=0.0759 smmd=0.4843 ct=11.4225 rec=1.3263 | train/val/test=1.000/0.780/0.754 | c=0.998347
[Epoch 0091] loss=13.6308 cls=0.0672 smmd=0.4095 ct=11.4177 rec=1.3182 | train/val/test=1.000/0.818/0.771 | c=0.998347
[Epoch 0092] loss=13.3382 cls=0.0612 smmd=0.3501 ct=11.4254 rec=1.3177 | train/val/test=1.000/0.794/0.776 | c=0.998347
[Epoch 0093] loss=13.5735 cls=0.0625 smmd=0.3989 ct=11.4155 rec=1.3197 | train/val/test=1.000/0.802/0.775 | c=0.998347
[Epoch 0094] loss=13.5988 cls=0.0740 smmd=0.3970 ct=11.4440 rec=1.3276 | train/val/test=1.000/0.792/0.765 | c=0.998347
[Epoch 0095] loss=14.3265 cls=0.1123 smmd=0.5267 ct=11.5023 rec=1.3453 | train/val/test=1.000/0.634/0.614 | c=0.998347
[Epoch 0096] loss=15.5568 cls=0.1417 smmd=0.7666 ct=11.5163 rec=1.3636 | train/val/test=1.000/0.758/0.740 | c=0.998347
[Epoch 0097] loss=15.3359 cls=0.1841 smmd=0.7051 ct=11.5824 rec=1.3608 | train/val/test=1.000/0.672/0.645 | c=0.998347
[Epoch 0098] loss=14.2905 cls=0.1081 smmd=0.5246 ct=11.4790 rec=1.3448 | train/val/test=1.000/0.780/0.783 | c=0.998347
[Epoch 0099] loss=13.8534 cls=0.0611 smmd=0.4502 ct=11.4398 rec=1.3189 | train/val/test=1.000/0.810/0.770 | c=0.998347
=== Best @ epoch 37: val=0.8280, test=0.7700 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-10-2 completed in 193.38 seconds.
==================================================
