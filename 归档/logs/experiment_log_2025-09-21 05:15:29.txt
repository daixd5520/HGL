Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2 - 2025-09-21 05:15:29:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.4092 cls=1.1100 smmd=5.6634 ct=7.2565 rec=1.4137 | train/val/test=0.444/0.459/0.430 | c=0.998347
[Epoch 0001] loss=53.7330 cls=1.0663 smmd=3.8066 ct=7.2134 rec=1.4151 | train/val/test=0.476/0.492/0.463 | c=0.998347
[Epoch 0002] loss=37.5135 cls=1.0761 smmd=2.2063 ct=7.1027 rec=1.4137 | train/val/test=0.560/0.564/0.555 | c=0.998347
[Epoch 0003] loss=40.8690 cls=1.0628 smmd=2.5428 ct=7.1015 rec=1.4135 | train/val/test=0.563/0.569/0.558 | c=0.998347
[Epoch 0004] loss=40.6187 cls=1.0237 smmd=2.5290 ct=7.0548 rec=1.4143 | train/val/test=0.561/0.569/0.554 | c=0.998347
[Epoch 0005] loss=35.7775 cls=0.9883 smmd=2.0566 ct=7.0049 rec=1.4157 | train/val/test=0.562/0.572/0.554 | c=0.998347
[Epoch 0006] loss=30.3187 cls=0.9581 smmd=1.5189 ct=6.9713 rec=1.4153 | train/val/test=0.563/0.570/0.552 | c=0.998347
[Epoch 0007] loss=33.6670 cls=0.9115 smmd=1.8615 ct=6.9453 rec=1.4101 | train/val/test=0.570/0.580/0.563 | c=0.998347
[Epoch 0008] loss=36.1179 cls=0.8579 smmd=1.9901 ct=7.5433 rec=1.4023 | train/val/test=0.630/0.633/0.626 | c=0.998347
[Epoch 0009] loss=29.7050 cls=0.8106 smmd=1.3748 ct=7.4273 rec=1.3941 | train/val/test=0.682/0.692/0.691 | c=0.998347
[Epoch 0010] loss=28.4100 cls=0.7755 smmd=1.2386 ct=7.4709 rec=1.3884 | train/val/test=0.709/0.717/0.711 | c=0.998347
[Epoch 0011] loss=30.1669 cls=0.7430 smmd=1.3994 ct=7.5547 rec=1.3836 | train/val/test=0.737/0.747/0.737 | c=0.998347
[Epoch 0012] loss=27.5569 cls=0.7137 smmd=1.1392 ct=7.5597 rec=1.3773 | train/val/test=0.744/0.750/0.752 | c=0.998347
[Epoch 0013] loss=27.0029 cls=0.6893 smmd=1.0961 ct=7.5057 rec=1.3713 | train/val/test=0.748/0.748/0.756 | c=0.998347
[Epoch 0014] loss=25.4728 cls=0.6691 smmd=0.9483 ct=7.4864 rec=1.3661 | train/val/test=0.771/0.774/0.774 | c=0.998347
[Epoch 0015] loss=25.5648 cls=0.6446 smmd=0.9578 ct=7.4925 rec=1.3592 | train/val/test=0.786/0.785/0.781 | c=0.998347
[Epoch 0016] loss=25.3667 cls=0.6307 smmd=0.9376 ct=7.4993 rec=1.3541 | train/val/test=0.790/0.787/0.789 | c=0.998347
[Epoch 0017] loss=23.2658 cls=0.6084 smmd=0.7285 ct=7.5017 rec=1.3474 | train/val/test=0.793/0.792/0.793 | c=0.998347
[Epoch 0018] loss=23.6382 cls=0.5879 smmd=0.7659 ct=7.5070 rec=1.3428 | train/val/test=0.797/0.792/0.798 | c=0.998347
[Epoch 0019] loss=22.9677 cls=0.5719 smmd=0.6998 ct=7.5078 rec=1.3371 | train/val/test=0.804/0.799/0.804 | c=0.998347
[Epoch 0020] loss=22.3901 cls=0.5598 smmd=0.6427 ct=7.5087 rec=1.3310 | train/val/test=0.809/0.806/0.808 | c=0.998347
[Epoch 0021] loss=21.8959 cls=0.5490 smmd=0.5974 ct=7.4917 rec=1.3273 | train/val/test=0.813/0.812/0.810 | c=0.998347
[Epoch 0022] loss=21.6308 cls=0.5327 smmd=0.5739 ct=7.4812 rec=1.3257 | train/val/test=0.815/0.816/0.814 | c=0.998347
[Epoch 0023] loss=21.1873 cls=0.5169 smmd=0.5240 ct=7.5144 rec=1.3207 | train/val/test=0.817/0.815/0.816 | c=0.998347
[Epoch 0024] loss=20.8122 cls=0.5096 smmd=0.4891 ct=7.5040 rec=1.3170 | train/val/test=0.819/0.816/0.819 | c=0.998347
[Epoch 0025] loss=20.3187 cls=0.5015 smmd=0.4452 ct=7.4792 rec=1.3153 | train/val/test=0.822/0.822/0.821 | c=0.998347
[Epoch 0026] loss=20.3432 cls=0.4902 smmd=0.4449 ct=7.4960 rec=1.3142 | train/val/test=0.825/0.826/0.822 | c=0.998347
[Epoch 0027] loss=19.8900 cls=0.4829 smmd=0.3983 ct=7.5045 rec=1.3131 | train/val/test=0.827/0.824/0.826 | c=0.998347
[Epoch 0028] loss=19.5851 cls=0.4766 smmd=0.3688 ct=7.5022 rec=1.3095 | train/val/test=0.827/0.828/0.825 | c=0.998347
[Epoch 0029] loss=19.6270 cls=0.4705 smmd=0.3779 ct=7.4794 rec=1.3083 | train/val/test=0.830/0.829/0.826 | c=0.998347
[Epoch 0030] loss=19.0798 cls=0.4657 smmd=0.3236 ct=7.4786 rec=1.3075 | train/val/test=0.830/0.828/0.829 | c=0.998347
[Epoch 0031] loss=19.1818 cls=0.4648 smmd=0.3311 ct=7.4927 rec=1.3062 | train/val/test=0.834/0.836/0.830 | c=0.998347
[Epoch 0032] loss=18.7945 cls=0.4607 smmd=0.2927 ct=7.4918 rec=1.3080 | train/val/test=0.835/0.835/0.832 | c=0.998347
[Epoch 0033] loss=18.8202 cls=0.4592 smmd=0.2960 ct=7.4885 rec=1.3069 | train/val/test=0.836/0.836/0.834 | c=0.998347
[Epoch 0034] loss=18.4421 cls=0.4595 smmd=0.2595 ct=7.4821 rec=1.3074 | train/val/test=0.837/0.840/0.835 | c=0.998347
[Epoch 0035] loss=18.5200 cls=0.4600 smmd=0.2677 ct=7.4791 rec=1.3104 | train/val/test=0.838/0.837/0.837 | c=0.998347
[Epoch 0036] loss=18.3060 cls=0.4618 smmd=0.2439 ct=7.4909 rec=1.3090 | train/val/test=0.839/0.841/0.838 | c=0.998347
[Epoch 0037] loss=18.1536 cls=0.4608 smmd=0.2283 ct=7.4925 rec=1.3113 | train/val/test=0.837/0.839/0.838 | c=0.998347
[Epoch 0038] loss=18.1464 cls=0.4620 smmd=0.2300 ct=7.4799 rec=1.3121 | train/val/test=0.840/0.844/0.835 | c=0.998347
[Epoch 0039] loss=17.9498 cls=0.4623 smmd=0.2097 ct=7.4826 rec=1.3137 | train/val/test=0.841/0.842/0.840 | c=0.998347
[Epoch 0040] loss=17.8689 cls=0.4618 smmd=0.2008 ct=7.4872 rec=1.3123 | train/val/test=0.840/0.840/0.840 | c=0.998347
[Epoch 0041] loss=17.8641 cls=0.4626 smmd=0.2006 ct=7.4854 rec=1.3125 | train/val/test=0.839/0.842/0.840 | c=0.998347
[Epoch 0042] loss=17.7383 cls=0.4619 smmd=0.1900 ct=7.4750 rec=1.3136 | train/val/test=0.840/0.840/0.837 | c=0.998347
[Epoch 0043] loss=17.6203 cls=0.4618 smmd=0.1770 ct=7.4813 rec=1.3131 | train/val/test=0.841/0.841/0.838 | c=0.998347
[Epoch 0044] loss=17.5565 cls=0.4613 smmd=0.1698 ct=7.4856 rec=1.3140 | train/val/test=0.840/0.839/0.840 | c=0.998347
[Epoch 0045] loss=17.4543 cls=0.4637 smmd=0.1617 ct=7.4746 rec=1.3132 | train/val/test=0.841/0.839/0.837 | c=0.998347
[Epoch 0046] loss=17.4279 cls=0.4648 smmd=0.1583 ct=7.4778 rec=1.3139 | train/val/test=0.839/0.838/0.837 | c=0.998347
[Epoch 0047] loss=17.4382 cls=0.4659 smmd=0.1583 ct=7.4823 rec=1.3156 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0048] loss=17.3717 cls=0.4692 smmd=0.1518 ct=7.4805 rec=1.3156 | train/val/test=0.839/0.841/0.839 | c=0.998347
[Epoch 0049] loss=17.3124 cls=0.4701 smmd=0.1464 ct=7.4772 rec=1.3171 | train/val/test=0.839/0.840/0.836 | c=0.998347
[Epoch 0050] loss=17.2118 cls=0.4722 smmd=0.1359 ct=7.4792 rec=1.3171 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0051] loss=17.2641 cls=0.4763 smmd=0.1401 ct=7.4831 rec=1.3168 | train/val/test=0.839/0.838/0.835 | c=0.998347
[Epoch 0052] loss=17.2574 cls=0.4750 smmd=0.1406 ct=7.4775 rec=1.3186 | train/val/test=0.841/0.839/0.835 | c=0.998347
[Epoch 0053] loss=17.2173 cls=0.4776 smmd=0.1371 ct=7.4747 rec=1.3174 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0054] loss=17.1504 cls=0.4761 smmd=0.1306 ct=7.4738 rec=1.3171 | train/val/test=0.841/0.840/0.834 | c=0.998347
[Epoch 0055] loss=17.1068 cls=0.4766 smmd=0.1251 ct=7.4793 rec=1.3179 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0056] loss=17.1194 cls=0.4789 smmd=0.1281 ct=7.4704 rec=1.3166 | train/val/test=0.840/0.842/0.837 | c=0.998347
[Epoch 0057] loss=17.1126 cls=0.4786 smmd=0.1274 ct=7.4699 rec=1.3186 | train/val/test=0.838/0.839/0.835 | c=0.998347
[Epoch 0058] loss=17.1772 cls=0.4810 smmd=0.1325 ct=7.4765 rec=1.3183 | train/val/test=0.839/0.842/0.837 | c=0.998347
[Epoch 0059] loss=17.1388 cls=0.4810 smmd=0.1302 ct=7.4685 rec=1.3194 | train/val/test=0.836/0.838/0.833 | c=0.998347
[Epoch 0060] loss=17.0511 cls=0.4835 smmd=0.1199 ct=7.4755 rec=1.3188 | train/val/test=0.839/0.841/0.837 | c=0.998347
[Epoch 0061] loss=17.0523 cls=0.4805 smmd=0.1217 ct=7.4676 rec=1.3193 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0062] loss=17.0817 cls=0.4814 smmd=0.1243 ct=7.4694 rec=1.3182 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0063] loss=17.0151 cls=0.4779 smmd=0.1185 ct=7.4659 rec=1.3193 | train/val/test=0.839/0.839/0.835 | c=0.998347
[Epoch 0064] loss=16.9602 cls=0.4793 smmd=0.1133 ct=7.4646 rec=1.3173 | train/val/test=0.839/0.838/0.835 | c=0.998347
[Epoch 0065] loss=17.0033 cls=0.4775 smmd=0.1168 ct=7.4685 rec=1.3184 | train/val/test=0.839/0.837/0.834 | c=0.998347
[Epoch 0066] loss=17.0139 cls=0.4776 smmd=0.1197 ct=7.4594 rec=1.3183 | train/val/test=0.838/0.840/0.836 | c=0.998347
[Epoch 0067] loss=17.0179 cls=0.4781 smmd=0.1192 ct=7.4639 rec=1.3188 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0068] loss=17.0814 cls=0.4796 smmd=0.1258 ct=7.4620 rec=1.3188 | train/val/test=0.838/0.845/0.835 | c=0.998347
[Epoch 0069] loss=17.1442 cls=0.4818 smmd=0.1315 ct=7.4642 rec=1.3206 | train/val/test=0.830/0.831/0.829 | c=0.998347
[Epoch 0070] loss=17.1783 cls=0.4878 smmd=0.1354 ct=7.4600 rec=1.3211 | train/val/test=0.829/0.835/0.828 | c=0.998347
[Epoch 0071] loss=17.1316 cls=0.4949 smmd=0.1291 ct=7.4654 rec=1.3250 | train/val/test=0.819/0.820/0.823 | c=0.998347
[Epoch 0072] loss=17.1980 cls=0.5032 smmd=0.1368 ct=7.4584 rec=1.3241 | train/val/test=0.820/0.823/0.815 | c=0.998347
[Epoch 0073] loss=17.2904 cls=0.5059 smmd=0.1438 ct=7.4673 rec=1.3288 | train/val/test=0.810/0.814/0.820 | c=0.998347
[Epoch 0074] loss=17.3804 cls=0.5129 smmd=0.1556 ct=7.4527 rec=1.3261 | train/val/test=0.819/0.821/0.814 | c=0.998347
[Epoch 0075] loss=17.2988 cls=0.5060 smmd=0.1449 ct=7.4664 rec=1.3281 | train/val/test=0.819/0.821/0.826 | c=0.998347
[Epoch 0076] loss=17.1909 cls=0.5014 smmd=0.1396 ct=7.4416 rec=1.3215 | train/val/test=0.834/0.841/0.832 | c=0.998347
[Epoch 0077] loss=17.0806 cls=0.4807 smmd=0.1279 ct=7.4505 rec=1.3199 | train/val/test=0.835/0.837/0.834 | c=0.998347
[Epoch 0078] loss=17.0003 cls=0.4789 smmd=0.1231 ct=7.4357 rec=1.3162 | train/val/test=0.840/0.842/0.836 | c=0.998347
[Epoch 0079] loss=16.9704 cls=0.4758 smmd=0.1184 ct=7.4446 rec=1.3181 | train/val/test=0.840/0.840/0.834 | c=0.998347
[Epoch 0080] loss=17.0191 cls=0.4798 smmd=0.1250 ct=7.4343 rec=1.3211 | train/val/test=0.841/0.840/0.836 | c=0.998347
[Epoch 0081] loss=17.0467 cls=0.4875 smmd=0.1258 ct=7.4418 rec=1.3234 | train/val/test=0.841/0.844/0.837 | c=0.998347
[Epoch 0082] loss=17.0619 cls=0.4880 smmd=0.1273 ct=7.4401 rec=1.3282 | train/val/test=0.841/0.841/0.838 | c=0.998347
[Epoch 0083] loss=17.0560 cls=0.4974 smmd=0.1273 ct=7.4350 rec=1.3280 | train/val/test=0.840/0.847/0.838 | c=0.998347
[Epoch 0084] loss=17.1309 cls=0.4919 smmd=0.1344 ct=7.4375 rec=1.3320 | train/val/test=0.838/0.838/0.837 | c=0.998347
[Epoch 0085] loss=17.1321 cls=0.5004 smmd=0.1369 ct=7.4239 rec=1.3305 | train/val/test=0.834/0.842/0.834 | c=0.998347
[Epoch 0086] loss=17.0947 cls=0.4935 smmd=0.1320 ct=7.4308 rec=1.3319 | train/val/test=0.829/0.831/0.831 | c=0.998347
[Epoch 0087] loss=17.0293 cls=0.5005 smmd=0.1281 ct=7.4162 rec=1.3319 | train/val/test=0.825/0.828/0.822 | c=0.998347
[Epoch 0088] loss=17.0019 cls=0.5021 smmd=0.1234 ct=7.4252 rec=1.3322 | train/val/test=0.819/0.821/0.825 | c=0.998347
[Epoch 0089] loss=17.0773 cls=0.5105 smmd=0.1325 ct=7.4152 rec=1.3342 | train/val/test=0.813/0.813/0.811 | c=0.998347
[Epoch 0090] loss=17.1352 cls=0.5141 smmd=0.1357 ct=7.4265 rec=1.3368 | train/val/test=0.807/0.810/0.817 | c=0.998347
[Epoch 0091] loss=17.2164 cls=0.5279 smmd=0.1455 ct=7.4148 rec=1.3364 | train/val/test=0.800/0.800/0.795 | c=0.998347
[Epoch 0092] loss=17.2664 cls=0.5305 smmd=0.1454 ct=7.4376 rec=1.3434 | train/val/test=0.803/0.807/0.812 | c=0.998347
[Epoch 0093] loss=17.2365 cls=0.5363 smmd=0.1469 ct=7.4154 rec=1.3368 | train/val/test=0.814/0.815/0.812 | c=0.998347
[Epoch 0094] loss=17.1045 cls=0.5128 smmd=0.1326 ct=7.4269 rec=1.3367 | train/val/test=0.821/0.821/0.823 | c=0.998347
[Epoch 0095] loss=17.0801 cls=0.5079 smmd=0.1340 ct=7.4112 rec=1.3283 | train/val/test=0.833/0.841/0.831 | c=0.998347
[Epoch 0096] loss=16.9906 cls=0.4864 smmd=0.1266 ct=7.4092 rec=1.3263 | train/val/test=0.833/0.835/0.833 | c=0.998347
[Epoch 0097] loss=16.8541 cls=0.4880 smmd=0.1143 ct=7.4026 rec=1.3248 | train/val/test=0.836/0.839/0.835 | c=0.998347
[Epoch 0098] loss=16.7866 cls=0.4854 smmd=0.1075 ct=7.4031 rec=1.3256 | train/val/test=0.837/0.840/0.836 | c=0.998347
[Epoch 0099] loss=16.8443 cls=0.4905 smmd=0.1137 ct=7.3991 rec=1.3278 | train/val/test=0.833/0.835/0.832 | c=0.998347
=== Best @ epoch 83: val=0.8473, test=0.8377 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2 - 2025-09-21 05:15:29:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=11830, val=3943, test=3944 (mode=public, shot=N/A)
[Epoch 0000] loss=72.4092 cls=1.1100 smmd=5.6634 ct=7.2565 rec=1.4137 | train/val/test=0.444/0.459/0.430 | c=0.998347
[Epoch 0001] loss=53.7330 cls=1.0663 smmd=3.8066 ct=7.2134 rec=1.4151 | train/val/test=0.476/0.492/0.463 | c=0.998347
[Epoch 0002] loss=37.5135 cls=1.0761 smmd=2.2063 ct=7.1027 rec=1.4137 | train/val/test=0.560/0.564/0.555 | c=0.998347
[Epoch 0003] loss=40.8690 cls=1.0628 smmd=2.5428 ct=7.1015 rec=1.4135 | train/val/test=0.563/0.569/0.558 | c=0.998347
[Epoch 0004] loss=40.6187 cls=1.0237 smmd=2.5290 ct=7.0548 rec=1.4143 | train/val/test=0.561/0.569/0.554 | c=0.998347
[Epoch 0005] loss=35.7775 cls=0.9883 smmd=2.0566 ct=7.0049 rec=1.4157 | train/val/test=0.562/0.572/0.554 | c=0.998347
[Epoch 0006] loss=30.3187 cls=0.9581 smmd=1.5189 ct=6.9713 rec=1.4153 | train/val/test=0.563/0.570/0.552 | c=0.998347
[Epoch 0007] loss=33.6670 cls=0.9115 smmd=1.8615 ct=6.9453 rec=1.4101 | train/val/test=0.570/0.580/0.563 | c=0.998347
[Epoch 0008] loss=36.1179 cls=0.8579 smmd=1.9901 ct=7.5433 rec=1.4023 | train/val/test=0.630/0.633/0.626 | c=0.998347
[Epoch 0009] loss=29.7050 cls=0.8106 smmd=1.3748 ct=7.4273 rec=1.3941 | train/val/test=0.682/0.692/0.691 | c=0.998347
[Epoch 0010] loss=28.4100 cls=0.7755 smmd=1.2386 ct=7.4709 rec=1.3884 | train/val/test=0.709/0.717/0.711 | c=0.998347
[Epoch 0011] loss=30.1669 cls=0.7430 smmd=1.3994 ct=7.5547 rec=1.3836 | train/val/test=0.737/0.747/0.737 | c=0.998347
[Epoch 0012] loss=27.5569 cls=0.7137 smmd=1.1392 ct=7.5597 rec=1.3773 | train/val/test=0.744/0.750/0.752 | c=0.998347
[Epoch 0013] loss=27.0029 cls=0.6893 smmd=1.0961 ct=7.5057 rec=1.3713 | train/val/test=0.748/0.748/0.756 | c=0.998347
[Epoch 0014] loss=25.4728 cls=0.6691 smmd=0.9483 ct=7.4864 rec=1.3661 | train/val/test=0.771/0.774/0.774 | c=0.998347
[Epoch 0015] loss=25.5648 cls=0.6446 smmd=0.9578 ct=7.4925 rec=1.3592 | train/val/test=0.786/0.785/0.781 | c=0.998347
[Epoch 0016] loss=25.3667 cls=0.6307 smmd=0.9376 ct=7.4993 rec=1.3541 | train/val/test=0.790/0.787/0.789 | c=0.998347
[Epoch 0017] loss=23.2658 cls=0.6084 smmd=0.7285 ct=7.5017 rec=1.3474 | train/val/test=0.793/0.792/0.793 | c=0.998347
[Epoch 0018] loss=23.6382 cls=0.5879 smmd=0.7659 ct=7.5070 rec=1.3428 | train/val/test=0.797/0.792/0.798 | c=0.998347
[Epoch 0019] loss=22.9677 cls=0.5719 smmd=0.6998 ct=7.5078 rec=1.3371 | train/val/test=0.804/0.799/0.804 | c=0.998347
[Epoch 0020] loss=22.3901 cls=0.5598 smmd=0.6427 ct=7.5087 rec=1.3310 | train/val/test=0.809/0.806/0.808 | c=0.998347
[Epoch 0021] loss=21.8959 cls=0.5490 smmd=0.5974 ct=7.4917 rec=1.3273 | train/val/test=0.813/0.812/0.810 | c=0.998347
[Epoch 0022] loss=21.6308 cls=0.5327 smmd=0.5739 ct=7.4812 rec=1.3257 | train/val/test=0.815/0.816/0.814 | c=0.998347
[Epoch 0023] loss=21.1873 cls=0.5169 smmd=0.5240 ct=7.5144 rec=1.3207 | train/val/test=0.817/0.815/0.816 | c=0.998347
[Epoch 0024] loss=20.8122 cls=0.5096 smmd=0.4891 ct=7.5040 rec=1.3170 | train/val/test=0.819/0.816/0.819 | c=0.998347
[Epoch 0025] loss=20.3187 cls=0.5015 smmd=0.4452 ct=7.4792 rec=1.3153 | train/val/test=0.822/0.822/0.821 | c=0.998347
[Epoch 0026] loss=20.3432 cls=0.4902 smmd=0.4449 ct=7.4960 rec=1.3142 | train/val/test=0.825/0.826/0.822 | c=0.998347
[Epoch 0027] loss=19.8900 cls=0.4829 smmd=0.3983 ct=7.5045 rec=1.3131 | train/val/test=0.827/0.824/0.826 | c=0.998347
[Epoch 0028] loss=19.5851 cls=0.4766 smmd=0.3688 ct=7.5022 rec=1.3095 | train/val/test=0.827/0.828/0.825 | c=0.998347
[Epoch 0029] loss=19.6270 cls=0.4705 smmd=0.3779 ct=7.4794 rec=1.3083 | train/val/test=0.830/0.829/0.826 | c=0.998347
[Epoch 0030] loss=19.0798 cls=0.4657 smmd=0.3236 ct=7.4786 rec=1.3075 | train/val/test=0.830/0.828/0.829 | c=0.998347
[Epoch 0031] loss=19.1818 cls=0.4648 smmd=0.3311 ct=7.4927 rec=1.3062 | train/val/test=0.834/0.836/0.830 | c=0.998347
[Epoch 0032] loss=18.7945 cls=0.4607 smmd=0.2927 ct=7.4918 rec=1.3080 | train/val/test=0.835/0.835/0.832 | c=0.998347
[Epoch 0033] loss=18.8202 cls=0.4592 smmd=0.2960 ct=7.4885 rec=1.3069 | train/val/test=0.836/0.836/0.834 | c=0.998347
[Epoch 0034] loss=18.4421 cls=0.4595 smmd=0.2595 ct=7.4821 rec=1.3074 | train/val/test=0.837/0.840/0.835 | c=0.998347
[Epoch 0035] loss=18.5200 cls=0.4600 smmd=0.2677 ct=7.4791 rec=1.3104 | train/val/test=0.838/0.837/0.837 | c=0.998347
[Epoch 0036] loss=18.3060 cls=0.4618 smmd=0.2439 ct=7.4909 rec=1.3090 | train/val/test=0.839/0.841/0.838 | c=0.998347
[Epoch 0037] loss=18.1536 cls=0.4608 smmd=0.2283 ct=7.4925 rec=1.3113 | train/val/test=0.837/0.839/0.838 | c=0.998347
[Epoch 0038] loss=18.1464 cls=0.4620 smmd=0.2300 ct=7.4799 rec=1.3121 | train/val/test=0.840/0.844/0.835 | c=0.998347
[Epoch 0039] loss=17.9498 cls=0.4623 smmd=0.2097 ct=7.4826 rec=1.3137 | train/val/test=0.841/0.842/0.840 | c=0.998347
[Epoch 0040] loss=17.8689 cls=0.4618 smmd=0.2008 ct=7.4872 rec=1.3123 | train/val/test=0.840/0.840/0.840 | c=0.998347
[Epoch 0041] loss=17.8641 cls=0.4626 smmd=0.2006 ct=7.4854 rec=1.3125 | train/val/test=0.839/0.842/0.840 | c=0.998347
[Epoch 0042] loss=17.7383 cls=0.4619 smmd=0.1900 ct=7.4750 rec=1.3136 | train/val/test=0.840/0.840/0.837 | c=0.998347
[Epoch 0043] loss=17.6203 cls=0.4618 smmd=0.1770 ct=7.4813 rec=1.3131 | train/val/test=0.841/0.841/0.838 | c=0.998347
[Epoch 0044] loss=17.5565 cls=0.4613 smmd=0.1698 ct=7.4856 rec=1.3140 | train/val/test=0.840/0.839/0.840 | c=0.998347
[Epoch 0045] loss=17.4543 cls=0.4637 smmd=0.1617 ct=7.4746 rec=1.3132 | train/val/test=0.841/0.839/0.837 | c=0.998347
[Epoch 0046] loss=17.4279 cls=0.4648 smmd=0.1583 ct=7.4778 rec=1.3139 | train/val/test=0.839/0.838/0.837 | c=0.998347
[Epoch 0047] loss=17.4382 cls=0.4659 smmd=0.1583 ct=7.4823 rec=1.3156 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0048] loss=17.3717 cls=0.4692 smmd=0.1518 ct=7.4805 rec=1.3156 | train/val/test=0.839/0.841/0.839 | c=0.998347
[Epoch 0049] loss=17.3124 cls=0.4701 smmd=0.1464 ct=7.4772 rec=1.3171 | train/val/test=0.839/0.840/0.836 | c=0.998347
[Epoch 0050] loss=17.2118 cls=0.4722 smmd=0.1359 ct=7.4792 rec=1.3171 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0051] loss=17.2641 cls=0.4763 smmd=0.1401 ct=7.4831 rec=1.3168 | train/val/test=0.839/0.838/0.835 | c=0.998347
[Epoch 0052] loss=17.2574 cls=0.4750 smmd=0.1406 ct=7.4775 rec=1.3186 | train/val/test=0.841/0.839/0.835 | c=0.998347
[Epoch 0053] loss=17.2173 cls=0.4776 smmd=0.1371 ct=7.4747 rec=1.3174 | train/val/test=0.840/0.839/0.836 | c=0.998347
[Epoch 0054] loss=17.1504 cls=0.4761 smmd=0.1306 ct=7.4738 rec=1.3171 | train/val/test=0.841/0.840/0.834 | c=0.998347
[Epoch 0055] loss=17.1068 cls=0.4766 smmd=0.1251 ct=7.4793 rec=1.3179 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0056] loss=17.1194 cls=0.4789 smmd=0.1281 ct=7.4704 rec=1.3166 | train/val/test=0.840/0.842/0.837 | c=0.998347
[Epoch 0057] loss=17.1126 cls=0.4786 smmd=0.1274 ct=7.4699 rec=1.3186 | train/val/test=0.838/0.839/0.835 | c=0.998347
[Epoch 0058] loss=17.1772 cls=0.4810 smmd=0.1325 ct=7.4765 rec=1.3183 | train/val/test=0.839/0.842/0.837 | c=0.998347
[Epoch 0059] loss=17.1388 cls=0.4810 smmd=0.1302 ct=7.4685 rec=1.3194 | train/val/test=0.836/0.838/0.833 | c=0.998347
[Epoch 0060] loss=17.0511 cls=0.4835 smmd=0.1199 ct=7.4755 rec=1.3188 | train/val/test=0.839/0.841/0.837 | c=0.998347
[Epoch 0061] loss=17.0523 cls=0.4805 smmd=0.1217 ct=7.4676 rec=1.3193 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0062] loss=17.0817 cls=0.4814 smmd=0.1243 ct=7.4694 rec=1.3182 | train/val/test=0.839/0.839/0.836 | c=0.998347
[Epoch 0063] loss=17.0151 cls=0.4779 smmd=0.1185 ct=7.4659 rec=1.3193 | train/val/test=0.839/0.839/0.835 | c=0.998347
[Epoch 0064] loss=16.9602 cls=0.4793 smmd=0.1133 ct=7.4646 rec=1.3173 | train/val/test=0.839/0.838/0.835 | c=0.998347
[Epoch 0065] loss=17.0033 cls=0.4775 smmd=0.1168 ct=7.4685 rec=1.3184 | train/val/test=0.839/0.837/0.834 | c=0.998347
[Epoch 0066] loss=17.0139 cls=0.4776 smmd=0.1197 ct=7.4594 rec=1.3183 | train/val/test=0.838/0.840/0.836 | c=0.998347
[Epoch 0067] loss=17.0179 cls=0.4781 smmd=0.1192 ct=7.4639 rec=1.3188 | train/val/test=0.837/0.839/0.835 | c=0.998347
[Epoch 0068] loss=17.0814 cls=0.4796 smmd=0.1258 ct=7.4620 rec=1.3188 | train/val/test=0.838/0.845/0.835 | c=0.998347
[Epoch 0069] loss=17.1442 cls=0.4818 smmd=0.1315 ct=7.4642 rec=1.3206 | train/val/test=0.830/0.831/0.829 | c=0.998347
[Epoch 0070] loss=17.1783 cls=0.4878 smmd=0.1354 ct=7.4600 rec=1.3211 | train/val/test=0.829/0.835/0.828 | c=0.998347
[Epoch 0071] loss=17.1316 cls=0.4949 smmd=0.1291 ct=7.4654 rec=1.3250 | train/val/test=0.819/0.820/0.823 | c=0.998347
[Epoch 0072] loss=17.1980 cls=0.5032 smmd=0.1368 ct=7.4584 rec=1.3241 | train/val/test=0.820/0.823/0.815 | c=0.998347
[Epoch 0073] loss=17.2904 cls=0.5059 smmd=0.1438 ct=7.4673 rec=1.3288 | train/val/test=0.810/0.814/0.820 | c=0.998347
[Epoch 0074] loss=17.3804 cls=0.5129 smmd=0.1556 ct=7.4527 rec=1.3261 | train/val/test=0.819/0.821/0.814 | c=0.998347
[Epoch 0075] loss=17.2988 cls=0.5060 smmd=0.1449 ct=7.4664 rec=1.3281 | train/val/test=0.819/0.821/0.826 | c=0.998347
[Epoch 0076] loss=17.1909 cls=0.5014 smmd=0.1396 ct=7.4416 rec=1.3215 | train/val/test=0.834/0.841/0.832 | c=0.998347
[Epoch 0077] loss=17.0806 cls=0.4807 smmd=0.1279 ct=7.4505 rec=1.3199 | train/val/test=0.835/0.837/0.834 | c=0.998347
[Epoch 0078] loss=17.0003 cls=0.4789 smmd=0.1231 ct=7.4357 rec=1.3162 | train/val/test=0.840/0.842/0.836 | c=0.998347
[Epoch 0079] loss=16.9704 cls=0.4758 smmd=0.1184 ct=7.4446 rec=1.3181 | train/val/test=0.840/0.840/0.834 | c=0.998347
[Epoch 0080] loss=17.0191 cls=0.4798 smmd=0.1250 ct=7.4343 rec=1.3211 | train/val/test=0.841/0.840/0.836 | c=0.998347
[Epoch 0081] loss=17.0467 cls=0.4875 smmd=0.1258 ct=7.4418 rec=1.3234 | train/val/test=0.841/0.844/0.837 | c=0.998347
[Epoch 0082] loss=17.0619 cls=0.4880 smmd=0.1273 ct=7.4401 rec=1.3282 | train/val/test=0.841/0.841/0.838 | c=0.998347
[Epoch 0083] loss=17.0560 cls=0.4974 smmd=0.1273 ct=7.4350 rec=1.3280 | train/val/test=0.840/0.847/0.838 | c=0.998347
[Epoch 0084] loss=17.1309 cls=0.4919 smmd=0.1344 ct=7.4375 rec=1.3320 | train/val/test=0.838/0.838/0.837 | c=0.998347
[Epoch 0085] loss=17.1321 cls=0.5004 smmd=0.1369 ct=7.4239 rec=1.3305 | train/val/test=0.834/0.842/0.834 | c=0.998347
[Epoch 0086] loss=17.0947 cls=0.4935 smmd=0.1320 ct=7.4308 rec=1.3319 | train/val/test=0.829/0.831/0.831 | c=0.998347
[Epoch 0087] loss=17.0293 cls=0.5005 smmd=0.1281 ct=7.4162 rec=1.3319 | train/val/test=0.825/0.828/0.822 | c=0.998347
[Epoch 0088] loss=17.0019 cls=0.5021 smmd=0.1234 ct=7.4252 rec=1.3322 | train/val/test=0.819/0.821/0.825 | c=0.998347
[Epoch 0089] loss=17.0773 cls=0.5105 smmd=0.1325 ct=7.4152 rec=1.3342 | train/val/test=0.813/0.813/0.811 | c=0.998347
[Epoch 0090] loss=17.1352 cls=0.5141 smmd=0.1357 ct=7.4265 rec=1.3368 | train/val/test=0.807/0.810/0.817 | c=0.998347
[Epoch 0091] loss=17.2164 cls=0.5279 smmd=0.1455 ct=7.4148 rec=1.3364 | train/val/test=0.800/0.800/0.795 | c=0.998347
[Epoch 0092] loss=17.2664 cls=0.5305 smmd=0.1454 ct=7.4376 rec=1.3434 | train/val/test=0.803/0.807/0.812 | c=0.998347
[Epoch 0093] loss=17.2365 cls=0.5363 smmd=0.1469 ct=7.4154 rec=1.3368 | train/val/test=0.814/0.815/0.812 | c=0.998347
[Epoch 0094] loss=17.1045 cls=0.5128 smmd=0.1326 ct=7.4269 rec=1.3367 | train/val/test=0.821/0.821/0.823 | c=0.998347
[Epoch 0095] loss=17.0801 cls=0.5079 smmd=0.1340 ct=7.4112 rec=1.3283 | train/val/test=0.833/0.841/0.831 | c=0.998347
[Epoch 0096] loss=16.9906 cls=0.4864 smmd=0.1266 ct=7.4092 rec=1.3263 | train/val/test=0.833/0.835/0.833 | c=0.998347
[Epoch 0097] loss=16.8541 cls=0.4880 smmd=0.1143 ct=7.4026 rec=1.3248 | train/val/test=0.836/0.839/0.835 | c=0.998347
[Epoch 0098] loss=16.7866 cls=0.4854 smmd=0.1075 ct=7.4031 rec=1.3256 | train/val/test=0.837/0.840/0.836 | c=0.998347
[Epoch 0099] loss=16.8443 cls=0.4905 smmd=0.1137 ct=7.3991 rec=1.3278 | train/val/test=0.833/0.835/0.832 | c=0.998347
=== Best @ epoch 83: val=0.8473, test=0.8377 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-False-5-2 completed in 192.14 seconds.
==================================================
