Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 - 2025-09-21 06:33:33:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1998 cls=1.9506 smmd=4.2028 ct=9.2686 rec=1.3889 | train/val/test=0.310/0.118/0.143 | c=0.998347
[Epoch 0001] loss=16.6791 cls=1.8965 smmd=2.8134 ct=9.1905 rec=1.3893 | train/val/test=0.759/0.412/0.433 | c=0.998347
[Epoch 0002] loss=15.1594 cls=1.7634 smmd=1.5304 ct=9.0885 rec=1.3886 | train/val/test=0.621/0.358/0.385 | c=0.998347
[Epoch 0003] loss=14.9065 cls=1.4968 smmd=1.5168 ct=9.1152 rec=1.3888 | train/val/test=0.931/0.566/0.591 | c=0.998347
[Epoch 0004] loss=14.5837 cls=1.1824 smmd=1.6901 ct=8.9533 rec=1.3789 | train/val/test=0.897/0.546/0.585 | c=0.998347
[Epoch 0005] loss=14.2280 cls=0.9478 smmd=1.6483 ct=8.9047 rec=1.3636 | train/val/test=0.931/0.530/0.558 | c=0.998347
[Epoch 0006] loss=13.6141 cls=0.6551 smmd=1.3943 ct=8.8719 rec=1.3465 | train/val/test=1.000/0.590/0.609 | c=0.998347
[Epoch 0007] loss=13.7029 cls=0.4559 smmd=1.0757 ct=9.5171 rec=1.3271 | train/val/test=1.000/0.584/0.601 | c=0.998347
[Epoch 0008] loss=13.3455 cls=0.3019 smmd=1.0427 ct=9.3894 rec=1.3057 | train/val/test=1.000/0.578/0.601 | c=0.998347
[Epoch 0009] loss=13.2790 cls=0.1894 smmd=1.2085 ct=9.3048 rec=1.2881 | train/val/test=1.000/0.582/0.595 | c=0.998347
[Epoch 0010] loss=13.2102 cls=0.1062 smmd=1.2896 ct=9.2766 rec=1.2689 | train/val/test=1.000/0.580/0.597 | c=0.998347
[Epoch 0011] loss=13.0316 cls=0.0574 smmd=1.1829 ct=9.2930 rec=1.2492 | train/val/test=1.000/0.600/0.610 | c=0.998347
[Epoch 0012] loss=12.8045 cls=0.0331 smmd=0.9685 ct=9.3344 rec=1.2343 | train/val/test=1.000/0.606/0.622 | c=0.998347
[Epoch 0013] loss=12.6229 cls=0.0202 smmd=0.7954 ct=9.3608 rec=1.2232 | train/val/test=1.000/0.616/0.627 | c=0.998347
[Epoch 0014] loss=12.5251 cls=0.0133 smmd=0.7104 ct=9.3703 rec=1.2156 | train/val/test=1.000/0.612/0.630 | c=0.998347
[Epoch 0015] loss=12.5160 cls=0.0102 smmd=0.7131 ct=9.3712 rec=1.2108 | train/val/test=1.000/0.608/0.631 | c=0.998347
[Epoch 0016] loss=12.4742 cls=0.0093 smmd=0.6873 ct=9.3637 rec=1.2070 | train/val/test=1.000/0.616/0.639 | c=0.998347
[Epoch 0017] loss=12.3667 cls=0.0098 smmd=0.5907 ct=9.3562 rec=1.2050 | train/val/test=1.000/0.624/0.642 | c=0.998347
[Epoch 0018] loss=12.2422 cls=0.0098 smmd=0.4794 ct=9.3459 rec=1.2036 | train/val/test=1.000/0.622/0.645 | c=0.998347
[Epoch 0019] loss=12.2056 cls=0.0093 smmd=0.4549 ct=9.3359 rec=1.2027 | train/val/test=1.000/0.622/0.644 | c=0.998347
[Epoch 0020] loss=12.1840 cls=0.0103 smmd=0.4397 ct=9.3283 rec=1.2028 | train/val/test=1.000/0.634/0.656 | c=0.998347
[Epoch 0021] loss=12.1312 cls=0.0121 smmd=0.3898 ct=9.3250 rec=1.2021 | train/val/test=1.000/0.646/0.669 | c=0.998347
[Epoch 0022] loss=12.0640 cls=0.0144 smmd=0.3166 ct=9.3326 rec=1.2002 | train/val/test=1.000/0.650/0.673 | c=0.998347
[Epoch 0023] loss=12.0277 cls=0.0185 smmd=0.2649 ct=9.3461 rec=1.1991 | train/val/test=1.000/0.654/0.676 | c=0.998347
[Epoch 0024] loss=12.0084 cls=0.0231 smmd=0.2360 ct=9.3531 rec=1.1981 | train/val/test=1.000/0.658/0.679 | c=0.998347
[Epoch 0025] loss=11.9923 cls=0.0297 smmd=0.2169 ct=9.3551 rec=1.1953 | train/val/test=1.000/0.662/0.686 | c=0.998347
[Epoch 0026] loss=11.9500 cls=0.0334 smmd=0.1880 ct=9.3447 rec=1.1919 | train/val/test=1.000/0.674/0.694 | c=0.998347
[Epoch 0027] loss=11.9257 cls=0.0377 smmd=0.1690 ct=9.3426 rec=1.1882 | train/val/test=1.000/0.672/0.710 | c=0.998347
[Epoch 0028] loss=11.9115 cls=0.0363 smmd=0.1700 ct=9.3378 rec=1.1837 | train/val/test=1.000/0.680/0.695 | c=0.998347
[Epoch 0029] loss=11.8903 cls=0.0377 smmd=0.1542 ct=9.3387 rec=1.1799 | train/val/test=1.000/0.678/0.722 | c=0.998347
[Epoch 0030] loss=11.8777 cls=0.0295 smmd=0.1597 ct=9.3354 rec=1.1765 | train/val/test=1.000/0.684/0.698 | c=0.998347
[Epoch 0031] loss=11.8456 cls=0.0245 smmd=0.1373 ct=9.3397 rec=1.1721 | train/val/test=1.000/0.686/0.713 | c=0.998347
[Epoch 0032] loss=11.7900 cls=0.0166 smmd=0.1114 ct=9.3275 rec=1.1673 | train/val/test=1.000/0.684/0.715 | c=0.998347
[Epoch 0033] loss=11.7813 cls=0.0142 smmd=0.1106 ct=9.3245 rec=1.1660 | train/val/test=1.000/0.686/0.700 | c=0.998347
[Epoch 0034] loss=11.7719 cls=0.0143 smmd=0.0928 ct=9.3308 rec=1.1670 | train/val/test=1.000/0.678/0.713 | c=0.998347
[Epoch 0035] loss=11.7530 cls=0.0123 smmd=0.0847 ct=9.3216 rec=1.1672 | train/val/test=1.000/0.682/0.710 | c=0.998347
[Epoch 0036] loss=11.7394 cls=0.0115 smmd=0.0765 ct=9.3188 rec=1.1662 | train/val/test=1.000/0.684/0.706 | c=0.998347
[Epoch 0037] loss=11.7235 cls=0.0125 smmd=0.0569 ct=9.3177 rec=1.1682 | train/val/test=1.000/0.678/0.718 | c=0.998347
[Epoch 0038] loss=11.7368 cls=0.0125 smmd=0.0689 ct=9.3140 rec=1.1707 | train/val/test=1.000/0.688/0.700 | c=0.998347
[Epoch 0039] loss=11.7305 cls=0.0149 smmd=0.0468 ct=9.3234 rec=1.1727 | train/val/test=1.000/0.678/0.723 | c=0.998347
[Epoch 0040] loss=11.7327 cls=0.0137 smmd=0.0602 ct=9.3138 rec=1.1726 | train/val/test=1.000/0.688/0.700 | c=0.998347
[Epoch 0041] loss=11.7109 cls=0.0154 smmd=0.0381 ct=9.3167 rec=1.1704 | train/val/test=1.000/0.688/0.715 | c=0.998347
[Epoch 0042] loss=11.6911 cls=0.0141 smmd=0.0347 ct=9.3056 rec=1.1683 | train/val/test=1.000/0.688/0.708 | c=0.998347
[Epoch 0043] loss=11.6815 cls=0.0148 smmd=0.0292 ct=9.3046 rec=1.1664 | train/val/test=1.000/0.696/0.705 | c=0.998347
[Epoch 0044] loss=11.6825 cls=0.0163 smmd=0.0243 ct=9.3096 rec=1.1662 | train/val/test=1.000/0.688/0.711 | c=0.998347
[Epoch 0045] loss=11.6795 cls=0.0163 smmd=0.0271 ct=9.3037 rec=1.1662 | train/val/test=1.000/0.696/0.704 | c=0.998347
[Epoch 0046] loss=11.6724 cls=0.0183 smmd=0.0178 ct=9.3041 rec=1.1661 | train/val/test=1.000/0.690/0.711 | c=0.998347
[Epoch 0047] loss=11.6663 cls=0.0173 smmd=0.0235 ct=9.2936 rec=1.1659 | train/val/test=1.000/0.696/0.705 | c=0.998347
[Epoch 0048] loss=11.6550 cls=0.0172 smmd=0.0135 ct=9.2935 rec=1.1654 | train/val/test=1.000/0.692/0.709 | c=0.998347
[Epoch 0049] loss=11.6465 cls=0.0164 smmd=0.0078 ct=9.2910 rec=1.1656 | train/val/test=1.000/0.690/0.712 | c=0.998347
[Epoch 0050] loss=11.6442 cls=0.0160 smmd=0.0028 ct=9.2926 rec=1.1664 | train/val/test=1.000/0.690/0.704 | c=0.998347
[Epoch 0051] loss=11.6433 cls=0.0164 smmd=0.0010 ct=9.2915 rec=1.1672 | train/val/test=1.000/0.692/0.715 | c=0.998347
[Epoch 0052] loss=11.6425 cls=0.0155 smmd=0.0095 ct=9.2808 rec=1.1683 | train/val/test=1.000/0.686/0.700 | c=0.998347
[Epoch 0053] loss=11.6486 cls=0.0174 smmd=0.0046 ct=9.2877 rec=1.1695 | train/val/test=1.000/0.690/0.717 | c=0.998347
[Epoch 0054] loss=11.6635 cls=0.0165 smmd=0.0179 ct=9.2855 rec=1.1718 | train/val/test=1.000/0.690/0.697 | c=0.998347
[Epoch 0055] loss=11.6829 cls=0.0200 smmd=0.0154 ct=9.3008 rec=1.1734 | train/val/test=1.000/0.686/0.721 | c=0.998347
[Epoch 0056] loss=11.6885 cls=0.0167 smmd=0.0403 ct=9.2870 rec=1.1723 | train/val/test=1.000/0.690/0.703 | c=0.998347
[Epoch 0057] loss=11.6396 cls=0.0139 smmd=0.0100 ct=9.2856 rec=1.1650 | train/val/test=1.000/0.694/0.712 | c=0.998347
[Epoch 0058] loss=11.6124 cls=0.0114 smmd=0.0011 ct=9.2772 rec=1.1614 | train/val/test=1.000/0.692/0.718 | c=0.998347
[Epoch 0059] loss=11.6423 cls=0.0125 smmd=0.0218 ct=9.2786 rec=1.1646 | train/val/test=1.000/0.692/0.705 | c=0.998347
[Epoch 0060] loss=11.6323 cls=0.0136 smmd=0.0056 ct=9.2841 rec=1.1645 | train/val/test=1.000/0.692/0.713 | c=0.998347
[Epoch 0061] loss=11.6067 cls=0.0123 smmd=-0.0039 ct=9.2719 rec=1.1632 | train/val/test=1.000/0.692/0.715 | c=0.998347
[Epoch 0062] loss=11.6212 cls=0.0142 smmd=0.0032 ct=9.2702 rec=1.1668 | train/val/test=1.000/0.684/0.699 | c=0.998347
[Epoch 0063] loss=11.6434 cls=0.0189 smmd=0.0048 ct=9.2765 rec=1.1716 | train/val/test=1.000/0.690/0.716 | c=0.998347
[Epoch 0064] loss=11.6472 cls=0.0181 smmd=0.0123 ct=9.2690 rec=1.1739 | train/val/test=1.000/0.676/0.703 | c=0.998347
[Epoch 0065] loss=11.6398 cls=0.0200 smmd=-0.0058 ct=9.2802 rec=1.1727 | train/val/test=1.000/0.682/0.712 | c=0.998347
[Epoch 0066] loss=11.6312 cls=0.0174 smmd=0.0067 ct=9.2642 rec=1.1715 | train/val/test=1.000/0.692/0.711 | c=0.998347
[Epoch 0067] loss=11.6170 cls=0.0183 smmd=-0.0080 ct=9.2689 rec=1.1688 | train/val/test=1.000/0.682/0.708 | c=0.998347
[Epoch 0068] loss=11.6062 cls=0.0161 smmd=-0.0097 ct=9.2645 rec=1.1677 | train/val/test=1.000/0.692/0.719 | c=0.998347
[Epoch 0069] loss=11.5992 cls=0.0159 smmd=-0.0110 ct=9.2613 rec=1.1665 | train/val/test=1.000/0.688/0.706 | c=0.998347
[Epoch 0070] loss=11.6022 cls=0.0162 smmd=-0.0120 ct=9.2645 rec=1.1668 | train/val/test=1.000/0.682/0.718 | c=0.998347
[Epoch 0071] loss=11.5968 cls=0.0147 smmd=-0.0105 ct=9.2587 rec=1.1670 | train/val/test=1.000/0.688/0.713 | c=0.998347
[Epoch 0072] loss=11.5964 cls=0.0158 smmd=-0.0129 ct=9.2591 rec=1.1672 | train/val/test=1.000/0.686/0.704 | c=0.998347
[Epoch 0073] loss=11.5894 cls=0.0157 smmd=-0.0186 ct=9.2560 rec=1.1682 | train/val/test=1.000/0.690/0.716 | c=0.998347
[Epoch 0074] loss=11.5920 cls=0.0157 smmd=-0.0156 ct=9.2539 rec=1.1690 | train/val/test=1.000/0.682/0.705 | c=0.998347
[Epoch 0075] loss=11.5941 cls=0.0177 smmd=-0.0188 ct=9.2535 rec=1.1708 | train/val/test=1.000/0.680/0.717 | c=0.998347
[Epoch 0076] loss=11.6026 cls=0.0170 smmd=-0.0121 ct=9.2532 rec=1.1723 | train/val/test=1.000/0.686/0.709 | c=0.998347
[Epoch 0077] loss=11.6030 cls=0.0192 smmd=-0.0194 ct=9.2583 rec=1.1725 | train/val/test=1.000/0.676/0.713 | c=0.998347
[Epoch 0078] loss=11.6127 cls=0.0176 smmd=0.0001 ct=9.2479 rec=1.1736 | train/val/test=1.000/0.682/0.706 | c=0.998347
[Epoch 0079] loss=11.6156 cls=0.0198 smmd=-0.0144 ct=9.2659 rec=1.1721 | train/val/test=1.000/0.684/0.712 | c=0.998347
[Epoch 0080] loss=11.6063 cls=0.0164 smmd=0.0003 ct=9.2482 rec=1.1707 | train/val/test=1.000/0.684/0.703 | c=0.998347
[Epoch 0081] loss=11.5863 cls=0.0161 smmd=-0.0178 ct=9.2538 rec=1.1671 | train/val/test=1.000/0.690/0.714 | c=0.998347
[Epoch 0082] loss=11.5775 cls=0.0145 smmd=-0.0176 ct=9.2504 rec=1.1651 | train/val/test=1.000/0.686/0.716 | c=0.998347
[Epoch 0083] loss=11.5799 cls=0.0140 smmd=-0.0122 ct=9.2453 rec=1.1664 | train/val/test=1.000/0.686/0.708 | c=0.998347
[Epoch 0084] loss=11.5846 cls=0.0159 smmd=-0.0201 ct=9.2528 rec=1.1680 | train/val/test=1.000/0.688/0.715 | c=0.998347
[Epoch 0085] loss=11.5770 cls=0.0154 smmd=-0.0203 ct=9.2439 rec=1.1690 | train/val/test=1.000/0.688/0.702 | c=0.998347
[Epoch 0086] loss=11.5767 cls=0.0166 smmd=-0.0227 ct=9.2434 rec=1.1697 | train/val/test=1.000/0.688/0.713 | c=0.998347
[Epoch 0087] loss=11.5794 cls=0.0168 smmd=-0.0225 ct=9.2445 rec=1.1704 | train/val/test=1.000/0.686/0.713 | c=0.998347
[Epoch 0088] loss=11.5819 cls=0.0170 smmd=-0.0192 ct=9.2393 rec=1.1724 | train/val/test=1.000/0.690/0.707 | c=0.998347
[Epoch 0089] loss=11.5874 cls=0.0193 smmd=-0.0223 ct=9.2454 rec=1.1726 | train/val/test=1.000/0.684/0.713 | c=0.998347
[Epoch 0090] loss=11.5962 cls=0.0182 smmd=-0.0107 ct=9.2379 rec=1.1754 | train/val/test=1.000/0.682/0.702 | c=0.998347
[Epoch 0091] loss=11.6190 cls=0.0228 smmd=-0.0143 ct=9.2586 rec=1.1760 | train/val/test=1.000/0.676/0.717 | c=0.998347
[Epoch 0092] loss=11.6651 cls=0.0211 smmd=0.0274 ct=9.2557 rec=1.1805 | train/val/test=1.000/0.672/0.694 | c=0.998347
[Epoch 0093] loss=11.6598 cls=0.0225 smmd=0.0139 ct=9.2748 rec=1.1743 | train/val/test=1.000/0.690/0.724 | c=0.998347
[Epoch 0094] loss=11.5935 cls=0.0137 smmd=-0.0015 ct=9.2525 rec=1.1644 | train/val/test=1.000/0.688/0.716 | c=0.998347
[Epoch 0095] loss=11.5777 cls=0.0108 smmd=-0.0014 ct=9.2455 rec=1.1614 | train/val/test=1.000/0.682/0.700 | c=0.998347
[Epoch 0096] loss=11.5969 cls=0.0128 smmd=-0.0042 ct=9.2607 rec=1.1637 | train/val/test=1.000/0.692/0.719 | c=0.998347
[Epoch 0097] loss=11.5730 cls=0.0114 smmd=-0.0148 ct=9.2514 rec=1.1625 | train/val/test=1.000/0.682/0.712 | c=0.998347
[Epoch 0098] loss=11.5786 cls=0.0121 smmd=-0.0070 ct=9.2423 rec=1.1656 | train/val/test=1.000/0.682/0.704 | c=0.998347
[Epoch 0099] loss=11.5875 cls=0.0153 smmd=-0.0209 ct=9.2533 rec=1.1699 | train/val/test=1.000/0.684/0.714 | c=0.998347
=== Best @ epoch 43: val=0.6960, test=0.7050 ===

==================================================
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 - 2025-09-21 06:33:33:
Running experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1...
Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=29, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=18.1998 cls=1.9506 smmd=4.2028 ct=9.2686 rec=1.3889 | train/val/test=0.310/0.118/0.143 | c=0.998347
[Epoch 0001] loss=16.6791 cls=1.8965 smmd=2.8134 ct=9.1905 rec=1.3893 | train/val/test=0.759/0.412/0.433 | c=0.998347
[Epoch 0002] loss=15.1594 cls=1.7634 smmd=1.5304 ct=9.0885 rec=1.3886 | train/val/test=0.621/0.358/0.385 | c=0.998347
[Epoch 0003] loss=14.9065 cls=1.4968 smmd=1.5168 ct=9.1152 rec=1.3888 | train/val/test=0.931/0.566/0.591 | c=0.998347
[Epoch 0004] loss=14.5837 cls=1.1824 smmd=1.6901 ct=8.9533 rec=1.3789 | train/val/test=0.897/0.546/0.585 | c=0.998347
[Epoch 0005] loss=14.2280 cls=0.9478 smmd=1.6483 ct=8.9047 rec=1.3636 | train/val/test=0.931/0.530/0.558 | c=0.998347
[Epoch 0006] loss=13.6141 cls=0.6551 smmd=1.3943 ct=8.8719 rec=1.3465 | train/val/test=1.000/0.590/0.609 | c=0.998347
[Epoch 0007] loss=13.7029 cls=0.4559 smmd=1.0757 ct=9.5171 rec=1.3271 | train/val/test=1.000/0.584/0.601 | c=0.998347
[Epoch 0008] loss=13.3455 cls=0.3019 smmd=1.0427 ct=9.3894 rec=1.3057 | train/val/test=1.000/0.578/0.601 | c=0.998347
[Epoch 0009] loss=13.2790 cls=0.1894 smmd=1.2085 ct=9.3048 rec=1.2881 | train/val/test=1.000/0.582/0.595 | c=0.998347
[Epoch 0010] loss=13.2102 cls=0.1062 smmd=1.2896 ct=9.2766 rec=1.2689 | train/val/test=1.000/0.580/0.597 | c=0.998347
[Epoch 0011] loss=13.0316 cls=0.0574 smmd=1.1829 ct=9.2930 rec=1.2492 | train/val/test=1.000/0.600/0.610 | c=0.998347
[Epoch 0012] loss=12.8045 cls=0.0331 smmd=0.9685 ct=9.3344 rec=1.2343 | train/val/test=1.000/0.606/0.622 | c=0.998347
[Epoch 0013] loss=12.6229 cls=0.0202 smmd=0.7954 ct=9.3608 rec=1.2232 | train/val/test=1.000/0.616/0.627 | c=0.998347
[Epoch 0014] loss=12.5251 cls=0.0133 smmd=0.7104 ct=9.3703 rec=1.2156 | train/val/test=1.000/0.612/0.630 | c=0.998347
[Epoch 0015] loss=12.5160 cls=0.0102 smmd=0.7131 ct=9.3712 rec=1.2108 | train/val/test=1.000/0.608/0.631 | c=0.998347
[Epoch 0016] loss=12.4742 cls=0.0093 smmd=0.6873 ct=9.3637 rec=1.2070 | train/val/test=1.000/0.616/0.639 | c=0.998347
[Epoch 0017] loss=12.3667 cls=0.0098 smmd=0.5907 ct=9.3562 rec=1.2050 | train/val/test=1.000/0.624/0.642 | c=0.998347
[Epoch 0018] loss=12.2422 cls=0.0098 smmd=0.4794 ct=9.3459 rec=1.2036 | train/val/test=1.000/0.622/0.645 | c=0.998347
[Epoch 0019] loss=12.2056 cls=0.0093 smmd=0.4549 ct=9.3359 rec=1.2027 | train/val/test=1.000/0.622/0.644 | c=0.998347
[Epoch 0020] loss=12.1840 cls=0.0103 smmd=0.4397 ct=9.3283 rec=1.2028 | train/val/test=1.000/0.634/0.656 | c=0.998347
[Epoch 0021] loss=12.1312 cls=0.0121 smmd=0.3898 ct=9.3250 rec=1.2021 | train/val/test=1.000/0.646/0.669 | c=0.998347
[Epoch 0022] loss=12.0640 cls=0.0144 smmd=0.3166 ct=9.3326 rec=1.2002 | train/val/test=1.000/0.650/0.673 | c=0.998347
[Epoch 0023] loss=12.0277 cls=0.0185 smmd=0.2649 ct=9.3461 rec=1.1991 | train/val/test=1.000/0.654/0.676 | c=0.998347
[Epoch 0024] loss=12.0084 cls=0.0231 smmd=0.2360 ct=9.3531 rec=1.1981 | train/val/test=1.000/0.658/0.679 | c=0.998347
[Epoch 0025] loss=11.9923 cls=0.0297 smmd=0.2169 ct=9.3551 rec=1.1953 | train/val/test=1.000/0.662/0.686 | c=0.998347
[Epoch 0026] loss=11.9500 cls=0.0334 smmd=0.1880 ct=9.3447 rec=1.1919 | train/val/test=1.000/0.674/0.694 | c=0.998347
[Epoch 0027] loss=11.9257 cls=0.0377 smmd=0.1690 ct=9.3426 rec=1.1882 | train/val/test=1.000/0.672/0.710 | c=0.998347
[Epoch 0028] loss=11.9115 cls=0.0363 smmd=0.1700 ct=9.3378 rec=1.1837 | train/val/test=1.000/0.680/0.695 | c=0.998347
[Epoch 0029] loss=11.8903 cls=0.0377 smmd=0.1542 ct=9.3387 rec=1.1799 | train/val/test=1.000/0.678/0.722 | c=0.998347
[Epoch 0030] loss=11.8777 cls=0.0295 smmd=0.1597 ct=9.3354 rec=1.1765 | train/val/test=1.000/0.684/0.698 | c=0.998347
[Epoch 0031] loss=11.8456 cls=0.0245 smmd=0.1373 ct=9.3397 rec=1.1721 | train/val/test=1.000/0.686/0.713 | c=0.998347
[Epoch 0032] loss=11.7900 cls=0.0166 smmd=0.1114 ct=9.3275 rec=1.1673 | train/val/test=1.000/0.684/0.715 | c=0.998347
[Epoch 0033] loss=11.7813 cls=0.0142 smmd=0.1106 ct=9.3245 rec=1.1660 | train/val/test=1.000/0.686/0.700 | c=0.998347
[Epoch 0034] loss=11.7719 cls=0.0143 smmd=0.0928 ct=9.3308 rec=1.1670 | train/val/test=1.000/0.678/0.713 | c=0.998347
[Epoch 0035] loss=11.7530 cls=0.0123 smmd=0.0847 ct=9.3216 rec=1.1672 | train/val/test=1.000/0.682/0.710 | c=0.998347
[Epoch 0036] loss=11.7394 cls=0.0115 smmd=0.0765 ct=9.3188 rec=1.1662 | train/val/test=1.000/0.684/0.706 | c=0.998347
[Epoch 0037] loss=11.7235 cls=0.0125 smmd=0.0569 ct=9.3177 rec=1.1682 | train/val/test=1.000/0.678/0.718 | c=0.998347
[Epoch 0038] loss=11.7368 cls=0.0125 smmd=0.0689 ct=9.3140 rec=1.1707 | train/val/test=1.000/0.688/0.700 | c=0.998347
[Epoch 0039] loss=11.7305 cls=0.0149 smmd=0.0468 ct=9.3234 rec=1.1727 | train/val/test=1.000/0.678/0.723 | c=0.998347
[Epoch 0040] loss=11.7327 cls=0.0137 smmd=0.0602 ct=9.3138 rec=1.1726 | train/val/test=1.000/0.688/0.700 | c=0.998347
[Epoch 0041] loss=11.7109 cls=0.0154 smmd=0.0381 ct=9.3167 rec=1.1704 | train/val/test=1.000/0.688/0.715 | c=0.998347
[Epoch 0042] loss=11.6911 cls=0.0141 smmd=0.0347 ct=9.3056 rec=1.1683 | train/val/test=1.000/0.688/0.708 | c=0.998347
[Epoch 0043] loss=11.6815 cls=0.0148 smmd=0.0292 ct=9.3046 rec=1.1664 | train/val/test=1.000/0.696/0.705 | c=0.998347
[Epoch 0044] loss=11.6825 cls=0.0163 smmd=0.0243 ct=9.3096 rec=1.1662 | train/val/test=1.000/0.688/0.711 | c=0.998347
[Epoch 0045] loss=11.6795 cls=0.0163 smmd=0.0271 ct=9.3037 rec=1.1662 | train/val/test=1.000/0.696/0.704 | c=0.998347
[Epoch 0046] loss=11.6724 cls=0.0183 smmd=0.0178 ct=9.3041 rec=1.1661 | train/val/test=1.000/0.690/0.711 | c=0.998347
[Epoch 0047] loss=11.6663 cls=0.0173 smmd=0.0235 ct=9.2936 rec=1.1659 | train/val/test=1.000/0.696/0.705 | c=0.998347
[Epoch 0048] loss=11.6550 cls=0.0172 smmd=0.0135 ct=9.2935 rec=1.1654 | train/val/test=1.000/0.692/0.709 | c=0.998347
[Epoch 0049] loss=11.6465 cls=0.0164 smmd=0.0078 ct=9.2910 rec=1.1656 | train/val/test=1.000/0.690/0.712 | c=0.998347
[Epoch 0050] loss=11.6442 cls=0.0160 smmd=0.0028 ct=9.2926 rec=1.1664 | train/val/test=1.000/0.690/0.704 | c=0.998347
[Epoch 0051] loss=11.6433 cls=0.0164 smmd=0.0010 ct=9.2915 rec=1.1672 | train/val/test=1.000/0.692/0.715 | c=0.998347
[Epoch 0052] loss=11.6425 cls=0.0155 smmd=0.0095 ct=9.2808 rec=1.1683 | train/val/test=1.000/0.686/0.700 | c=0.998347
[Epoch 0053] loss=11.6486 cls=0.0174 smmd=0.0046 ct=9.2877 rec=1.1695 | train/val/test=1.000/0.690/0.717 | c=0.998347
[Epoch 0054] loss=11.6635 cls=0.0165 smmd=0.0179 ct=9.2855 rec=1.1718 | train/val/test=1.000/0.690/0.697 | c=0.998347
[Epoch 0055] loss=11.6829 cls=0.0200 smmd=0.0154 ct=9.3008 rec=1.1734 | train/val/test=1.000/0.686/0.721 | c=0.998347
[Epoch 0056] loss=11.6885 cls=0.0167 smmd=0.0403 ct=9.2870 rec=1.1723 | train/val/test=1.000/0.690/0.703 | c=0.998347
[Epoch 0057] loss=11.6396 cls=0.0139 smmd=0.0100 ct=9.2856 rec=1.1650 | train/val/test=1.000/0.694/0.712 | c=0.998347
[Epoch 0058] loss=11.6124 cls=0.0114 smmd=0.0011 ct=9.2772 rec=1.1614 | train/val/test=1.000/0.692/0.718 | c=0.998347
[Epoch 0059] loss=11.6423 cls=0.0125 smmd=0.0218 ct=9.2786 rec=1.1646 | train/val/test=1.000/0.692/0.705 | c=0.998347
[Epoch 0060] loss=11.6323 cls=0.0136 smmd=0.0056 ct=9.2841 rec=1.1645 | train/val/test=1.000/0.692/0.713 | c=0.998347
[Epoch 0061] loss=11.6067 cls=0.0123 smmd=-0.0039 ct=9.2719 rec=1.1632 | train/val/test=1.000/0.692/0.715 | c=0.998347
[Epoch 0062] loss=11.6212 cls=0.0142 smmd=0.0032 ct=9.2702 rec=1.1668 | train/val/test=1.000/0.684/0.699 | c=0.998347
[Epoch 0063] loss=11.6434 cls=0.0189 smmd=0.0048 ct=9.2765 rec=1.1716 | train/val/test=1.000/0.690/0.716 | c=0.998347
[Epoch 0064] loss=11.6472 cls=0.0181 smmd=0.0123 ct=9.2690 rec=1.1739 | train/val/test=1.000/0.676/0.703 | c=0.998347
[Epoch 0065] loss=11.6398 cls=0.0200 smmd=-0.0058 ct=9.2802 rec=1.1727 | train/val/test=1.000/0.682/0.712 | c=0.998347
[Epoch 0066] loss=11.6312 cls=0.0174 smmd=0.0067 ct=9.2642 rec=1.1715 | train/val/test=1.000/0.692/0.711 | c=0.998347
[Epoch 0067] loss=11.6170 cls=0.0183 smmd=-0.0080 ct=9.2689 rec=1.1688 | train/val/test=1.000/0.682/0.708 | c=0.998347
[Epoch 0068] loss=11.6062 cls=0.0161 smmd=-0.0097 ct=9.2645 rec=1.1677 | train/val/test=1.000/0.692/0.719 | c=0.998347
[Epoch 0069] loss=11.5992 cls=0.0159 smmd=-0.0110 ct=9.2613 rec=1.1665 | train/val/test=1.000/0.688/0.706 | c=0.998347
[Epoch 0070] loss=11.6022 cls=0.0162 smmd=-0.0120 ct=9.2645 rec=1.1668 | train/val/test=1.000/0.682/0.718 | c=0.998347
[Epoch 0071] loss=11.5968 cls=0.0147 smmd=-0.0105 ct=9.2587 rec=1.1670 | train/val/test=1.000/0.688/0.713 | c=0.998347
[Epoch 0072] loss=11.5964 cls=0.0158 smmd=-0.0129 ct=9.2591 rec=1.1672 | train/val/test=1.000/0.686/0.704 | c=0.998347
[Epoch 0073] loss=11.5894 cls=0.0157 smmd=-0.0186 ct=9.2560 rec=1.1682 | train/val/test=1.000/0.690/0.716 | c=0.998347
[Epoch 0074] loss=11.5920 cls=0.0157 smmd=-0.0156 ct=9.2539 rec=1.1690 | train/val/test=1.000/0.682/0.705 | c=0.998347
[Epoch 0075] loss=11.5941 cls=0.0177 smmd=-0.0188 ct=9.2535 rec=1.1708 | train/val/test=1.000/0.680/0.717 | c=0.998347
[Epoch 0076] loss=11.6026 cls=0.0170 smmd=-0.0121 ct=9.2532 rec=1.1723 | train/val/test=1.000/0.686/0.709 | c=0.998347
[Epoch 0077] loss=11.6030 cls=0.0192 smmd=-0.0194 ct=9.2583 rec=1.1725 | train/val/test=1.000/0.676/0.713 | c=0.998347
[Epoch 0078] loss=11.6127 cls=0.0176 smmd=0.0001 ct=9.2479 rec=1.1736 | train/val/test=1.000/0.682/0.706 | c=0.998347
[Epoch 0079] loss=11.6156 cls=0.0198 smmd=-0.0144 ct=9.2659 rec=1.1721 | train/val/test=1.000/0.684/0.712 | c=0.998347
[Epoch 0080] loss=11.6063 cls=0.0164 smmd=0.0003 ct=9.2482 rec=1.1707 | train/val/test=1.000/0.684/0.703 | c=0.998347
[Epoch 0081] loss=11.5863 cls=0.0161 smmd=-0.0178 ct=9.2538 rec=1.1671 | train/val/test=1.000/0.690/0.714 | c=0.998347
[Epoch 0082] loss=11.5775 cls=0.0145 smmd=-0.0176 ct=9.2504 rec=1.1651 | train/val/test=1.000/0.686/0.716 | c=0.998347
[Epoch 0083] loss=11.5799 cls=0.0140 smmd=-0.0122 ct=9.2453 rec=1.1664 | train/val/test=1.000/0.686/0.708 | c=0.998347
[Epoch 0084] loss=11.5846 cls=0.0159 smmd=-0.0201 ct=9.2528 rec=1.1680 | train/val/test=1.000/0.688/0.715 | c=0.998347
[Epoch 0085] loss=11.5770 cls=0.0154 smmd=-0.0203 ct=9.2439 rec=1.1690 | train/val/test=1.000/0.688/0.702 | c=0.998347
[Epoch 0086] loss=11.5767 cls=0.0166 smmd=-0.0227 ct=9.2434 rec=1.1697 | train/val/test=1.000/0.688/0.713 | c=0.998347
[Epoch 0087] loss=11.5794 cls=0.0168 smmd=-0.0225 ct=9.2445 rec=1.1704 | train/val/test=1.000/0.686/0.713 | c=0.998347
[Epoch 0088] loss=11.5819 cls=0.0170 smmd=-0.0192 ct=9.2393 rec=1.1724 | train/val/test=1.000/0.690/0.707 | c=0.998347
[Epoch 0089] loss=11.5874 cls=0.0193 smmd=-0.0223 ct=9.2454 rec=1.1726 | train/val/test=1.000/0.684/0.713 | c=0.998347
[Epoch 0090] loss=11.5962 cls=0.0182 smmd=-0.0107 ct=9.2379 rec=1.1754 | train/val/test=1.000/0.682/0.702 | c=0.998347
[Epoch 0091] loss=11.6190 cls=0.0228 smmd=-0.0143 ct=9.2586 rec=1.1760 | train/val/test=1.000/0.676/0.717 | c=0.998347
[Epoch 0092] loss=11.6651 cls=0.0211 smmd=0.0274 ct=9.2557 rec=1.1805 | train/val/test=1.000/0.672/0.694 | c=0.998347
[Epoch 0093] loss=11.6598 cls=0.0225 smmd=0.0139 ct=9.2748 rec=1.1743 | train/val/test=1.000/0.690/0.724 | c=0.998347
[Epoch 0094] loss=11.5935 cls=0.0137 smmd=-0.0015 ct=9.2525 rec=1.1644 | train/val/test=1.000/0.688/0.716 | c=0.998347
[Epoch 0095] loss=11.5777 cls=0.0108 smmd=-0.0014 ct=9.2455 rec=1.1614 | train/val/test=1.000/0.682/0.700 | c=0.998347
[Epoch 0096] loss=11.5969 cls=0.0128 smmd=-0.0042 ct=9.2607 rec=1.1637 | train/val/test=1.000/0.692/0.719 | c=0.998347
[Epoch 0097] loss=11.5730 cls=0.0114 smmd=-0.0148 ct=9.2514 rec=1.1625 | train/val/test=1.000/0.682/0.712 | c=0.998347
[Epoch 0098] loss=11.5786 cls=0.0121 smmd=-0.0070 ct=9.2423 rec=1.1656 | train/val/test=1.000/0.682/0.704 | c=0.998347
[Epoch 0099] loss=11.5875 cls=0.0153 smmd=-0.0209 ct=9.2533 rec=1.1699 | train/val/test=1.000/0.684/0.714 | c=0.998347
=== Best @ epoch 43: val=0.6960, test=0.7050 ===

Experiment Cora-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-1 completed in 22.93 seconds.
==================================================
