Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 - 2025-09-21 05:45:18:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5640 cls=1.1058 smmd=5.6110 ct=11.2767 rec=1.4136 | train/val/test=0.462/0.312/0.351 | c=0.998347
[Epoch 0001] loss=22.4363 cls=1.0960 smmd=3.9709 ct=11.2542 rec=1.4135 | train/val/test=0.308/0.416/0.407 | c=0.998347
[Epoch 0002] loss=24.7800 cls=1.0966 smmd=4.9045 ct=11.2636 rec=1.4137 | train/val/test=0.308/0.416/0.407 | c=0.998347
[Epoch 0003] loss=23.6985 cls=1.0805 smmd=4.5024 ct=11.1954 rec=1.4136 | train/val/test=0.615/0.558/0.587 | c=0.998347
[Epoch 0004] loss=18.3208 cls=1.0535 smmd=2.4217 ct=11.0330 rec=1.4134 | train/val/test=0.769/0.670/0.656 | c=0.998347
[Epoch 0005] loss=19.9128 cls=1.0285 smmd=3.0657 ct=11.0278 rec=1.4128 | train/val/test=0.692/0.600/0.626 | c=0.998347
[Epoch 0006] loss=20.4629 cls=0.9936 smmd=3.3353 ct=10.9215 rec=1.4123 | train/val/test=0.692/0.620/0.641 | c=0.998347
[Epoch 0007] loss=18.5332 cls=0.9575 smmd=2.5891 ct=10.8762 rec=1.4112 | train/val/test=0.923/0.654/0.667 | c=0.998347
[Epoch 0008] loss=16.4232 cls=0.9262 smmd=1.7611 ct=10.8527 rec=1.4094 | train/val/test=0.923/0.658/0.674 | c=0.998347
[Epoch 0009] loss=18.3779 cls=0.9022 smmd=2.5356 ct=10.8834 rec=1.4087 | train/val/test=0.846/0.676/0.670 | c=0.998347
[Epoch 0010] loss=18.4710 cls=0.8746 smmd=2.5793 ct=10.8812 rec=1.4085 | train/val/test=0.846/0.680/0.679 | c=0.998347
[Epoch 0011] loss=16.4989 cls=0.8406 smmd=1.5612 ct=11.4721 rec=1.4074 | train/val/test=0.923/0.682/0.696 | c=0.998347
[Epoch 0012] loss=18.4616 cls=0.8142 smmd=2.3702 ct=11.4258 rec=1.4065 | train/val/test=0.923/0.684/0.687 | c=0.998347
[Epoch 0013] loss=18.4057 cls=0.7670 smmd=2.3540 ct=11.4347 rec=1.4050 | train/val/test=0.923/0.684/0.701 | c=0.998347
[Epoch 0014] loss=16.4269 cls=0.7033 smmd=1.5825 ct=11.4191 rec=1.4001 | train/val/test=0.923/0.688/0.695 | c=0.998347
[Epoch 0015] loss=16.6273 cls=0.6473 smmd=1.6663 ct=11.4405 rec=1.3948 | train/val/test=0.923/0.688/0.699 | c=0.998347
[Epoch 0016] loss=16.6848 cls=0.5985 smmd=1.7036 ct=11.4313 rec=1.3903 | train/val/test=0.923/0.686/0.698 | c=0.998347
[Epoch 0017] loss=16.0513 cls=0.5636 smmd=1.4567 ct=11.4342 rec=1.3868 | train/val/test=0.923/0.692/0.705 | c=0.998347
[Epoch 0018] loss=15.6349 cls=0.5416 smmd=1.2920 ct=11.4414 rec=1.3854 | train/val/test=0.923/0.698/0.707 | c=0.998347
[Epoch 0019] loss=15.6847 cls=0.5332 smmd=1.3220 ct=11.4200 rec=1.3863 | train/val/test=0.923/0.708/0.712 | c=0.998347
[Epoch 0020] loss=15.8135 cls=0.5268 smmd=1.3772 ct=11.4132 rec=1.3878 | train/val/test=0.923/0.696/0.707 | c=0.998347
[Epoch 0021] loss=15.3082 cls=0.5180 smmd=1.1611 ct=11.4508 rec=1.3913 | train/val/test=0.923/0.718/0.717 | c=0.998347
[Epoch 0022] loss=15.9494 cls=0.4994 smmd=1.4019 ct=11.5008 rec=1.3883 | train/val/test=1.000/0.716/0.721 | c=0.998347
[Epoch 0023] loss=15.1920 cls=0.4589 smmd=1.1414 ct=11.4175 rec=1.3830 | train/val/test=0.923/0.708/0.717 | c=0.998347
[Epoch 0024] loss=15.3227 cls=0.4172 smmd=1.1790 ct=11.4778 rec=1.3773 | train/val/test=1.000/0.726/0.711 | c=0.998347
[Epoch 0025] loss=14.6180 cls=0.3730 smmd=0.9340 ct=11.4119 rec=1.3690 | train/val/test=1.000/0.722/0.717 | c=0.998347
[Epoch 0026] loss=14.8172 cls=0.3440 smmd=1.0250 ct=11.4004 rec=1.3645 | train/val/test=1.000/0.718/0.723 | c=0.998347
[Epoch 0027] loss=14.4573 cls=0.3243 smmd=0.8736 ct=11.4307 rec=1.3609 | train/val/test=1.000/0.716/0.724 | c=0.998347
[Epoch 0028] loss=14.5074 cls=0.3175 smmd=0.8801 ct=11.4678 rec=1.3610 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0029] loss=14.3751 cls=0.3186 smmd=0.8461 ct=11.4184 rec=1.3641 | train/val/test=1.000/0.734/0.734 | c=0.998347
[Epoch 0030] loss=14.6991 cls=0.3239 smmd=0.9722 ct=11.4232 rec=1.3670 | train/val/test=1.000/0.730/0.735 | c=0.998347
[Epoch 0031] loss=14.6672 cls=0.3173 smmd=0.9437 ct=11.4660 rec=1.3664 | train/val/test=1.000/0.730/0.725 | c=0.998347
[Epoch 0032] loss=14.8374 cls=0.2964 smmd=1.0134 ct=11.4738 rec=1.3638 | train/val/test=1.000/0.728/0.735 | c=0.998347
[Epoch 0033] loss=14.1760 cls=0.2687 smmd=0.7860 ct=11.3991 rec=1.3550 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0034] loss=14.2619 cls=0.2399 smmd=0.7982 ct=11.4726 rec=1.3477 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0035] loss=14.0026 cls=0.2227 smmd=0.7168 ct=11.4282 rec=1.3422 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0036] loss=13.8071 cls=0.2131 smmd=0.6460 ct=11.4152 rec=1.3405 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0037] loss=13.8987 cls=0.2176 smmd=0.6745 ct=11.4324 rec=1.3423 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0038] loss=13.9010 cls=0.2234 smmd=0.6696 ct=11.4425 rec=1.3456 | train/val/test=1.000/0.732/0.731 | c=0.998347
[Epoch 0039] loss=14.1663 cls=0.2325 smmd=0.7717 ct=11.4456 rec=1.3505 | train/val/test=1.000/0.734/0.741 | c=0.998347
[Epoch 0040] loss=14.2655 cls=0.2351 smmd=0.8061 ct=11.4575 rec=1.3504 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0041] loss=14.2373 cls=0.2182 smmd=0.8081 ct=11.4341 rec=1.3478 | train/val/test=1.000/0.738/0.737 | c=0.998347
[Epoch 0042] loss=13.8592 cls=0.1985 smmd=0.6497 ct=11.4660 rec=1.3395 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0043] loss=13.7875 cls=0.1764 smmd=0.6419 ct=11.4278 rec=1.3335 | train/val/test=1.000/0.736/0.731 | c=0.998347
[Epoch 0044] loss=13.6404 cls=0.1681 smmd=0.5827 ct=11.4349 rec=1.3296 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0045] loss=13.5513 cls=0.1607 smmd=0.5564 ct=11.4154 rec=1.3291 | train/val/test=1.000/0.744/0.740 | c=0.998347
[Epoch 0046] loss=13.6148 cls=0.1692 smmd=0.5681 ct=11.4440 rec=1.3320 | train/val/test=1.000/0.734/0.731 | c=0.998347
[Epoch 0047] loss=13.7086 cls=0.1738 smmd=0.6018 ct=11.4484 rec=1.3375 | train/val/test=1.000/0.746/0.748 | c=0.998347
[Epoch 0048] loss=13.9042 cls=0.1866 smmd=0.6831 ct=11.4330 rec=1.3403 | train/val/test=1.000/0.716/0.701 | c=0.998347
[Epoch 0049] loss=14.1739 cls=0.1923 smmd=0.7656 ct=11.4884 rec=1.3505 | train/val/test=1.000/0.726/0.745 | c=0.998347
[Epoch 0050] loss=13.9889 cls=0.1936 smmd=0.6971 ct=11.4773 rec=1.3439 | train/val/test=1.000/0.714/0.702 | c=0.998347
[Epoch 0051] loss=13.8558 cls=0.1669 smmd=0.6562 ct=11.4616 rec=1.3408 | train/val/test=1.000/0.736/0.732 | c=0.998347
[Epoch 0052] loss=13.5560 cls=0.1411 smmd=0.5479 ct=11.4531 rec=1.3252 | train/val/test=1.000/0.734/0.735 | c=0.998347
[Epoch 0053] loss=13.6312 cls=0.1137 smmd=0.5918 ct=11.4386 rec=1.3125 | train/val/test=1.000/0.746/0.729 | c=0.998347
[Epoch 0054] loss=13.3108 cls=0.1117 smmd=0.4792 ct=11.4007 rec=1.3127 | train/val/test=1.000/0.742/0.737 | c=0.998347
[Epoch 0055] loss=13.4524 cls=0.1202 smmd=0.5125 ct=11.4519 rec=1.3181 | train/val/test=1.000/0.748/0.744 | c=0.998347
[Epoch 0056] loss=13.6378 cls=0.1373 smmd=0.5860 ct=11.4405 rec=1.3270 | train/val/test=1.000/0.766/0.752 | c=0.998347
[Epoch 0057] loss=13.9255 cls=0.1570 smmd=0.6949 ct=11.4422 rec=1.3348 | train/val/test=1.000/0.720/0.711 | c=0.998347
[Epoch 0058] loss=14.1301 cls=0.1714 smmd=0.7587 ct=11.4743 rec=1.3469 | train/val/test=1.000/0.752/0.756 | c=0.998347
[Epoch 0059] loss=13.9031 cls=0.1807 smmd=0.6739 ct=11.4569 rec=1.3421 | train/val/test=1.000/0.692/0.676 | c=0.998347
[Epoch 0060] loss=13.8315 cls=0.1710 smmd=0.6288 ct=11.4986 rec=1.3509 | train/val/test=1.000/0.730/0.746 | c=0.998347
[Epoch 0061] loss=13.4338 cls=0.1413 smmd=0.5010 ct=11.4463 rec=1.3288 | train/val/test=1.000/0.744/0.739 | c=0.998347
[Epoch 0062] loss=13.4283 cls=0.0937 smmd=0.5265 ct=11.4121 rec=1.3064 | train/val/test=1.000/0.738/0.730 | c=0.998347
[Epoch 0063] loss=13.1647 cls=0.0882 smmd=0.4197 ct=11.4186 rec=1.3055 | train/val/test=1.000/0.756/0.744 | c=0.998347
[Epoch 0064] loss=13.2242 cls=0.1027 smmd=0.4280 ct=11.4456 rec=1.3142 | train/val/test=1.000/0.720/0.730 | c=0.998347
[Epoch 0065] loss=13.6630 cls=0.1213 smmd=0.5997 ct=11.4407 rec=1.3248 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0066] loss=14.0235 cls=0.1499 smmd=0.7179 ct=11.4859 rec=1.3359 | train/val/test=1.000/0.664/0.676 | c=0.998347
[Epoch 0067] loss=14.3928 cls=0.1922 smmd=0.8329 ct=11.5306 rec=1.3675 | train/val/test=1.000/0.654/0.669 | c=0.998347
[Epoch 0068] loss=14.1255 cls=0.1934 smmd=0.7439 ct=11.4943 rec=1.3497 | train/val/test=1.000/0.712/0.686 | c=0.998347
[Epoch 0069] loss=13.7489 cls=0.1268 smmd=0.6109 ct=11.4915 rec=1.3336 | train/val/test=1.000/0.758/0.741 | c=0.998347
[Epoch 0070] loss=13.2700 cls=0.0661 smmd=0.4638 ct=11.4289 rec=1.2969 | train/val/test=1.000/0.754/0.738 | c=0.998347
[Epoch 0071] loss=13.2754 cls=0.0752 smmd=0.4687 ct=11.4133 rec=1.3054 | train/val/test=1.000/0.724/0.714 | c=0.998347
[Epoch 0072] loss=13.3204 cls=0.0716 smmd=0.4852 ct=11.4194 rec=1.3041 | train/val/test=1.000/0.754/0.745 | c=0.998347
[Epoch 0073] loss=13.1993 cls=0.0745 smmd=0.4222 ct=11.4544 rec=1.3043 | train/val/test=1.000/0.774/0.752 | c=0.998347
[Epoch 0074] loss=13.4411 cls=0.1027 smmd=0.5144 ct=11.4439 rec=1.3197 | train/val/test=1.000/0.730/0.712 | c=0.998347
[Epoch 0075] loss=13.9817 cls=0.1427 smmd=0.7160 ct=11.4477 rec=1.3453 | train/val/test=1.000/0.768/0.751 | c=0.998347
[Epoch 0076] loss=14.0087 cls=0.1462 smmd=0.6934 ct=11.5338 rec=1.3364 | train/val/test=1.000/0.712/0.708 | c=0.998347
[Epoch 0077] loss=13.9665 cls=0.1334 smmd=0.7236 ct=11.4234 rec=1.3348 | train/val/test=1.000/0.766/0.744 | c=0.998347
[Epoch 0078] loss=13.4686 cls=0.1081 smmd=0.5302 ct=11.4297 rec=1.3188 | train/val/test=1.000/0.744/0.731 | c=0.998347
[Epoch 0079] loss=13.2444 cls=0.0952 smmd=0.4388 ct=11.4429 rec=1.3136 | train/val/test=1.000/0.764/0.745 | c=0.998347
[Epoch 0080] loss=13.2537 cls=0.0977 smmd=0.4587 ct=11.4008 rec=1.3145 | train/val/test=1.000/0.736/0.719 | c=0.998347
[Epoch 0081] loss=13.2022 cls=0.0970 smmd=0.4359 ct=11.4051 rec=1.3179 | train/val/test=1.000/0.758/0.739 | c=0.998347
[Epoch 0082] loss=13.1350 cls=0.1044 smmd=0.3914 ct=11.4440 rec=1.3205 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0083] loss=13.4208 cls=0.1214 smmd=0.4979 ct=11.4492 rec=1.3324 | train/val/test=1.000/0.680/0.666 | c=0.998347
[Epoch 0084] loss=14.0038 cls=0.1511 smmd=0.7274 ct=11.4352 rec=1.3492 | train/val/test=0.923/0.614/0.622 | c=0.998347
[Epoch 0085] loss=14.4514 cls=0.2426 smmd=0.8131 ct=11.5961 rec=1.4027 | train/val/test=0.923/0.560/0.552 | c=0.998347
[Epoch 0086] loss=14.5483 cls=0.2861 smmd=0.8624 ct=11.5641 rec=1.3702 | train/val/test=1.000/0.700/0.682 | c=0.998347
[Epoch 0087] loss=13.6685 cls=0.0873 smmd=0.5987 ct=11.4656 rec=1.3252 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0088] loss=13.4099 cls=0.0507 smmd=0.5171 ct=11.4461 rec=1.2913 | train/val/test=1.000/0.722/0.720 | c=0.998347
[Epoch 0089] loss=13.4302 cls=0.0386 smmd=0.5186 ct=11.4644 rec=1.3003 | train/val/test=1.000/0.706/0.681 | c=0.998347
[Epoch 0090] loss=13.3509 cls=0.0280 smmd=0.5176 ct=11.3986 rec=1.2887 | train/val/test=1.000/0.726/0.726 | c=0.998347
[Epoch 0091] loss=13.3383 cls=0.0424 smmd=0.4887 ct=11.4461 rec=1.2986 | train/val/test=1.000/0.750/0.736 | c=0.998347
[Epoch 0092] loss=13.2526 cls=0.0435 smmd=0.4359 ct=11.4909 rec=1.3004 | train/val/test=1.000/0.722/0.698 | c=0.998347
[Epoch 0093] loss=13.8918 cls=0.0905 smmd=0.6969 ct=11.4402 rec=1.3284 | train/val/test=1.000/0.672/0.674 | c=0.998347
[Epoch 0094] loss=14.2361 cls=0.1550 smmd=0.7585 ct=11.5760 rec=1.3726 | train/val/test=1.000/0.638/0.599 | c=0.998347
[Epoch 0095] loss=14.4820 cls=0.2254 smmd=0.8811 ct=11.4908 rec=1.3515 | train/val/test=1.000/0.600/0.608 | c=0.998347
[Epoch 0096] loss=14.2019 cls=0.2136 smmd=0.7200 ct=11.5912 rec=1.4078 | train/val/test=1.000/0.608/0.605 | c=0.998347
[Epoch 0097] loss=13.8048 cls=0.2100 smmd=0.6364 ct=11.4365 rec=1.3447 | train/val/test=1.000/0.742/0.745 | c=0.998347
[Epoch 0098] loss=13.4012 cls=0.0573 smmd=0.5212 ct=11.4196 rec=1.2997 | train/val/test=1.000/0.730/0.727 | c=0.998347
[Epoch 0099] loss=13.3254 cls=0.0690 smmd=0.4674 ct=11.4696 rec=1.3056 | train/val/test=1.000/0.652/0.647 | c=0.998347
=== Best @ epoch 73: val=0.7740, test=0.7520 ===

==================================================
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 - 2025-09-21 05:45:18:
Running experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2...
Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 output:
Parsed from filename: Pretrained backbone is Hyperbolic.
Pretrain dataset overridden by checkpoint: PubMed -> Computers
Split sizes | train=13, val=500, test=1000 (mode=few, shot=5)
[Epoch 0000] loss=26.5640 cls=1.1058 smmd=5.6110 ct=11.2767 rec=1.4136 | train/val/test=0.462/0.312/0.351 | c=0.998347
[Epoch 0001] loss=22.4363 cls=1.0960 smmd=3.9709 ct=11.2542 rec=1.4135 | train/val/test=0.308/0.416/0.407 | c=0.998347
[Epoch 0002] loss=24.7800 cls=1.0966 smmd=4.9045 ct=11.2636 rec=1.4137 | train/val/test=0.308/0.416/0.407 | c=0.998347
[Epoch 0003] loss=23.6985 cls=1.0805 smmd=4.5024 ct=11.1954 rec=1.4136 | train/val/test=0.615/0.558/0.587 | c=0.998347
[Epoch 0004] loss=18.3208 cls=1.0535 smmd=2.4217 ct=11.0330 rec=1.4134 | train/val/test=0.769/0.670/0.656 | c=0.998347
[Epoch 0005] loss=19.9128 cls=1.0285 smmd=3.0657 ct=11.0278 rec=1.4128 | train/val/test=0.692/0.600/0.626 | c=0.998347
[Epoch 0006] loss=20.4629 cls=0.9936 smmd=3.3353 ct=10.9215 rec=1.4123 | train/val/test=0.692/0.620/0.641 | c=0.998347
[Epoch 0007] loss=18.5332 cls=0.9575 smmd=2.5891 ct=10.8762 rec=1.4112 | train/val/test=0.923/0.654/0.667 | c=0.998347
[Epoch 0008] loss=16.4232 cls=0.9262 smmd=1.7611 ct=10.8527 rec=1.4094 | train/val/test=0.923/0.658/0.674 | c=0.998347
[Epoch 0009] loss=18.3779 cls=0.9022 smmd=2.5356 ct=10.8834 rec=1.4087 | train/val/test=0.846/0.676/0.670 | c=0.998347
[Epoch 0010] loss=18.4710 cls=0.8746 smmd=2.5793 ct=10.8812 rec=1.4085 | train/val/test=0.846/0.680/0.679 | c=0.998347
[Epoch 0011] loss=16.4989 cls=0.8406 smmd=1.5612 ct=11.4721 rec=1.4074 | train/val/test=0.923/0.682/0.696 | c=0.998347
[Epoch 0012] loss=18.4616 cls=0.8142 smmd=2.3702 ct=11.4258 rec=1.4065 | train/val/test=0.923/0.684/0.687 | c=0.998347
[Epoch 0013] loss=18.4057 cls=0.7670 smmd=2.3540 ct=11.4347 rec=1.4050 | train/val/test=0.923/0.684/0.701 | c=0.998347
[Epoch 0014] loss=16.4269 cls=0.7033 smmd=1.5825 ct=11.4191 rec=1.4001 | train/val/test=0.923/0.688/0.695 | c=0.998347
[Epoch 0015] loss=16.6273 cls=0.6473 smmd=1.6663 ct=11.4405 rec=1.3948 | train/val/test=0.923/0.688/0.699 | c=0.998347
[Epoch 0016] loss=16.6848 cls=0.5985 smmd=1.7036 ct=11.4313 rec=1.3903 | train/val/test=0.923/0.686/0.698 | c=0.998347
[Epoch 0017] loss=16.0513 cls=0.5636 smmd=1.4567 ct=11.4342 rec=1.3868 | train/val/test=0.923/0.692/0.705 | c=0.998347
[Epoch 0018] loss=15.6349 cls=0.5416 smmd=1.2920 ct=11.4414 rec=1.3854 | train/val/test=0.923/0.698/0.707 | c=0.998347
[Epoch 0019] loss=15.6847 cls=0.5332 smmd=1.3220 ct=11.4200 rec=1.3863 | train/val/test=0.923/0.708/0.712 | c=0.998347
[Epoch 0020] loss=15.8135 cls=0.5268 smmd=1.3772 ct=11.4132 rec=1.3878 | train/val/test=0.923/0.696/0.707 | c=0.998347
[Epoch 0021] loss=15.3082 cls=0.5180 smmd=1.1611 ct=11.4508 rec=1.3913 | train/val/test=0.923/0.718/0.717 | c=0.998347
[Epoch 0022] loss=15.9494 cls=0.4994 smmd=1.4019 ct=11.5008 rec=1.3883 | train/val/test=1.000/0.716/0.721 | c=0.998347
[Epoch 0023] loss=15.1920 cls=0.4589 smmd=1.1414 ct=11.4175 rec=1.3830 | train/val/test=0.923/0.708/0.717 | c=0.998347
[Epoch 0024] loss=15.3227 cls=0.4172 smmd=1.1790 ct=11.4778 rec=1.3773 | train/val/test=1.000/0.726/0.711 | c=0.998347
[Epoch 0025] loss=14.6180 cls=0.3730 smmd=0.9340 ct=11.4119 rec=1.3690 | train/val/test=1.000/0.722/0.717 | c=0.998347
[Epoch 0026] loss=14.8172 cls=0.3440 smmd=1.0250 ct=11.4004 rec=1.3645 | train/val/test=1.000/0.718/0.723 | c=0.998347
[Epoch 0027] loss=14.4573 cls=0.3243 smmd=0.8736 ct=11.4307 rec=1.3609 | train/val/test=1.000/0.716/0.724 | c=0.998347
[Epoch 0028] loss=14.5074 cls=0.3175 smmd=0.8801 ct=11.4678 rec=1.3610 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0029] loss=14.3751 cls=0.3186 smmd=0.8461 ct=11.4184 rec=1.3641 | train/val/test=1.000/0.734/0.734 | c=0.998347
[Epoch 0030] loss=14.6991 cls=0.3239 smmd=0.9722 ct=11.4232 rec=1.3670 | train/val/test=1.000/0.730/0.735 | c=0.998347
[Epoch 0031] loss=14.6672 cls=0.3173 smmd=0.9437 ct=11.4660 rec=1.3664 | train/val/test=1.000/0.730/0.725 | c=0.998347
[Epoch 0032] loss=14.8374 cls=0.2964 smmd=1.0134 ct=11.4738 rec=1.3638 | train/val/test=1.000/0.728/0.735 | c=0.998347
[Epoch 0033] loss=14.1760 cls=0.2687 smmd=0.7860 ct=11.3991 rec=1.3550 | train/val/test=1.000/0.726/0.727 | c=0.998347
[Epoch 0034] loss=14.2619 cls=0.2399 smmd=0.7982 ct=11.4726 rec=1.3477 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0035] loss=14.0026 cls=0.2227 smmd=0.7168 ct=11.4282 rec=1.3422 | train/val/test=1.000/0.726/0.725 | c=0.998347
[Epoch 0036] loss=13.8071 cls=0.2131 smmd=0.6460 ct=11.4152 rec=1.3405 | train/val/test=1.000/0.726/0.722 | c=0.998347
[Epoch 0037] loss=13.8987 cls=0.2176 smmd=0.6745 ct=11.4324 rec=1.3423 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0038] loss=13.9010 cls=0.2234 smmd=0.6696 ct=11.4425 rec=1.3456 | train/val/test=1.000/0.732/0.731 | c=0.998347
[Epoch 0039] loss=14.1663 cls=0.2325 smmd=0.7717 ct=11.4456 rec=1.3505 | train/val/test=1.000/0.734/0.741 | c=0.998347
[Epoch 0040] loss=14.2655 cls=0.2351 smmd=0.8061 ct=11.4575 rec=1.3504 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0041] loss=14.2373 cls=0.2182 smmd=0.8081 ct=11.4341 rec=1.3478 | train/val/test=1.000/0.738/0.737 | c=0.998347
[Epoch 0042] loss=13.8592 cls=0.1985 smmd=0.6497 ct=11.4660 rec=1.3395 | train/val/test=1.000/0.730/0.730 | c=0.998347
[Epoch 0043] loss=13.7875 cls=0.1764 smmd=0.6419 ct=11.4278 rec=1.3335 | train/val/test=1.000/0.736/0.731 | c=0.998347
[Epoch 0044] loss=13.6404 cls=0.1681 smmd=0.5827 ct=11.4349 rec=1.3296 | train/val/test=1.000/0.732/0.730 | c=0.998347
[Epoch 0045] loss=13.5513 cls=0.1607 smmd=0.5564 ct=11.4154 rec=1.3291 | train/val/test=1.000/0.744/0.740 | c=0.998347
[Epoch 0046] loss=13.6148 cls=0.1692 smmd=0.5681 ct=11.4440 rec=1.3320 | train/val/test=1.000/0.734/0.731 | c=0.998347
[Epoch 0047] loss=13.7086 cls=0.1738 smmd=0.6018 ct=11.4484 rec=1.3375 | train/val/test=1.000/0.746/0.748 | c=0.998347
[Epoch 0048] loss=13.9042 cls=0.1866 smmd=0.6831 ct=11.4330 rec=1.3403 | train/val/test=1.000/0.716/0.701 | c=0.998347
[Epoch 0049] loss=14.1739 cls=0.1923 smmd=0.7656 ct=11.4884 rec=1.3505 | train/val/test=1.000/0.726/0.745 | c=0.998347
[Epoch 0050] loss=13.9889 cls=0.1936 smmd=0.6971 ct=11.4773 rec=1.3439 | train/val/test=1.000/0.714/0.702 | c=0.998347
[Epoch 0051] loss=13.8558 cls=0.1669 smmd=0.6562 ct=11.4616 rec=1.3408 | train/val/test=1.000/0.736/0.732 | c=0.998347
[Epoch 0052] loss=13.5560 cls=0.1411 smmd=0.5479 ct=11.4531 rec=1.3252 | train/val/test=1.000/0.734/0.735 | c=0.998347
[Epoch 0053] loss=13.6312 cls=0.1137 smmd=0.5918 ct=11.4386 rec=1.3125 | train/val/test=1.000/0.746/0.729 | c=0.998347
[Epoch 0054] loss=13.3108 cls=0.1117 smmd=0.4792 ct=11.4007 rec=1.3127 | train/val/test=1.000/0.742/0.737 | c=0.998347
[Epoch 0055] loss=13.4524 cls=0.1202 smmd=0.5125 ct=11.4519 rec=1.3181 | train/val/test=1.000/0.748/0.744 | c=0.998347
[Epoch 0056] loss=13.6378 cls=0.1373 smmd=0.5860 ct=11.4405 rec=1.3270 | train/val/test=1.000/0.766/0.752 | c=0.998347
[Epoch 0057] loss=13.9255 cls=0.1570 smmd=0.6949 ct=11.4422 rec=1.3348 | train/val/test=1.000/0.720/0.711 | c=0.998347
[Epoch 0058] loss=14.1301 cls=0.1714 smmd=0.7587 ct=11.4743 rec=1.3469 | train/val/test=1.000/0.752/0.756 | c=0.998347
[Epoch 0059] loss=13.9031 cls=0.1807 smmd=0.6739 ct=11.4569 rec=1.3421 | train/val/test=1.000/0.692/0.676 | c=0.998347
[Epoch 0060] loss=13.8315 cls=0.1710 smmd=0.6288 ct=11.4986 rec=1.3509 | train/val/test=1.000/0.730/0.746 | c=0.998347
[Epoch 0061] loss=13.4338 cls=0.1413 smmd=0.5010 ct=11.4463 rec=1.3288 | train/val/test=1.000/0.744/0.739 | c=0.998347
[Epoch 0062] loss=13.4283 cls=0.0937 smmd=0.5265 ct=11.4121 rec=1.3064 | train/val/test=1.000/0.738/0.730 | c=0.998347
[Epoch 0063] loss=13.1647 cls=0.0882 smmd=0.4197 ct=11.4186 rec=1.3055 | train/val/test=1.000/0.756/0.744 | c=0.998347
[Epoch 0064] loss=13.2242 cls=0.1027 smmd=0.4280 ct=11.4456 rec=1.3142 | train/val/test=1.000/0.720/0.730 | c=0.998347
[Epoch 0065] loss=13.6630 cls=0.1213 smmd=0.5997 ct=11.4407 rec=1.3248 | train/val/test=1.000/0.718/0.708 | c=0.998347
[Epoch 0066] loss=14.0235 cls=0.1499 smmd=0.7179 ct=11.4859 rec=1.3359 | train/val/test=1.000/0.664/0.676 | c=0.998347
[Epoch 0067] loss=14.3928 cls=0.1922 smmd=0.8329 ct=11.5306 rec=1.3675 | train/val/test=1.000/0.654/0.669 | c=0.998347
[Epoch 0068] loss=14.1255 cls=0.1934 smmd=0.7439 ct=11.4943 rec=1.3497 | train/val/test=1.000/0.712/0.686 | c=0.998347
[Epoch 0069] loss=13.7489 cls=0.1268 smmd=0.6109 ct=11.4915 rec=1.3336 | train/val/test=1.000/0.758/0.741 | c=0.998347
[Epoch 0070] loss=13.2700 cls=0.0661 smmd=0.4638 ct=11.4289 rec=1.2969 | train/val/test=1.000/0.754/0.738 | c=0.998347
[Epoch 0071] loss=13.2754 cls=0.0752 smmd=0.4687 ct=11.4133 rec=1.3054 | train/val/test=1.000/0.724/0.714 | c=0.998347
[Epoch 0072] loss=13.3204 cls=0.0716 smmd=0.4852 ct=11.4194 rec=1.3041 | train/val/test=1.000/0.754/0.745 | c=0.998347
[Epoch 0073] loss=13.1993 cls=0.0745 smmd=0.4222 ct=11.4544 rec=1.3043 | train/val/test=1.000/0.774/0.752 | c=0.998347
[Epoch 0074] loss=13.4411 cls=0.1027 smmd=0.5144 ct=11.4439 rec=1.3197 | train/val/test=1.000/0.730/0.712 | c=0.998347
[Epoch 0075] loss=13.9817 cls=0.1427 smmd=0.7160 ct=11.4477 rec=1.3453 | train/val/test=1.000/0.768/0.751 | c=0.998347
[Epoch 0076] loss=14.0087 cls=0.1462 smmd=0.6934 ct=11.5338 rec=1.3364 | train/val/test=1.000/0.712/0.708 | c=0.998347
[Epoch 0077] loss=13.9665 cls=0.1334 smmd=0.7236 ct=11.4234 rec=1.3348 | train/val/test=1.000/0.766/0.744 | c=0.998347
[Epoch 0078] loss=13.4686 cls=0.1081 smmd=0.5302 ct=11.4297 rec=1.3188 | train/val/test=1.000/0.744/0.731 | c=0.998347
[Epoch 0079] loss=13.2444 cls=0.0952 smmd=0.4388 ct=11.4429 rec=1.3136 | train/val/test=1.000/0.764/0.745 | c=0.998347
[Epoch 0080] loss=13.2537 cls=0.0977 smmd=0.4587 ct=11.4008 rec=1.3145 | train/val/test=1.000/0.736/0.719 | c=0.998347
[Epoch 0081] loss=13.2022 cls=0.0970 smmd=0.4359 ct=11.4051 rec=1.3179 | train/val/test=1.000/0.758/0.739 | c=0.998347
[Epoch 0082] loss=13.1350 cls=0.1044 smmd=0.3914 ct=11.4440 rec=1.3205 | train/val/test=1.000/0.732/0.727 | c=0.998347
[Epoch 0083] loss=13.4208 cls=0.1214 smmd=0.4979 ct=11.4492 rec=1.3324 | train/val/test=1.000/0.680/0.666 | c=0.998347
[Epoch 0084] loss=14.0038 cls=0.1511 smmd=0.7274 ct=11.4352 rec=1.3492 | train/val/test=0.923/0.614/0.622 | c=0.998347
[Epoch 0085] loss=14.4514 cls=0.2426 smmd=0.8131 ct=11.5961 rec=1.4027 | train/val/test=0.923/0.560/0.552 | c=0.998347
[Epoch 0086] loss=14.5483 cls=0.2861 smmd=0.8624 ct=11.5641 rec=1.3702 | train/val/test=1.000/0.700/0.682 | c=0.998347
[Epoch 0087] loss=13.6685 cls=0.0873 smmd=0.5987 ct=11.4656 rec=1.3252 | train/val/test=1.000/0.718/0.725 | c=0.998347
[Epoch 0088] loss=13.4099 cls=0.0507 smmd=0.5171 ct=11.4461 rec=1.2913 | train/val/test=1.000/0.722/0.720 | c=0.998347
[Epoch 0089] loss=13.4302 cls=0.0386 smmd=0.5186 ct=11.4644 rec=1.3003 | train/val/test=1.000/0.706/0.681 | c=0.998347
[Epoch 0090] loss=13.3509 cls=0.0280 smmd=0.5176 ct=11.3986 rec=1.2887 | train/val/test=1.000/0.726/0.726 | c=0.998347
[Epoch 0091] loss=13.3383 cls=0.0424 smmd=0.4887 ct=11.4461 rec=1.2986 | train/val/test=1.000/0.750/0.736 | c=0.998347
[Epoch 0092] loss=13.2526 cls=0.0435 smmd=0.4359 ct=11.4909 rec=1.3004 | train/val/test=1.000/0.722/0.698 | c=0.998347
[Epoch 0093] loss=13.8918 cls=0.0905 smmd=0.6969 ct=11.4402 rec=1.3284 | train/val/test=1.000/0.672/0.674 | c=0.998347
[Epoch 0094] loss=14.2361 cls=0.1550 smmd=0.7585 ct=11.5760 rec=1.3726 | train/val/test=1.000/0.638/0.599 | c=0.998347
[Epoch 0095] loss=14.4820 cls=0.2254 smmd=0.8811 ct=11.4908 rec=1.3515 | train/val/test=1.000/0.600/0.608 | c=0.998347
[Epoch 0096] loss=14.2019 cls=0.2136 smmd=0.7200 ct=11.5912 rec=1.4078 | train/val/test=1.000/0.608/0.605 | c=0.998347
[Epoch 0097] loss=13.8048 cls=0.2100 smmd=0.6364 ct=11.4365 rec=1.3447 | train/val/test=1.000/0.742/0.745 | c=0.998347
[Epoch 0098] loss=13.4012 cls=0.0573 smmd=0.5212 ct=11.4196 rec=1.2997 | train/val/test=1.000/0.730/0.727 | c=0.998347
[Epoch 0099] loss=13.3254 cls=0.0690 smmd=0.4674 ct=11.4696 rec=1.3056 | train/val/test=1.000/0.652/0.647 | c=0.998347
=== Best @ epoch 73: val=0.7740, test=0.7520 ===

Experiment PubMed-/mnt/data1/Graph/HypGraphLoRA/pre_trained_gnn/Computers.GRACE.GAT.hyp_True.True.20250912-232538.pth-True-5-2 completed in 193.20 seconds.
==================================================
